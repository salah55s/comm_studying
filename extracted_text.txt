Binomial Distribution
•The binomial distribution occurs inmany cases of
interest tous.
•Wewill useittofind theprobability ofaspecified
number ofbiterrors inadigital transmission .
•This distribution isillustrated viatheexperiment of
tossing acoin𝑛
Binomial Distribution
•Consider theexperiment oftossing acoin𝑛times .Let
theprobability ofheads be𝑝,andoftails be𝑞.
•Now suppose wewish tofind theprobability of𝑘
heads outof𝑛tosses .
•One possible sequence is:thefirst𝑘tosses areheads,
andtheremaining 𝑛-𝑘tosses aretails.
Binomial Distribution
•Theprobability ofthatparticular sequence is
𝑝𝑘𝑞𝑛−𝑘
•Another possible sequence istostart with𝑛-𝑘tails,
followed by𝑘heads .
•The probability ofthis second sequence any other
ordering ordering of𝑘heads and𝑛-𝑘tails is
𝑝𝑘𝑞𝑛−𝑘
Binomial Distribution
•Theoverall probability of𝑘heads (inanyposition) is
found byadding theindividual probabilities together .
•Thus ,weneed tomultiply theprobability ofthe
sequence bythenumber ofways wecandistribute 𝑘
heads among𝑛positions .
•This number isfound by
Binomial Distribution
where 𝑛
𝑘is the binomial coefficient. 
•Thus, the probability of 𝑘heads is given by 
𝑃𝑋=𝑘=𝑛
𝑘𝑝𝑘𝑞𝑛−𝑘=𝑛!
𝑘!𝑛−𝑘!𝑝𝑘𝑞𝑛−𝑘
Uniform Distribution
•Itisthesimplest continuous density function .
•The value of the density function is a constant over a 
range of the x -axis.

Uniform Distribution
•That constant must beselected such thatthetotal area
under thedensity function isunity .
•Asanapplication example, suppose theoperation of
turning onasinusoidal generator .The output ofthe
generator willbe:
𝑠𝑡=𝐴𝑐𝑜𝑠(2𝜋𝑓𝑜𝑡+𝜑)
Uniform Density
•The absolute time ofturning onthegenerator is
random, andthus theangle𝜑israndom .
•Itisexpected that𝜑isuniformly distributed between
0and2𝜋.
•Thus a=0,b=2𝜋
Block Diagram of a Digital System


Source Encoder
•Operates upon one ormore analog signals toproduce a
periodic train ofsymbols .
•May contain amultiplexer, with channels which areused to
communicate from more than onesource atthesame time.

Encryptor
•Provides security byensuring that only theintended
receiver canunderstand themessage .
•Encryption isameans ofsecuring data byencoding it
mathematically such that itcan only beread, or
decrypted, bythose with thecorrect keyorcipher .
•Encryption iscrucial inadigitally -connected world to
keep private information, messages, and financial
transactions private andsecure .
Channel Encoder
•Itincreases efficiency and decreases the effects of
transmission errors duetonoise .
•000,001,010,011,100,101,110,111
If101istransmitted andanerror occur inthethird bit,100
isreceived .There isnoway forthereceiver toknow that
100wasnotthetransmitted word .
•0000,0011,0101,0110,1001,1010,1100,1111
If1010 istransmitted, and1011 isreceived, thisisnotone
oftheeight acceptable words .
Line Coding
•Is the process of converting digital data to baseband 
signals. 

Carrier Modulator
•Baseband signals cannotbetransmitted over aradio linkor
satellites, because this would require large antennas to
efficiently radiate thelowfrequency spectrum ofthesignal .
•Aspectrum shift tohigher frequencies isalso required to
transmit several messages simultaneously, bysharing the
large bandwidth ofthetransmission medium (FDM) .
Carrier Modulator

Carrier Modulator

Spread Spectrum Modulator
•Provides additional security .
•Multiple access techniques combine signals that might
originate from different sources, sothat they canshare a
portion ofthecommunication resource (e.g.,spectrum,
time) .
•Frequency spreading canproduce asignal thatisrelatively
invulnerable tointerference andcanbeused toenhance the
system privacy .Itisavaluable technique used formultiple
access .
Receiver and Symbol Synchronizer
•Thereceiver issimply amirror image ofthetransmitter .
•The only difference isthat thecarrier modulator ofthe
transmitter hasbeen replaced bythecarrier demodulator
andthesymbol synchronizer .
•The symbol synchronizer partitions theoverall signal into
segments corresponding toeach symbol and toeach
message .
Advantages of Digital Systems
•Errors often canbecorrected
Theonly decision atthereceiver istheselection between two
possible pulses, notthedetails ofthepulse shape .

Advantages of Digital Systems
•The regenerative repeater not only performs the function 
of amplification, but also cleans up the signal. 

Advantages of Digital Systems
2.Signal manipulation (e.g.encryption) issimple to
perform
Digital systems deals with numbers, rather than
waveforms .These numbers can bemanipulated by
simple logic circuits .Analog operations require complex
hardware .
Disadvantages of Digital Systems
•Generally requires more bandwidth than analog. 
•Synchronization is required. 
DIGITAL
COMMUNICATIONS
Fundamentals and Applications
Second Edition
BERNARD SKLAR
Communications Engineering Services, Tarzana, California
and
University of California, Los Angeles
Prentice Hall P T R
Upper Saddle River, New Jersey 07458
www.phptr.com
Contents
PREFACE xvii
1 SIGNALS AND SPECTRA 1
1.1 Digital Communication Signal Processing, 3
1.1.1 Why Digital?, 3
1.1.2 Typical Block Diagram and Transformations, 4
1.1.3 Basic Digital Communication Nomenclature, 11
1.1.4 Digital versus Analog Performance Criteria, 13
1.2 Classification of Signals, 14
1.2.1 Deterministic and Random Signals, 14
1.2.2 Periodic and Nonperiodic Signals, 14
1.2.3 Analog and Discrete Signals, 14
1.2.4 Energy and Power Signals, 14
1.2.5 The Unit Impulse Function, 16
1.3 Spectral Density, 16
1.3.1 Energy Spectral Density, 17
1.3.2 Power Spectral Density, 17
1.4 Autocorrelation, 19
1.4.1 Autocorrelation of an Energy Signal, 19
1.4.2 Autocorrelation of a Periodic (Power) Signal, 20
1.5 Random Signals, 20
1.5.1 Random Variables, 20
1.5.2 Random Processes, 22
1.5.3 Time Averaging and Ergodicity, 25
1.5.4 Power Spectral Density of a Random Process, 26
1.5.5 Noise in Communication Systems, 30
v
1.6 Signal Transmission through Linear Systems, 33
1.6.1 Impulse Response, 34
1.6.2 Frequency Transfer Function, 35
1.6.3 Distortionless Transmission, 36
1.6.4 Signals, Circuits, and Spectra, 42
1.7 Bandwidth of Digital Data, 45
1.7.1 Baseband versus Bandpass, 45
1.7.2 The Bandwidth Dilemma, 47
1.8 Conclusion, 51
2 FORMATTING AND BASEBAND MODULATION 55
2.1 Baseband Systems, 56
2.2 Formatting Textual Data (Character Coding), 58
2.3 Messages, Characters, and Symbols, 61
2.3.7 Example of Messages, Characters, and Symbols, 61
2.4 Formatting Analog Information, 62
2.4.1 The Sampling Theorem, 63
2.4.2 Aliasing, 69
2.4.3 Why Oversample? 72
2.4.4 Signal Interface for a Digital System, 75
2.5 Sources of Corruption, 76
2.5.7 Sampling and Quantizing Effects, 76
2.5.2 Channel Effects, 77
2.5.3 Signal-to-Noise Ratio for Quantized Pulses, 78
2.6 Pulse Code Modulation, 79
2.7 Uniform and Nonuniform Quantization, 81
2.7.7 Statistics of Speech Amplitudes, 81
2.7.2 Nonuniform Quantization, 83
2.7.3 Companding Characteristics, 84
2.8 Baseband Modulation, 85
2.8.1 Waveform Representation of Binary Digits, 85
2.8.2 PCM Waveform Types, 85
2.8.3 Spectral Attributes of PCM Waveforms, 89
2.8.4 Bits per PCM Word and Bits per Symbol, 90
2.8.5 M-ary Pulse Modulation Waveforms, 91
2.9 Correlative Coding, 94
2.9.7 Duobinary Signaling, 94
2.9.2 Duobinary Decoding, 95
2.9.3 Preceding, 96
2.9.4 Duobinary Equivalent Transfer Function, 97
2.9.5 Comparison of Binary with Duobinary Signaling, 98
2.9.6 Poly binary Signaling, 99
2.10 Conclusion, 100
vi Contents
3 BASEBAND DEMODULATION/DETECTION 104
3.1 Signals and Noise, 106
3.1.1 Error-Performance Degradation in Communication Systems, 106
3.1.2 Demodulation and Detection, 107
3.1.3 A Vectorial View of Signals and Noise, 110
3.1.4 The Basic SNR Parameter for Digital Communication Systems, 117
3.1.5 Why Eb/N0 Is a Natural Figure of Merit, 118
3.2 Detection of Binary Signals in Gaussian Noise, 119
3.2.1 Maximum Likelihood Receiver Structure, 119
3.2.2 The Matched Filter, 122
3.2.3 Correlation Realization of the Matched Filter, 124
3.2.4 Optimizing Error Performance, 127
3.2.5 Error Probability Performance of Binary Signaling, 131
3.3 Intersymbol Interference, 136
3.3.1 Pulse Shaping to Reduce ISI, 138
3.3.2 Two Types of Error-Performance Degradation, 142
3.3.3 Demodulation/Detection of Shaped Pulses, 145
3.4 Equalization, 149
3.4.1 Channel Characterization, 149
3.4.2 Eye Pattern, 151
3.4.3 Equalizer Filter Types, 152
3.4.4 Preset and Adaptive Equalization, 158
3.4.5 Filter Update Rate, 160
3.5 Conclusion, 161
4 BANDPASS MODULATION AND DEMODULATION/
DETECTION 167
4.1 Why Modulate? 168
4.2 Digital Bandpass Modulation Techniques, 169
4.2.1 Phasor Representation of a Sinusoid, 171
4.2.2 Phase Shift Keying, 173
4.2.3 Frequency Shift Keying, 175
4.2.4 Amplitude Shift Keying, 175
4.2.5 Amplitude Phase Keying, 176
4.2.6 Waveform Amplitude Coefficient, 176
4.3 Detection of Signals in Gaussian Noise, 177
4.3.1 Decision Regions, 177
4.3.2 Correlation Receiver, 178
4.4 Coherent Detection, 183
4.4.1 Coherent Detection of PSK, 183
4.4.2 Sampled Matched Filter, 184
4.4.3 Coherent Detection of Multiple Phase Shift Keying, 188
4.4.4 Coherent Detection of FSK, 191
Contents vii
4.5 Noncoherent Detection, 194
4.5.1 Detection of Differential PSK, 194
4.5.2 Binary Differential PSK Example, 196
4.5.3 Noncoherent Detection of FSK, 198
4.5.4 Required Tone Spacing for Noncoherent Orthogonal FSK, 200
4.6 Complex Envelope, 204
4.6.1 Quadrature Implementation of a Modulator, 205
4.6.2 D8PSK Modulator Example, 206
4.6.3 D8PSK Demodulator Example, 208
4.7 Error Performance for Binary Systems, 209
4.7.1 Probability of Bit Error for Coherently Detected BPSK, 209
4.7.2 Probability of Bit Error for Coherently Detected
Differentially Encoded Binary PSK, 211
4.7.3 Probability of Bit Error for Coherently Detected
Binary Orthogonal FSK, 213
4.7.4 Probability of Bit Error for Noncoherently Detected
Binary Orthogonal FSK, 213
4.7.5 Probability of Bit Error for Binary DPSK, 216
4.7.6 Comparison of Bit Error Performance for Various
Modulation Types, 218
4.8 M-ary Signaling and Performance, 219
4.8.1 Ideal Probability of Bit Error Performance, 219
4.8.2 M-ary Signaling, 220
4.8.3 Vectorial View of MPSK Signaling, 222
4.8.4 BPSK and QPSK Have the Same Bit Error Probability, 223
4.8.5 Vectorial View of MFSK Signaling, 225
4.9 Symbol Error Performance for M-ary Systems (M > 2), 229
4.9.1 Probability of Symbol Error for MPSK, 229
4.9.2 Probability of Symbol Error for MFSK, 230
4.9.3 Bit Error Probability versus Symbol Error Probability
for Orthogonal Signals, 232
4.9.4 Bit Error Probability versus Symbol Error Probability
for Multiple Phase Signaling, 234
4.9.5 Effects of Intersymbol Interference, 235
4.10 Conclusion, 236
5 COMMUNICATIONS LINK ANALYSIS 242
5.1 What the System Link Budget Tells the System Engineer, 243
5.2 The Channel, 244
5.2.7 The Concept of Free Space, 244
5.2.2 Error-Performance Degradation, 245
5.2.3 Sources of Signal Loss and Noise, 245
viii Contents
5.3 Received Signal Power and Noise Power, 250
5.3J The Range Equation, 250
5.3.2 Received Signal Power as a Function of Frequency, 254
5.3.3 Path Loss is Frequency Dependent, 256
5.3.4 Thermal Noise Power, 258
5.4 Link Budget Analysis, 259
5.4.1 Two E//NQ Values of Interest, 262
5.4.2 Link Budgets are Typically Calculated in Decibels, 263
5.4.3 How Much Link Margin is Enough? 264
5.4.4 Link Availability, 266
5.5 Noise Figure, Noise Temperature, and System Temperature, 270
5.5J Noise Figure, 270
5.5.2 Noise Temperature, 273
5.5.3 Line Loss, 274
5.5.4 Composite Noise Figure and Composite Noise Temperature, 276
5.5.5 System Effective Temperature, 277
5.5.6 Sky Noise Temperature, 282
5.6 Sample Link Analysis, 286
5.6.1 Link Budget Details, 287
5.6.2 Receiver Figure of Merit, 289
5.6.3 Received Isotropic Power, 289
5.7 Satellite Repeaters, 290
5.7.7 Nonregenerative Repeaters, 291
5.7.2 Nonlinear Repeater Amplifiers, 295
5.8 System Trade-Offs, 296
5.9 Conclusion, 297
6 CHANNEL CODING: PART 1 304
6.1 Waveform Coding and Structured Sequences, 305
6.1.1 Antipodal and Orthogonal Signals, 307
6.1.2 M-ary Signaling, 308
6.1.3 Waveform Coding, 309
6.1.4 Waveform-Coding System Example, 313
6.2 Types of Error Control, 315
6.2.1 Terminal Connectivity, 315
6.2.2 Automatic Repeat Request, 316
6.3 Structured Sequences, 317
6.3.1 Channel Models, 318
6.3.2 Code Rate and Redundancy, 320
6.3.3 Parity Check Codes, 321
6.3.4 Why Use Error-Correction Coding? 323
Contents ix
6.4 Linear Block Codes, 328
6.4.1 Vector Spaces, 329
6.4.2 Vector Subspaces, 329
6.4.3 A (6, 3) Linear Block Code Example, 330
6.4.4 Generator Matrix, 331
6.4.5 Systematic Linear Block Codes, 333
6.4.6 Parity-Check Matrix, 334
6.4.7 Syndrome Testing, 335
6.4.8 Error Correction, 336
6.4.9 Decoder Implementation, 340
6.5 Error-Detecting and Correcting Capability, 342
6.5.1 Weight and Distance of Binary Vectors, 342
6.5.2 Minimum Distance of a Linear Code, 343
6.5.3 Error Detection and Correction, 343
6.5.4 Visualization of a 6-Tuple Space, 347
6.5.5 Erasure Correction, 348
6.6 Usefulness of the Standard Array, 349
6.6.1 Estimating Code Capability, 349
6.6.2 An (n, k) Example, 351
6.6.3 Designing the (8, 2) Code, 352
6.6.4 Error Detection versus Error Correction Trade-Offs, 352
6.6.5 The Standard Array Provides Insight, 356
6.7 Cyclic Codes, 356
6.7.7 Algebraic Structure of Cyclic Codes, 357
6.7.2 Binary Cyclic Code Properties, 358
6.7.3 Encoding in Systematic Form, 359
6.7.4 Circuit for Dividing Polynomials, 360
6.7.5 Systematic Encoding with an (n - k)-Stage Shift Register, 363
6.7.6 Error Detection with an (n - k)-Stage Shift Register, 365
6.8 Weil-Known Block Codes, 366
6.8.1 Hamming Codes, 366
6.8.2 Extended Golay Code, 369
6.8.3 BCH Codes, 370
6.9 Conclusion, 374
7 CHANNEL CODING: PART 2 381
7.1 Convolutional Encoding, 382
7.2 Convolutional Encoder Representation, 384
7.2.1 Connection Representation, 385
7.2.2 State Representation and the State Diagram, 389
7.2.3 The Tree Diagram, 391
7.2.4 The Trellis Diagram, 393
7.3 Formulation of the Convolutional Decoding Problem, 395
7.3.1 Maximum Likelihood Decoding, 395
x Contents
7.3.2 Channel Models: Hard versus Soft Decisions, 396
7.3.3 The Viterbi Convolutional Decoding Algorithm, 401
7.3.4 An Example of Viterbi Convolutional Decoding, 401
7.3.5 Decoder Implementation, 405
7.3.6 Path Memory and Synchronization, 408
7.4 Properties of Convolutional Codes, 408
7.4.1 Distance Properties of Convolutional Codes, 408
7.4.2 Systematic and Nonsystematic Convolutional Codes, 413
7.4.3 Catastrophic Error Propagation in Convolutional Codes, 414
7.4.4 Performance Bounds for Convolutional Codes, 415
7.4.5 Coding Gain, 416
7.4.6 Best Known Convolutional Codes, 418
7.4.7 Convolutional Code Rate Trade-Off, 420
7.4.8 Soft-Decision Viterbi Decoding, 420
7.5 Other Convolutional Decoding Algorithms, 422
7.5.1 Sequential Decoding, 422
7.5.2 Comparisons and Limitations of Viterbi and Sequential Decoding, 425
7.5.3 Feedback Decoding, 427
7.6 Conclusion, 429
8 CHANNEL CODING: PART 3 436
8.1 Reed-Solomon Codes, 437
8.1.1 Reed-Solomon Error Probability, 438
8.1.2 Why R-S Codes Perform Well Against Burst Noise, 441
8.1.3 R-S Performance as a Function of Size,
Redundancy, and Code Rate, 441
8.1.4 Finite Fields, 445
8.1.5 Reed-Solomon Encoding, 450
8.1.6 Reed-Solomon Decoding, 454
8.2 Interleaving and Concatenated Codes, 461
8.2.1 Block Interleaving, 463
8.2.2 Convolutional Interleaving, 466
8.2.3 Concatenated Codes, 468
8.3 Coding and Interleaving Applied to the Compact Disc
Digital Audio System, 469
8.3.1 CIRC Encoding, 470
8.3.2 CIRC Decoding, 472
8.3.3 Interpolation and Muting, 474
8.4 Turbo Codes, 475
8.4.1 Turbo Code Concepts, 477
8.4.2 Log-Likelihood Algebra, 481
8.4.3 Product Code Example, 482
8.4.4 Encoding with Recursive Systematic Codes, 488
8.4.5 A Feedback Decoder, 493
Contents xi
8.4.6 The MAP Decoding Algorithm, 498
8.4.7 MAP Decoding Example, 504
8.5 Conclusion, 509
Appendix 8A The Sum of Log-Likelihood Ratios, 510
9 MODULATION AND CODING TRADE-OFFS 520
9.1 Goals of the Communications System Designer, 521
9.2 Error Probability Plane, 522
9.3 Nyquist Minimum Bandwidth, 524
9.4 Shannon-Hartley Capacity Theorem, 525
9.4.1 Shannon Limit, 528
9.4.2 Entropy, 529
9.4.3 Equivocation and Effective Transmission Rate, 532
9.5 Bandwidth Efficiency Plane, 534
9.5.7 Bandwidth Efficiency ofMPSK and MFSK Modulation, 535
9.5.2 Analogies Between Bandwidth-Efficiency
and Error Probability Planes, 536
9.6 Modulation and Coding Trade-Offs, 537
9.7 Defining, Designing, and Evaluating Digital
Communication Systems, 538
9.7.7 M-ary Signaling, 539
9.7.2 Bandwidth-Limited Systems, 540
9.7.3 Power-Limited Systems, 541
9.7.4 Requirements for MPSK and MFSK Signaling, 542
9.7.5 Bandwidth-Limited Uncoded System Example, 543
9.7.6 Power-Limited Uncoded System Example, 545
9.7.7 Bandwidth-Limited and Power-Limited
Coded System Example, 547
9.8 Bandwidth-Efficient Modulation, 555
9.5.7 QPSK and Offset QPSK Signaling, 555
9.8.2 Minimum Shift Keying, 559
9.8.3 Quadrature Amplitude Modulation, 563
9.9 Modulation and Coding for Bandlimited Channels, 566
9.9.7 Commercial Telephone Modems, 567
9.9.2 Signal Constellation Boundaries, 568
9.9.3 Higher Dimensional Signal Constellations, 569
9.9.4 Higher-Density Lattice Structures, 572
9.9.5 Combined Gain: N-Sphere Mapping and Dense Lattice, 573
9.10 Trellis-Coded Modulation, 573
9.70.7 The Idea Behind Trellis-Coded Modulation (TCM), 574
9.10.2 TCM Encoding, 576
9.10.3 TCM Decoding, 580
9.10.4 Other Trellis Codes, 583
xii Contents
9.10.5 Trellis-Coded Modulation Example, 585
9.10.6 Multi-Dimensional Trellis-Coded Modulation, 589
9.11 Conclusion, 590
10 SYNCHRONIZATION 598
10.1 Introduction, 599
10.1.1 Synchronization Defined, 599
10.1.2 Costs versus Benefits, 601
10.1.3 Approach and Assumptions, 602
10.2 Receiver Synchronization, 603
10.2.1 Frequency and Phase Synchronization, 603
10.2.2 Symbol Synchronization—Discrete Symbol Modulations, 625
10.2.3 Synchronization with Continuous-Phase Modulations (CPM), 631
10.2.4 Frame Synchronization, 639
10.3 Network Synchronization, 643
10.3.1 Open-Loop Transmitter Synchronization, 644
10.3.2 Closed-Loop Transmitter Synchronization, 647
10.4 Conclusion, 649
11 MULTIPLEXING AND MULTIPLE ACCESS 656
11.1 Allocation of the Communications Resource, 657
11.1.1 Frequency-Division Multiplexing/Multiple Access, 660
11.1.2 Time-Division Multiplexing/Multiple Access, 665
11.1.3 Communications Resource Channelization, 668
11.1.4 Performance Comparison ofFDMA and TDMA, 668
11.1.5 Code-Division Multiple Access, 672
11.1.6 Space-Division and Polarization-Division Multiple Access, 674
11.2 Multiple Access Communications System and Architecture, 676
11.2.1 Multiple Access Information Flow, 677
11.2.2 Demand Assignment Multiple Access, 678
11.3 Access Algorithms, 678
11.3.1 ALOHA, 678
11.3.2 Slotted ALOHA, 682
11.3.3 Reservation-ALOHA, 683
11.3.4 Performance Comparison ofS-ALOHA and R-ALOHA, 684
11.3.5 Polling Techniques, 686
11.4 Multiple Access Techniques Employed with INTELSAT, 689
11.4.1 Preassigned FDM/FM/FDMA or MCPC Operation, 690
11.4.2 MCPC Modes of Accessing an INTELSA T Satellite, 690
11.4.3 SPADE Operation, 693
11.4.4 TDMA in INTELSAT, 698
11.4.5 Satellite-Switched TDMA in INTELSAT, 704
Contents xiii
11.5 Multiple Access Techniques for Local Area Networks, 708
11.5.1 Carrier-Sense Multiple Access Networks, 708
11.5.2 Token-Ring Networks, 710
11.5.3 Performance Comparison of CSMA/CD and Token-Ring Networks, 711
11.6 Conclusion, 713
12 SPREAD-SPECTRUM TECHNIQUES 718
12.1 Spread-Spectrum Overview, 719
12.1.1 The Beneficial Attributes of Spread-Spectrum Systems, 720
12.1.2 A Catalog of Spreading Techniques, 724
12.1.3 Model for Direct-Sequence Spread-Spectrum
Interference Rejection, 726
12.1.4 Historical Background, 727
12.2 Pseudonoise Sequences, 728
72.2.1 Randomness Properties, 729
12.2.2 Shift Register Sequences, 729
12.2.3 PN Autocorrelation Function, 730
12.3 Direct-Sequence Spread-Spectrum Systems, 732
12.3.1 Example of Direct Sequencing, 734
12.3.2 Processing Gain and Performance, 735
12.4 Frequency Hopping Systems, 738
12.4.1 Frequency Hopping Example, 740
12.4.2 Robustness, 741
12.4.3 Frequency Hopping with Diversity, 741
12.4.4 Fast Hopping versus Slow Hopping, 742
12.4.5 FFH/MFSK Demodulator, 744
12.4.6 Processing Gain, 745
12.5 Synchronization, 745
12.5.1 Acquisition, 746
12.5.2 Tracking, 751
12.6 Jamming Considerations, 754
12.6.1 The Jamming Game, 754
12.6.2 Broadband Noise Jamming, 759
12.6.3 ^Partial-Band Noise Jamming, 760
12.6.4 . Multiple-Tone Jamming, 763
12.6.5 Pulse Jamming, 763
12.6.6 Repeat-Back Jamming, 765
12.6.7 BLADES System, 768
12.7 Commercial Applications, 769
12.7.1 Code-Division Multiple Access, 769
12.7.2 Multipath Channels, 771
12.7.3 The FCC Part 15 Rules for Spread-Spectrum Systems, 772
12.7.4 Direct Sequence versus Frequency Hopping, 773
12.8 Cellular Systems, 775
12.8.1 Direct Sequence CDMA, 776
xiv Contents
12.8.2 Analog FM versus TDMA versus CDMA, 779
12.8.3 Interference-Limited versus Dimension-Limited Systems, 781
12.8.4 IS-95 CDMA Digital Cellular System, 782
12.9 Conclusion, 795
13 SOURCE CODING 803
13.1 Sources, 804
13.1.1 Discrete Sources, 804
13.1.2 Waveform Sources, 809
13.2 Amplitude Quantizing, 811
13.2.1 Quantizing Noise, 813
13.2.2 Uniform Quantizing, 816
13.2.3 Saturation, 820
13.2.4 Dithering, 823
13.2.5 Nonuniform Quantizing, 826
13.3 Differential Pulse-Code Modulation, 835
13.3.1 One-Tap Prediction, 838
13.3.2 N-Tap Prediction, 839
13.3.3 Delta Modulation, 841
13.3.4 Sigma-Delta Modulation, 842
13.3.5 Sigma-Delta A-to-D Converter (ADC), 847
13.3.6 Sigma-Delta D-to-A Converter (DAC), 848
13.4 Adaptive Prediction, 850
13.4.1 Forward Prediction, 851
13.4.2 Synthesis/Analysis Coding, 852
13.5 Block Coding, 853
13.5.1 Vector Quantizing, 854
13.6 Transform Coding, 856
13.6.1 Quantization for Transform Coding, 857
13.6.2 Subband Coding, 857
13.7 Source Coding for Digital Data, 859
13.7.1 Properties of Codes, 860
13.7.2 Huffman Codes, 862
13.7.3 Run-Length Codes, 866
13.8 Examples of Source Coding, 870
13.8.1 Audio Compression, 870
13.8.2 Image Compression, 875
13.9 Conclusion, 884
14 ENCRYPTION AND DECRYPTION 890
14.1 Models, Goals, and Early Cipher Systems, 891
14.1.1 A Model of the Encryption and Decryption Process, 893
14.1.2 System Goals, 893
14.1.3 Classic Threats, 893
Contents xv
14.1.4 Classic Ciphers, 894
14.2 The Secrecy of a Cipher System, 897
14.2.1 Perfect Secrecy, 897
14.2.2 Entropy and Equivocation, 900
14.2.3 Rate of a Language and Redundancy, 902
14.2.4 Unicity Distance and Ideal Secrecy, 902
14.3 Practical Security, 905
14.3.1 Confusion and Diffusion, 905
14.3.2 Substitution, 905
14.3.3 Permutation, 907
14.3.4 Product Cipher Systems, 908
14.3.5 The Data Encryption Standard, 909
14.4 Stream Encryption, 915
14.4.1 Example of Key Generation Using a Linear
Feedback Shift Register, 916
14.4.2 Vulnerabilities of Linear Feedback Shift Registers, 917
14.4.3 Synchronous and Self-Synchronous Stream
Encryption Systems, 919
14.5 Public Key Cryptosystems, 920
14.5.1 Signature Authentication using a Public Key Cryptosystem, 921
14.5.2 A Trapdoor One-Way Function, 922
14.5.3 The Rivest-Shamir-Adelman Scheme, 923
14.5.4 The Knapsack Problem, 925
14.5.5 A Public Key Cryptosystem based on a Trapdoor Knapsack, 927
14.6 Pretty Good Privacy, 929
14.6.1 Triple-DBS, CAST, and IDEA, 931
14.6.2 Diffie-Hellman (Elgamal Variation) and RSA, 935
14.6.3 PGP Message Encryption, 936
14.6.4 PGP Authentication and Signature, 937
14.7 Conclusion, 940
15 FADING CHANNELS 944
15.1 The Challenge of Communicating over Fading Channels, 945
15.2 Characterizing Mobile-Radio Propagation, 947
75.2.7 Large-Scale Fading, 951
15.2.2 Small-Scale Fading, 953
15.3 Signal Time-Spreading, 958
75.3.7 Signal Time-Spreading Viewed in the Time-Delay Domain, 958
15.3.2 Signal Time-Spreading Viewed in the Frequency Domain, 960
15.3.3 Examples of Flat Fading and Frequency-Selective Fading, 965
15.4 Time Variance of the Channel Caused by Motion, 966
75.4.7 Time Variance Viewed in the Time Domain, 966
15.4.2 Time Variance Viewed in the Doppler-Shift Domain, 969
15.4.3 Performance over a Slow-and Flat-Fading Rayleigh Channel, 975
xvi Contents
15.5 Mitigating the Degradation Effects of Fading, 978
75.5.7 Mitigation to Combat Frequency-Selective Distortion, 980
75.5.2 Mitigation to Combat Fast-Fading Distortion, 982
15.5.3 Mitigation to Combat Loss in SNR, 983
15.5.4 Diversity Techniques, 984
15.5.5 Modulation Types for Fading Channels, 987
15.5.6 The Role of an Interleaver, 988
15.6 Summary of the Key Parameters Characterizing Fading Channels, 992
75.6.7 Fast Fading Distortion: Case 1, 992
15.6.2 Frequency-Selective Fading Distortion: Case 2, 993
15.6.3 Fast-Fading and Frequency-Selective Fading Distortion: Case 3, 993
15.7 Applications: Mitigating the Effects of Frequency-Selective Fading, 996
75.7.7 The Viterbi Equalizer as Applied to GSM, 996
15.7.2 The Rake Receiver as Applied to Direct-Sequence
Spread-Spectrum (DS/SS) Systems, 999
15.8 Conclusion, 1001
A A REVIEW OF FOURIER TECHNIQUES 1012
A.I Signals, Spectra, and Linear Systems, 1012
A.2 Fourier Techniques for Linear System Analysis, 1012
A2.7 Fourier Series Transform, 1014
A.2.2 Spectrum of a Pulse Train, 1018
A.2.3 Fourier Integral Transform, 1020
A.3 Fourier Transform Properties, 1021
A.3.1 Time Shifting Property, 1022
A.3.2 Frequency Shifting Property, 1022
A.4 Useful Functions, 1023
A.4.1 Unit Impulse Function, 1023
A.4.2 Spectrum of a Sinusoid, 1023
A.5 Convolution, 1025
A5.7 Graphical Example of Convolution, 1027
A.5.2 Time Convolution Property, 1028
A.5.3 Frequency Convolution Property, 1030
A.5.4 Convolution of a Function with a Unit Impulse, 1030
A.5.5 Demodulation Application of Convolution, 1031
A.6 Tables of Fourier Transforms and Operations, 1033
B FUNDAMENTALS OF STATISTICAL DECISION THEORY 1035
B.I Bayes' Theorem, 1035
5.7.7 Discrete Form of Bayes'Theorem, 1036
B.1.2 Mixed Form of Bayes'Theorem, 1038
B.2 Decision Theory, 1040
5.2.7 Components of the Decision Theory Problem, 1040
Contents xvii
B.2.2 The Likelihood Ratio Test and the Maximum
A Posteriori Criterion, 1041
B.2.3 The Maximum Likelihood Criterion, 1042
B.3 Signal Detection Example, 1042
B.3.1 The Maximum Likelihood Binary Decision, 1042
B.3.2 Probability of Bit Error, 1044
C RESPONSE OF A CORRELATOR TO WHITE NOISE 1047
D OFTEN-USED IDENTITIES 1049
E s-DOMAIN, z-DOMAIN AND DIGITAL FILTERING 1051
E.I The Laplace Transform, 1051
£.7.7 Standard Laplace Transforms, 1052
E.1.2 Laplace Transform Properties, 1053
E.1.3 Using the Laplace Transform, 1054
E.1.4 Transfer Function, 1055
E.1.5 RC Circuit Low Pass Filtering, 1056
E.1.6 Poles and Zeroes, 1056
E.1.7 Linear System Stability, 1057
E.2 The z-Transform, 1058
E.2.1 Calculating the z-Transform, 1058
E.2.2 The Inverse z-Transform, 1059
E.3 Digital Filtering, 1060
E.3.1 Digital Filter Transfer Function, 1061
E.3.2 Single Pole Filter Stability, 1062
E.3.3 General Digital Filter Stability, 1063
E.3.4 z-Plane Pole-Zero Diagram and the Unit Circle, 1063
£.3.5 Discrete Fourier Transform of Digital Filter Impulse Response, 1064
E.4 Finite Impulse Response Filter Design, 1065
E.4.1 FIR Filter Design, 1065
E.4.2 The FIR Differentiator, 1067
E.5 Infinite Impulse Response Filter Design, 1069
E.5.1 Backward Difference Operator, 1069
£.5.2 HR Filter Design using the Bilinear Transform, 1070
E.5.3 The IIR Integrator, 1071
F LIST OF SYMBOLS 1072
INDEX 1074
xviii Contents
Preface
This second edition of Digital Communications: Fundamentals and Applications
represents an update of the original publication. The key features that have been
updated are:
• The error-correction coding chapters have been expanded, particularly in
the areas of Reed-Solomon codes, turbo codes, and trellis-coded modula-
tion.
• A new chapter on fading channels and how to mitigate the degrading ef-
fects of fading has been introduced.
• Explanations and descriptions of essential digital communication concepts
have been amplified.
• End-of-chapter problem sets have been expanded. Also, end-of-chapter
question sets (and where to find the answers), as well as end-of-chapter
CD exercises have been added.
• A compact disc (CD) containing an educational version of the design soft-
ware System View by ELANIX® accompanies the textbook. The CD con-
tains a workbook with over 200 exercises, as well as a concise tutorial on
digital signal processing (DSP). CD exercises in the workbook reinforce
material in the textbook; concepts can be explored by viewing waveforms
with a windows-based PC and by changing parameters to see the effects on
the overall system. Some of the exercises provide basic training in using
System View; others provide additional training in DSP techniques.
xix
The teaching of a one-semester university course proceeds in a very different
manner compared with that of a short-course in the same subject. At the university,
one has the luxury of time—time to develop the needed skills and mathematical tools,
time to practice the ideas with homework exercises. In a short-course, the treatment is
almost backwards compared with the university. Because of the time factor, a short-
course teacher must "jump in" early with essential concepts and applications. One of
the vehicles that I found useful in structuring a short course was to start by handing out
a check list. This was not merely an outline of the curriculum. It represented a collec-
tion of concepts and nomenclature that are not clearly documented, and are often mis-
understood. The short-course students were thus initiated into the course by being
challenged. I promised them that once they felt comfortable describing each issue, or
answering each question on the list, they would be well on their way toward becoming
knowledgeable in the field of digital communications. I have learned that this list of es-
sential concepts is just as valuable for teaching full-semester courses as it is for short
courses. Here then is my "check list" for digital communications.
1. What mathematical dilemma is the cause for there being several definitions
of bandwidth? (See Section 1.7.2.)
2. Why is the ratio of bit energy-to-noise power spectral density, Eb/N0, a nat-
ural figure-to-merit for digital communication systems? (See Section 3.1.5.)
3. When representing timed events, what dilemma can easily result in confusing
the most-significant bit (MSB) and the least-significant bit (LSB)? (See Sec-
tion 3.2.3.1.)
4. The error performance of digital signaling suffers primarily from two degra-
dation types, a) loss in signal-to-noise ratio, b) distortion resulting in an irre-
ducible bit-error probability. How do they differ? (See Section 3.3.2.)
5. Often times, providing more Eb/N0 will not mitigate the degradation due to
intersymbol interference (ISI). Explain why. (See Section 3.3.2.)
6. At what location in the system is Eb/N0 defined? (See Section 4.3.2.)
7. Digital modulation schemes fall into one of two classes with opposite behav-
ior characteristics, a) orthogonal signaling, b) phase/amplitude signaling. De-
scribe the behavior of each class. (See Sections 4.8.2 and 9.7.)
8. Why do binary phase shift keying (BPSK) and quaternary phase shift keying
(QPSK) manifest the same bit-error-probability relationship? Does the same
hold true for M-ary pulse amplitude modulation (M-PAM) and M2-ary quad-
rature amplitude modulation (M2-QAM) bit-error probability? (See Sections
4.8.4 and 9.8.3.1.)
9. In orthogonal signaling, why does error-performance improve with higher di-
mensional signaling? (See Section 4.8.5.)
10. Why is free-space loss a function of wavelength? (See Section 5.3.3.)
11. What is the relationship between received signal to noise (S/N) ratio and car-
rier to noise (C/N) ratio? (See Section 5.4.)
12. Describe four types of trade-offs that can be accomplished by using an error-
correcting code. (See Section 6.3.4.)
xx Preface
13. Why do traditional error-correcting codes yield error-performance degrada-
tion at low values of Eb/N0? (See Section 6.3.4.)
14. Of what use is the standard array in understanding a block code, and in eval-
uating its capability? (See Section 6.6.5.)
15. Why is the Shannon limit of -1.6 dB not a useful goal in the design of real sys-
tems? (See Section 8.4.5.2.)
16. What are the consequences of the fact that the Viterbi decoding algorithm
does not yield a posteriori probabilities? What is a more descriptive name for
the Viterbi algorithm? (See Section 8.4.6.)
17. Why do binary and 4-ary orthogonal frequency shift keying (FSK) manifest
the same bandwidth-efficiency relationship? (See Section 9.5.1.)
18. Describe the subtle energy and rate transformations of received signals: from
data-bits to channel-bits to symbols to chips. (See Section 9.7.7.)
19. Define the following terms: Baud, State, Communications Resource, Chip,
Robust Signal. (See Sections 1.1.3 and 7.2.2, Chapter 11, and Sections 12.3.2
and 12.4.2.)
20. In a fading channel, why is signal dispersion independent of fading rapidity?
(See Section 15.1.1.1.)
I hope you find it useful to be challenged in this way. Now, let us describe the
purpose of the book in a more methodical way. This second edition is intended
to provide a comprehensive coverage of digital communication systems for se-
nior level undergraduates, first year graduate students, and practicing engineers.
Though the emphasis is on digital communications, necessary analog fundamentals
are included since analog waveforms are used for the radio transmission of digital
signals. The key feature of a digital communication system is that it deals with a fi-
nite set of discrete messages, in contrast to an analog communication system in
which messages are defined on a continuum. The objective at the receiver of the
digital system is not to reproduce a waveform with precision; it is instead to deter-
mine from a noise-perturbed signal, which of the finite set of waveforms had been
sent by the transmitter. In fulfillment of this objective, there has arisen an impres-
sive assortment of signal processing techniques.
The book develops these techniques in the context of a unified structure. The
structure, in block diagram form, appears at the beginning of each chapter; blocks in
the diagram are emphasized, when appropriate, to correspond to the subject of that
chapter. Major purposes of the book are to add organization and structure to a field
that has grown and continues to grow rapidly, and to insure awareness of the "big
picture" even while delving into the details. Signals and key processing steps are
traced from the information source through the transmitter, channel, receiver, and
ultimately to the information sink. Signal transformations are organized according to
nine functional classes: Formatting and source coding, Baseband signaling, Band-
pass signaling, Equalization, Channel coding, Muliplexing and multiple access,
Spreading, Encryption, and Synchronization. Throughout the book, emphasis is
placed on system goals and the need to trade off basic system parameters such as
signal-to-noise ratio, probability of error, and bandwidth expenditure.
Preface xxi
ORGANIZATION OF THE BOOK
Chapter 1 introduces the overall digital communication system and the basic signal
transformations that are highlighted in subsequent chapters. Some basic ideas of
random variables and the additive white Gaussian noise (AWGN) model are re-
viewed. Also, the relationship between power spectral density and autocorrelation,
and the basics of signal transmission through linear systems are established. Chap-
ter 2 covers the signal processing step, known as formatting, in order to render an
information signal compatible with a digital system. Chapter 3 emphasizes base-
band signaling, the detection of signals in Gaussian noise, and receiver optimiza-
tion. Chapter 4 deals with bandpass signaling and its associated modulation and
demodulation/detection techniques. Chapter 5 deals with link analysis, an im-
portant subject for providing overall system insight; it considers some subtleties
that are often missed. Chapters 6, 7, and 8 deal with channel coding—a cost-
effective way of providing a variety of system performance trade-offs. Chapter 6
emphasizes linear block codes, Chapter 7 deals with convolutional codes, and Chap-
ter 8 deals with Reed-Solomon codes and concatenated codes such as turbo codes.
Chapter 9 considers various modulation/coding system trade-offs dealing with
probability of bit-error performance, bandwidth efficiency, and signal-to-noise
ratio. It also treats the important area of coded modulation, particularly trellis-coded
modulation. Chapter 10 deals with synchronization for digital systems. It covers
phase-locked loop implementation for achieving carrier synchronization. It covers
bit synchronization, frame synchronization, and network synchronization, and it
introduces some ways of performing synchronization using digital methods.
Chapter 11 treats multiplexing and multiple access. It explores techniques that
are available for utilizing the communication resource efficiently. Chapter 12 intro-
duces spread spectrum techniques and their application in such areas as multiple
access, ranging, and interference rejection. This technology is important for both
military and commercial applications. Chapter 13 deals with source coding which is
a special class of data formatting. Both formatting and source coding involve digiti-
zation of data; the main difference between them is that source coding additionally
involves data redundancy reduction. Rather than considering source coding imme-
diately after formatting, it is purposely treated in a later chapter so as not to inter-
rupt the presentation flow of the basic processing steps. Chapter 14 covers basic
encryption/decryption ideas. It includes some classical concepts, as well as a class of
systems called public key cryptosystems, and the widely used E-mail encryption
software known as Pretty Good Privacy (PGP). Chapter 15 deals with fading chan-
nels. Here, we deal with applications, such as mobile radios, where characteriza-
tion of the channel is much more involved than that of a nonfading one. The design
of a communication system that will withstand the degradation effects of fading can
be much more challenging than the design of its nonfading counterpart. In this
chapter, we describe a variety of techniques that can mitigate the effects of fading,
and we show some successful designs that have been implemented.
It is assumed that the reader is familiar with Fourier methods and convolu-
tion. Appendix A reviews these techniques, emphasizing those properties that are
xxii Preface
particularly useful in the study of communication theory. It also assumed that the
reader has a knowledge of basic probability and has some familiarity with random
variables. Appendix B builds on these disciplines for a short treatment on statistical
decision theory with emphasis on hypothesis testing—so important in the under-
standing of detection theory. A new section, Appendix E, has been added to serve
as a short tutorial on s-domain, z-domain, and digital filtering. A concise DSP tu-
torial also appears on the CD that accompanies the book.
If the book is used for a two-term course, a simple partitioning is suggested;
the first seven chapters can be taught in the first term, and the last eight chapters
in the second term. If the book is used for a one-term introductory course, it is sug-
gested that the course material be selected from the following chapters: 1, 2, 3, 4,
5, 6, 7, 9, 10, 12.
ACKNOWLEDGMENTS
It is difficult to write a technical book without contributions from others. I have re-
ceived an abundance of such assistance, for which I am deeply grateful. For their
generous help, I want to thank Dr. Andrew Viterbi, Dr. Chuck Wheatley, Dr. Ed
Tiedeman, Dr. Joe Odenwalder, and Serge Willinegger of Qualcomm. I also want
to thank Dr. Dariush Divsalar of Jet Propulsion Laboratory (JPL), Dr. Bob
Bogusch of Mission Research, Dr. Tom Stanley of the Federal Communications
Commission, Professor Larry Milstein of the University of California, San Diego,
Professor Ray Pickholtz of George Washington University, Professor Daniel
Costello of Notre Dame University, Professor Ted Rappaport of Virginia Poly-
technic Institute, Phil Kossin of Lincom, Les Brown of Motorola, as well as
Dr. Bob Price and Frank Amoroso.
I also want to acknowledge those people who played a big part in helping me
with the first edition of the book. They are: Dr. Maurice King, Don Martin and
Ned Feldman of The Aerospace Corporation, Dr. Marv Simon of JPL, Dr. Bill
Lindsey of Lincom, Professor Wayne Stark of the University of Michigan, as well
as Dr. Jim Omura, Dr. Adam Lender, and Dr. Todd Citron.
I want to thank Dr. Maurice King for contributing Chapter 10 on Synchro-
nization, and Professor Fred Harris of San Diego State University for contributing
Chapter 13 on Source Coding. Also, thanks to Michelle Landry for writing the sec-
tions on Pretty Good Privacy in Chapter 14, and to Andrew Guidi for contributing
end-of-chapter problems in Chapter 15.
I am particularly indebted to my friends and colleagues Fred Harris, Profes-
sor Dan Bukofzer of California State University at Fresno, and Dr. Maury Schiff
of Elanix, who put up with my incessant argumentative discussions anytime that I
called on them. I also want to thank my very best teachers—they are my students at
the University of California, Los Angeles, as well as those students all over the
world who attended my short courses. Their questions motivated me and provoked
me to write this second edition. I hope that I have answered all their questions with
clarity.
Preface xxiii
I offer special thanks for technical clarifications that my son, Dean Sklar,
suggested; he took on the difficult role of being his father's chief critic and "devil's
advocate." I am particularly indebted to Professor Bob Stewart of the University of
Strathclyde, Glasgow, who contributed countless hours of work in writing and
preparing the CD and in authoring Appendix E. I thank Rose Kernan, my editor,
for watching over me and this project, and I thank Bernard Goodwin, Publisher at
Prentice Hall, for indulging me and believing in me. His recommendations were
invaluable. Finally, I am extremely grateful to my wife, Gwen, for her encourage-
ment, devotion, and valuable advice. She protected me from the "slings and ar-
rows" of everyday life, making it possible for me to complete this second edition.
BERNARD SKLAR
Tarzana, California
xxiv Preface




































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Discrete Memoryless Channels
•The channel ismemoryless when𝑆𝑜𝑢𝑡(𝑛)depends only
upon𝑆𝑖𝑛(𝑛)and notupon𝑆𝑖𝑛(𝑛−1)orany other input
sample values .

Transition Matrix
•The memoryless channel can becharacterized bya
Transition Matrix composed composed ofconditional
probabilities .
𝑇=𝑃00𝑃01
𝑃10𝑃11
•With no noise nor distortion
𝑇=10
01
•summation of probabilities in any column is unity. 
Transition Diagram
•Analternative way ofdisplaying transition probabilities is
byuseoftheTransition Diagram .
•Forthebinary channel
•Thesummation ofprobabilities leaving anynode isunity .

Binary Symmetric Channel (BSC)
•Achannel inwhich the two conditional error
probabilities areequal .

Tandem Connections of BSCs
•Suppose intransmitting adigital signal over along
distance ,thesignal path includes anumber ofrepeaters .
•Further, suppose thatthepath between each repeater and
thefollowing repeater canbeviewed asBSC .
•The overall channel can beviewed asatandem
connections ofBSCs .

Source Coding
Delta Modulation Techniques
Delta Modulation
•Delta modulation isasimple technique forreducing thedynamic range ofthe
numbers tobecoded .
•Instead ofsending each sample value, wesend thedifference between a
sample andtheprevious sample .
•Delta modulation quantizes thisdifference using only onebitofquantization .
Thus, a“1”issent ifthedifference ispositive, anda“0”issent ifthedifference
isnegative .
•Weshall refer tothese two possibilities aseither +𝞓or-𝞓.Atevery sample
point, thequantized waveform canincrease ordecrease by𝞓.
Implementation of the quantizer using a comparator and a staircase generator
The resulting bit train               1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 

•Since thequantized waveform can only either increase ordecrease by𝞓ateach
sample point, weshall attempt tofitastaircase approximation totheanalog
waveform .
•Ifthestaircase isbelow theanalog sample value, thedecision istoincrement
positively (an upstep) .Ifthestaircase isabove, weincrement negatively
(down step) .
•Thekeytoeffective useofdelta modulation istheintelligent choice ofthetwo
parameters, step sizeandsampling rate.
•These parameters must bechosen such that thestaircase signal isaclose
approximation totheanalog waveform .
•Increasing thesampling frequency means that thedelta -modulated waveform
requires alarger bandwidth .Increasing the step size increases the
quantization error .
Too small step size                         Too large step size

•Ifthesteps aretoosmall, wecan experience aslope overload condition, where
thestaircase cannot track rapid changes intheanalog signal .
•If,ontheother hand, thesteps aretoolarge, considerable overshoot will occur
during periods when the signal isnot changing rapidly .We have significant
quantization noise, known asgranular noise .
•InPCM, asingle biterror will cause anerror inreconstructing the associated
sample value .The error willaffect only that single reconstructed sample .
•Ifabiterror occurs indelta modulation, theD/A converter inthereceiver willgo
upinstead ofdown (orvise versa) and alllater values willcontain anerror .
Differential PCM
•Is another technique for sending information about changes in the 
samples rather than about the sample values themselves. 
•The modulator sends the difference between a sample and its 
predicted value. 
CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

Principles of Probability
𝑃𝑟𝐴=lim
𝑁→∞𝑛𝐴
𝑁
Where 𝑛𝐴is the number of times that the event 𝐴occurs in 𝑁
performances of the experiment. 
•In the coin tossing experiment, we may expect that out of a 
million tosses of a fair coin, about one half of them will 
show up heads. 
𝑃𝑟𝐴𝑜𝑟𝐵=𝑃𝑟𝐴+𝑃𝑟𝐵
Distribution Function and Probability Density 
Function 
•Iftherandom variable isdenoted by𝑋,then thedistribution
function𝐹𝑥is:
𝐹𝑥0=𝑃𝑟{𝑋≤𝑥0}
•Weusecapital letters forrandom variables, andlower case
letters forthevalues they take on.
•Thus,𝑋=𝑥0means thattherandom variable𝑋isequal to
thenumber𝑥0
Distribution Function and Probability Density 
Function 
•Note :
𝐹−∞=0 𝐹+∞=1
•Wenow define thederivative ofthedistribution function
as:
𝑃𝑥=𝑑𝐹𝑥
𝑑𝑥
Where𝑃𝑥isknown asthedensity function ofthe
random variable 𝑋.
Distribution Function and Probability Density 
Function 
𝐹𝑥0= 
−∞𝑥0
𝑃𝑥𝑑𝑥
𝑃𝑟{𝑥1≤𝑋≤𝑥2}= −∞𝑥2𝑃𝑥𝑑𝑥 −∞𝑥1𝑃𝑥𝑑𝑥
𝑃𝑟{𝑥1≤𝑋≤𝑥2}= 𝑥1𝑥2𝑃𝑥𝑑𝑥
Distribution Function and Probability Density 
Function 
•The probability that𝑋isbetween anytwolimits isfound
byintegrating thedensity function between these two
limits .
•This explains thereason fortheterminology “density
function” .
•Thetotal area under thedensity function isunity
 
−∞+∞
𝑃𝑥𝑑𝑥=1
Gaussian Density
•Isthemost common density encountered intherealworld .
𝑃𝑥=1
𝜎2𝜋exp(−(𝑥−𝑚)2
2𝜎2)
𝑚and𝜎areconstants .
•Theparameter 𝑚indicates thecenter position orsymmetry
point ofthedensity .
•Theparameter 𝜎indicates thespread ofthedensity .
Gaussian Density

Gaussian Density
•The Gaussian density issufficiently important, such that
this integral hasbeen computed and tabulated under the
name error function .
erf𝑥=2
𝜋 
0𝑥
𝑒−𝑢2𝑑𝑢
erfc𝑥=1−erf𝑥=2
𝜋 
𝑥∞
𝑒−𝑢2𝑑𝑢
•
•
Information Theory
Entropy Coding 
Information Theory
•The concept ofinformation content isrelated to
predictability .
•The more likely aparticular message, the less
information isgiven bytransmitting thatmessage .
𝐼𝑥=𝑙𝑜𝑔1
𝑃𝑥
𝐼𝑥:information content ofmessage𝑥.
𝑃𝑥:probability ofoccurrence ofthemessage𝑥.
Information Theory
•Entropy :isdefined astheaverage information per
message .
𝐻= 
𝑖=1𝑛
𝑃𝑥𝑖𝐼𝑥𝑖
•Information content can beinterpreted asthe
minimum number ofbinary digits required toencode a
message .
Information Theory
•Entropy can be interpreted as the minimum number of 
digits per message required, on average, for encoding. 
•Consider a communication scheme made up of two 
possible messages 
𝑃𝑥2=1−𝑃𝑥1
𝐻=𝑃𝑥1𝑙𝑜𝑔1
𝑃𝑥1+1−𝑃𝑥1log(1
1−𝑃𝑥1)
Information Theory
•The result is sketched as shown:
•Note thataseither ofthetwomessages becomes more
likely, theentropy decreases .
•When either message hasprobability 1,theentropy
goes tozero.

Information Theory
•This isreasonable, since atthese points, theoutcome
iscertain .
•If𝑃𝑥1=1,weknow that thefirst message will be
sent allthetime.Noinformation istransmitted by
sending themessage .
•Maximum Entropy occurs incase ofequally probable
messages .
Coding
•Given Mpossible messages, wewish toconvert them
intoMpossible code words .
•The code words canbeselected toachieve objectives
such asefficiency, error correction orsecurity .
•We discuss codes toachieve efficiency under the
heading entropy coding .Such codes attempt tosend the
information using theminimum number ofbits.
Coding 
•We illustrate error detection and correction using
block Hamming codes .
•The prefix property :nocode word forms thestarting
sequences (known astheprefix) ofany other code
word .
•The prefix restriction issufficient but necessary
conditions forthecodes tobeuniquely decodable .
Entropy Coding: Huffman and Shannon
•Itisofinterest tofind uniquely decodable codes of
minimum length .
•This isachieved byassigning theshorter codewords to
themost probable messages .
•Different messages are encoded into words of
different lengths .
Entropy Coding: Huffman and Shannon
•When talking about thelength ofacode, wetherefore
must refer totheaverage length ofthecode words .
•This average iscomputed bytaking theprobabilities of
each message intoaccount .
•For binary -coding alphabets, the average code word
length isgrater than orequal totheentropy .
•Fortheaverage length tobeequal totheentropy, the
probability ofevery message must beinverse power of2.
 
 
 
&
 
 
Fourier Series
 
© Basem Hesham Lecture 1
 
 
Introduction 
 
 
 
 
 سريعاكدا هنعرف  أي الحاجات اللي هندرسها في الكورس دا وهنعرف ليه بندرسها ومدى  
أهميته عشان دا هيخليك تقدر تمشي في الكورس من غير ما تحس انك تايه.  
 
هتدينا د/هناء شاكر والكورس هندرسه في   22  محاضرة وكلأسبوع  هناخد محاضرتين بس  
ورا بعض ودي توزيع الدرجات في الاليحة  
 
Grading 
50 Coursework 
100 Final Exam 
150 Total 
 
 اعمال السنة من50  درجة متضمنه درجة الميدترموالسكاشن وهيكون فيه كويز في  
المحاضرات وفيه بونص للناس اللي هتجاوب بسرعة وتشارك, وتفاصيل توزيع اعمال السنة   
الدكتور هتبلغنا بيها ان شاء هللا.  
 
 
المادة بشكل عام امتحانها من المحاضرات والسكاشن ولو عايز تتعمق اكتر ممكن تشوف الـ  
references  :دي 
 
" Digital Communications Fundamentals and Applications , by Bernard Sklar  " 
" Modern digital and analog communication systems , by B.p. lathi  " 
" Digital and Data Communication Systems , by Martin S. Roden " 
" Digital Communication Systems -Wiley , by Simon S. Haykin " 
 
: وهتالقيهم موجودين في اللينك دا https://bit.ly/3tgsoFw  
 
 
 
 
 
 
 Introductio n
n 
 
Course Outlines  
 
▪ Block Diagram of digital communication system  
 هنرسم block diagram كامل للـ digital system  من بداية مااإلشارة تدخل وتتبعت من الـ transmitter  
 ويكون اصلها analog أي اللي بيتم عليها وبتمر ب ـ stages ومراحل مختلفة لحد ما الـ receiver يستقبلها, 
وكل عمليه تمت في الـ transmitter   تم عكسها فيالـ receiver . 
 كل block تقريبا هو شابتر في المنهج هيتشرح فيه بالتفصيل.    
 
▪ Theory of Probability  
 نظريــة اتحتمــاتت مهمــة جــدا وبتــوفرأســس رياضــية قويــة لفهــم وتحليــل الظــواهر العشــوا ية فــي مجــال 
اتتصاتت. 
الـ digital  signal بعد ما يتعمــل لهــا modulation وتتبعــت فــي الهــوا والــ ـ receiver  يســتقبلها وارد  اإلشــارة 
خــالل يريقهــا تكــون تعرضــت لــ ـ noise ووارد يحصــل مشــاكل فــي اتنتشــار multi path propagation 
problems    ودا ببساية واحنا بننقلاإلشارات  في الهوا ممكن تصيدم باي مبنى وترتد منــه مــرن تانيــة معنــى 
كدا ان  اإلشارة  هتوصل للـ receiver  بـ phase  غير التاني ويبعا المشكلة هنا اناإلشارات دي الـ receiver 
  هيجمعها ولو عندي اشارتين  sinewave مثال و حصل   delayسبب shift  phase 180   كدا مجموعهم بقى0. 
 
فنظرا للـ noise والـ propagation problems  تزم نحي للــ ـ digital system   احتمــاتت الوصــول الصــحي
واحتمــاتت الوصــول الخيــت فالمتخصصــين بيعملــوا planning  ويدرســوا ظــروف النظــام ويحســبوا احتمــال
وصول  اإلشارة  بشكل صحي  بكذا واحتمال وصول  اإلشارة  بشكل خيت بكذا  واحتمال  اإلشارة  تكــون  ضــعيفة 
في المكان دا بكذا  ,فمفهوم الـ Probability للـ digital system مهم جدا. 
  
▪ Source Coding  
اإلشارة  اللي بنبعتها عند الـ  transmitter    بيكون اصلها analog    فتول مرحلة هتمــر بيهــا اإلشــارة خــاللالــ ـ 
digital system  هو تحويل اإلشارة دي من continuous  فيالــ ـ time للمكــافل ليهــا فــي الــ ـ binary  واللــي
بنقول عليه دوا ر  Analog to digital converter . 
  واسمه Source ألنه بيشتغل على اإلشارة األصلية source signal ويحولها ألصفار ووحايد. 
 
 
 
 
 
▪
 
Channel Coding
 
هو عملية تهدف الى اكتشاف األخياء
  
اللي ممكن تحصل اثناء عملية اترسال
. 
اإلشارة
 
بيتم ارسالها فــي وســي 
مليان  
noise
  
  ولو حصل
أي
  
error
  
 هيبقى مشكلة كبيرن تن داتا كتيــر هتضــيع فعشــان كــدا تزم يكــون موجــود
عندنا حاجة بتعمل  
Error Detection and Correction
.
 
 
 الــ
ـ
 
Channel Encoding
 
 بيضــيف
bits
 
 زيــادة
بنســميها 
R
edundant
 
Bits
 
 او
Check bits
 
 او
parity bits
 
  عشان نعمل
Error Detection and Correction
 
ال
ـ
 
R
edundant
 
Bits
 
 بتزود حاجة اســمها
distance
 
 مــا بــين اتكــواد و
الــ 
ـ
 
distance
 
هــو عــدد
 
الــ 
ـ
 
bits
 
 اللــي
بيختلف فيها كود عن كود اخر
 
.
 
 
 
مثال لو عندي 
3 bits
 
 يبقى كدا عندي
8
 
 احتماتت وهضيف
bit
 
 رابعه اللي هي في
الصــورة
 
دي 
Y
 
 يبقــى كــدا
معايا 
4 bits
 
 يعني
16
 
 احتمال بس مستخدم منهم
8
 
 بس ي
ب 
قى فيه كدا 
8
 
 تانيين مش مســتخدمين
وهــم دول اللــي 
هيدوا فرصة ان لو حصل خيت في  
bit
  
بحصل على كود تاني مش واحد من ال
ـ
  
8
  
  المتعارف عليهم ما بين
ا
لـــ
 
transmitter
  
 و
ا
لـ
  
receiver
  
  وهنا
ا
لـ
  
receiver
  
 يقــدر يكتشــف الخيــت دا, وكــل مــا الــ
ـ
 
distance
 
 تزيــد كــل مــا
إمكانية
  
تصحي   
األخياء
  
تزيد.
 
 
 
لو انا باعت كود 
1
01
 
 ووصل
100
 
لل
ـ
 
receiver
  
ك
د
ا
 
مش هيكتشف الخيت 
ألنه
 
كود مــن اللــي متعــارف عليــه 
لكن اما  
المسافة
  
زات اتكواد زادت فتصب  في  
8
  
اكواد مش هيستخدهم وبكدن هعرف اكتشف الخيت
  
زي اننــا 
نبعت  
1010
 
  ويستقبلها
1011
 
هيكتشف الخيت تن دا مش من ضمن اتكواد اللي متعارف عليها
.
 
فيه شــابتر كامــل هنــدرس فيــه يريقــة او اتنــين للــ 
ـ
 
Error Detection and Correction
 
 وهنعــرف بــتم ازاي
بالتفصيل
.
  
 
 
واســمه 
Channel 
Encoding
 
تن
 
دا خــاص باكتشــاف 
األخيــاء
 
الناتجــة عــن الــ 
ـ
 
channel
 
 ودا الوســي اللــي
بيتبعت فيه 
اإلشارة
 
وهو دا اللي سبب ان فيه 
أخياء
 
تحصل في ال
ـ
 
signal
 
 فبضيف
bits
 
زيادة
 
فيه
.
 
 
واحنا بندرس الشابتر دا فيه شابتر يعتبر اقرب لل
ـ
 
Source 
C
oding
 
 ولكن هندرسه هنا وهو عن حاجــة اســمها
Information Theory
 

 
▪
 
Information Theory
 
 عشان نقدر نعمل
Error Detection and Correction
 
 و
نعمل 
Source coding
 
 باقــل عــدد ممكــن مــن الــ
ـ
 
bits
 
 فهنعمل حاجة اسمها
Lossless Compression
 
 ضــبي
اإلشــارات
 
وتقليــل حجمهــا ليــه نــوعين ,النــوع 
األول
 
 
Lossy Compression
 ودا بيبقــى فيــه فقــد للــ
ـ
 
information
  
  والنوع التاني
Compression
  
Lossless
  
ودا بيضغي ال
ـ
 
information
 
 من غير ما يحصــل فقــد
يعني هحول عدد اتصفار والوحايد الكبير لعدد اقل من غير ما افقد حاجة من البيانات ودا عشان اعملــه تزم 
أكون
  
دارس 
Information Theory
.
 
 
 
▪
 
Line Coding
 
اإلشارة
 
ال
ـ
 
Analog
 
 حولناها لشوية اصفار ووحايد ومــثال هنعتبــر ان
binary 1
 
 هــو
5 v
 
 و
binary 0
 
 هــو
-
5
 
v
 
 و
دا
 
احد اشكال تمثيل
 
ال
ـ
 
 
Baseband Signal
 
ولكن مش دا الشكل الوحيــد وفيــه عيــوب
 
زي ان لــو بعــت مــثال 
عدد اصفار كبير ورا بعض او عدد وحايد كبير مش هيحصل  
transition
  
  
لل
ـ
  
signal
  
 لفترة ودا هيسبب مشاكل
في ال
ـ
  
timing
  
 و
ال 
ـ 
  
synchronization
  
  تن بيكــون فيــه
clock
  
  فــي
transmitter
  
  و
clock
  
  فــي
receiver
  
 اتتنــين
دول تزم يكونـــوا 
synchronized
 
 مـــع بعـــض فلحظـــات اتنتقـــال دي هـــي اللـــي بتســـاعدنا فـــي تحقيـــق ال
synchronization
 
  العيب التاني ان لو تي سبب من األسباب اإلشارة اتقلبت
180
  
درجة كل اتصفار
 
هتبقى وحايــد وكــل الوحايــد 
هتبقى وحايد والداتا كلها بقت غلي
 
اسم
  
الشكل
  
اللي فات  
non
 
return to zero 
(
NRZ
)
  
,
وأحد
 
ال
ـ
  
forms
  
  اللي هندرسها
ال
ـ
  
binary 1
 
 هعبرعنه ب
ـ
5 
v
 
 في النص األول و
 
-
5
 
v
 
في النص التاني وال
ـ
 
binary 0
 
 هعبرعنه ب
ـ
 
-
5
 
v
 
 في الــنص األول و
 
5
 
v
 
 فــي الــنص
التاني والصورة دي بتوض  الفرق بين النوعين  
 
 
 
ال 
شكل اللي فات حل مشكلة ال 
ـ 
  
synchronization
  
تن ال
ـ
  
transitions
  
زاد
،
  
ولكــن المشــكلة التانيــه لســه موجــود
ن
 
وهي ان لو اإلشارة اتقلبت 
180
 
.درجة
 
شابتر ال
ـ
Line Coding 
 
 خاص للتعرف على اتشــكال المختلفــة للــ
ـ
Baseband Signals 
 
 ومزايــا وعيــوب كــل
شكل فيها وهندرس بعض الدوا ر البسيية اللي من خاللها نعمل 
generation
 
لألشكال المختلفة
 
لل
ـ
Baseband 
Signals
 
 

 
▪ Carrier  Modulation  
الـ Baseband Signal إشارة ترددها قليل ت تصل  انها تتبعت في الهوا فالزم اعمــل modulation  واحملهــا
على  carrier    وهيإشارة  sinewave  ترددها عالي  (الـ  carrier    دايما analog)   وبعد عمليــة الـــ  modulation 
  بتيلعإشارة معدلة modulated signal 
أنواع  ال modulation  في ال digital communications: 
▪ ASK (Amplitude Shift Keying)  
▪ FSK (Frequency Shift Keying)  
▪ PSK (Phase Shift Keying)  
▪ Spread spectrum modulation  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
هنراجع سريعا في الجــزء دا علــى  المعنــى  والمفهــوم  تن دا أســاس ت غنــى عنــه  ومــش هنتيــرق إلثباتــات 
ومسا ل في جزء  Fourier Series  و    Fourier Transform 
 
   Fourier Series  و Fourier Transform  هما تحليالت رياضية الهدف منها إيجاد محتــوى أي إشــارة مــن
الترددات ودا بنحتاج نعرفه لكذا سبب زي ان  فيه  ترددات لــبعض اإلشــارات  unwanted  وفيــه تــرددات الـــ 
receiver   بيعديها ومب يعديش غيرها ,انا عايز مثال الـ receiver  يســتقبل اإلشــارة مــن الموبايــل بتــاعي علــى
تردد معين ويرفض باقي اإلشارات ودا بيتم باستخدام دوا ر الفلتر ويبعا مينفعش نصمم الفلتر من غيــر مــا 
نعرف محتوى اإلشــارة مــن التــرددات  ونعــرف نعــدي تــرددات أي ونــرفض أي ودا  بنــاءا    علــى اإلشــارات 
المرغوب فيها أكون عارف تردداتها كام. 
 
في Fourier Transform  رسمنا حاجة اسمها amplitude spectrum  وهو عالقة بين التــردد علــى المحــور
اتفقي والـ  magnitude    على المحور الرأســيو الـــ  magnitude    دا هــو amplitude  لإلشــارة  عنــد  تــرددات  
مختلفة. 
 
في المثال دا لـ amplitude spectrum  بيوض  ان عندنا قيملإلشارة من 0Hz  لحــد 1000 Hz  ولكــن عنــد 
1000 Hz تحديدا قيمة الـ amplitude بصفر وبالتالي الرسمة دي وضــحت محتــوى اإلشــارة مــن التــرددات 
وقيمة  الـ amplitude عند كل تردد. 
 
 
 
 
 
 
 
 
 
في اإلشارة دي نقدر نقول ان الـ Band Width  )(هو الفرق بين اعلى تــردد واقــل تــرددهــو  1000 Hz 
 وهو نياق التردد اللي تحتويــه اإلشــارة وممكــن نقــول عليــه Band Limited to 1000 Hz  يعنــي اعلــى
تردد فيها هو  1000 Hz. 
 
 واض  من الرسم ان الترددات مش موجودن بنفس النسبة في اإلشارة وفيه ترددات موجودن بشكل اكبــر مــن
الترددات األخرى ومن دا نقدر نقول ان الـ amplitude spectrum  هو قيمة تعبر عن نسبة وجــود التــردد دا
مقارنة بباقي الترددات الموجودة في اإلشارة. 
 
ومن الرسمة هنالقــي فيــ ه DC Voltage  تن موجــود قيمــةلإلشــارة فــي amplitude spectrum  عنــد تــردد
صفر وفي الـ Time domain   هتظهراإلشارة وهي فيها shift   في المحور الرأسيبمقدار قيمة  الـ DC  Fourier Analysis  
amp spectrum  
𝒇 
1000 Hz  0 Hz 
 
 
 
 متسلسلة فورير هيأداة رياضية تستخدم في تحويل أي إشارة دورية Periodic Signal  مهمــا كانــت درجــة
تعقيدها إلى مجموعات من اإلشارات المبسية Sin and Cos waves   بترددات مختلفةكما  يلي: 
𝑓(𝑡)= 𝑎𝑜+ ∑𝑎𝑛cos(𝑛𝜔𝑜𝑡)∞
𝑛=1+ ∑𝑏𝑛sin(𝑛𝜔𝑜𝑡)∞
𝑛=1 
𝜔𝑜=2𝜋𝑓𝑜 
𝑓𝑜=1
𝑇 
 
في الرسمة اتتيه  هنالحظ ان اإلشارة تتكرر كل  𝑇  ولكن هل دا معنان اناإلشارة ترددها 𝑓𝑜=1
𝑇 ؟ يبعا ت 
تردد  𝑓𝑜    دا التردداألساسي  اللي ظاهر على الرسمة لكــن فعليــا اإلشــارة دي مكونــه مــن عــدد كبيــر جــدا مــن 
الترددات وعشــان نعــرف التــرددات دي نرجــع لمعادلــة Fourier series  زي مــا كنــا بنحســبها قبــل كــدا لمــا
درسناها. 
اإلشارة  الـ  periodic   دي مكونه من عدد كبير جدا من الترددات هم𝑓𝑜  ومضاعفاته الصحيحة فقي وكل تــردد
موجــود بنســبة او weight  او Amplitude مختلــف عــن التــاني واللــي بيعبــر عــن نســبة التــردد هــي الــ ـ 
coefficient s     وكل ماكان الرقم دا كبير   فدا معنــان ان التــردد دا موجــود  بـــ  Amplitude  كبيــر  داخــل الموجــة 
األصلية والعكس بالعكس, واحيانا واحنا بنحل مسا ل فورير كنــا بنالقــي قــيم   الــ ـ  coefficients  عنــد  n   معينــه
بصفر ودا معنان ان التردد المناظر للـ n دا مش موجود  في اتشارة. 
 
 
 
  
 
 
 
 
Fourier Series  
𝜔𝑜→  𝑓𝑢𝑛𝑑𝑎𝑚𝑒𝑛𝑡𝑎𝑙  𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦   
𝑇 
 
لو رسمنا الـ  sin  والـ  cos   في ورقة مثال تحت بعــض وبتــرددات𝑓𝑜  ومضــاعفاتها وجمعــتهم نــاتج تجمــيعهم
هينتج في اتخر شكل  periodic مهما كان عدد الـ sin والـ cos 
دا مثال بسيي لـ 3  ترددات مجموعهم بينتج إشارة periodic 
 
 
 ازاي كل اإلشاراتالـ periodic يتكونوا  من  sin  وcos وفي نفس الوقت ليهم اشكال مختلفة؟ 
بسبب اختالف الـ  fundamental  frequency اللي بنبدا بيه ولو مثال 𝑓𝑜=100  𝐻𝑧  اإلشارة هتتكــون مــن 
التــرددات 100 , 200 , 300 , 400 , 500 ,…  ولــو كــان 𝑓𝑜=1000  𝐻𝑧   إلشــارة تانيــه هتتكــون مــن
الترددات 1000 , 2000 , 3000 , 4000 , 5000 ,…   ودا هيخلي الشكل يختلف 
سبب تاني ممكن يخلي الشكل يختلف وهو  قيم الـ coefficient s  .لو اختلفت الشكل هيختلف 
 
تردد إشارة الـ sin والـ cos  هــو ترددهــا األساســي fundamental  frequency  𝑓𝑜  فقــي ودي اإلشــارة اللــي
ممكن أقول انها تتكون من تردد واحد, وعشان كدا بنقول ان الترددات  الموجودة في اإلشارات موجــودن فــي 
شكل sin  وcos , ولما نعمل Fourier Transform  إلشارةsin  وcos  بتيلع فيالـ spectrum  عند التردد𝑓𝑜 
impulse  signal تدل ان الـ weight بتاع الـ signal عالي جدا عند 𝑓𝑜  تن مفــيش تــردد غيــرن فــي الرســمة
اصال 

 
 
 
 
 
 
   
 
©
 
Basem Hesham
Binomial Distribution & Random Process
Lecture
 
4
 
 
 
 هنستكمل في المحاضرةإيجاد الـ probability    الخاصــبالــ ـ random variable   ــا المــر  دت هنت امــل
مع discrete random variables  ومن اشهر التوزي ات المُستخدمب في الت امل مع المتغير ال شــوا ي
المنفصل هي Binomial Distribution 
 
 المتغير العشوائيالمنفصل  ( Discrete Random Variable)  هو نوع من المتغيرات ال شــوا يب التــي
تأخذ قيماً محددة. ي ني ذلك أن النتا ج الممكنب للمتغير ال شوا ي المنفصل تكون قا لب لل د وغير متصــلب 
   ضها ال  ض. على س يل المثال، إذا كنا نرمز لرمــي النــرد  ــرق ، فــين المتغيــر ال شــوا ي  المنفصــل 
سيكون قيمه هي األرقا  من 1   إلى6. 
 
 المتغير العشوائيالمستتمر  ( Continuous Random Variable)  فهــو نــوع مخــر مــن المتغيــرات
ال شوا يب  يمكن أن يأخــذ قيمــاً فــي نتــا  مســتمر. ي نــي ذلــك أن النتــا ج الممكنــب للمتغيــر ال شــوا ي 
المستمر  يمكن أن تشمل أت قيمب ضمن نتا  محدد. على ســ يل المثــال درجــب الحــرارة ال تأخــذ قــي  
محدد  ويمكن ان تأخذ  عدد النها ي من القي  ضمن نتا  محدد.   
gaussian density   اللي ت  دراسته المحاضرة السا قب ي  ر عن احتماالت المتغير ال شوا يالمستمر.   
 
▪ Consider the experiment of tossing a coin 𝒏 times. Let the probability of heads 
be 𝑝, and of tails be 𝑞. 
𝒒=𝟏−𝒑 
▪ Now suppose we want  to find the probability of 𝑘 heads out of 𝑛 tosses.  
فــي تجر ــب القــال ال ملــب نفتــرض اجــرال التجر ــب عــدد 𝑛  مــن المــرات ولــتكن10  مــرات ونفتــرض
ان احتمــال حــدو  head  هــو𝒑   واحتمــال حــدوtail  هــو𝒒  ,ومجمــوع االحتمــالين يســاوت واحــد
و التالي  𝒒=𝟏−𝒑   
 
   نفترض اننا عايزين نجيب احتمال حدوالــ ـ  heads    عــدد𝒌    مــن المــرات ولنفتــرض ان 𝒌=𝟕  ي نــي
هات احتمال حدو   7  مرات heads  من ضمنالـ 10 .مرات اللي عملنا فيه  التجر ب 
 
▪ One possible sequence is  to start with 𝒏−𝒌 tails followed by 𝒌 heads . 
▪ The probability of any ordering of 𝒌 heads and 𝒏−𝒌 tails 
𝒑𝒌 𝒒𝒏−𝒌 
 
افترضنا ان احتمال حدو    الـ  head   الواحدة𝒑   اذا احتمال حدو head s 7  هي𝒑𝟕   وتضرب قــي  الــ ـ 
𝒑  ألننا  عايزين  الـ  7    دول يظهروا مع   ــض فــي نفــاالــ ـ  sequence     و المثــل احتمــال حــدوالــ ـ  tail 
 الواحدة𝒒   و التالي احتمال حدوالـ tail 3  هو𝒒𝟑, اذا االحتمــال الكلــي لحــدو  7  مــرات heads  و 3 
 مراتtails   هو  𝒑𝟕 𝒒𝟑  وت  ضرب االحتماالت ألنها تحد  م ا كـ sequence واحد 
 Binomial Distribution  
 
  اذا 𝒑𝒌 𝒒𝒏−𝒌 هي احتمال حدو  عدد م ين من الـ  heads   و الـ  tails   ــا االحتمــال داللــ ـ sequence 
 الواحد  ا. ي ني عندتعدد ك ير من الـ sequence s   ممكنت حصل وعشان اقدر حصل علــى الــ ـ  total 
probability   هحتاج احسبعدد الـ sequences 
  عندت احتماالت sequences : كتير ممكن تحصل زت مثال 
TTTHHHHHHH , HHHHHHHTTT , HTHTHTHHHH , HHHHHTTTHH , …….  
عشان اعرف احسب عدد الـ sequence s    المختلفب اللي هيظهر فيه7   مرات heads  و 3   مراتtails  
 هحس ه  عن تريق الت اديل والتوافيق من القانون 
(𝒏
𝒌)=𝒏!
𝒌!(𝒏−𝒌)! 
 
▪ The overall probability of 𝑘 heads (in any position) is  found by adding the 
individual probabilities together  
 
▪ Thus, we need to multiply the probability of the  sequence by the number of 
ways we can distribute 𝑘 heads among 𝑛 positions.   
This number is found by : 
(𝒏
𝒌)=𝒏!
𝒌!(𝒏−𝒌)! 
where (𝒏
𝒌) is the binomial coefficient  
Thus, the probability of 𝑘 heads is given by  
 
𝒑(𝑿=𝑲)=(𝒏
𝒌) 𝒑𝒌 𝒒𝒏−𝒌=𝒏!
𝒌!(𝒏−𝒌)!𝒑𝒌 𝒒𝒏−𝒌 
 
 م نى القانون انالـ Random variable X لقيمب محدد  وهي K   يساوت عددالـ sequences   الممكنب
(𝒏
𝒌)  في احتمالالـ sequence الواحد  𝒑𝒌 𝒒𝒏−𝒌 
 في تجر ب القال ال ملب10  مرات متلوب احتمال ظهور 7 Heads  فيالـ  10 مرات 
 
 𝑿 هي المتغير ال شوا ي وهي احتمال ظهور الـ  Head 
𝑲  هي الرق  المحدد اللي المتغير ال شوا ي هيساويه وفي المثال داK  تساوت7   واللي  ت  ر عن عدد
ظهور الـ Head 
 𝒏 عدد مرات اجرال التجر ب وفي المثال دا يساوت 10 
𝒑  : probability of Head              ,𝒒  : probability of tails    
 
 
 
 
A noisy transmission channel has a per -digit error probability 𝑷𝒆=𝟎.𝟎𝟏 
calculate the probability of having more than one error in 10 received digits.  
Solution  
 عندت channel     ت فيها  sequence binary مكون من 10 bits   تت رضلـ noise  واحتمال ان
يحصل ختأ في كل bit    هو 𝑃𝑒=0.01  ومتلوب احتمال ان يحصل error  في اكتر منbit 
 
Random variable X  هنا هو عدد الـ bits  اللي فيها ختأ 
 𝒑 هو احتمال الختأ في الـ bit ( الواحدة probability of error ) 
 
𝒑=𝟎.𝟎𝟏 
𝒒=𝟏−𝒑=𝟏−𝟎.𝟎𝟏=𝟎.𝟗𝟗 
𝒏=𝟏𝟎 
𝒑(𝑿=𝟎)+𝒑(𝑿=𝟏) + 𝒑(𝑿=𝟐)+⋯+ 𝒑(𝑿=𝟏𝟎)=𝟏 
 
 متلوب احتمال ان يحصل اكتر من error  و التالي االحتماالت عندت هي  دايب من احتمال ان يحصل  
2 error  لحد احتمال ان يحصل 10 error  ي ني هجمع  كل االحتماالت دت وت  ا دا هياخد وقت ك ير  
في الحل.  
لو الحظنا ان مجموع االحتماالت   دايب من عد  حدو  error  𝒑(𝑿=𝟎)  لحد احتمال ان الـ 10 bits  
  كله  يحصل فيه error  𝒑(𝑿=𝟏𝟎)   يساوت واحد, و التالي لو ترحنا من1   احتمال ان ميحصلش
error  واحتمال ان يحصل error   واحد هحصل على ناتج احتمال حدو  اكتر من error . 
 
𝒑(𝑿>𝟏)=𝟏−𝒑(𝑿=𝟏)−𝒑(𝑿=𝟎) 
𝒑(𝑿>𝟏)=𝟏−(𝟏𝟎
𝟎)(𝟎.𝟎𝟏)𝟎 (𝟎.𝟗𝟗)𝟏𝟎−(𝟏𝟎
𝟏)(𝟎.𝟎𝟏)𝟏 (𝟎.𝟗𝟗)𝟗 
=𝟏−𝟏𝟎!
𝟎!(𝟏𝟎)!(𝟎.𝟎𝟏)𝟎 (𝟎.𝟗𝟗)𝟏𝟎−𝟏𝟎!
𝟏!(𝟗)!(𝟎.𝟎𝟏)𝟏 (𝟎.𝟗𝟗)𝟗 
= 𝟎.𝟎𝟎𝟒𝟐  
 
 
 
 
 Example  1 
 
 
 
   
Expected Value   هي نفسهاالمتوست الحسا ي  ( mean)  وتُ ت ر مقياسًا للقيمب المتوستب المتوق ب لمتغيــر  
عشوا ي. 
 
The mean value 𝒎𝒙, or expected value of a random variable X, is defined by  
 
𝑬{𝑿}=𝒎𝒙= ∫𝒙 𝒑(𝒙) 𝒅𝒙∞
−∞ 
 
 𝐸{𝑋} تُقرأ  الـ   Expected Valueللـ Random Variable X  
 
 لو عندنا قي  discrete  مثال كنا  نجيبالـ mean     عن تريق ضرب كل قيمب في احتمال حــدوثها ثــ  يــت
جمع النواتج ولو االحتماالت لكل قيمب متساويب ممكن نحسب المتوست عن تريق جمع القي  ثــ  نقســمه  
على عدده . 
 
عشان نجيب الـ mean   اوالــ ـ average value  لمتغيــر عشــوا ي continuous   ــدل مــا ناخــد كــل قيمــب
وضر ها في احتمال حــدوثها هناخــد المتغيــر ال شــوا ي كأنــه القــي   تاعتنــا  ــا  continuous   و ــدل مــا
اضرب في احتمال حدوثه ألنه مش رق  محدد ف ضــرب فــي probability density function 𝒑(𝒙)  
  تاعب المتغير ال شوا ي و دل جم ه  هكامل من∞-   الى∞  
 
 من المقاييااإلحصا يب  المهمب واللي اخدنا فكر  عنه المحاضرة اللي فاتت وهي الت اين variance 
 
𝝈𝟐=𝑬{(𝑿−𝒎𝒙)𝟐}= ∫(𝒙−𝒎𝒙)𝟐 𝒑(𝒙) 𝒅𝒙∞
−∞ 
 
لحساب الت ــاين variance ، يــت  حســاب الفــر   ــين كــل قيمــب ممكنــب للمتغيــر ال شــوا ي ( Random 
variable  𝑿 ) والقيمب المتوق ب او المتوست (𝒎𝒙 mean  )  حتى نحسب مدى  ُ د القي  عــنالــ  ـ mean   ،
ث  يت  رفع هذا الفر  إلى األا الثاني حتى نتالشى القي  السال ب الن قيمب الت اين ممكن تكون موجــب او 
سالب.  
 
 
 
 
 
 
 
 
 Expected Values  (mean)  
 
 
X is u niformly distributed as shown in the following figure. Find  𝑬{𝑿}, 𝑬{𝑿𝟐},  
𝑬{𝒄𝒐𝒔 𝑿} and 𝑬{(𝑿−𝒎𝒙)𝟐} 
 
 
 
 
 
 
 
Solution  
 
Uniform Distribution is the simplest continuous density function.  The value of the density 
function is a constant over a  range of the x -axis. 
 
 الشكل السا ق هو من اشكالالـ probability density function  وهو uniform distribution   وهو توزيع
له دالب احتمال 𝑝(𝑥)  ثا تب ودا س ب تسميتها uniform    ألنها لديها قيمب ثا ته تساوت1
2𝜋  في هذا المثال 
 
1) 𝑬{𝑿} 
 
𝐸{𝑋}=∫ 𝑥 𝑝(𝑥)𝑑𝑥2𝜋
0=∫ 𝑋 1
2𝜋𝑑𝑥=1
2𝜋 𝑋2
2|
02𝜋2𝜋
0=𝜋 
 
2) 𝑬{𝑿𝟐} 
 
𝐸{𝑋2}=∫ 𝑥2 𝑝(𝑥)𝑑𝑥2𝜋
0=∫ 𝑋2 1
2𝜋𝑑𝑥=1
2𝜋 𝑋3
3|
02𝜋2𝜋
0=4
3𝜋2 
 
 
3) 𝑬{𝒄𝒐𝒔 𝑿} 
 
𝐸{𝑐𝑜𝑠 𝑋}=∫ 𝑐𝑜𝑠 𝑥 𝑝(𝑥)𝑑𝑥2𝜋
0=∫ 𝑐𝑜𝑠 𝑥 1
2𝜋𝑑𝑥=1
2𝜋 𝑠𝑖𝑛 𝑥|02𝜋2𝜋
0=0 
 
 
 Example  2 
𝟐𝝅 𝟏
𝟐𝝅 
𝑿 𝒑(𝒙) 
Probability density function of X  
 
 𝑬{𝒄𝒐𝒔 𝑿}   م ناها اني  جيب المتوست لقي cos x   و التاليقيمتها هتساوت صفر ورياضيا لو حلناها  فدا 
تكامل  cos x    على cycle    كامله من0    الى2𝜋  والتكامل هو المساحب تحت المنحنى و التالي  الـ  positive half 
cycle  هتالشيالـ negative half cycle 
 
 
4) 𝑬{(𝑿−𝒎𝒙)𝟐} 
 كانه  يقول احسبالـ variance 
 𝒎𝒙 هوالـ Expected Values  اللي ج نا  في اول متلوب 
 
𝐸{(𝑋−𝑚𝑥)2}=∫ (𝑥−𝑚𝑥)2 𝑝(𝑥)𝑑𝑥2𝜋
0=∫ (𝑥−𝜋)2 1
2𝜋𝑑𝑥2𝜋
0 
1
2𝜋 (𝑥−𝜋)3
3|
02𝜋
=𝜋2
3 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
X is a Gaussian -distributed random variable with density function  
𝑷(𝒙)= 𝟏
√𝟐𝝅 𝝈 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 
1) Find 𝑬{𝑿}  
2) Find 𝑬{(𝑿−𝒎𝒙)𝟐} 
Solution  
 في المتلوب األول كانه عــايز يث ــت ان الـــ mean    تــاع Gaussian density function  هــو𝒎 
والمتلوب التاني كانه عايز يث ت ان الـ variance  يساوت𝝈𝟐  ودا احنا عارفينه من الرســ  ولكــن هنــا
عايزين نث ته رياضياً  
1) Find 𝑬{𝑿}  
𝑬{𝑿}=𝒎𝒙= ∫𝒙 𝒑(𝒙) 𝒅𝒙∞
−∞=𝟏
√𝟐𝝅 𝝈∫ 𝒙 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞ 
 
  عشان ن رف نحل التكامل دا الز  نوجد تفاضل االا قداالـ exponential  
 
 :تفاضل االا 
−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐→−𝟐(𝒙−𝒎)
𝟐 𝝈𝟐=−(𝒙−𝒎)
 𝝈𝟐 
 
𝑬{𝑿}=𝟏
√𝟐𝝅 𝝈∫(𝒙−𝒎+𝒎) 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞ 
 
=𝟏
√𝟐𝝅 𝝈∫(𝒙−𝒎) 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙+𝟏
√𝟐𝝅 𝝈∞
−∞∫ 𝒎 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞ 
 
 
الجزل التاني من التكامل ع ــارة عــن تكامــل ثا ــت اللــي هــو  𝒎  مضــروب فــي 𝑝(𝑥)  اللــي هــي الـــ 
gaussian  وتكامل 𝑝(𝑥)   من−∞  الى∞ يساوت واحد الن المساحب كلها تحــت أت density 
function  تساوت واحد و التاليتكامل  الجزل دا يساوت  𝒎 
 
𝑬{𝒄𝒐𝒏𝒔𝒕𝒂𝒏𝒕 }=𝒄𝒐𝒏𝒔𝒕𝒂𝒏𝒕   
 
 
 Example  3 
 
=−𝝈𝟐
√𝟐𝝅 𝝈∫−(𝒙−𝒎)
𝝈𝟐 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙+∞
−∞𝒎 
ضر نا في  −𝝈𝟐
−𝝈𝟐 عشان نوجد تفاضل االا قدا  الـ exponential 
=−𝝈
√𝟐𝝅 [ 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐|
−∞∞
] +𝒎=(𝟎−𝟎)+𝒎=𝒎 
∴𝑬{𝑿}=𝒎 
 
 
2) Find 𝑬{(𝑿−𝒎𝒙)𝟐} 
 
𝑬{(𝑿−𝒎)𝟐}= ∫(𝒙−𝒎)𝟐 𝒑(𝒙) 𝒅𝒙∞
−∞ 
=𝟏
√𝟐𝝅 𝝈∫(𝒙−𝒎)𝟐  𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞ 
=−𝝈𝟐
√𝟐𝝅 𝝈∫(𝒙−𝒎)−(𝒙−𝒎)
𝝈𝟐  𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞ 
 
نحلها عن تريق التكامل   التجزتل  
 
𝒖=(𝒙−𝒎)              𝒅𝒗=−(𝒙−𝒎)
𝝈𝟐  𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐𝒅𝒙 
𝒅𝒖=𝒅𝒙            𝒗=𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 
 
∫𝒖 𝒅𝒗=𝒖𝒗− ∫𝒗 𝒅𝒖 
 
∴𝑬{(𝑿−𝒎)𝟐}=−𝝈𝟐
√𝟐𝝅 𝝈[(𝒙−𝒎) 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐|
−∞∞
−∫ 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞] 
=−𝝈𝟐[𝟏
√𝟐𝝅 𝝈(𝒙−𝒎) 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐|
−∞∞
−𝟏
√𝟐𝝅 𝝈∫ 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐 𝒅𝒙∞
−∞] 
=−𝝈𝟐[(𝟎−𝟎)−𝟏]=𝝈𝟐 
 
   المساحب كلها تحت
المنحنى تساوت واحد  
 
 
 
 
▪ A random or stochastic process is a collection of  infinite number of sample 
functions together with  associated statistical properties.  
ال مليب ال شوا يب Random Process  هي مجموعب من المتغيرات ال شوا يب يرت ت كــل منهــا  نقتــب زمنيــب
محددة.  في ال مليب دت  جيب  samples     عددها ك يــر جــداً عشــان نقــدر نحــدد الخــوااإلحصــا يب   لمتغيــر 
عشوا ي. 
 
لنفترض أننا نقو   قياا درجب الحرارة في مدينب ما في الساعب 12   ظهرًا يوميًا ونرصد هــذال يانــات علــى  
مدى فترة تويلب,  مثال  سنقو   رصد درجب الحرارة في  اليو  األول الساعب 12  ظهراًوقيمب درجــب الحــرارة  
في اليو  التاني الساعب  12    ظهراًوهكذا في  اقي   األيا   ي ني هاخد  samples    كتير جدا على مدارأيا   كتيــر 
الساعب 12  ومنالـ  samples  دت هقدر اجيب المقاييااإلحصا يب.    
في هذا المثال يمكننا أن ن ت ر هذ  السلسلب مــن القــرالات ك مليــب عشــوا يب Random Process ، ودرجــب
الحرارة هي المتغير ال شوا ي الذت يرت ت  كل نقتب زمنيب.    
 
Example: Random Process to represent the  temperature of a city  
 
 
 
األر ع رسومات نفا اإلشارة  𝑥(𝑡)  او نفا المتغير  ولكن في أيا   مختلفب 
 
 
 
 
Random Process  
 
 
 
 منضمن المقاييا اإلحصا يب هي الـ Correlation    وزت ما عرفنا في المحاضرات اللي فاتت انــه  يحــدد
مدى ترا ت و تشا ه  اإلشارات  ومن ضــمن انواعــه هــو  autocorrelation   ودا  يقــيا مــدى التشــا ه  ــين
نفا  اإلشارات. 
 
في المثال  اللي في الصورة  عندنا  متغيرين عشوا يين االول اسمه  𝑥(𝑡)   والتاني  𝑦(𝑡)   وكالهما ع ارة عن  
Random variable  و function  فــي الــزمن𝑡   ,و اللحظتــين اللــي هقــارن عنــده𝑡1  و𝑡2  ونفتــرض ان
المتغير ال شوا ي دا هو درجب الحرارة  قيسها على مدار اليو  وعايز اعرف ال القــب  ــين درجــب الحــرارة 
عند  𝑡1    ولنفترض انها12    الضهر و𝑡2    الساعب3   ال صر وعايز اعرف درجب الحرارة في اللحظتين دول
متقار ين في القي  وال مت اعدين ,ودا اللي هيقيسه الـ  autocorrelation    هيجيب مدى التشا ه ما  ــين قــيالــ ـ 
Random variables .عند لحظتين مختلفتين 
 
المتغير ال شوا ي  𝑥(𝑡)    في اول sample    هنالحظ ان القيمتين عند𝑡1    و𝑡2   كالهما موجــب وقــري ين مــن
  ض وفي تاني  sample    القيمتين سال ين وقري ين من   ض وهكذا في  اقيالـ  samples   ,نستنتج من كدا
ان قيمب  الـ  autocorrelation    للمتغير ال شوا ي 𝑥(𝑡)  هيتلععالي  الن التشا ه  ين القي  ك ير 
في المتغيــر ال شــوا ي  𝑦(𝑡)   هــنالحظ ان فــي كــل sample  القــي  عنــد اللحظتــين𝑡1  و𝑡2   قــيمه  مختلفــب
و  يد  عن   ض و التالي الـ  autocorrelation   للمتغير ال شوا ي 𝑦(𝑡)   هيتلعقليل  
  
 
 
لو عايز اجيب م ادلب ت  ر عن الـ  autocorrelation   وكنا اخدنا فكر  ق ل كدا عن القانون وهوتكامل  
حاصل ضرب اشارتين او نقتتين  وفي المثال اللي فات هو تكامل  حاصل ضرب النقتتين  𝑡1   و𝑡2  
 
𝑹𝒙𝒙(𝒕𝟏 ,𝒕𝟐)=𝑬{𝒙(𝒕𝟏) ,𝒙(𝒕𝟐)}=𝒙(𝒕𝟏) 𝒙(𝒕𝟐) ̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅     
 
Correlation Function of a Random Process  
 
←𝑅𝑥𝑥  هي رمز الـــ  correlation  و𝑥𝑥 م ناهــا لــنفا المتغيــر ال شــوا ي 𝑥  ألنهــا autocorrelation 
 واحيانا ممكن يشيلها ويكتــب𝑅  فــنفه  انهــا autocorrelation  امــا لــو كانــت cross  correlation   الز
تتكتب  زت مثال  𝑅𝑥𝑦  
 
 ت ريفالـ correlation  هوالـ Expected Value    لحاصل ضرب قيمبالمتغير ال شوا ي عند اللحظــب  
𝑡1  في قيمب المتغير ال شوا ي عند اللحظــب𝑡2  ,وزت مــا ت لمنــا انالــ ـ Expected Value    هــيأصــال 
المتوست او  الـ average  و التالي ممكن نقــول الت ريــف  تريقــهتانيــب وهــي ان الــ ـ correlation  هــي
متوست حاصل الضــرب للمتغيــر ال شــوا ي عنــد  𝑡1    و𝑡2   و شــكل عــا    الــ ـ  correlation   هــو متوســت
حاصل الضرب  ين حاجتين عايز اجيب  التشا ه ما  ينه .  
 
الكال   دا منتقي  الن لو شوفنا المتغير ال شــوا ي 𝑥(𝑡)  مــثال وحســ نا حاصــل الضــرب عنــد𝑡1  و𝑡2 
  لكل sample  وج نا المتوست  هيتلع رق  ك ير  الن القي  قري ه من   ض  والرق   الك ير دا  يدل ان مدى 
التشا ه  ين القيمتين ك ير. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A process with overall statistical properties  that are independent of time  
 هي عمليب خصا صهااإلحصا يب ال ت تمد على الزمن ي ني مهما تغير الوقت الخوا  اإلحصا يب 
ثا ته.  
 
The random process representing the temperature of a city  is an example of 
non-stationary random process.  The temperature statistics (mean value), for 
example,  depend on the time of the day. 
 في المثال السا ق لدرجب الحرارى لو قمنا  قياا متوست درجب الحرارة فيأوقات مختلفب في نفا  
اليو  هنالقي القي   تختلف وتسمى هذ  ال مليب non-stationary random process . 
 
 فيه حالب خاصب منهاو اللي هنشتغل عليها في المسا ل وهي Wide -Sense Stationary Process  
 
 
 
 
 
A process in which only the mean and autocorrelation function are independent 
of time.  
 هي حالب خاصب من Stationary Process  فيهــاالــ ـ  mean  والــ ـ autocorrelation function  فقــتال 
ي تمدوا على الزمن و التالي متوست المتغير ال شوا ي  𝑥(𝑡)   يساوت constant 
𝒙(𝒕)̅̅̅̅̅̅=𝐜𝐨𝐧𝐬𝐭𝐚𝐧𝐭  
 
autocorrelation function  ال ت تمد على مكان وجود اللحظتين𝑡1  و𝑡2 فين ولكن ت تمد على الفــر  
الزمني   ينه  فقت  وال ت تمد على الزمن نفسه وتالمــا الفــر  ثا ــت هتتلــع قيمــب الــ ـ autocorrelation 
  ثا ته و التالي هيكون function  في الفر  ما  ينه  فقت𝝉 
  و التالي هن دل القانون ونشيل𝒕𝟏    ونكت ها𝒕   ونشيل𝒕𝟐   ونكت ها 𝒕+𝝉 وتكون  الشكل االتي 
 
𝑹𝒙𝒙(𝒕𝟏 ,𝒕𝟐)=𝑹𝒙𝒙( 𝒕𝟐−𝒕𝟏)=𝑹𝒙𝒙(𝝉)    ,   𝝉= 𝒕𝟐−𝒕𝟏 
𝑹𝒙𝒙(𝝉)=𝑬{𝒙(𝒕) ∙  𝒙(𝒕+𝝉)} 
 
 
 Stationary Process  
Wide -Sense Stationary Process  
 
اي اللي ممكن نستفاده من الت autocorrelation  ؟ 
1. Frequency content  of the process  
The frequency content of a process depends on the  rapidity of the amplitude change 
with time.  This can be measured by correlating amplitudes at  
𝑡 and 𝑡+ 𝜏. The process 𝑥(𝑡) is a slowly varying process  compared to the process 
𝑦(𝑡) . 
Thus, the autocorrelation function provides valuable  information about the frequency 
content of the  process.  
حسا نا للـ autocorrelation function  إلشارة م ينه ممكن ن رف منه محتوىاإلشارة من التــرددات ,لــو 
قيمب  الـ  correlation    عاليب ما  ين النقت دا م نا  ان التغير في االشــارة اللــي  يحصــل  ــين النقتتــين دول
 تئ و التالي التــرددات قليلــه, وال كــا لــو قيمــب الــ ـ correlation  قليلــه دا م نــا  ان التغييــرات  تحصــل
 سرعب و التالي التردد عالي. 
 
2. Average power of the signal  
We have a way of finding the average power of the  output of a system when the 
input is a stochastic  process.  
value at 𝜏=0 is equal to the average power of the signal  
 
ممكن نحسب الـ power  تاع الـ system  من خاللالت ويض    ـ 𝜏=0   في قانونالـ autocorrelation 
𝑅𝑥𝑥(𝜏=0)=𝐸{𝑥(𝑡) ,𝑥(𝑡)}=𝐸{𝑥(𝑡)2}=∫ 𝑥(𝑡)2 𝑝(𝑥) 𝑑𝑥∞
−∞   
 
الـ  Expected هي الـ average  و 𝐸{𝑥(𝑡)2}  م ناهــا average  الدالــب تر يــع وهــو دا ال ــاور  تــاع
اإلشارة 
 
ملحوظة :  الـ  Energy     تاعباإلشارة  هي مقياا للت  ير عن حجــ   اإلشــارة  ,وحجــ  االشــارة ع ــارة 
عن  amplitude    و duration    ولو كانت االشارة ليهــا amplitude    و duration  ك يــر اذا الــ ـ  Energy 
 تاعتها  عاليب و التالي الـ power عالي 
 
 
 
 
 
 
 
 
 
 
you are given
 
a statistic process 
𝒙
(
𝒕
)
 
with mean value 
𝒎
𝒙
 
and autocorrelation 
function 
𝑹
(
𝝉
)
. obviously ,the process is at least wide
-
sense stationary. If not, 
the mean and  autocorrelation could not have been given in 
this
 
form. Find the 
mean and autocorrelation of a process 
𝒚
(
𝒕
)
, where 
 
𝒚
(
𝒕
)
=
𝒙
(
𝒕
)
−
𝒙
(
𝒕
−
𝑻
)
 
Solution
 
واضح من التوصيف ان ال
ـ
 
process
 
 هي
Wide
-
Sense Stationary
 
 الن
ال
ـ
 
mean
 
  م تى قيمب ثا ته
و 
ال
ـ
 
autocorrelation
 
  دالب في
𝝉
 
 
1)
 
Mean
 
𝒎
𝒚
=
𝑬
{
𝒚
(
𝒕
)
}
=
𝑬
{
𝒙
(
𝒕
)
−
𝒙
(
𝒕
−
𝑻
)
}
 
=
𝑬
{
𝒙
(
𝒕
)
}
−
𝑬
{
𝒙
(
𝒕
−
𝑻
)
}
=
𝒎
𝒙
−
𝒎
𝒙
=
𝟎
 
 
 
𝐸
{
𝑥
(
𝑡
)
}
  و
𝐸
{
𝑥
(
𝑡
−
𝑇
)
}
 
  كالهما يساوت
𝑚
𝑥
 
 الن ال مليه هنا
Wide
-
Sense Stationary
 
  فمهما
ا
غير اللحظب اللي  حسب عندها 
ال
ـ
 
mean
 
 قيمته ثا ته
 
 
2)
 
Autocorrelation
 
 
𝑹
𝒚𝒚
(
𝝉
)
=
𝑬
{
𝒚
(
𝒕
)
 
𝒚
(
𝒕
+
𝝉
)
}
 
=
𝑬
{
[
𝒙
(
𝒕
)
−
𝒙
(
𝒕
−
𝑻
)
]
 
[
𝒙
(
𝒕
+
𝝉
)
−
𝒙
(
𝒕
−
𝑻
+
𝝉
)
]
}
 
=
𝑬
{
𝒙
(
𝒕
)
 
𝒙
(
𝒕
+
𝝉
)
}
−
𝑬
{
𝒙
(
𝒕
)
 
𝒙
(
𝒕
−
𝑻
+
𝝉
)
}
 
−
𝑬
{
𝒙
(
𝒕
−
𝑻
)
 
𝒙
(
𝒕
+
𝝉
)
}
+
 
𝑬
{
𝒙
(
𝒕
−
𝑻
)
 
𝒙
(
𝒕
−
𝑻
+
𝝉
)
}
 
 
Remember that 
for 
Wide
-
Sense Stationary
: 
𝑬
{
𝒙
(
𝒕
𝟏
)
 
𝒙
(
𝒕
𝟐
)
}
=
𝑹
𝒙𝒙
(
 
𝒕
𝟐
−
𝒕
𝟏
)
=
𝑹
𝒙𝒙
(
𝝉
)
 
 
 
 
=
𝑹
𝒙𝒙
(
 
(
𝒕
+
𝝉
)
−
(
𝒕
)
)
−
𝑹
𝒙𝒙
(
(
𝒕
−
𝑻
+
𝝉
)
−
(
𝒕
)
)
−
 
𝑹
𝒙𝒙
(
(
𝒕
+
𝝉
)
−
(
𝒕
−
𝑻
)
)
+
𝑹
𝒙𝒙
(
(
𝒕
−
𝑻
+
𝝉
)
−
(
𝒕
−
𝑻
)
)
 
 
 
𝑹
𝒙𝒙
(
𝝉
)
−
𝑹
𝒙𝒙
(
𝝉
−
𝑻
)
−
𝑹
𝒙𝒙
(
𝝉
+
𝑻
)
+
𝑹
𝒙𝒙
(
𝝉
)
 
=
𝟐
𝑹
𝒙𝒙
(
𝝉
)
−
𝑹
𝒙𝒙
(
𝝉
−
𝑻
)
−
𝑹
𝒙𝒙
(
𝝉
+
𝑻
)
 
Example
 
4
 
Error Detection and Correction 
Linear Block Coding
Algebraic Codes
•We now investigate  an 
organized  technique  for 
formulating  code  words  and 
recovering  the original  words  
and identifying  errors  at the 
receiver . 
•Consider  the single  parity  
check  code : 

Algebraic Codes
•The coding  process  can be described  as an addition  of 
a fourth  bit such that the total number  of ones is even . 
•We now generalize  this type of coding . Suppose  that 
the message  words  consist  of 𝑚 bits. 
•We have  2𝑚 distinct  message  words . 
•Consider  code  words  that add 𝑛 parity  bits to the 𝑚 
message  bits to end up with code  words  of length
    𝑚+𝑛
Algebraic Codes
•The code word will be of the form:
•Each check bit is chosen to achieve even parity when 
combined with specific message bits. 
•Matrix skills and Boolean algebra permit us to express 
this relationship in general format.

Algebraic Codes
•The check bits are chosen to satisfy the following:

Algebraic Codes
•Note  that the 𝑛𝑥𝑛 matrix  formed  by partitioning  the 
right  part of 𝐻 is an identity  matrix . 
•If the number  of ones is even,  the sum is zero:

Example

Example

Decoding at the receiver
•The receiver  forms  the product  of the received  word  
with the matrix  𝐻.
•If the product  is not equal  to zero,  the receiver  knows  
that at least one error  was made  during  transmission,  
and the received  word  is not one of the acceptable  
code  words . 
•The value  of algebraic  coding  arises  in multiple  error  
detection  and in error  correction . 
Decoding at the receiver
•We define  an error  vector  which  contains  a “1” in 
each bit position  in which  an error  occurs . The 
received  vector  is therefore  of the form :
•Error  detection  is possible  if the error  vector  can be 
isolated  at the  receiver .

Decoding at the receiver
•If we multiply  the received  vector  by 𝐻, we have
•The syndrome  characterizes  the specific  bit error . 
•The result  is a vector  that is identical  to one column  of 
𝐻, that column  being  the one corresponding  to the bit 
position  in error . 

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner


Chapter 2 
Source Coding 
Quantization Noise: Uniform Quantization
•Thequantization noise isdefined as:
where𝑓(𝑛𝑇𝑠)istheoriginal sample value and𝑓𝑞(𝑛𝑇𝑠)is
thequantized sample value .
•The amplitude range ofthesignal isdivided into L
uniformly spaced intervals, each ofwidth∆𝑠

Quantization Noise: Uniform Quantization
•Themaximum quantization error is−+∇𝑠
2.
•Thequantization error liesintherange (-𝛻𝑠
2,+𝛻𝑠
2).
•Assuming that the error isequally likely tolie
anywhere inthisrange .
•Themean squared quantization error isgiven by
Quantization Noise: Uniform Quantization

Non-Uniform Quantization
•The most common form ofnon-uniform quantization
isknown ascompanding .
•Theuniform quantization provides thesame resolution
level athigh levels asatlow.

Non-Uniform Quantization
•Forsome signals, likevoice signals, itisdesirable touse
small quantization steps atthelower levels, and larger
steps atthehigher levels .

Non-Uniform Quantization
•The average quantization error may well decease using
thisapproach .
•Prior toquantization, thesignal iscompressed .
•The operation compresses the extreme values while
enhancing thesmall values ofthesignal .
Non-Uniform Quantization

Non-Uniform Quantization
•Theanalog signal forms theinput tothecompressor, and
theoutput isuniformly quantized .
•Theresult isequivalent toquantizing with steps thatstart
outsmall andgetlarger with higher signal values .
•Atthereceiver, expansion isapplied sothat theoverall
transmission isnotdistorted .
Non-Uniform Quantization
•The processing pair (compression and expansion) is 
called companding . 

Non-Uniform Quantization
•The most common application ofcompanding isin
voice transmission .

Non-Uniform Quantization
•North America and Japan have adopted astandard
compression curve known as𝝻-lowcompanding .
•Europe hasadopted another standard known asA-law
companding .

Random Process
Definition of a Random Process
•Arandom orstochastic process isacollection of
infinite number ofsample functions together with
associated statistical properties .
•This infinite number ofsamples form theensemble .
•Forevery specific value oftime𝑡=𝑡𝑜,𝑥(𝑡0)isa
random variable .
Example: Random Process to represent the 
temperature of a city

Correlation Function of a Random Process

Correlation Function of a Random Process
•For x𝑡,the amplitudes at 𝑡1and  𝑡1+𝜏are similar, 
that is have stronger correlation. 
𝑅𝑥𝑥𝑡1,𝑡2=𝐸{𝑥𝑡1𝑥𝑡2}=𝑥𝑡1𝑥𝑡2
•The correlation iscomputed by multiplying
amplitudes at𝑡1and𝑡2ofasample function andthen
averaging thisproduct over theensemble .
Stationary Process
•Aprocess with overall statistics that areindependent of
time.
•Therandom process representing thetemperature ofacity
isanexample ofnon-stationary random process .
•The temperature statistics (mean value), forexample,
depend onthetime oftheday.
Wide -Sense Stationary Process
•Aprocess inwhich only themean andautocorrelation
function areindependent oftime.
𝑥𝑡=constant
𝑅𝑥𝑥𝑡1,𝑡2=𝑅𝑥𝑥(𝜏),𝜏=𝑡2−𝑡1
𝑅𝑥𝑥𝑡1,𝑡2=𝑅𝑥𝑥(𝑡2-𝑡1)=𝐸𝑥𝑡1𝑥𝑡2
𝑅𝑥𝑥(𝑡,𝑡+𝜏)=𝑅𝑥𝑥(𝜏)=𝐸𝑥𝑡𝑥𝑡+𝜏
Wide -Sense Stationary Process
•𝑅𝑥𝑥(𝑡,𝑡+𝜏)does notdepend ontheactual values of
𝑡1and𝑡2,butonly upon thedifference .
•One ofthemost important characteristics ofarandom
process isitsautocorrelation function, which leads to
thespectral information oftherandom process .
•The frequency content ofaprocess depends onthe
rapidity oftheamplitude change with time.
Wide -Sense Stationary Process
•This can bemeasured bycorrelating amplitudes at
𝑡and𝑡+𝜏.
•The process x𝑡 isaslowly varying process
compared totheprocessy𝑡.
•Thus, theautocorrelation function provides valuable
information about the frequency content ofthe
process .
Wide -Sense Stationary Process
•The Power Spectral Density (PSD) ofx𝑡isthe
Fourier transform oftheautocorrelation function .
𝐺𝑓=𝐹𝑅𝑡= 
−∞∞
𝑅(𝑡)𝑒−𝑗2𝜋𝑓𝑡𝑑𝑡
Where𝐺𝑓isthePSD.
𝑅𝑡= 
−∞∞
𝐺𝑓𝑑𝑓=𝑅(0)
Wide -Sense Stationary Process
𝑅𝑥𝑥(𝜏)=𝐸𝑥𝑡𝑥𝑡+𝜏
𝑅0=𝐸{𝑥2(𝑡)}
•Theobservation that𝑅0istheaverage power makes
agreat deal ofsense .
•Wehave away offinding theaverage power ofthe
output ofasystem when theinput isastochastic
process .
Difference Between Correlation and  Convolution
•Correlation isameasurement ofthesimilarity between two
signals/sequences .
•Convolution isameasurement oftheeffect ofonesignal onthe
other signal .
•Convolution isthe common operation alinear and time
invariant system canperform onagiven input signal .
•Inconvolution, there issome input -output relationship, sothis
actslikeafiltering operation .
Lecture 1
Angle Modulation
References
•Modern Digital and Analog Communication Systems
B.P .Lathi
•Communication Systems
Simon Haykin
Amplitude Modulation


Frequency Modulation (FM)

𝑓𝑚𝑡=𝑎𝑐𝑜𝑠2 𝜋𝑓𝑚𝑡
𝑓𝑐𝑡=𝐴𝑐𝑜𝑠(2𝜋𝑓𝑐𝑡+𝜙)
note:                                     𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 𝜃𝑡
𝑓𝑖=ሶ𝜃
2𝜋
ሶ𝜃=2𝜋𝑓𝑖
𝜃𝑡=2𝜋׬0𝑡𝑓𝑖𝑑𝑡
𝑓𝑖=𝑓𝑐+𝑘𝑓𝑓𝑚𝑡
∴𝜃𝑡=2𝜋𝑓𝑐𝑡+𝑘𝑓׬0𝑡𝑓𝑚𝑡𝑑𝑡
∴𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 2𝜋𝑓𝑐𝑡+2𝜋𝑘𝑓׬0𝑡𝑓𝑚𝑡𝑑𝑡𝑐𝑜𝑠2 𝜋𝑓𝑡
          𝜃𝑡=2𝜋𝑓
ሶ𝜃
2𝜋= 𝑓
•The frequency of 𝑓𝑠𝑡 varies from 𝑓𝑐+𝑘𝑓(min 𝑜𝑓𝑓𝑚𝑡 )
                                                          to   𝑓𝑐+𝑘𝑓(m𝑎𝑥𝑜𝑓𝑓𝑚𝑡 )
                                        If 𝑓𝑚𝑡=𝑎 𝑐𝑜𝑠2 𝜋𝑓𝑚𝑡
 ∴𝑎𝑘𝑓=∆𝑓𝑐                           Maximum Frequency Deviation
The maximum change in carrier frequency
•The modulation index is defined as
𝛽=∆𝑓𝑐
𝑓𝑚
•For sinusoidal signals
                 ∴ 𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 2𝜋𝑓𝑐𝑡+2𝜋𝑘𝑓
2𝜋𝑓𝑚𝑎 𝑠𝑖𝑛2𝜋𝑓𝑚𝑡 
 ∴ 𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 2𝜋𝑓𝑐𝑡+𝛽𝑠𝑖𝑛2𝜋𝑓𝑚𝑡 𝑓𝑚𝑡=𝑎 𝑐𝑜𝑠2 𝜋𝑓𝑚𝑡
Phase Modulation PM
𝑓𝑚𝑡=𝑎 𝑐𝑜𝑠2 𝜋𝑓𝑚𝑡
                                       𝑓𝑐𝑡=𝐴 𝑐𝑜𝑠(2𝜋𝑓𝑐𝑡+𝜙)
                                            𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 𝜃𝑡
 𝜃𝑡= 2𝜋𝑓𝑐𝑡+𝑘𝑝𝑓𝑚𝑡
 ∴ 𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 2𝜋𝑓𝑐𝑡+𝑘𝑝𝑓𝑚𝑡
 𝑓𝑖=ሶ𝜃
2𝜋=𝑓𝑐+𝑘𝑝
2𝜋𝑑
𝑑𝑡𝑓𝑚𝑡
•For sinusoidal signals
𝑓𝑠𝑡=𝐴𝑐𝑜𝑠 2𝜋𝑓𝑐𝑡+𝑎𝑘𝑝𝑐𝑜𝑠2 𝜋𝑓𝑚𝑡 
 𝑓𝑖=ሶ𝜃
2𝜋=𝑓𝑐+𝑘𝑝
2𝜋𝑎2𝜋 𝑓𝑚(−𝑠𝑖𝑛 2𝜋𝑓𝑚𝑡 )
 𝑓𝑖=𝑓𝑐−𝑎𝑘𝑝𝑓𝑚 𝑠𝑖𝑛 2𝜋𝑓𝑚𝑡 
Chapter 2 
Source Coding 
Sampling Theory
•States that, iftheFourier transform ofatime function is
zero for𝑓>𝑓𝑚,andthevalues ofthetime function are
known for𝑡=𝑛𝑇𝑠,forallinteger values of𝑛,then the
time function isknown forallvalues of𝑡provided that
thesamples areclose enough together .
•Therestriction isthat
𝑓𝑠≥2𝑓𝑚
where𝑓𝑠isthesampling frequency .
Pulse Code Modulation (PCM)
•Isatechnique forrounding -offtheamplitudes ofa
waveform .
•This isthe second operation after the sampling
process .
•The rounding -offoperation isknown asquantization,
and theround -offerror isknown asquantization
noise .
Pulse Code Modulation (PCM)
•PCM codes thevarious levels into binary numbers, and
sends thebinary code corresponding tothe particular
round -offlevel .

PCM Modulators
•APCM modulator isnothing more than ananalog -to-
digital converter .
•The converter first samples thewaveform, and then
quantizes each sample value .
•There arethree generic forms forthequantizer :
Counting Quantizer
Serail Quantizer
Parallel quantizer
Counting Quantizer

Counting Quantizer

Counting Quantizer
•Theramp generator starts ateach sampling point, and
abinary counter issimultaneously started .
•The time duration oftheramp, and therefore the
duration ofthecount isproportional tothesample
value (ramp slope isconstant) .
•The ramp slope must besufficient toreach the
maximum possible values within onesampling period .
Counting Quantizer
•The clock frequency issuch thatthecounter hasenough
time tocount toitshighest count foraramp duration
corresponding tothemaximum possible sample .
•Theending counts onthecounter willcorrespond binary
equivalent value oftheinput .
Serial Quantizer

•Theserial quantizer successively divides theordinates intotworegions .
•Itfirstdivides theaxisinhalf, andobserves whether thesample isinthe
upper orlower half.
•The result ofthisobservation generates themost significant bitinthe
code word .
Serial 3 -bit quantizer
•The half region inwhich thesample liesisthen subdivided into two
regions, andacomparison isagain performed .This generates thenext
bit.
•Theprocess continues anumber oftimes equal tothenumber ofbitsof
encoding .
•Thefigure isshown for3-bitcode words andforarange ofinput values
between 0and1volt.
•Iftheinput range ofsignal sample values were not0to1,thesignal
could benormalized toachieve values within thisrange .Serial 3 -bit quantizer
•Ifmore orfewer bits are required, the appropriate
comparison blocks canbeadded orremoved .
•Note :
Value greater than1
2-1
2>1
4
Value greater than1
2>=3
4=6
8
Value greater than6
8-1
4-1
2>1
8
Value greater than6
8>1
8+1
2+1
4>7
8Serial 3 -bit quantizer
Parallel Quantizer
3-bit Parallel Encoder

•The parallel quantizer isthefastest inoperation, since it
develops allbitsofthecode words simultaneously .
•Itisalso the most complex, requiring anumber of
comparators thatisonly onelessthan thenumber oflevels
ofquantization .
•Theblock labeled“coder” observes theoutput oftheseven
comparators .Parallel Quantizer
•Ifallseven outputs are1(yes), thecoder output is111,
since thesample value hadtobegreater than 7/8.
•Ifcomparator outputs 1through 6are1,andoutput 7is0,
thecoder output is110,since thesample hadtobebetween
6/8and7/8.
•We continue through alllevels and finally, ifall
comparator outputs arelow, thesample hadtobelessthan
1/8,sothecoder output is000.Parallel Quantizer
 
 
 
 
 
 
© Basem Hesham Lecture 
2
 
 
Digital System & Fourier Transform
 
 
 
 في الجزء دا هناخد overview  عنأي نظام رقمي Digital System  بشكل عام بيتكون منأي وفيه كالم كتيررر 
من اللي اتشرح المحاضرة اللي فاتت هيتكرر المحاضرة دي تاني بس بتفاصيل اكتر. 
اإلشارة اللي بنبدأ بيها اصلها analog وفي كورس الر analog communication  السنة اللي فاتت كنررا بنتمامررل
مرر  اإلشررارات زي مررا هرري  analog  ودرسررنا الررر modulation techniques  المختلفررة زيAM  وFM  وPM 
   ,السنة دي هنتمامل ماإلشارة digital   وهنتملمأي المميزات اللي خلتنا نتمامل م  الر Digital System. 
 
  زي ما قولنااإلشارة  في اصلها  analog    زيإشارة  الصوت  مثال،  ولكن  اإلشارة  اللي ببمتها من الموبايررل  مررثال 
بتكون  digital   وهنتملم النهاردهاإلشارة دي تحولت ازاي واي المراحل اللي مرت عليها لحد ما استقبلناها. 
 
 
 
 
 
 
 
 
 
 
كل  block    من اللي في الصورة عليها chapter   مفصل في المنهج , هناخررد فرري محاضرررة النهرراردة overview 
عموما عن الر digital system  بيتكون منأي وبردو هناخد فكرة عن المنهج اللي هندرسرره وفرري نفررس الوقررت 
هي مش مقدمة دا سؤال وارد يجي في االمتحان كنظري زي رسم الر block diagram  او أجزاء منه او شرررح
بمض األجزاء. 
 
 
 
الر Source Encoder   ببساطة هو الدايرة اللي بتحول اإلشررارة المتصررلة للمكررافالرر ر binary  بتاعهررا واللرري
بنقول عليها  analog to digital converter (ADC)   
فيه شابتر كامل هناخده على الر Source Encoder  هنتمرف على بمض دواير ADC  ونمرف بتشتغل ازاي
والشابتر دا فيه نظري ومسائل, وهنتملم بمض الر  techniques  اللي بتمكني من التحويل للمكاف  الر  binary 
  باقل عدد ممكنمن الر  bits   ودا عشان احنا مقيدين بررر  bandwidth  محرردد  وكررل نظررام ليرره نطرران ممررين مررن 
الترددات بيشتغل فيه وعشان يكون فيه استخدام امثررل efficient utilization) ) للررر bandwidth  وابمررت عليرره
عدد كبير من الر  users  فالزم ابمت للر  user  الواحد عدد اقل من الر  bits,  وعشان اقدر ابمت البيانات بر bit 
rate  سري الزم اضغطهم و اقلل عددهم الن الر bit rate  السري بيحتررا  bandwidth كبيررر (بنقرريس ممرردل 
نقل البيانات ب ر  bits per second) Block Diagram of Digital System  
Block Diagram of Digital System  
Source Encoder  
 
  في الشابتر دا هنتملم طرن تحويلاإلشارة المتصلة للمكاف    الر  binary   لمرردد قليررل مررن الررر  bits   ولمررا نرردخل
علررى شررابتر الرر ر Information theory  هنررتملم طرررن للررر compression  اسررمها Lossless compression 
techniques  زي Huffman coding  هنمرف من خالل الطرن دي اننا نبمت الداتا باقل عدد ممكن من الررر 
bits   من غير ما يحصلأي فقد للر information. 
 فيرره طرررن compression  ممكررن افقررد فيهررا جررزء مررن الررر information  وبنسررميها lossy compression 
techniques   وبيحصل فقد في جزء من المملومررات مررن غيررر مررا يرر ثر علررى الررر  reconstruction   بترراا الررر 
signal ( يمني فيرره حاجررة ممينرره انررا ممكررن اسررتغنى عنهررا Redundant Information  )  يمنرري حاجررة زيررادة
واالستغناء عنها مي ثرش كتير على  quality اإلشارة المرسلة. 
 
▪ Operates upon one or more analog signals to produce a periodic train of symbols.  
  بيشتغل علىإشارة  analog   او اكتر وهنمرف في حالة اكتر دي بنتمامررل مماهررا ازاي عشرران احصررل علررى
periodic train of symbols   ودا المقصود بيه االصفار والوحايد 
 
 
▪ May contain a multiplexer with channels which are used to communicate from 
more than one source at the same time.  
   لو انا بتمامل م System    بيبمت اكتر منإشارة  في وقت واحد ,و عملياً كل الر Systems  بتبمررت اكتررر مررن
إشارة في وقت واحد الن الر Systems بيشتغل على عدد كبير من الر users. 
 طالما انا شررغال علررى عررددكبيررر مررن الرر  ر users  يبقررى الزم يكررون فيرره  technique بت مكنرري مررن انرر ي ابمررت 
المملومات الخاصة بالر  users   في وقت واحد, الررر  operation    دي بنقررول عليهررا multiplexing    يمنرري اجمرر
مملومات لمدد من الرر ر users  بطريقررة مررا تضررمنعرردم حرردو   interference  مررا بررين مملومررات الررر users 
وبمضهم. 
 
ودي طرن مختلفة للر Multiplexing : 
FDMA (Frequency Division Multiple Access)  
TDMA (Time Division Multiple Access)  
CDMA (Code Division Multiple Access)  
 
Multiple Access  ممناها الوصول المتمدد يمني عنرردي channel  و عرردد مررن الررر users  يقرردروا يبمترروا لررر 
Receiving station  في نفس الوقت من غير ما يحصل interference ويقدر الر Receiver  يستقبلها برر على
جودة.    
 
ملحوظة : أي حاجة فيها كلمة   Encoder .يبقى اكيد خرجة اصفار ووحايد 
 
 
 
 
 
FDMA (Frequency Division Multiple Access)  
هبمت عررن طريررل فصررل الرر ر users  فرري الررر Frequency domain  يمنرري مملومرراتهم فرري الررر Frequency 
domain هتكون منفصلة وبالتالي اقدر ابمتها واستقبلها بشكل صحيح. 
تملمنا في المحاضرة السابقة ان بمد ما بحسب Fourier Transform  بقدر مررن خاللرره ارسررم عالقررة مررا بررين
التررردد علررى المحررور االفقرري و الررر amplitude spectrum  علررى المحررور الرأسرريوقيمررة الررر amplitude 
spectrum  هرري قيمررة بتررديني الررر weight  اونسرربة تواجررد كررل frequency  فررياإلشررارة مقارنررة بررالترددات 
األخرى  الموجودة. 
لنفترض ان عندنا 3 users  مثال واشارة الصوت من 300 Hz  لحد 4 KHz   ولنفترررض انهررا مررن0 Hz  لحررد
4KHz والرسم االتي هنفترض انه الر frequency spectrum  بتاا كلإشارة (كلمررة spectrum  دايمررا بترردل
على تردد) 
 
 
 
 
 
 
 
 
 
 
نفتكر من المحاضرة اللي فاتت ان اإلشارة 𝒇𝟐  مثالبنقو ل انهررا Bandlimited to 3KHz  يمنرري حرردودها فرري
نطان الترددات لحد قبل  3KHz   الن عند 3KHz .تحديدا قيمته بصفر 
اإلشارات زي ما واضح من الرسم انهررا متداخلررة فرري frequency domain  فمشرران اقرردر ابمتهررا بررر FDMA 
الزم افصلهم عن بمض وهنا تيجي فائدة الر Modulation techniques  وهو انه بياخررد كررلإشررارة ويحملهررا 
على carrier    مختلفبتردد عالي والتحميل على ال ر  carrier   بيمملshift  للر spectrum  بتاااإلشارة.  
 
 
 
 
 
 
 
 
 
 amp spectrum  
𝒇 
1KHz 0 Hz 3KHz 4KHz  𝒇𝟏 
𝒇𝟐 
𝒇𝟑 
𝒇 
𝒇𝑪𝟏 amp spectrum  
𝒇𝑪𝟐 𝒇𝑪𝟑 
 
ومن خالل الشكل اللي فات واضح اني هقدر ابمتهم على نفس الر channel  من غير تررداخل النفيرره band 
pass filter  في الر  receiver   مصمم انه يمدينطان ممين  من الترددات وميمديش الباقي فبيتم تصميم  band 
pass filter عند الر receiver يمدي إشررارة الرر ر user األول وفيرره band pass filter  يمررديإشررارة الرر ر user 
 التانيو  band pass filter   تالت عشان يمديإشارة الر user التالت. 
 
TDMA ( Time  Division Multiple Access)  
هنا بيقسم الر channel  الواحدة لمجموعة frames  وكل frame  يحتوي على عدد من الررر time slots  وكررلuser 
 ليه time slot خاص بيه يمني بيررتم تحديررد وقررت ممررين للرر ر user  يرسررل فيرره البيانررات بتاعترره ولمررا يخلررص بيررتم
تخصيص الوقت دا لر  user    تانيو هكذا ,و هنا الر  carrier   الواحد هبمت عليه مجموعررة مررن الررر  users   و بفصررل
اإلشارات في الر time domain عشان ميحصلش تداخل لو بمتهم على نفس الر carrier .في نفس الوقت 
 
مررثال هفرررض ان عنرردنا 3 users  بررس المررره دي هنشررتغل time domain  وعنرردي تررالتإشررارات اصررلهم 
analog   و  وزي ما هندرس في  PCM  بنبدا اول خطوة بالر  sampling  ,  من الشكل االتي هررنالحظ ان مفرريش
تداخل بين ال samples دي وبمضها في الر time domain   الن كل واحد بيبمت في  time slot .مختلف 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
𝒕 𝒔𝟏(𝒕) 
𝒕 
𝒕 𝒔𝟐(𝒕) 
𝒔𝟑(𝒕) Sampling 
interval  
11 
12 
13 21 31 
22 32 
23 33 
 
sampling interval  هي الفترة الزمنيررة بررين كررل sample  والتانيرره لررنفساإلشررارة ومقلوبهررا هررو التررردد 𝑓𝑠 
( sampling frequency  ) ونفتكر ان شرط التردد دا انه يكون اكبر من او يساوي ضمف اعلى تردد موجود
في الر information signal   
 
  ب بسطاألفكار  فيه زي  switch    بيقفل اول مره عند اول sample    بتاعة اولuser    وبمد كداالر  switch   يتنقل
للر  sample بتاعة الر user التاني وبمدها يتنقل للر sample  بتاعررة الررر user  التالررت, الررر switch  كانرره بيلررف
على الر users 3  وياخد sample   من كل. user 
 
 كدا خلصت sample  واحده من كلuser  بيحطإشررارة تحرردد ان الرر ر frame  انتهررى اسررمها synchronizing 
pulse  بيكررون الررر level  بتاعهررا عررادةً  اكبررر مررن بقيررة ال samples  عشرران ال receiver  لمررا يسررتقبلهايقرردر 
يميزها ويمرف ان دي نهاية الر  frame. 
 
 عشان يحصل تزامن صحيح بين الررر  transmitter   والررر  receiver   الزم يبقررى عنررده حاجررة تحددلرره امتررى الررر 
frame   ابتدى وامتى انتهى,في المثال دا ال ر  frame  الواحررد مكررون مررن تالترره samples  واحررده لكررلuser 
فلما يجيي يوزا عند الر  receiver    هيحط اول sample   تخررص الررر  user  األول  ويحررط  اول  sample   تخررص
الر user التاني ويحط اول sample  تخررص الررر user  التالررتوبمرردها synchronizing pulse  وبمررد كرردا الررر 
switch  هياخد تاني sample  من اولuser  وتاني  sample من تانيuser  وتاني  sample من تالررت   user 
 وبمدها synchronizing pulse  عشان يمرررف ان الررر frame  خلررص وطبمررا الممليررة مسررتمرة لحررد مررا ياخررد
samples  لكاملاإلشارة. 
 
 
 
 
 
 
 
 
 
 
ممكن لسرربب مررا يحصررل عنررد اسررتقبال اإلشررارة delay  وتررداخل بررين الررر  symbol s  وبمضررها وبالتررالي مررش
هتوصل في الزمن المحدد  فالر  frame  األول  عدى شويه واخررد مررن الرر ر  frame   الترراني وهكررذا, وكرردا ترتيررب
المملومة هيتلخبط ودا اسمه (ISI)  Inter symbol interference. 
 
 
 
 𝑺(𝒕) Synchronizing  
pulse  
11 12 13 21 22 23 22 31 33 𝒕 
 
 
▪ provide security by ensuring that only the intended receiver can understand the 
message.  
▪ Encryption is a means of securing data by encoding it  mathematically such that 
it can only be read, or  decrypted, by those with the correct key or cipher.  
 
Encryption   التشررفيربيرروفر security  للررر system  عررن طريررل انرره بيضررمن ان الررر receiver  اللرري عررايز
اوصله اإلشارة هو  بس اللي يقدر يفهمها. 
عملية التشفير تستخدم عشان مفيش غيررر مجموعررة محرردده اللرري تقرردر تفهررم  اإلشررارة  زي القنرروات المشررفرة 
المشتركين بس اللي يستقبلوها وتشتغل عندهم. 
 
ابسط  أفكار  التشفير اللي نمرفها هي عن طريل عمل  xor    مثال اوإضافة  bits    تانيهلإلشارة،  ولكن مجموعة 
الر bits تكون بهدف التشفير وميمرفهاش غير الر receiver   اللي عايز ابمتله فقط بهدفإخفاء المملومة. 
 
خر  الر source encoder  بيحولألصفار ووحايد ودول بيمثلرروا المملومررة االصررلية وبمررد التشررفير الخررر  
بردو اصفار ووحايد ولكن جزء من االصفار والوحايد بيمثلوا المملومة االصلية  عندي وجزء تاني مضرراف 
بهدف التشفير. 
 
التشفير دا موضوا كبير جدا  مش هندرسه في الكررورس  ومطلرروب مننررا  الفكرررة البسرريطة اللرري اخرردناها فرري 
المحاضرة. 
 
 
 
 
It increases efficiency and decreases the effects of transmission errors due to noise.  
 
هو عملية تهدف الى اكتشاف األخطاء  اللي ممكن تحصررل اثنرراء عمليررة االرسررال.  اإلشررارة  بيررتم ارسررالها فرري 
وسط مليان noise  وممكن يحصل multipath propagation  ولو حصررلأي error  هيبقررى مشرركلة كبيررره الن
داتا كتير هتضي  فمشان كدا الزم يكون موجود عندنا حاجة بتممل  Error Detection and Correction. 
 
 الررر Channel Encoding  بيضرريفbits  زيررادة بنسررميها Redundant Bits  او Check bits  او parity bits 
  عشان نممل Error Detection and Correction 
 
الر  Redundant  Bits  بتزود حاجة اسمها distance  ما بررين االكررواد والرر ر distance هررو عرردد الرر ر bits  اللرري
بيختلف فيها كود عن كود اخر . 
 
ملحوظة مهمه : الر message بمد الر source encoder  بيقولو عليها message word  واللي بيخر  من
الر channel encoder  بنقول عليه code word. Encryptor  
Channel Encoder  
 
If 101 is transmitted and an error occur in the third bit, 100 is received. There is no way 
for the receiver to know that 100 was not the transmitted word  
0000 , 001 1 , 010 1 , 011 0 , 100 1 , 101 0 , 110 0 , 111 1 
If 1010 is transmitted and 1011 is received, this is  not one of the received words.  
 
 مثال لو عندي 3 bits  يبقى كدا عندي8  احتماالت وهضيفbit  رابمه اللي هي في الصررورة ديY  يبقررى كرردا
ممايا 4 bits  يمني16  احتمال بس مستخدم منهم8  بس يبقى فيه كدا8  تانيين مش مسررتخدمين وهررم دول اللرري
هيدوا فرصة ان لو حصل خط  في  bit  بحصل على كود تاني مش واحد من الر  8    المتمارف عليهم ما بينالررر 
transmitter   والر  receiver    وهناالر  receiver   يقرردر يكتشررف الخطرر  دا, وكررل مررا الررر distance  تزيررد كررل مررا
إمكانية  تصحيح  األخطاء  تزيد. 
 
 
لو انا باعت كود  101    ووصل100  للر  receiver   كدا  الر receiver   مش هيكتشف الخطألنه كود من اللرري 
متمارف عليه لكن امررا  المسررافة  زات االكررواد زادت ف صرربح فرري 8  اكررواد مررش هيسررتخدهم وبكررده همرررف
اكتشررف الخطرر . زي اننررا نبمررت 1010  ويسررتقبلها 1011  هيكتشررف الخطرر  الن دا مررش مررن ضررمناالكررواد 
المتمارف عليها 
 
واسمه Channel Encoding الن دا خاص باكتشاف األخطاء الناتجررة عررن الرر ر channel  ودا الوسررط اللرري
بيتبمت فيه اإلشارة وهو دا اللي سبب ان فيه أخطاء تحصل في الر signal. 
 
 اللي فات دا بالنسبة الكتشاف االخطاء اما بالنسبة لتصحيحاألخطاء الر Receiver بيشوف الر code word 
 اللي وصلت ليه اقرب لمينمن الر code word 8 الموجودين يمني لو فيه فرن بينها وبين احد الر 8  اكررواد
في  bit    واحده هيفترض ان دي هي الbit   , الصحيحة اللي كان المفروض تتبمتو احتمال ان الخط  يكون 
في bit  .واحده اكتر من ان يحصل خطا في اتنين او تالته   
 
عدد الر parity bits الزم يكون متناسب م  الر propagation problem  الن فيه دراسة بتم للنظام وبنمرف
احتمال حدو  خط  او اتنين او تالته ومنها نممل  design للر channel encoder  بمددbits .مناسب 
 

 
 
is the process of converting digital data to baseband signal
 
 
 زي ما تملمنا السنة اللي فاتت عشان نبمررت
أي
 
إشررارة
 
بيبقررى ممانررا 
اإلشررارة
 
األصررلية 
information signal
 
 و
carrier
 
 وبتحصررل عمليررة
modulation
 
.
 
 الرر
ر
 
information signal
 
 هرري الرر
ر
 
Baseband
 
 و
الررر 
carrier
 
 هررو
 
إشارة
 
sinusoidal signal
 
.بتردد عالي
 
 
خر  ال
ر
 
Line Encoder
 
 بنقول عليرره
Baseband signal
 
 وبنسررميها كرردا عشرران نميزهررا عررن الرر
ر
 
analog
 
 الن
اإلشارة
 
دي يمتبر لسه 
analog
 
  ولكن بتاخد قيمتين فقط
ومررثال هنمتبررر ان 
binary 1
 
 هررو
5
 
v
 
 و
binary 0
 
 هررو
  
-
5
v
 
ودا احد اشكال تمثيل ال
ر
 
 
Baseband Signal
 
 ولكن مش دا الشكل الوحيد وفيه عيوب
عرفناها المحاضرررة 
اللي فاتت  
زي ان لو بمت مثال عدد اصفار كبير ورا بمض او عدد وحايد كبير مررش هيحصررل 
transition
  
 للرر
ر
 
signal
  
لفترة ودا هيسبب مشاكل في ال
ر
 
timing
 
 و
الر 
ر 
 
synchronization
 
 الن بيكون فيه
clock
 
 فرري
transmitter
 
 و
clk
 
 فرري
receiver
 
 االتنررين دول الزم يكونرروا
synchronized
 
 مرر  بمررض فلحظررات االنتقررال دي هرري اللرري
بتساعدنا في تحقيل ال  
synchronization
 
 
  الميب التاني ان لو الي سبب من األسباب اإلشارة اتقلبت
180
  
 درجة كل االصفار هتبقى وحايررد وكررل الوحايررد
هتبقى  
اصفار
 
والداتا كلها بقت غلط
 
 
اسم
  
الشكل اللي فات  
non return to zero 
(
NRZ
)
  
,وأحد ال
ر
  
forms
  
  اللي هندرسها
ال
ر
  
binary 1
 
 همبرعنه ب
ر
5
 
v
 
 في النص األول و
 
-
5
 
v
 
في النص التاني وال
ر
 
binary 0
 
 همبرعنه ب
ر
-
5
 
v
 
 
 فرري الررنص األول و
 
5 v
 
 فرري الررنص
التاني والصورة دي بتوضح الفرن بين النوعين
.
 
 
 
ال 
شكل اللي فرات حرل مشركلة الر 
ر 
  
synchronization
  
 الن الرر
ر
  
transitions
  
 دايمررا موجررود فرري نررص الرر
ر
  
bit
،
  
ولكررن 
المشكلة التانيه لسه موجود
ه
  
وهي ان لو اإلشارة اتقلبت 
180
 
درجة
 
الداتا كلها هتبوظ
.
 
هندرس في الشابتر دا
 
االشكال المختلفة لل
ر
Baseband Signals 
 
 ومزايا وعيرروب كررل شرركل فيهررا وهنرردرس
بمض الدوائر البسيطة اللي من خاللها نممل  
generation
 
لألشكال المختلفة
  
لل
ر
Baseband Signals 
.
 
Baseband Signal
 
 هرري دي
الرر 
ر
 
information signal
 
 اللرري بنحملهررا علررى
الرر 
ر
 
carrier
 
ونبمتهررا ل
لرر 
ر
 
digital 
system
 
 
Line
 
Encoder
 
 
 
▪ Baseband signals cannot be transmitted over radio links or satellites , because this 
would require large antennas to efficien tly radiate the low frequency spectrum of 
the signal.  
 
▪ A spectrum shift to higher frequencies is also required to  transmit several 
messages simultaneously, by sharing the  large bandwidth of the transmission 
medium (FDM) . 
 
 دي المرحلة اللي بيتم فيها عمليةالر Modulation  .بمد ما حصلنا علىالر information signal  واللي بنسميها
Baseband Signal   وديإشارة بترددات منخفضة ال تصررلح انهررا تتبمررت علررى communication channel 
لمسافات بميده ودا لكذا سبب منها ان  في  الر  antenna    احد المفاهيم بتقول انه لو عنديإشارة  بتردد مررنخفض 
حجم  الر antenna  المطلوب عشانت ر radiate اإلشارة دي كبير اما لما الترددات بتملى الحجم دا بيقل. 
 
السبب التاني عدم حدو  تداخل بين اإلشارات الن اإلشارات في اصلها متداخلة في الر ر  frequency domain 
 وبالتالي من خالل عمليةالر ر  modulation  بنحمل كلإشارة على carrier مختلف وبالتالي اقدر ابمت اكتر من 
إشارة في وقت واحد من غير ما يحصل بينهم تداخل. 
 
 المفررروض نممررل modulation مررن خررالل اننررا نحمررل اإلشررارة علررى carrier  وبيررتم التمررديل علررى احرردالرر ر 
Parameter  بتاعةالر carrier طبقا للر information signal  وزي ما قولنا المحاضرة اللي فاتت ان فيرره شررابتر
هندرس أنواا الر modulation  ومن ضمناألنواا دي هو Amplitude Shift Key (ASK)  وزي ما واضح من
الصورة انه    بيتم عن طريل ضرب الر Baseband  فيالر carrier  حي  واحد فياإلشارة هيدي نفس اإلشررارة 
وصفر في اإلشارة هيدي صفر ودا technique  منASK  اسمه on–off keying (OOK) 
 
 
 
Carrier Modulator  
1 1 1 0 
Carrier  
signal  
Modulated  
signal  Modulating  
signal  0 
 
 
CDMA (Code Division Multiple Access) is the access  technique in the 3rd 
generation cellular mobile network, that  uses spread spectrum which enables 
multiple users to  transmit at the same time and the same frequency, but with  
different spreading code (sequence).  
 
 الر  Spread spectrum  داال ر  technique  المستخدم في CDMA  في الجيل التالت من الموبايل3G  ,وفكرته
مبنيه على فرد ال ر   Spectrum   بتاااإلشارة. 
لنفترض ان عندنا  إشارة حسبنا ال ر   Fourier transform  :بتاعها وطل  بالشكل االتي 
 
 
 
 
 
 
 
 
بمد عملية  ال ر   modulation  شكلال ر   spectrum   بتاااإلشارة ال ر   modulated  : هيطل  بالشكل االتي 
 
 
 
 
 
 
 
 
هنالحظ ان ال ر   spectrum بقى مفروض والر amplitude spectrum  قل  جدا. 
 
فكرة الموضوا ببساطة ان عندي  data  ممينه  عايز ابمتها مكونه من اصفار ووحايررد هضررربهم فرري code 
 ممين مكون من اصفار ووحايد بردو وال ر  code دا ليرره مواصررفات ممينرره الن فرري األصررل مررش أي عمليررة 
ضرب تحصل يبقى spreading  واللي هيحدد هوال ر  rate  بتااال ر  code . لو كررانالر ر  rate  بتررااالر ر   code 
  اعلى بكتير منال ر   rate    بتااالر ر   data    يبقررى spreading    وحاصررل ضررربهم هيطلررإشررارة الر ر  rate  بتاعهررا
عالي ودا بيفسر ليه حصل  spreading .الن التردد زاد 
 
 
 Spread Spectrum Modulator  
𝒇 Amp spectrum  
𝟏 𝑲𝑯𝒛 𝟏𝟎 𝑲𝑯𝒛 
𝒇 Amp spectrum  
 
It’s used to increase the signal bandwidth and decrease its  amplitude, to 
provide additional security.  
The signal is harder to be interpreted as the need of the code (sequence) to get 
the message.  
 
 من مميزاتال ر  Spread spectrum  انه بيوفر security  فيالر system  الن فيه code  خاص بكررلuser 
و الر spectrum لما بيتفرد كدا بيكون غير ممررروف للرر ر unintended receiver  الن الشرركل دا يبرران الي
receiver  كانرره noise  ولكررنالرر ر receiver  المقصررود هررو عررارف ان دي مررش  noise انمررا اشررارة الررر 
spectrum    بتاعها مفرود, وميزه تانيه وهي انرري اقرردر اخلرري كررذاuser  فرري نفررس الوقررت علررى نفررس الر ر  
carrier    بس كلuser   بياخد code   مختلف واالكواد دي بيكون محقل فيها شرطال ر   orthogonality 
 
Orthogonal  ممناها  متمامد يمني االكواد مختلفة عن بمض  ودي الطريقة اللي  الرر ر  receiver   هيفرقنررا فيهررا
عن بمض ودا ممناه ان الر cross correlation  ما بين االكواد يسرراوي صررفر وهررو تكامررل حاصررل ضرررب
اشارتين. 
 
 correlation    يمنيالتشابه بين  االكواد  او االشارات, لو عنرردنا اشررارتين   sinewave  زي بمررض بالضرربط
بنفس التردد والقيمة وحسبنا حاصل ضربهم الجزء الموجب في الموجب هيدي موجب والجزء السالب في 
السالب هيدي موجب وبالتالي لما نممل تكامل هيديني ناتج التكامل قيمة كبيره مما يدل ان الر ر  correlation  
 كبير والتشابه كبير. ولو جبنا مثالsin  وcos  ودول بنقول عليهم orthogonal signal  الن لما واحده فرريهم
max  التانيهmin  والمكس فلما نحسب تكامل حاصل ضربهم الناتج هيبقى صررفروبالتررالي الر ر  correlation  
بصفر يمني مفيش اي تشابه بينهم  ,و دا بيفسررر ليرره االكررواد بتاعررة  CDMA    الزم تكررون orthogonal   يمنرري
مختلفين عن بمض عشان ال ر   receiver .يقدر يفرقهم عن بمض 
 
بتم عملية ضرب في ال ر   transmitter من خالل انه بياخد إشررارة كررل user  ويضررربها فرري code  ونفررسالر ر  
code    موجود منه نسخة فيال ر   receiver  عشاناإلشارة ال ر  modulated  تتضرب في نفس الكررود الموجررود
في ال ر   receiver   وتحصل عملية عكسيه( dispreading ) ونقدر نسترج   اإلشارة. 
 
باختصار عشان يحصل  spreading  و dispreading : فيه شرطين 
1.  الر   transmitter    وال ر   receiver   يكون ليهم نفسال ر   code 
2.   الطرفين الزم يكونوا synchronize .م  بمض 
 
فيه نوعين من  ال ر   correlation : 
Autocorrelation  :   يقيس مدى تشابه بينإشارة و shifted version من نفس  اإلشارة 
cross  correlation     : يقيس مدى تشابه ما بين اشارتين مخلفتين 
 
 
 
 في الررر CDMA  الررر Autocorrelation  مررا بررينإشررارة و shifted version  منررهصررغير جرردا ودا ممنرراه انرر ه 
بالنسبة  للر receiver  الررر shifted version  مررن نفررس اإلشررارة فيرفضرره وبكرردا بيممررل reject  للرر ر delay 
component  وبكداحل  مشكلة ISI 
 
  فيالر  CDMA  الر  Autocorrelation  صغير جدا وبالتالي هيحررل مشرركلةISI  و cross-correlation  صررغير
جدا وبالتالي  الر receiver هيمرف يفرن بين االكواد. 
 
وارد الر system  يكون فيه spread spectrum ووارد ميكونش فيه. 
 
 
 
 
Receiver  is simply a mirror image of transmitter . 
the only difference is that the carrier modulator of the transmitter has been replaced 
by carrier demodulator and the symbol synchronizer .   
 
  الر Receiver    بيتم فيه عكس اللي تم فيالر  transmitter    وكل بلوك في  الر  Receiver    يناظر بلوك فرريالررر 
transmitter   ما عدابلوك زيادة اسمه symbol synchronizer  عشان موضوا الر synchronization  
 
 الزمالرر ر digital system  فرريالرر ر stages  المختلفررة يراعرري موضررواالرر ر synchronization  بررينالرر ر 
transmitter    والر  receiver   والتزامن ممناها ان يكون في طريقة ما تحدد امتىالر bit  انتهت وبالتررالي فيرره
bit    جديدهبدأت  وامتى  الر  frame    انتهى وبدا وامتىالر  message    دي خلصت وامتىالر  message   التانيه
اتبمتت وهكذا. 
 
The symbol synchronizer  partitions the overall signal into  segments 
corresponding to each symbol and to each  messag e. 
  بيقسماإلشارة االصلية لر segments  وفي symbols ويحدد بدايتها ونهايتها. 
 
 
 
 
 
 
 
 Receiver and Symbol Synchronizer  
 
 
 زي ما عرفنا فرري المحاضرررة اللرري فاتررت ان الهرردف مررن Fourier transform  الحصررول علررى محترروى
اإلشارة من الترددات ومن خاللها نمرف خصائص اإلشارة فرري الرر ر frequency domain  سررواء كانررت
periodic  او nonperiodic  ,هنفهم سريما في المحاضرة دي قانون تحويل فورير 
 
𝑭(𝝎)= ∫𝒇(𝒕) 𝒆−𝒋𝝎𝒕 𝒅𝒕∞
−∞ 
 
  قيمة magnitude    الممادلة السابقة |𝑭(𝝎)|    يدل على نسبة وجود التردد فياإلشارة  بكررام ونمرررف هررو 
موجود  بنسبه كبيره  او   قليله او مش موجود اصال. 
 ليه  رياضيا  حاصل ضرب  اإلشارة  اللي عايز اجيب تردداتها في  𝒆−𝒋𝝎𝒕    واكامل واخدالر magnitude 
 الناتج يديني رقم يدل على نسبة وجود التردد فياإلشارة؟ 
 
فيه حاجة مهمة  درسناها اسمها  correlation   وهي تكامل حاصل ضرب اشارتين وبنمرف منه التشررابه
بين اإلشارات، ولكن  أي عالقة دا باننا نمرف ترددات اإلشارة؟ 
 
اإلشارة  𝒇(𝒕)    فيها ترررددات كتيررر انررا مررش عارفهررا و 𝒆−𝒋𝝎𝒕    لمررا بنفكهررا بترردينيsin   وcos  والتكامررل
عندي من ماالنهاية لسالب ماالنهاية يمني هضرب  اإلشارة  𝒇(𝒕)  في كل الترررددات  فمنررد كررل تررردد لررو 
موجررود فرري اإلشررارة االصررلية 𝒇(𝒕)   بشرركل ملحرروظ هيتطررابل مررsin  وcos  وبالتررالي تكامررل حاصررل
ضربهم هيدي  correlation  بقيمة عالية عند التردد دا،  ولكن لو  اإلشارة  مفيهاش تردد ممين اذا ناتج  الرر ر 
correlation  بتاعرره صررفرولررو موجررود بنسرربة صررغيره هيرردي  correlation بقيمررة قليلررة, ولررو لقيررت  الرر ر 
magnitude  كبير يبقى التردد موجود بنسبه او weight   كبير والمكس. 
 
باختصار تحويل فورير رياضيا هو correlation  يوجررد مرردى التشررابه مررا بررين التررردداتالموجررودة فرري 
اإلشارة وما بين مجموعة دوال  sin  وcos .بترددات النهائية 
 
 Fourier Transform  
 
 
 
 
 
 
© Basem Hesham Lecture 
3
 
 
Probabilty & Channel Modelling 
 
 
 
𝑷𝒓{𝑨}= 𝐥𝐢𝐦
𝑵→∞𝒏𝑨
𝑵 
𝒏𝑨 : number of times that the event A occurs in N performances of the experiment  
 هو عدد مرات ظهور الحدثA  في تجربة تم اجراؤها عددN من المرات  
 
In the coin tossing experiment, we may expect that out of a million tosses of a fair 
coin, about one half of them will show up heads.  
  فيتجربة القاء العملةةة لةةو جربتهةةا مةة     10     مةةرات وارد لعلةةه مةةالهم مةة8 Head   وبالتةةالي احتمالةة8
10 
  واتاللنtail    وبالتالي احتمال2
10  فهل معالى كدا ان احتمال ظهور  أي واحد مالهم  1
2   غلع؟ 
عبعا ال  الن عشان القاالون لكون صحلح  بالفترض  ان  عدد مرات اجراء التجربة لةةؤول الةةى ماالالهالةةة  لعالةةي 
لةةو كررالةةا تجربةةة القةةاء العملةةة عةةدد ال الهةةاحي مةةن المةةرات احتمةةال ظهةةور أي واحةةد مةةالهم هةةةو 1
2 
 
𝑷𝒓{𝑨 𝒐𝒓 𝑩}=𝑷𝒓{𝑨}+𝑷𝒓{𝑩} 
𝑷𝒓{𝑨 𝒂𝒏𝒅 𝑩}=𝑷𝒓{𝑨}∙𝑷𝒓{𝑩} 
 
or  معالاها جمه two event  وand  معالاها ضرب two event  .وهالفهم اكتر من المساحل 
 
 
 
 
If the random variable is X ,then the distribution function 𝑭(𝒙𝐨) is  
𝑭(𝒙𝐨)=𝑷𝒓 { 𝑿 ≤𝒙𝐨 } 
 
where 𝑷𝒓 { 𝑿 ≤𝒙𝐨 } is the probability that the value taken by the random 
variable X is less than or equal to a real number 𝒙𝐨 
 
Distribution Function هي دالة تععي توزله االحتماالت لمتغلر عشواحي و بقدر احسب من خ لهةةا 
احتمال ان تكون قلمة   المتغلر العشواحي X  اقل من او تساوي قلمة معلال  من القةةلم الممكالةةةلقةةلم المتغلةةر  
العشواحي. 
 
 Random Variableالمتغلر العشواحي X  هوأي متغلر قلمت  مش  ابت  عالد قلمةةة محةةدد  لةةكلس سةةالقوم 
بدراسة احتماللة ان تكون قلمة هكا المتغلر عالد  range  معةةلن مةةن القةةلم.  زي مةة   درجةةة الحةةرار  فةةي 
مالعقة معلال  ومالعرفش درجة الحرار  هتعله بالضبع كام فبالاخد  range   محددمن الدرجات . Probability  
Distribution Function     𝑭(𝒙𝐨) 
 
 م ال تاالي : لو عالدي baseband signal  اللي االا باعتها فةةيالةة ة system  والفتةةرض االالةةا هالعبةةر عةةن 1 
binary  بة 5 V  ومعبر عن0 binary  بة 0 V  واالشار  عملتلها modulation  وبعتهةةاا ااشةةار  دي وارد
تتعرض لة noise  ووارد تتعرضلة propagation problems  وبسبب داااشار  احتمال متوصةةلش 5 
V  او0 V  بالضبع ووارد لبقى فل  زلاد  او القصان عن القلمة المحدد  وبالتالي مقةةدرش احةةدد بالضةةبع
هتوصل كام للة Receiver ا المتغلر العشواحي هالا هو الة level voltage ( قلمة الةةة volt  اللةةي واصةةل للةةة 
Receiver  للتعبلر عةةن binary  0  او binary  1) ا بالتةةالي القلمةةة اللةةي واصةةل  للةة ة Receiver  هةةي قلمةةة
random  وغلر محدد  فبلحع threshold  معلن لقارن بل  وهالفترض اال  هالا 2.5 V  واي قلمة اعلى من
2.5    هالفترض االها معبر  عن1    واي قلمةاقل  من 2.5  هالفترض االها معبةةر  عةةن0ا وبالتةةالي لةةو عةةالز 
احسب  م    احتمال وصول الصفر  هجلب الة  probability  ان الجهد الواصل للة  Receiver   قلمت  تكةةون
اقل مةةن 2.5  االلةةي بلحسةةب الكةة م دا دالةةة اسةةمها distribution function  ولةةو جلالةةا العبةةل دا علةةى
القاالون 
 
𝑭(𝒙𝐨)=𝑷𝒓 { 𝑿 ≤𝒙𝐨 } 
 𝑿هي الة level voltage  )(المتغلر العشواحيالمعبر عن binary 1  و binary 0  
 𝒙𝐨  في الم ال اللي قولالا علل  هي 2.5 V 
 𝑭(𝒙𝐨) هي احتمال الجهد الواصل للة Receiver  لكون اقل من2.5 
 
Notes : we use capital letters for the random variables and lowercase letters for 
the values they take.  
 
The distribution function 𝑭(𝒙𝐨) has the following properties:  
𝑭(−∞)=𝑷𝒓 { 𝑿 ≤−∞ }=𝟎     
 احتمال ان الة random variable ألا كاالت قلمت  اقل من  −∞   وعبعا مفلش قلمة اقل من−∞ 
 وبالتالي قلمت  بصفر 
𝑭(+∞)=𝑷𝒓 { 𝑿 ≤∞ }=𝟏        
احتمال ان الة random variable   اقل من∞   بواحد الن اكلد مهما كاالت قلمت  هتكون اقل من∞  ودا
احتمال اكلد واالحتمال األكلد قلمت  بواحد.   
𝟎≤𝑭(𝒙𝐨)≤𝟏  
أي احتمال قلمت  في الة range  بلن0  و1 
 
الة  probability    منالحاجات  المهمةةة جةةدا فةةي مجةةال ا لةة ة  communications   حلةةث االةة  لةةتم اسةةتخدامها
لمعرفة وجود noise  اوأخعاء في االرسالا و من المصعلحات المهمة في اي digital system هةةي الةة ة 
probability of error  احتمال الخعأ لعالي احتمال االي ابعت1 فةةي الةة ة transmitter  ول وصةةل 0 فةةي الةة ة 
receiver  
 
 
 
probability density function
 
  هي دالة توفر احتماللة وقوع قلمة متغلر عشواحي بلن العاق معلن من
القلم.
 
The 
derivative of the distribution function is :
 
 
𝑷
(
𝒙
)
=
𝒅𝑭
(
𝒙
𝐨
)
𝒅𝒙
 
𝑭
(
𝒙
𝐨
)
=
𝑷
𝒓
 
{
 
𝑿
 
≤
𝒙
𝐨
 
}
=
 
∫
𝑷
(
𝒙
)
 
𝒅𝒙
𝒙
𝐨
−
∞
 
 
Where 
𝑷
(
𝒙
)
 
is known as the 
probability density function
 
of the random variable X
 
 
The probability that 
𝑋
 
is between any two limits is found by integrating the density 
function between these two limits
 
 تسمى الدالة
𝑷
(
𝒙
)
 
 ب
ة
 
probability density function
  
  وهي دالة المساحة تحت المالحالى الخاص بها بلن
القعتلن هو 
احتمال حدوث ال
ة
 
X
 
 .بلن هاتلن الالقعتلن
 
 
ال
ة
 
probability density function
 
 هي دوال
إحصاحلة
 
للها اشكال مختلفة ومالها 
Gaussian distribution
  
 )(شكل المالحالى الجرسي
 
والمالحالى دا ع قة بلن المتغلر العشواحي 
X
  
  على المحور االفقي وبلن
𝑷
(
𝒙
)
 
.على المحور الراسي
 
لحساب احتمال ان المتغلر العشواحي 
X
 
  لقه في المدى بلن
𝒙
𝟏
 
  و
𝒙
𝟐
 
  لجب حساب
المساحة اسفل المالحالى زي ما واضح من الرسم البلاالي االتي: 
 
 
 
 
 
 
 
 
 
𝑷
𝒓
 
{
 
𝒙
𝟏
 
≤
𝑿
≤
𝒙
𝟐
 
}
=
 
∫
𝑷
(
𝒙
)
 
𝒅𝒙
𝒙
𝟐
𝒙
𝟏
 
 
 
 
 
probability density function
  
𝑷
(
𝒙
)
 
𝒙
𝟏
 
𝒙
𝟐
 
𝑷
(
𝒙
)
 
𝑷
𝒓
 
{
 
𝒙
𝟏
 
≤
𝑿
≤
𝒙
𝟐
 
}
 
 
  لو م  المالحالى دا بلعبر عن احتماالت درجات الحرار  في مالعقة معلال   و الفترض ان   𝒙𝟏=25° C     و  
𝒙𝟐=30° C    وعالز  اجلب  احتمال درجةةة الحةةرار  فةةي المالعقةةة دي تقةةه مةةا بةةلن 25  و 30 االحةةل عةةن 
عرلد حساب المساحة تحت المالحالى بلن الالقعتلن باستخدام  probability density function 
 
∫𝑷(𝒙) 𝒅𝒙∞
−∞=1 
  التكامل دا معالا  االي بجلب مساحة المالحالى كلها و كل جزء تحت المالحالى بلعبر عن احتمال لبقى اكلد
التكامل دا بلعبر عن مجموع االحتماالت الكلي وبالتالي هلساوي واحد  
 
 
 
 
Is the most common density encountered in the real world  
  لعتبر هكا التوزله من اك ر التوزلعات االحتماللة استخداما  وللها تعبلقات كتلر جدا   
بالكتبها Density اختصار لة probability density function 
 
𝑷(𝒙)= 𝟏
√𝟐𝝅 𝝈 𝒆−(𝒙−𝒎)𝟐
𝟐 𝝈𝟐  
m , 𝝈  are constants.  
m (mean) : indicates the center position or symmetry  point of the density.  
 مالحالى هكا التوزله متما ل حول القعة الوسع الحسابي( mean) ا ودا بلعبر عن الة symmetry point 
وهي قلمة للة random variable  بتخلي المالحالى حواللها symmetry. 
 
𝝈𝟐 (Variance) : indicates the spread of the density.  
 بتحددالة spread االالتشار او االتساع بتاع المالحالىا وبلقلس مقدار تشتت القلم عن الوسع الحسابي ولو  
كاالت قلمت  كبلر  دا معالا  ان القلم متباعد  عن بعضها وعن الوسع الحسابي لعالي اتساع المالحالى بلزلد  
ولو كاالت قلمت  صغلر  دا معالا  ان القلم متقاربة من بعضها ومن الوسع الحسابي.  
قلمة المتوسع الحسابي لوحد  مش كافي الزم الجلب قلمة توضح مدى بُعد القلم عن  الة mean    
 
 
 
 
 
 Gaussian Density  
 
 
 
 
 
 
 
 
 
 
 
 
 
 التكامل
اللي فات 
 
ر
ل
ت
ك
 
م
د
خ
ت
س
ل
ب
 
 
ال
ا
 
 
ا
ر
ظ
ال
عملول  
جداول وعالدي دالتلن بلسهلوا التكامل وهم : 
 
1.
 
Error function   
𝒆𝒓𝒇
(
𝒙
)
 
𝒆𝒓𝒇
(
𝒙
)
=
𝟐
√
𝝅
 
∫
𝒆
−
 
𝒖
𝟐
 
𝒅𝒖
𝒙
𝟎
 
 
2.
 
Error function complementary  
𝒆𝒓𝒇𝒄
(
𝒙
)
 
𝒆𝒓𝒇𝒄
(
𝒙
)
=
𝟐
√
𝝅
 
∫
𝒆
−
 
𝒖
𝟐
 
𝒅𝒖
∞
𝒙
=
𝟏
−
𝒆𝒓𝒇
(
𝒙
)
 
 
Notes:
 
𝟐
√
𝝅
 
∫
𝒆
−
 
𝒖
𝟐
 
𝒅𝒖
𝒙
𝟐
𝒙
𝟏
=
𝒆𝒓𝒇
(
𝒙
𝟐
)
−
𝒆𝒓𝒇
(
𝒙
𝟏
)
 
𝒆𝒓𝒇
(
∞
)
=
𝟏
 
 
 
 
 
 
 
,
 
 
 
 
 
𝒆𝒓𝒇
(
𝟎
)
=
𝟎
 
 
 
 
 
 
 
,
 
 
 
 
 
𝒆𝒓𝒇
(
−
𝒙
)
=
−
𝒆𝒓𝒇
(
𝒙
)
 
 
 
 
 
   دول دوال تكاملتهم محسوبة وجاهز
والجدول م   موجود فل  قلم 
X
  
  و
𝒆𝒓𝒇
(
𝒙
)
 
 وعالد
X
  
  بككا بجلب
قلمة  
𝒆𝒓𝒇
(
𝒙
)
 
 وفل  جدول تاالي خاص
ب 
ة
 
𝒆𝒓𝒇𝒄
(
𝒙
)
 
 
اللي هعمل  االي هكامل ال
ة
 
density function
 
 وهوصل التكامل الحد الصورتلن اللي فوق حسب
المسالة
  
لعالي بشوف اللي هتسهل الحل واوصل 
ال
ة
 
Gaussian
 
 .الحد الصورتلن دول
 
 
دا للالس الجدول :    
 
https://bit.ly/3M6ou8H
 
 
Error Function
 
 
𝑷
(
𝒙
)
 
𝑷
(
𝒙
)
 
𝑷
(
𝒙
)
 
𝑷
(
𝒙
)
 
𝑚
=
2
 
𝑚
=
2
 
𝑚
=
−
2
 
𝑚
=
−
2
 
𝑋
 
𝑋
 
𝑋
 
𝑋
 
 
 
A binary communication system is one that sends only two possible messages. 
The simplest form of binary system is one in which either zero or one volt is sent. 
Consider such a system in which the transmitted voltage is corrupted by additive 
atmospheric noi se. If the receiver receives anything above 0.5 volt is assumes that 
a one was sent. If it receives anything below 0.5 volt it assumes that a zero was 
sent. Measurements have shown that if one volt is transmitted the received signal 
level is random and has  a Gaussian density with m = 1 and  𝝈=𝟎.𝟓 ,find the 
probability that a transmitted one will be interpreted as a zero at the 
receiver (i.e., a bit error ) 
Solution  
الررر   system   دا بيبعت1 V  او 0 V   وافترض انه بيتعرضلررر   noise   جايه من الهوا, ولوالررر  Receiver  
 استقبل قيمة اعلى منV 1
2   هيعتبرها واحد ولو اقل منV 1
2  هيفترض انه باعت0. 
  القياسات والتجارب وضحت ان لوالر  system    دايما  بيبعت 1 V   عشان يشوفأي  اللي بيحصل  لإلشار   
اللي وصلت للر  Receiver   وهي random   غير محدده واي حاجة random    يبقى الزم ليها probability 
density function    بتعبر عن احتمراالت حردوثهرا وشررررركرل الردالرةفي المثرال دا   اللي بتعبر عن احتمراالت  
الوصول  هي Gaussian . 
  بيعمل تجاربإلرسال  الواحد وبيشوف  الرر   system    هيستقبلهاأي  ومن التجارب الكتير دي رسم منحنى   
density function بيعبر عن احتماالت وصول الواحد. 
 
مطلوب احتمال لما ابعت  1    منالررر   Transmitter    وتوصل0   عندالررر   Receiver , وممكن يصيغ السؤال  
بشركل تاني ويقول هات  الرررررر   bit error   او احتمال الالطأ probability of error في ارسرال الواحد. كأني  
بقوله هات احتمال اني  ابعت واحد ويوصل صفر للر Receiver. 
 
 اول الطو  عشان احسبالر probability   هكاملالر density function  واللي هي هنا Gaussian  
حدود التكامل من  ∞-  الى0.5   عشررانالررررررر   Receiver  هيقرأ الواحد اللي اتبعت انه صررفر في حالة ان
الجهد اقل من  0.5  
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=∫𝑃(𝑥) 𝑑𝑥0.5
−∞ 
𝑃(𝑥)= 1
√2𝜋 𝜎 𝑒−(𝑥−𝑚)2
2 𝜎2= 1
√2𝜋 × 1
2 𝑒−(𝑥−1)2
2 (1
2)2
 
𝑃(𝑥)= √2
√𝜋 𝑒−2(𝑥−1)2 Example  1 
 
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=√2
√𝜋  ∫𝑒−2(𝑥−1)2 𝑑𝑥0.5
−∞ 
 
 الالطو  اللي بعد كدا اني  أوصل المعادلة اللي فاتت الحد اشكال معادالت الر error: 
𝑒𝑟𝑓(𝑥)=2
√𝜋 ∫𝑒− 𝑢2 𝑑𝑢𝑥
0        ,        𝑒𝑟𝑓𝑐(𝑥)=2
√𝜋 ∫𝑒− 𝑢2 𝑑𝑢∞
𝑥 
  الزم نغيرالر variable x  الىu  ونغير حدود التكامل 
 
To reduce this to a form that can be found in a table of error functions. we make the 
change of variable  
Let  𝑢2=2(𝑥−1)2 
𝑢=√2 (𝑥−1) 
𝑑𝑢= √2 𝑑𝑥  
  نغيرحدود التكامل :   
𝑥=1
2  →𝑢=√2 (1
2−1)=−1
√2  
𝑥=−∞  →𝑢=−∞   
Then we get  
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=√2
√𝜋  ∫𝑒−𝑢2 𝑑𝑢
√2−1
√2
−∞=1
√𝜋 ∫𝑒−𝑢2 𝑑𝑢−1
√2
−∞ 
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=  1
2 2
√𝜋 ∫𝑒−𝑢2 𝑑𝑢−1
√2
−∞ 
 
 واضح من المعادلة ان االسهلاني أوصل للشكل التاني  ← 𝑒𝑟𝑓𝑐(𝑥)=2
√𝜋 ∫𝑒− 𝑢2 𝑑𝑢∞
𝑥   ولكن
هنحتاج نعدل في حدود التكامل  والثوابت.  
 
الفكر  ببساطة اننا هنبدل حدود التكامل ونغير اإلشارات ودا الن الدالة  even  يعني متماثلة حول محورy  
 يعني لو عملنا تكامل من−∞  الى−𝟏
√𝟐   هو نفس التكامل لو عملناه من𝟏
√𝟐  الى∞ 
 
 ازاي الدالة دي symmetry  وقيمةm=1  مش0  يعني مش متماثلة حول محورy   ؟ 
هنا مش بنكامل الدالة الر  gaussian    األصلية وبنكامل الدالة الجديد  اللي فيها المتغير𝑢   ولو رسمنا
𝑒−𝑢2  هنالقيها متماثلة حول محورy 
 
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)= 1
√𝜋 ∫𝑒−𝑢2 𝑑𝑢∞
1
√2 
 
the second equality is true since the integrand is even. this is now seen related to the 
complementary error function  
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=0.5 𝑒𝑟𝑓𝑐(1
√2)=0.5×0.322=0.16 
 
 
 
thus, on the average, one would expect 16 out of every 100 transmitted 1's to be 
misinterpreted as 0's at the receiver.   
 
 
 ممكن حلها باستالدامerf : 
𝑃𝑟(𝑒𝑟𝑟𝑜𝑟)=  1
2 2
√𝜋 ∫𝑒−𝑢2 𝑑𝑢−1
√2
−∞=1
2(erf(−1
√2)−erf(−∞)) 
=1
2(−erf(1
√2)+erf(∞))=1
2(−0.67780+1)=0.16 
 
 
 
 
𝒙=𝟏
√𝟐=𝟎.𝟕𝟎𝟕 
 
 
Channel Modelling  هيعرلقة هالدسلة لتم لل ال ة Channel  بشكل مبسع لسهل قراءت  والتعامل مع   
رلاضلا . 
The channel is memoryless when 𝑺𝒐𝒖𝒕(𝒏) depends only  on 𝑺𝒊𝒏(𝒏)  and not on 
𝑺𝒊𝒏(𝒏−𝟏) or any other input  sample values  
 
 التوصلفاللي هالدرس  على  memoryless channel  تعتمد على الدخل الحالي فقع 
 
 
 
 
عالدي عرلقتلن اعمل بلهم Modelling 
 
1. Transition  Diagram  
An alternative way of displaying transition probabilities is  by use of the Transition 
Diagram.  
 شكل بسلع لوضح االحتماالتالممكالة في الة System 
For the binary channel  
 
 
 
𝑷𝟏𝟎 
 
 تقرأ كاالتي: احتمال االي ابعت  0   واستقبل1   
𝑷𝟎𝟎+ 𝑷𝟏𝟎=𝟏    ,     𝑷𝟎𝟏+ 𝑷𝟏𝟏=𝟏  
 
The summation of probabilities leaving any node is unity .  
مجموع االحتماالت الخارجة من الفس الة node   لساوي واحد 
Channel Modelling  
Channel  𝑺𝒊𝒏(𝒏) 𝑺𝒐𝒖𝒕(𝒏) 
 المرسل المستقبل  
 
Binary Symmetric Channel (BSC)  
A channel in which the two conditional error probabilities are equal  
  هي الموكجرلاضي لُ ستخدم لوصف قالا  اتصال  الاحلة الحالة لمكن أن تحدث بها أخعاء في القل البتات.  
بلتم القلهم من الة transmitter  الىالة receiver  عبرالة  channel  وممكن لحصل خعأ في قلمة البت
المرسلة من  0 الى1  او من1  الى0 .   بلتم تم لل التغللر اللي بلحصل دا عن عرلد قلاس احتماللة حدوث
الخعأ في كل بت تم القل  عبر الة  channel 
احتمال االي ابعت صفر  واستقبل واحد تساوي احتمال االي ابعت واحد واستقبل صفرا واحتمال االي ابعت  
واحد واستقبل واحد تساوي احتمال االي ابعت صفر واستقبل صفر.  
𝑷𝟏𝟎= 𝑷𝟎𝟏= 𝑷    ,  𝑷𝟎𝟎= 𝑷𝟏𝟏=𝟏− 𝑷   
𝑷  : احتماللة حدوث error    
 
 
Tandem Connections of BSCs  
Suppose in transmitting a digital signal over a long distance , the signal path 
includes a number of repeaters.  
Further, suppose that the path between each repeater and  the following repeater 
can be viewed as BSC.  The overall channel can be viewed as a tandem  connection  
of BSCs.  
 هو مجموعة من BSCs ورا بعضها   
BSC دا تعبلر عن الة Channel وبالحع االحتماالت على الة diagram دا. هحع repeater   في الالص بلن
الة transmitter  والة receiver   
 

 
 بلنالة transmitter  والة repeater   هعمل modelling   على االBSC ا الة repeater دا كأال  
transmitter للة receiver  األخلر وهعمل  بردو modelling   على االBSC   ا والك م دا بالستفلد مال  في
االالا الحسب الة total probability of error للة system 
 
 لو علز احسبالة total probability of error   هحسب عالد حالة فلها repeater  واحد فقع وبعد كدا من
الع قة اللي هوصلها هعممها على مجموعة من الة repeaters . 
 
 لو عالز احسب total probability of error  همشي منالة diagram   على كل المسارات اللي توصلالي من
1  فيالة transmitter لة 0  فيالة receiver  اوامشي على كل المسارات اللي توصلالي من0  فيالة 
transmitter لة 1  فيالة  receiver  .وبعد كدا اجمه المسارات دي 
 
لحساب total probability of error  : للم ال دا هالعمل االتي 
 
𝑷𝒆=𝑷𝟏𝟎 𝑷(𝟎)+𝑷𝟎𝟏 𝑷(𝟏)  
 
  𝑃10  المشي على المسارات اللي بعتالا فلها0  واستقبلالا فلها1          𝑃(0)   احتمال ارسال0 
  𝑃01  المشي على المسارات اللي بعتالا فلها1  واستقبلالا فلها0          𝑃(1)  احتمال ارسال1    
 
𝑷𝒆=[𝑷𝟎𝟎𝑷𝟏𝟎+𝑷𝟏𝟎𝑷𝟏𝟏 ] 𝑷(𝟎)+[𝑷𝟎𝟏𝑷𝟎𝟎+𝑷𝟏𝟏 𝑷𝟎𝟏] 𝑷(𝟏)  
=[(𝟏−𝑷)𝑷+𝑷(𝟏−𝑷) ] 𝑷(𝟎)+[𝑷(𝟏−𝑷)+(𝟏−𝑷)𝑷 ] 𝑷(𝟏)  
 
عشان الجلب اول مسار  (𝟏−𝑷)𝑷   بعتالا فل0    واستقبلالا فل1   هالمشي المسار االول  (𝟏−𝑷)   وبعدها
المسار التاالي 𝑷   وهالا هالضربهم في بعضالن دا مسار مكتمل على بعض   الزم لحصل االتاللن  مع بعض   
لعالي and . 
 
 المسار التاالي 𝑷(𝟏−𝑷)   الليبعتالا فل  0   واستقبلالا فل1    هالجمع  على المساراألول   (𝟏−𝑷)𝑷   الن
دا مسار جدلد واالحتمال عالدي االي امشي المسار اللي فوق او or   لمشي على المسار اللي تحت لحد ما
لوصل 1 
 
 وكل دا هالضرب  في احتمال االي ابعت0  𝑷(𝟎)  أص من الة transmitter    الن اللي فوق مشروع االي
أكون باعت 0  أص ومادام condition  الزم لحصل مه بعض لبقى الزم وقتهاالة and دي تترجم لضرب 
وبالم ل في احتمال االي ابعت 1   ولوصل0   هضربة في احتمال ان ابعت1 𝑃(1)  
 
  هالاخد 𝑷(𝟏−𝑷)   : عامل مشترس والبسع المعادلة كاالتي 
𝑷𝒆=[𝑷(𝟏−𝑷)+𝑷(𝟏−𝑷) ] [𝑷(𝟎)+𝑷(𝟏)] 
 اكلد احتمال االي ابعت صفر او واحد= 1           𝑷(𝟎)+𝑷(𝟏)=1  
 
𝑷𝒆=[𝑷(𝟏−𝑷)+𝑷(𝟏−𝑷) ]=𝟐𝑷(𝟏−𝑷)≅𝟐𝑷 
𝑷  قلمة صغلر  ممكن الهملها بالالسبة للواحد 
 
For n BSC   →  𝐏𝐞=𝐧𝑷  
 لما كان عالديBSC 2 الاتج الة total probability of error  عله تقرلبا𝟐𝑷   ومعالى كدا ان لو عالدي
BSC 10  الة total probability of error  هلساوي10𝑷  
 
  هل معالى كدا لما بزود repeaters  الة total probability of error   بتزلد ؟ 
اكلد أل  الن قلمة الة 𝑷  فيBSC 1  غلر فيBSC  2  او 10 BSC  الن كل ما الزود عددالة repeaters 
 قلمةالة 𝑷  بتقل جدا  وبالتالي محصلة total probability of error هتقل   
 
 
2. Transition Matrix  
The memoryless channel can be characterized by a Transition Matrix composed 
of conditional probabilities.  
[𝑻]= [𝑷𝟎𝟎𝑷𝟎𝟏
𝑷𝟏𝟎𝑷𝟏𝟏] 
 
 في العرلقة دي هالحع االحتماالت في شكل مصفوفة بدلالة diagram   وكل عمود بلعبر عن احتمال
ارسال bit  واحد  واستقبالها في اكتر من حال.  
م   العمود األول بلعبر عن ارسال 0   واستقبال0  𝑷𝟎𝟎  ااو ارسال0  واستقبال1 𝑷𝟏𝟎   ا والعمود
التاالي بلعبر عن ارسال  1  واستقبال0   𝑷𝟎𝟏  ااو ارسال1  واستقبال1  𝑷𝟏𝟏 
 الستالتج من كدا ان مجموع احتماالت العمود الواحد لساوي1    الن كل عمود لعبر عن احتماالت
ارسال bit   واحد 
 
With no noise nor distortion  
 في حالة عدم وجود noise  او distortion  تكون المصفوفة الالاتجة هي مصفوفة الوحد 
الن في الحالة دي احتمال االي ابعت 0    واستقبل0  هو احتمال اكلد و احتمال االي ابعت1   واستقبل1 
  هو احتمال اكلدوبالتالي  باقي االحتماالت  𝑷𝟎𝟏  ا𝑷𝟏𝟎   اللي بتدل ان حصل خعأ في االرسالة قلمتهم
بصفر   
[𝑻]= [𝟏𝟎
𝟎𝟏] 
 
summation of probabilities in any column is unity.  
 = مجموع عالاصر العمود الواحد1 
 
 
A digital communication  system has a symbol alphabet composed of four 
entries and the transition matrix is given by  
 
[𝑻] =
[     𝟏𝟒⁄𝟏𝟐⁄𝟏𝟔⁄𝟏𝟔⁄
𝟏𝟒⁄𝟏𝟔⁄𝟏𝟐⁄𝟏𝟔⁄
𝟏𝟒⁄𝟏𝟔⁄𝟏𝟔⁄𝟏𝟑⁄
𝟏𝟒⁄𝟏𝟔⁄𝟏𝟔⁄𝟏𝟑⁄]     
 
 
(a) Find the probability of a  single transmitted symbol being in error assuming 
that all four input symbols are equally probable at any time  
Solution  
 
[𝑻] =[𝑷𝟎𝟎𝑷𝟎𝟏𝑷𝟎𝟐𝑷𝟎𝟑
𝑷𝟏𝟎𝑷𝟏𝟏𝑷𝟏𝟐𝑷𝟏𝟑
𝑷𝟐𝟎𝑷𝟐𝟏𝑷𝟐𝟐𝑷𝟐𝟑
𝑷𝟑𝟎𝑷𝟑𝟏𝑷𝟑𝟐𝑷𝟑𝟑] 
 
𝑷𝒆|𝟎 𝒔𝒆𝒏𝒕= 𝑷𝟏𝟎+ 𝑷𝟐𝟎+ 𝑷𝟑𝟎=𝟏
𝟒+𝟏
𝟒+𝟏
𝟒=𝟑
𝟒 
𝑷𝒆|𝟏 𝒔𝒆𝒏𝒕= 𝑷𝟎𝟏+ 𝑷𝟐𝟏+ 𝑷𝟑𝟏=𝟏
𝟐+𝟏
𝟔+𝟏
𝟔=𝟓
𝟔 
𝑷𝒆|𝟐 𝒔𝒆𝒏𝒕= 𝑷𝟎𝟐+ 𝑷𝟏𝟐+ 𝑷𝟑𝟐=𝟏
𝟔+𝟏
𝟐+𝟏
𝟔=𝟓
𝟔 
𝑷𝒆|𝟑 𝒔𝒆𝒏𝒕= 𝑷𝟎𝟑+ 𝑷𝟏𝟑+ 𝑷𝟐𝟑=𝟏
𝟔+𝟏
𝟔+𝟏
𝟑=𝟐
𝟑 
  
The total probability of error is given by the average of four quantities  
بعد ما حسبالا احتمال الخعأ في ارسال كل bit  لوحد هالحسب الة average probability of error 
 هو قالي انالة four symbols  دول equally probable  عشان اجلبالة  average هجلب مجموعهم  
واقسمهم على عددهم اللي هو 4 
 
𝑷𝒆=(𝟑
𝟒+ 𝟓
𝟔+ 𝟓
𝟔+ 𝟐
𝟑)𝟏
𝟒=𝟑𝟕
𝟒𝟖 Example  2 
 
لو الة symbols  مكاالتش equally probable  هالحسبالة average   باالالا الضرب كل قلمة في احتمال
حدو ها واجمه الالواتج  باستخدام الع قة:  
𝑃𝑒=∑(𝑃𝑒|𝑖 𝑠𝑒𝑛𝑡)3
𝑖=0 𝑃(𝑖) 
 𝑃(𝑖)  هو احتمال ارسال كل symbol  وفي الم ال دا احتمال ارسال كل symbol   هو1
4 
=3
4×1
4+ 5
6×1
4+ 5
6×1
4+ 2
3 ×1
4=37
48 
القاالون دا لصلح في حالة االهم equally probable  او nonequally probable 
 
(b) Find the probability of correct symbol transmission  
  معلوب حساب االرسالالصحلح للة symbol 
𝑷𝒆+𝑷𝒄=𝟏 
𝑷𝒄=𝟏−𝑷𝒆=𝟏− 𝟑𝟕
𝟒𝟖= 𝟏𝟏
𝟒𝟖 
 
 ممكن الحل من المصفوفة عن عرلد الجلب احتماالت االرسال الصحلح والضرب كل قلمة في السبتها
او الضربهم كلهم في  𝟏
𝟒 الالهم equally probable 
 
𝑷𝒄=𝑷𝟎𝟎 𝑷(𝟎)+ 𝑷𝟏𝟏 𝑷(𝟏)+ 𝑷𝟐𝟐 𝑷(𝟐)+ 𝑷𝟑𝟑 𝑷(𝟑) 
=𝟏
𝟒×𝟏
𝟒+ 𝟏
𝟔×𝟏
𝟒+ 𝟏
𝟔×𝟏
𝟒+ 𝟏
𝟑 ×𝟏
𝟒=𝟏𝟏
𝟒𝟖 
 
 
 
(c) If the symbols are denoted A , B , C, and D ,find the probability BADCAB  that 
the transmitted sequence will be received as DADDAB   
 
Transmitted                                 Received  
B A D C A B                                  D A D D A B 
1  0 3  2  0 1                                   3  0  3 3  0  1 
 
 
 0 1 2 3 
 
 معلوب حساب احتمال ارسال BADCAB (103201)  واستقبال DADDAB  (303301) 
 
𝑷𝒆|𝟏𝟎𝟑𝟐𝟎𝟏 𝒔𝒆𝒏𝒕 𝒂𝒏𝒅 𝟑𝟎𝟑𝟑𝟎𝟏 𝒓𝒆𝒄𝒆𝒊𝒗𝒆𝒅= 
 𝑷𝟑𝟏 𝑷𝟎𝟎  𝑷𝟑𝟑 𝑷𝟑𝟐  𝑷𝟎𝟎  𝑷𝟏𝟏 = 
=𝟏
𝟔 × 𝟏
𝟒 × 𝟏
𝟑 × 𝟏
𝟔 × 𝟏
𝟒 × 𝟏
𝟔= 𝟏
𝟏𝟎𝟑𝟔𝟖 
 تم ضرب االحتماالت ألالها تحدث معا  في الفس الوقتك ة sequence واحد 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
دا تكملة للمحاضر  اللي فاتت وهالالكتفي باالالا العرف ملزتلن وعلبلن فقع
 
1. Errors often can be corrected
 
The only decision at the 
receiver is the selection between two possible pulses, 
not the details of the pulse shape.
 
إمكااللة
  
تصحلح  
األخعاء
  
في ال
ة
 
digital
 
  اقوى بكتلر من
ال
ة
 
analog
 
ألالالا
 
بالتعامل مه قلمتلن فقع 
 
 
 
 
 
 
 
 
 
 
 
The regenerative repeater not only performs the 
function of amplification, but 
also cleans up the signal
 
ال
ة
 
Repeater
 
 هو
amplifier
  
 
ي
ف
 
م
د
خ
ت
س
ُ
م
ال
ة
 
system
 
digital
  
 ومش بلكبر
ااشار 
 
فقع ولكن بلعمل  
regenerate
 
 لإلشار
 
بشكل صحلح لعالي بلاخد 
ااشار 
 
وفلها 
noise
 
 ولخرجها
إشار 
 
cleaned
  
 
د
د
ح
ل
 
ف
ر
ع
ل
ب
 
ن
ال
 
ا
ه
ر
ب
ك
ل
 
ا
م
 
د
ع
ب
 
 
ا
م
ا
م
ت
ال
ة
 
binary 0
 
 و
binary 1
 
 عن عرلد المقارالة ب
ال
ة
 
threshold
 
 او
أي
 
أفكار
 
تااللة. بلن 
ال 
ة
 
Transmitter
 
 و
ال
ة
 
Receiver
 
  بلكون فل
Repeaters
 
   على مسافات معلال
 
هال حظ من الصور  ان 
ال
ة
 
amplifier
 
 بلكبر
ال
ة
 
signal
 
 ب
ال
ة
 
noise
 
بتاعتها
 
 
 
 
 
 
 
 
 
 
 
 
وحتى لو 
ااشار 
 
وصلت وفلها بعض 
األخعاء
  
فل  عرق ل
ل
ة
 
error detection and correction
 
  القدر
الحدد الخعأ والصلح  
 
Advantages of Digital Systems
 
 
 
2. Signal manipulation (e.g. encryption) is simple to perform  
Digital systems deal with numbers, rather than waveforms. These numbers can be 
manipulated by simple logic circuits. Analog operations require complex hardware.  
 التعامل مهااشار  في حالة الة digital  سهل تالفلك  ألال  بلتعامل مه ارقام ودي سهل التعامل معاهاألالالا  
بالستخدم في التعامل معاها simple logic gates 
 
 
 
 
 
1.  Generally, requires more bandwidth than analog.  
 بلحتاج bandwidth  ممكن لكون اكبر منالة analog  وعبعا فل  حلول للمشكلة دي وهي bandwidth 
utilization techniques  ومشكلةالة large bandwidth للة user الواحد اال  بلقلل عدد الة users   ومن
اشهر الة techniques  المستخدمة لحل هك  المشكلة هي CDMA   
 
2. Synchronization is required.  
أي Digital system    الزم لكون فل  synchronizationبلن الة transmitter   والة Receiver 
 Disadvantages of Digital Systems  
 
 
 
 
 
 
   
 
©
 
Basem Hesham
PCM & Companding & Counting Quantizer 
Lecture
 
5
 
 
 اول Block  معانا فيالـ Digital communication system  الدايرة اللي بتحول اإلشارة المتصلة للمكافئ
الـ binary  بتاعها واللي بنقول عليها analog to digital converter (ADC)  
 في الشابتر دا هندرس طرق التحويل من analog  الى digital  وهنبدأ بأول  طريقة وهي Pulse code 
modulation  
 
 
 
 
 
PCM  هي طريقة تستخدم في تحويلاإلشارة  من analog  الى digital  ,والعملية تتم على 3   خطوات 
 - 1  اولخطوة بعمل sampling لإلشارة   
- 2  تاني خطوة بعمل حاجة اسمها Quantization  وهنا بحدد مجموعة منالـ Levels  علىاإلشارة  
وبقرب كل sample  ألقرب Level 
- 3 تالت خطو ة بعمل coding  او المكافئالـ binary  ألقرب Quantization Level  
 
 
 
 
States that, if the Fourier transform of a time function is zero for 𝒇 > 𝒇𝒎, and 
the values of the time function are known for 𝒕 = 𝒏𝑻𝒔 , for all integer values of 
𝒏, then the time function is known for all values of 𝒕 provided that the samples 
are close enough together.  
 
 
 
Source Coding  
1-Sampling  Pulse Code Modulation  
 
 اول مرحلة من مراحل تحويلاإلشارة  التماثلية analog  الى رقميه digital   هي عملية اخذ العينات
Sampling   والتي تحولاإلشارة  التناظرية الى إشارة منفصلة discrete   والتي تتم وفقا لنظرية اخذ
العينات Sampling Theory والتي تنص على:  اذا كانت 𝒔(𝒕) إشارة تماثلية ذات نطاق ترددي من  
صفر الى 𝒇𝒎 (أي اعلى تردد تحتويه هو  𝒇𝒎  وممكن نقول انه  الـ Bandwidth للـ signal  وقيمة 
Fourier transform  لهذهاإلشارة  الي تردد اكبر من  𝒇𝒎  )تساوي صفرفيمكن تمثيلها بواسطة عينات  
منها تؤخذ على فترات متساوية  𝒕 = 𝒏𝑻𝒔   (بنبعت مناإلشارة  قيم محدده فقط عند  𝑛𝑇𝑠   حيث𝑛  رقم
صحيح )  
 
لكي نستطيع تحويل اإلشارة الى عينات يسمح باسترجاعها بعد ذلك في الـ Receiver  بالشكلوالجودة   
المناسبة يجب ان يكون تردد اخذ العينات وفقا للعالقة األتية :  
𝒇𝒔≥𝟐 𝒇𝒎 
 : حيث 
 𝒇𝒔 : معدل او تردد اخذ العينات ( sampling frequency or sampling rate)   ويساوي𝟏
𝑻𝒔  
 𝑻𝒔 : هي الـ sampling period  حيث يتم اخذ عينات مناإلشارة  االصلية كل  𝑻𝒔  ثانيه 
 
التردد 𝑓𝑠=2 𝑓𝑚   يعرف بـ Nyquist rate   وهو قيمة اقل تردد مسموح بيه ووحدته samples/sec  
 يتم استرجاعاإلشارة  االصلية من اإلشارة المجزئة في الـ Receiver  بواسطة Low pass filter 
 
 اقل عدد samples  ممكن اخده خالل cycle  كامله هو 2 𝑓𝑚  ولو قل عن كدا لن يتم التعرف علىاإلشارة  
في الـ Receiver  ,وكل ما نزيد هيكون افضل من ناحية استرجاعاإلشارة  بشكل صحيح و  الـ bit rate  
هيزيد يعني الداتا هتتبعت اسرع ولكن دا هيزود الـ Bandwidth 
 
ممكن نحصل على الـ bit rate  ( bits/sec )من خالل ضرب التردد  𝒇𝒔  في عدد البتات لكل sample 
 (بنحددها على حسب  A/D Converter الليهستخدمه )  
𝒃𝒊𝒕 𝒓𝒂𝒕𝒆  𝑹𝒃=𝒇𝒔(𝒔𝒂𝒎𝒑𝒍𝒆𝒔
𝒔𝒆𝒄) × 𝒏(𝒃𝒊𝒕𝒔
𝒔𝒂𝒎𝒑𝒍𝒆)→𝒃𝒊𝒕𝒔
𝒔𝒆𝒄 
 
 
 
 
 
 
 
 
 
 نفترض ان عندناإشارة  في الـ time domain   اسمها 𝑠(𝑡)  وعشان اعمل sampling  هضرباإلشارة  
𝑠(𝑡)  في train of pulses 𝑝(𝑡)  periodic وهي دالة Even   
 
 
Using Fourier series  
Time domain  
𝒑(𝒕)=𝒂𝒐+∑𝒂𝒏𝐜𝐨𝐬(𝒏 𝝎𝒔 𝒕)∞
𝒏=𝟏 
𝑺𝒔(𝒕)=𝑺(𝒕) (𝒂𝒐+∑𝒂𝒏𝐜𝐨𝐬(𝒏 𝝎𝒔 𝒕)∞
𝒏=𝟏) 
=𝒂𝒐𝑺(𝒕)+∑𝒂𝒏 𝑺(𝒕)𝐜𝐨𝐬( 𝟐𝝅𝒏  𝒇𝒔𝒕)∞
𝒏=𝟏 
 
Frequency  domain  
𝑺𝒔(𝒇)=𝒂𝒐𝑺(𝒇)+∑ 𝒂𝒏
𝟐 [𝑺(𝒇−𝒏𝒇𝒔)+𝑺(𝒇+𝒏𝒇𝒔)]∞
𝒏=𝟏 
 
  جزء𝒃𝒏  غير موجود في المعادلة الن الدالة Even   
period اإلشارة  𝑝(𝑡)   هو𝑇𝑠  وبالتالي التردد الـ Fundamental هنا هو مقلوب الـ  𝑓𝑠=1
𝑇𝑠 ←𝑇𝑠  
 فمهم اننا نكتبه في المعادلة𝑓𝑠   مش𝑓𝑜 
Proof  
 
الـ Amplitude Spectrum  لإلشارة االصلية  𝑆(𝑓) 
 
 
الـ Amplitude Spectrum لإلشارة  𝑆𝑠(𝑓)  بعد عملية الـ sampling   
 
 ضرباإلشارة 𝑆(𝑡)  في مجموع اشارات cos(2𝜋𝑛 𝑓𝑠𝑡)  في الـ time domain يكافئ في الـ 
frequency  domain   عملshift  الى اليمين والى اليسار بمسافات تساوي 𝒏 𝒇𝒔   حيث𝒏  أي عدد 
صحيح ( integer)   
 
A low pass filter with a cut off frequency of 𝐟𝐦 can be used to recover the 
information signal  
 
 باستخدامLPF في الـ Receiver  يعدي الترددات لحد𝒇𝒎 ويلغي باقي الترددات كلها 
 
 
 

 
 ألخذ العينات بشكل صحيح البد ان يتحقق الشرط 
𝒇𝒔≥𝟐 𝒇𝒎      𝒐𝒓        𝑻𝒔≤𝟏
𝟐𝒇𝒎    
 
 اذا لم يتحقق فسوف ينتج خطأ يسمى Aliasing error   حيث ال يمكن الحصول علىاإلشارة  االصلية 
كامله وبشكل صحيح نتيجة وجود تداخل بين اإلشارات. واالشكال التالي  ة توضح الحاالت المختلفة : 
 
 
 
 
 
 
Is a technique for rounding -off the amplitudes of a waveform.  
 هي عملية تقريب كل sample  منالـ samples  المأخوذة ضمن مستوى معين من الـ Quantization 
Levels (L) ويتم تقريب كل عينه الى اقرب Quantization Level  تبعا لقيمةالـ Amplitude  لهذه
العينة.  
ال يمكن عمل coding  للـ samples مباشرة الن A/D Converter   هيتعامل مع عدد كبير جدا من
العينات لكن بعد عملية الـ Quantization  بيتعامل مع عدد محدود من Quantization Levels 
 
 
 
2-Quantization   
 
The rounding -off operation is known as quantization, and the round -off error is 
known as quantization error . 
quantization error  .هو الفرق بين القيمة الحقيقية للعينه والقيمة التقريبية لها 
ينتج عن الـ Quantization   خطأ نتيجة لعملية التقريبودا بسبب  ان بعد عملية التقريب بيكون فيه 
فرق بين القيمة الحقيقية  للـ sample  والقيمة التقريبية  ليها ,مثال لو قيمة العينة 2.1   فولت بعد التقريب
بقت 2 فولت.  
 
لتقليل هذا الخطأ نزود عدد  Quantization Levels (L)  حيث انها تزيد من كفاءة عمليةالـ 
Quantization  ولكن دا بيزود عددالـ bits  في الكود مما سيؤدي الى زيادةال ـ bit rate    وبالتاليزيادة 
الـ Bandwidth  :حيث 
𝒃𝒊𝒕 𝒓𝒂𝒕𝒆  𝑹𝒃=𝒇𝒔 × 𝒏  ↑          ,    𝑩𝑾𝒎𝒊𝒏 =𝑹𝒃
𝟐   ↑ 
 لذلك نحاو ل قدر اإلمكان ان يتواجد اتزان ( trade off ) بين عدد المستويات (𝑳)  وعددالـ bits  (𝒏) 
 
 يتم تقسيمالـ  DR  لمجموعة منالـ  Levels  وعدد هذه المستويات سوف يعتمد على عددالـ  bits   التي
نريد ان نعبر بها عن الـ sample 
Quantization Levels  𝑳=𝟐𝒏  
𝒏→number of bits  required to represent each sample  
𝑳→number of Quantization Levels     
 
 بعد حساب عدد المستويات نقوم بحساب قيمةالـ  step  التي تفصل بين كل مستويين 
∆𝒔= 𝑫𝑹
𝑳 
 
∆𝒔→ separation between two quantization level.  
 
 𝐷𝑅  هو الفرق بين اعلى واقل amplitude  فياإلشارة  
𝑫𝑹 =𝒂𝒎𝒑𝒍𝒊𝒕𝒖𝒅 𝒆𝒎𝒂𝒙 −𝒂𝒎𝒑𝒍𝒊𝒕𝒖𝒅 𝒆𝒎𝒊𝒏 
 
 
 
 
 
 
PCM codes the various levels into binary numbers and  sends the binary code 
corresponding to the particular round -off level.  
 كل مستوى يأخذ كود معينويتم تمثيل كل sample  بناءً على المستوى الواقعةفيه  وبالتالي يتم تحويل  
العينات الى اكواد   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3-Encoding   
 
 
▪ The processing pair (compression and expansion) is  called companding.  
▪ The most common form of non -uniform quantization is known as companding.  
Companding  هي اختصار لدمج مصطلحين مع بعض Compression and Expansion    وهي من
اشهر صور non-uniform quantization 
 
▪ The uniform quantization provides the same resolution at high levels as at low.  
▪ For some signals like voice signals it is desirable to use small quantization 
steps at lower levels and larger steps at higher levels.  
 
اللي تم شرحه في المحاضرة هو uniform quantization   وفي النوع دا المسافة بينالـ Levels  (∆𝒔) 
 ثابته, ودا ممكن يعمل مشكلة في بع ض اإلشارات زي اشارة الصوت مثال معظم التفاصيل موجوده في 
القيم المنخفضة لإلشارة وبكدا كل الـ samples  في المنطقة اللي تحتهيتم تقريبهم  لنفس الـ Level  و  
الـ Quantization Error   هيكون كبير 
 
اإلشارة دي عشان نقدر نعملها Quantization  و Coding   بشكل صحيح المفروض ازود عدد
المستويات في الجزء اللي فيه تفاصيل اكتر من غير ما نزود عدد الـ Levels  فياإلشارة  كلها و  الـ  
Bandwidth يزيد.  
 
الحل هنا في استخدام non-uniform quantization  ومن خالله اقدر ازود عددالـ Levels  في
األجزاء اللي فيها تفاصيل اكتر  يعني استخدم  ∆𝒔   صغيره واالجزاء اللي فيها تفاصيل اقل استخدم∆𝒔 
كبيره وبكدا الـ Quantization Error  Average هيقل.  
 
كدا عملنا توازن ما بين اننا مفقدناش المعلومات المهمة الموجودة في التفاصيل اللي تحت وبين استخدام  
افضل للـ  Bandwidth 
 
Companding  
 
▪ The average quantization error may well dec rease using this approach.  
▪ Prior to quantization, the signal is compressed  by a function similar to the one 
show below that.  
 الدوائر المتاحة عندنا uniform quantiz er    يعني بتعمل quantization  بـ  ∆𝒔  ثابته.  
ازاي هنستفيد من الدواير دي ونعمل بيها non-uniform quantization ؟ 
الحل اننا نضغط اإلشارة  compression  من خاللدايرة اسمها  الـ compressor   ونضغطهابشكل  
يناسب التطبيق اللي هنشتغل عليه والتطبيق اللي عندنا دا الـ compressor   بيضغط القيم الكبيرة ويحافظ
على القيم الصغيرة وبعدها ناخد اإلشارة المضغوطة وندخلها على uniform quantiz er   وبكدا عملناnon-
uniform quantization 
 خصائصالـ compressor  تختلف على حسب التطبيق الليهستخدمه   الن ممكن تكون تفاصيل اإلشارة 
في القيم العاليه مثال او تكون متغيره فبنشوف تفاصيل اإلشارة فين  بالضبط  واعمل enhance   لتفاصيل
اإلشارة و  compress لألجزاء اللي مش مهمة اوي في االشارة 
 
 
الشكل دا يوضح عالقة الدخل والخرج للـ compressor   دا 
 
 
This operation compresses the extreme values of the wave form while enhancing 
the small values  
 واضح منالرسم ان القيم الصغيرة بتطلع تقريبا بنفس القيمة  والقيم الكبيرة بتخرج قريبه من بعض 
يعني عمل ضغط للقيم الكبيرة فقط يعني بيضغط  القيم العالية من اإلشارة ويحافظ على القيم القليلة 
 
Compression   characteristic  
 
▪ The analog signal forms the input to the compressor, and the output is 
uniformly quantized. The result is equivalent to quantizing with steps that start 
out small and get larger with higher signal values.  
▪ At the receiver, expansion is applied so that the overall transmission is not 
distorted.  
 فيالـ Receiver بنعمل  عكس العملية دي وهي  Expansion 
 
North America and Japan have adopted a standard compression curve known 
as 𝝻-low companding. Europe has adopted another standard known as A-law 
companding.  
دي قوانين او  standards للـ  compression 𝝻-low  للنظاماألمريكي   و  A-law  للنظام االوروبي 
 
لالطالع فقط 
القانون و الـ curve  في الصورةيوضحوا خصائص  𝝻-low    
𝑭(𝒔)=𝒔𝒈𝒏  (𝒔) 𝒍𝒏(𝟏+𝝁|𝒔|)
𝒍𝒏(𝟏+𝝁) 
 
 درجة انحناء المنحنى بيحددها parameter اسمه 𝝁  
 لو 𝝁=𝟎   فدا معناه ان مفيش compression ولو عوضنا في المعادلة بالقيمة دي هنالقي الدخل زي  
الخرج بالضبط, واقصى قيمة ممكنه   𝝁=𝟐𝟓𝟓   
  على حسب قيمة𝝁 بنحدد شكل العالقة بين الدخل والخرج واي اللي هيتضغط  ونسبة الـ  
compression  وعلى حسباإلشارة  و متطلباتها بنحدد القيمة المناسبة 
 
 

 
 
A PCM modulator is nothing more than an analog -to digital converter. The 
converter first samples the waveform, and then quantizes each sample value.  
 
There are three generic forms for the quantizer:  
➢ Counting Quantizer  
➢ Serial Quantizer  
➢ Parallel quantizer  
 
 
 
 
 
Sample and hold  بابسط األمثلة عبارة عن switch يعمل sampl ing    يعني ياخد قيمةاإلشارة  عند  
لحظة معينه  و مكثف يعمل Hold  يعني يحتفظ بالقيمة لمدة sampling period  𝑇𝑠   وبعدين يرجع ياخد
sample  تانية وهكذا 
 
Comparator تقارن خرج S/H   و ramp generator  وعند تساوي القيمتين تخرجإشارة stop 
 
في البداية بتخرج إشارة start  بتشغل binary counter   و S/H   بالتزامن مع بعض وتدخلاإلشارة  
التناظرية  𝑓(𝑡)  علىS/H وتاخد اول sample   وتعملHold   ليها يعنييثبتها خالل فترة زمنيه   𝑇𝑠 
 وفي نفس الوقت دايرةالـ ramp  بتطلعإشارة بميل ثابت لحد ما الـ comparator   يالقي خرج
االشارتين S/H   و ramp generator  تساووا هيخرجإشارة stop للـ binary counter  و  يوقف عد .  
 
العده اللي هيقف عندها الـ counter  تعبر عنالـ sample  بالـ binary  وبعدها الدايرة تعمل reset للـ  
binary counter  و ramp generator  ونعملstart  عند بداية sample   جديدهوهكذا في باقي االشارة .  
PCM Modulators  
Counting Quantizer  
 
 
The ramp generator starts at each sampling point, and
 
a binary counter is 
simultaneously started.
 
مع بداية كل
 
sample
  
 يبدأ
ال 
ـ
 
ramp generator
 
 &
S/H
 
 &
binary counter
 
العمل في نفس الوقت
 
 
 
فيه 
شرطين 
 
عشان الدايرة تشتغل صح 
الشرط 
األول
  
خاص ب 
ال
ـ
 
 
ramp
 
slope
 والتاني خاص ب
ال
ـ
 
 
clock frequency
 
 :
 
▪
 
The time duration of the ramp, and therefore the
 
duration of the count is
 
proportional to the sample
 
value 
(ramp slope is constant).
 
 الزم ميل
ال
ـ
 
ramp generator
 
 يكون ثابت عشان تطلع الخرج صح الن مينفعش عند كل
sample
 
يكون فيه ميل مختلف
.
 
زمن 
ال
ـ
 
ramp
 
 
ة
م
ي
ق
 
ع
م
 
ً
ا
ي
د
ر
ط
 
ب
س
ا
ن
ت
ي
ال
ـ
 
sample
 
 يعني كل ما كانت قيمة
ال
ـ
  
sample
 
 اكبر زمن
ال
ـ
 
ramp
 
اللي هيحتاجة عشان يوصل للقيمة دي هيزداد
 
وبالتالي عند اقصى قيمة  
لإلشارة
 
هيكون اكبر زمن 
لل
ـ
 
ramp
 
ومن دا بنحصل على ميل ثابت من العالقة
 
األتية: 
 
𝒓𝒂𝒎𝒑
 
𝒔𝒍𝒐𝒑𝒆
=
𝑫𝑹
𝑻
𝒓𝒂𝒎𝒑
  
 
  
𝐷𝑅
 هو اعلى
قيمة في 
اإلشارة
 
و  
𝑇
𝑟𝑎𝑚𝑝
 
 هو الزمن الالزم للوصول العلى قيمة في االشارة
 
 
▪
 
The ramp slope must be 
sufficient to reach the
 
maximum possible values 
within one sampling period.
 
 ميل
ال
ـ
ramp 
 
 الزم يكون كافي انه يوصل ألقصى قيمة في
اإلشارة
 
في فترة
 
ال
ـ
  
𝑇
𝑠
 
 يعني الزمن اللي
هتحتاجة 
إشارة
 
ال
ـ
 
ramp
 
 الزم يكون اقل من او يساوي
𝑇
𝑠
 
  الن بعد زمن
𝑇
𝑠
 
 فترة
ال
ـ
 
Hold
 
هتنتهي
  
و 
  
S/H
 
 هتعمل
 
sample
 
جديده
 
وبالتالي لو كان
 
زمن
 
ال
ـ
ramp 
 
  اكبر من
𝑇
𝑠
 
  مش هنحصل على قيمة
digital
 
مقابلة ألعلى جهد
 
النه هياخد 
sample
 
جديده قبل ما يوصل للقيمة المطلوبة
.
 
فالزم 
إشارة
 
ال
ـ
ramp 
 
توصل ألعلى قيمة
 
في زمن
 
اقل من او يساوي  
𝑇
𝑠
 
 
𝑻
𝒓𝒂𝒎𝒑
≤
 
𝑻
𝒔
 
S/H o/p
 
ramp o/p
 
 هنا تساوى االثنين فيقف
الـ  
counter
 
  عن العد
عند قيمة 
الـ 
sample
 
 
▪ The clock frequency is such that the counter has enough time to count to its 
highest count for a ramp duration corresponding to the maximum possible 
sample.  
 الزمالـ clock frequency تمكن الـ counter   انه يبقى عنده زمن كافي عشان يوصلألقصى  عده 
واللي ب تكون مناظرة ألعلى قيمة لإلشارة الن الكود اللي خارج من الـ binary counter   يتناسب مع
قيمة الـ sample  وبالتالي اقصى عده تكونمناظرة ألقصى  قيمة في اإلشارة  
 
The ending counts on the counter will correspond binary  equivalent value of the 
input.  
 فينهاية العد   خرج الـ binary counter  هيعبر عن المكافئالـ binary  لإلشارة   
 
Counting Quantizer  بيشتغلعلى القيم الموجبة فقط ولو  اإلشارة كان فيها قيم سالبة مثال من -10 الى 
10  هنعملshift  لإلشارة بمقدار  10 وبالتالي  الـ range الجديد: 0→20  
 
 : قوانين مهمة في حل المسائل 
time of all counts =𝑻𝒓𝒂𝒎𝒑  
 𝑇𝑟𝑎𝑚𝑝  هو زمن الوصول القصى قيمةلإلشارة  ويناظر  اقصى عده   
مثال لو اقصى قيمة لإلشارة 10 V  وعندنا 4-bit counter  وبالتالي عند تساوي قيمةالـ ramp   مع قيمة
الـ sample = 10  الزم يوصلالـ counter  آلخر عده← 1111 
 
time of one count =𝑻𝒓𝒂𝒎𝒑
𝐧𝐮𝐦𝐛𝐞𝐫  𝐨𝐟 𝐜𝐨𝐮𝐧𝐭𝐬=𝑻𝒓𝒂𝒎𝒑
𝟐𝐧 
 زمن العده الواحده والليهنستفيد منه في حساب التردد  
clock frequency of counter =𝟏
𝐭𝐢𝐦𝐞  𝐨𝐟 𝐨𝐧𝐞  𝐜𝐨𝐮𝐧𝐭   
clock frequency  هو عدد العدات counts في الثانيه الواحدة  وهو مقلوب زمن العده الواحدة 
 
 
 
 
 
 
Give the ramp slope is 𝟏𝟎𝟔 V/s, signal amplitude's range from 0 to 10 V, and a 
4-bit counter is used, what should the clock frequency be for a voice signal, 
that is a signal with maximum frequency of 3KHz.  
Solution  
 
𝑟𝑎𝑚𝑝  𝑠𝑙𝑜𝑝𝑒 =𝐷𝑅
𝑇𝑟𝑎𝑚𝑝→106=10
𝑇𝑟𝑎𝑚𝑝 
𝑇𝑟𝑎𝑚𝑝 =0.01 𝑚𝑠 
0.01 𝑚𝑠                  counter  counts  (24) 
1 𝑠                             𝑓𝑐𝑙𝑜𝑐𝑘  
𝑓𝑐𝑙𝑜𝑐𝑘 =24
0.01 𝑚𝑠=1.6 𝑀𝐻𝑧  
 
 حل اخر 
time of all counts =𝑇𝑟𝑎𝑚𝑝 =0.01 𝑚𝑠 
time of one count =𝑇𝑟𝑎𝑚𝑝
number  of counts=0.01 𝑚𝑠
24=0.625  𝜇𝑠 
𝑓𝑐𝑙𝑜𝑐𝑘 =1
0.625  𝜇𝑠=1.6 𝑀𝐻𝑧  
 
  0.01 𝑚𝑠  دا الزمن اللي هتاخدهالـ ramp   عشان توصلألعلى قيمة وهي  10 V   وبالتالي الزمالـ  
counter   يعد اقصى عده ليه في زمن اقل من او يساوي𝑇𝑠 
 
 هعمل check  عشانأتأكد ان ramp  slope    كان كافي انه يوصلنيألقصى  قيمة في اإلشارة خالل  
زمن اقل من  𝑇𝑠 
𝑓𝑠 𝑚𝑖𝑛 =2 𝑓𝑚=6𝐾𝐻𝑧  
𝑇𝑠=1
6 𝐾𝐻𝑧=0.1667  𝑚𝑠 
∴𝑇𝑟𝑎𝑚𝑝 < 𝑇𝑠  
 Example  
 
The only reason for worrying about maximum frequency of the signal is to see if the 
ramp slope is sufficient to reach the maximum possible value within one sampling 
period.  With a maximum signal frequency of 3KHz the minimum sampling rate for 
recovery is 6KHz ,so the maximum sampling period is 1
6 𝑚𝑠. 
 
Since the ramp can reach the 10 V maximum in 0.01 .it is sufficient fast to avoid 
problems.  
 
 
 
 
 
 
 
   
 
 
 
©
 Basem HeshamLinear Block Coding & Line Coding
Error Detection and Correction & 
Lecture
 
9
 
 
Error Detection and Correction هي عملية تهدف الىىا اشتفىى ف وتصىىحي  األخطىى   اللىىي ممشىىص تحصىى  
اثن   عملية االرس  . اإلف رة بيتم ارس له  فىىي وسىىط مليىى ص noise   وفيىى  Propagation problems وب لتىى لي
وارد يحص   error   في الىbits   اثن   االرس. 
 
الىى ى Channel Encoding  بيضىىيفbits  زيىى دة بنسىىميه او parity bits   عفىى ص نلمىى Error Detection and 
Correction 
 
الى  parity bits    بتزود ح جة اسمه minimum distance   م  بيص االشواد والى distance هو عدد الى bits  اللىىي
بيختلف فيه  شود عص شود اخر وش  م  زادت الى distance زادت إمش نية اشتف ف وتصحي  األخط  . 
 
هنتللم فىىي المح ضىىرة دد احىىد الطىىرن اللىىي بنلمىى  مىىص خ لهىى  Error Detection and Correction  لشىىص
هنتللم األو  مفهىىوم مهىىم وهىىو اص عىىدد األخطىى   اللىىي ممشىىص نشتفىىفه  ونصىىححه  مىىرتبط ب لىى ى minimum 
distance .وفيم  يلي فرح للمفهوم دا 
 
▪ In general, if the minimum distance between code words is 𝐷𝑚𝑖𝑛 ,errors involving up to 
𝐷𝑚𝑖𝑛−1  bits can be detected.  
▪ If 𝐷𝑚𝑖𝑛 is an even number, errors up to 𝐷𝑚𝑖𝑛
2−1 can be corrected  
▪ If 𝐷𝑚𝑖𝑛 is an odd number, errors up to 𝐷𝑚𝑖𝑛
2−1
2 can be corrected  
 
 لو عنددأد code words   وحسبت اق distance  مىى  بيىىنهم قيمتىى  بنسىىميه  (𝐷𝑚𝑖𝑛)  هقىىدر اشتفىىف errors 
عدده  يس ود  (𝐷𝑚𝑖𝑛−1)   ,    وب لنسبة لتصىىحياألخطىى    هقىىدر اصىىح   errors   عىىدده  (𝐷𝑚𝑖𝑛
2−1)    لىىو
ش نت  (𝐷𝑚𝑖𝑛)  رقم زوجي و( 𝐷𝑚𝑖𝑛
2−1
2)     لو ش نت(𝐷𝑚𝑖𝑛) رقم فردد. 
 
 𝐷𝑚𝑖𝑛   نقدر نحسبه  مص خ   مق رنة االشواد ببلضونفو ف  أد  اق  فرن م  بيص شود  والت ني  ودا هيوضىى  
اشتر مع  المس ئ  
 
For example ,by combining groups of 3 bits, we can form eight possible message words :  
000 , 001 , 010 , 011 , 100 , 101 , 110 and 111  
Since every possible 3 -bit combination is used as a message, the received 3 -bit 
combination will be identical to one of the code words.  If 101 is transmitted and an error 
occur in the third bit, 100 is received. There is no way for the receiver to know that 100 
was not the transmitted word . 
 Error Detection and Correction  
 
 مث  لو عندد 3 bits وب لت لي عندد 8 اشواد وبلت مث  101   وحص error  في ت لتbit  ووصلت100  مش
هقدر اشتفف الخطأ الص  100   دا احد االشواد المُتل رف عليهم بيصال ى transmitter  والىى ى Receiver  ومنقىىدرش
نلرف ه  دا هو الشود  األصلي اللي اتبلت وال حص  في  خطأ. 
 
في الح لة دد ش  اتنيص شود بيختلفوا عص بلض علا االق  في 1 bit  وب لتىى لي 𝐷𝑚𝑖𝑛=1  ولىىو عوضىىن  فىىي
الل قة بت عة حس ب عدد الى errors    اللي ممشص اشتففه 
 
𝐷𝑚𝑖𝑛=1 
 
 error can be detected  =𝐷𝑚𝑖𝑛−1  →1−1  =0 
 
error can be corrected  =𝐷𝑚𝑖𝑛
2−1
2  →1
2−1
2  =0 
 
No errors  can be detected  or corrected . 
 اذاً مقدرش اشتفف او  اصح  أد خطأ 
 
في المث   اللي ف ت هنضيف Even parity bit  ودد بتخلي عدد الوح يد في الشىىود زوجىىيو دد االشىىواد اللىىي 
هنحص   عليه : 
0000 , 001 1 , 010 1 , 011 0 , 100 1 , 101 0 , 110 0 , 111 1 
The minimum distance between code words is 2   
 
𝐷𝑚𝑖𝑛=2 
error can be detected  =𝐷𝑚𝑖𝑛−1  →2−1  =1   
error can be corrected  =𝐷𝑚𝑖𝑛
2−1 →2
2−1   =0  
errors up to 1 can be detected, and no errors can be corrected  
  هقدر اشتفف خطأ واحد لشص مقدرش اصحأد خطأ. 
 
Suppose that 0101 is transmitted and it is received as 0111. This is not one of the eight 
acceptable words, and the system can tell that an error was made  
 
 لو بلتن 0101  في الى transmitter  و الى  receiver استقبله 0111 اذاً الى  receive هي قىىدر ي شتفىىف الخطىىأ الص 
دا مش مص ضمص االشواد المتل رف عليه . لشص الى  receive r مش هيقدر ي صح  الخطأ الص فيىى  شىىذا احتمىى   
للشود الصحي  اللي تم ارس ل  مث   ش ص  1111  والخطأ  ش ص في او  بت او  0011   والخطأ ش ص في ت ني بت او
0101    والخطأ ش ص في ت لت بت او 0110   والخطأ ش ص في رابع بت وبم  اص التصىىحي  ممشىىص يىىتم بىى شتر مىىص
طريقة اذاً مقدرش اعم   correction  
 
 
As an example, suppose that the system consisted of three code words, 01111 , 10011 
and 01000. The minimum distance between code words is 3  
𝐷𝑚𝑖𝑛=3 
 
𝐷𝑚𝑖𝑛−1  →3−1  =2  error can be detected  
𝐷𝑚𝑖𝑛
2−1
2 →3
2−1
2   =1   error can be corrected  
في المث   دا هنقدر نشتفف لحد  2 error  وهنقدر نصح   error   .واحد فقط 
 
If 01111 were transmitted and the second bit were in error ,00111 would be received  
 
00111   مش مص ضمص االشواد المتل رف عليه  وب لت لي هنقدر نشتفف الخطأ, ونقدر نصىىح  الخطىىأ دا مىىص
خ   اص الى  receiver     بيفوف الشود اللي وص 00111     اقرب لميص مص االشواد المُتل رف عليه  وفي المث
دا هو الشود  01111. 
    احتم   اص يحص error    واحد اعلا مص احتم   حصو  اشتر مص error    وب لت لي الى receiver   وافترض اص
error   واحد بس اللي حص واخت ر  01111. 
 
 نستنتج مىىص اللىىي فىى ت اص شلمىى  زادت minimum distance 𝐷𝑚𝑖𝑛    شلمىى  زادت امش نيىى  اشتفىى ف وتصىىحي
األخط    ودا مص خ   زي دة الى parity bits  او check bits. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Algebraic codes involve the production of check bits by adding together different groups 
of information bits. The type of addition used is normal binary addition without carr y. 
 
Linear Block Coding  هي احدأنواع الى coding   المستخدمة في اشتفىى ف وتصىىحياألخطىى   حيىى  يىىتم 
إض فة check  bits  عص طريق تجميع الى information bits  والجمع المقصود هن  هو جمىىع binary  عىى دد
ولشص بدوص  .carry    يلني مث  لو بنجمع1+1  الن تج10   ولشص هن خد0  فقط وهنفي  الى carry  اللي هو بى1  
 
check bits  هي ن تج عملية الجمع بدوص carry    وممشص نلرفهىى  بطريقىىة ت نيىىة وهىىي عمىى XOR  بىىيص الىىى
information bits  وبلضه  وممشص نلرفه  بطريقة ت لت  وهي اص الىbit  المٌض فة هتخلي عىىدد الوح يىىد فىىي
الشود زوجي  (هنلم   Even parity  ودد ابسط طريقة إلض فة parity check bits) 
0 ⊕0 =0 
0 ⊕1 =1 
1 ⊕0 =1 
1 ⊕1 =0 
 
 
ممشص نلرفه  بطريق  رابل  وهي انن  لو جملن  الى message bits  مع الى check bit  جمع بدوص carry 
.الزم الن تج يطلع صفر ودد ق عده مهمة هنحت جه  قدام   
 0+0+0=0      ,     0+1+1 =0 
1+0+1 =0     ,     1+1+0 =0 
 
 في ح لة انن  ع يزيص نضيف اشتر مص parity bit    وطبل  مينفلش نحطه  بطريقة عفوائية وب لت لي يجب
اص يوجد ع قة بينه  وبيص الى Information bits   :مص خ   االتي 
 
We can generalize this type of coding. Suppose that the message word consists of m 
bits ⇒2𝑚 distinct message word.  
We add 𝑛 parity bits to the 𝑚 message bits to end up with code words of length (𝑚+
𝑛) bits. ⇒ number of code words become 2𝑚+𝑛. 
⇒ Note that of the 2𝑚+𝑛 possible code words only 2𝑚 are used  
 
 نفترض اص الى information bits   االصلية (بنقو  عليه message word  وبلدإض فة parity bit s    هنقو
عليه  code word  ) ونفترض اص عددbits   الى message   هو𝑚   وب لت لي عندن message words 2𝑚 
 مختلفيص ,وعدد الىbits   اللي هضيفهم𝑛  وب لت لي عندن  code words   2𝑚+𝑛   مختلفيص منهم2𝑚   فقط
مستخدميص يلني هم دو   فقط اللي متل رف عليهم بيص الى transmitter  والى receiver   وهم دو  اللي
هلرف اعم  مص خ لهم error detection and error correction 
 
 Linear Block Coding  
Information bits  Check bit  
 
 الى Code Word  هيبقا ب الفش  االتي حي  مص𝑎1  الا𝑎𝑚  هي message bits  و مص𝑐1  الىىا𝑐𝑛  هىىي
الى parity bits 
[𝑎1  𝑎2 𝑎3……𝑎𝑚  𝑐1  𝑐2 𝑐3……𝑐𝑛] 
Each check bit  is chosen to achieve even parity when combined with specific message 
bits. 
 الى check bit  اللي بنضيفه  عف ص تحقق even parity  مع عدد مليص مص الىىى message bits  . مىىش شلهىى
مث  لو الشود 1011  وع يز اضيف 2 parity bit  ممشص اخت ر الىbit األولىىا تحقىىق even parity   مىىع او
وت لت  bit   في الشودمث  وممشص اخت ر الى  bit   الت ني  تحقق even parity  معت لت ورابع   bit   
 الى combination  المختلفة دد علاأس س اص االشىىواد اللىىي هنحصىى  عليهىى  فىىي االخىىر ن قيهىى  بتحقىىق  
𝐷𝑚𝑖𝑛  ملين  توافق احتي ج تي في الى system  في الى Error detection and correction 
 
إض فة الى  parity bits : مش هيتم بطريقة يدوي  ولشص ب ستخدام المصفوف ت مص خ   الل قة األتية 
 
The check bits is chosen to satisfy:  
[𝐻] 𝑇̅=0 
 
 بنخت ر الى check bits    مص خضرب المصفوفتيص  [𝐻]   و𝑇̅ ونس ود ح ص  ضربهم بصفر  وهنفهم  
أد هم المصفوف ت دد ولي  بنس ود ح ص  ضربهم بصفر.  
 
𝑇̅    اسمه transmitted vector  وهي عب رة عص مصفوفة  تمث  الى Code Word   وهو عب رة عص الى
message bits  عددهم𝑚 (مص 𝑎1 الا𝑎𝑚 )   مُض ف عليه parity bits   عددهم𝑛 (مص 𝑐1  الا𝑐𝑛  )
 وب لت لي عدد صفوف المصفوفة𝑚+𝑛  .وتتشوص مص عمود واحد 
 
𝑇̅=
[            𝑎1
𝑎2
𝑎3
.
.
.
𝑎𝑚
𝑐1
𝑐2
.
.
.
𝑐𝑛]            
 
 
 
[𝐻] is 𝑛×(𝑛+𝑚) matrix  
[𝐻]=[ℎ11ℎ12…ℎ1𝑚1000…0
ℎ21ℎ22…ℎ2𝑚0100…0
..….....…0
ℎ𝑛1ℎ𝑛2…ℎ𝑛𝑚0000…1] 
 
 [𝐻]   اسمه parity check  matrix    وابل ده   ← 𝑛×(𝑛+𝑚)   حي  عدد الصفوف𝑛  وعدد
االعمدة 𝑛+𝑚 , والمصفوفة  [𝐻]    تتشوص مص جزئيص الجزاألو  عب رة عص مصفوفة مص قيم الى ℎ 
حجمه   𝑛×𝑚  والجز  الت ني عب رة عص مصفوفة الوحدة Identity matrix    حجمه𝑛×𝑛 
 
 قيمℎ عب رة عص اصف ر ووح يد  ودد القيم اللي بخت ر مص خ له  ش  parity bit   هتلم even  parity  مع
أد bits  في الى message  وهنفهم دا بيتم ازاد.  
 
مص خ   الل قة االتي  بيشوص مل ن  مجهو  وهو قيم الى  check bits  (مص 𝑐1 الا𝑐𝑛)     هنحسبهم مص خ
ضرب المصفوفتيص في بلض ب نن  نضرب ش  صف مص المصفوفة   [𝐻]  في المصفوفة𝑇̅   اللي بتتشوص
مص عمود واحد وهنحص  علا عدد  𝑛  مص المل دالتمص خ   المل دالت دد هنقدر نحسب قيم الى  𝑐 
اللي في ش  مل دلة 
 
[𝐻] 𝑇̅=[ℎ11ℎ12…ℎ1𝑚1000…0
ℎ21ℎ22…ℎ2𝑚0100…0
..….....…0
ℎ𝑛1ℎ𝑛2…ℎ𝑛𝑚0000…1] 
[            𝑎1
𝑎2
𝑎3
.
.
.
𝑎𝑚
𝑐1
𝑐2
.
.
.
𝑐𝑛]            
=0  
 
  ش  صف في  قيمℎ عدده  𝑚  هيتضربو في الى message bits مص 𝑎1 الا𝑎𝑚  وفي ش  مل دلة هيطلع
مل ن  parity bit   واحد بس الص الى parity bits  مص 𝑐1  الا𝑐𝑛 .هيتضربو في صف مص مصفوفة الوحدة   
 
 
 
 
If the number of ones is even, the sum is zero:  
ℎ11 𝑎1+ℎ12 𝑎2+⋯+ℎ1𝑚 𝑎𝑚+𝑐1=0 
ℎ21 𝑎1+ℎ22 𝑎2+⋯+ℎ2𝑚 𝑎𝑚+𝑐2=0 
  
  
ℎ𝑛1 𝑎1+ℎ𝑛2 𝑎2+⋯+ℎ𝑛𝑚 𝑎𝑚+𝑐𝑛=0 
 
 واض  اص ش  مل دلة بتطلع احد الىىى𝑐  اللىىي هىىي الىىى parity bit  وزد مىى  قولنىى  اص قىىيم الىىى ℎعبىى رة عىىص  
اصف ر ووح يد ومث  في او  مل دلة لو ش نت ℎ11=1  ,   ℎ12=0   ,  ℎ13=1  ,  وب قي قىىيمالىىى  
ℎ لحد ℎ1𝑚   :يس ووا صفر هنحص  علا المل دلة األتية 
𝑎1+ 𝑎3+𝑐1=0 
ملنا المل دلىىة دد اص او  𝑐1 parity bit  بتحقىىق Even parity  بىىيص او  وت لىىت bit (𝑎3 و 𝑎1)  فىىي الىىى
message  
 
 زد م  قولن  انن  لو جملن الى message bits  مع الى check bit  جمع بدوص carry  الزم الن تج يطلع صفر
ودا سبب انن  بنس ود ن تج ضرب المصفوفتيص بصفر. 
  
الفشرة ب ختص ر ب ستخدامن  للل قة [𝐻] 𝑇̅=0   لم  بنفشه  بتطلىىع مجموعىىة ملىى دالت شىى  مل دلىىة مىىنهم
خ صة بإيج د احد الى parity bit  وش  مل دلة فيهم عب رة عص جمع الىىى message bits  مىىع الىىى check bit 
 جمع بدوص carry  وب لت لي عف ص الى Even parity تتحقق الزم الن تج يس ود صفر وب لت لي نقىىدر نحسىىب 
الى  𝑐  ب نن  نخت ر قيمته  اللي تخلي ن تج المل دلة يس ود صفر  او نخت ر قيمة الى  𝑐   اللي تخلي عدد الوح يىىد
زوجي في المل دلة. 
 
اللي فرحن ه دا ازاد الى transmitter  بيشوص الى parity bits ازاد وهنفوف ازاد الى receiver  هيسىىتخدم
الش م دا في ان  يشتفف ويصح  االخط    بلد المسألة الج ي  اص ف   للا . 
 
 
 
 
 
 
 
 
 .
.
. 
 
 
Given the algebraic code with 4 -bit message words and 3 parity -check bits , [𝑯] is 
defined as  
[𝑯]=[𝟏𝟏𝟎𝟏𝟏𝟎𝟎
𝟏𝟎𝟏𝟏𝟎𝟏𝟎
𝟎𝟏𝟏𝟏𝟎𝟎𝟏] 
Find the code words corresponding to 16 possible message words.  
Solution  
  عندن message  عدد الىbits  يس ود4  (𝑚=4  ) وعدد الىbits parity  يس ود 3(𝑛=3)   وملطا
المصفوفة [𝐻]  اللي هحسب مص خ له  الىbits parity  
 مطلوب نجيب الى 16 code words يلني هنحسب الى bits parity   لش message word  مص 0000   الا
1111 
[𝐻] 𝑇̅=[1101100
1011010
0111001] 
[      𝑎1
𝑎2
𝑎3
𝑎4
𝑐1
𝑐2
𝑐3]      
=0 
The three equations corresponding to the matrix equation [𝐻] 𝑇̅=0 are  
𝑎1+𝑎2+𝑎4+𝑐1=0 
𝑎1+𝑎3+𝑎4+𝑐2=0 
𝑎2+𝑎3+𝑎4+𝑐3=0 
 
  ن حظ اص𝑐1  بتحقق Even parity  بيص او  وت ني ورابعbit   ,و 𝑐2  بتحقق Even parity  بيص او  وت لت
ورابع bit   ,و𝑐3  بتحقق Even parity  بيص ت ني وت لت ورابعbit 
 
 هنلوض بقيم الى message bits (𝑎1 𝑎2 𝑎3 𝑎4)   في المل دالت الس بقة ومنه  نحسبbits parity    لش
شود  
 
For message 0000  
0+0+0+𝑐1=0→  𝑐1=0 
0+0+0+𝑐2=0→  𝑐2=0 
0+0+0+𝑐3=0→  𝑐3=0 
 Example  1 
 
For message 0001  
0+0+1+𝑐1=0→  𝑐1=1 
0+0+1+𝑐2=0→  𝑐2=1 
0+0+1+𝑐3=0→  𝑐3=1 
  في ش  مل دلة في الشود 0001  عف ص يتحقق Even parity  الزم عدد الوح يد يشوص زوجي فهنحط1   في
ش  مل دلة وممشص نقوله  بطريقة ت ني  وهي الجمع بدوص carry .يس ود صفر 
 
For message 1010 
1+0+0+𝑐1=0→  𝑐1=1 
1+1+0+𝑐2=0→  𝑐2=0 
0+1+0+𝑐3=0→  𝑐3=1 
 
 وهشذا مع ب قي االحتم الت المختلفةوهنحص  منهم علا الى  code words  االتي  بلدإض فة الى parity bit  
  لش message word  والجدو  االتي يوض  ش  الى code words  بلدإض فة الى parity bits 
 
 
Code Word s Message Word s 
0000  000 0000  
0001  111  0001  
0010  011 0010  
0011  100 0011  
0100  101 0100  
0101  010 0101  
0110  110 0110  
0111  001 0111  
1000  110 1000  
1001  001 1001  
1010  101 1010  
1011  010 1011  
1100  011 1100  
1101  100 1101  
1110 000 1110 
1111  111 1111  
 
 
Suppose that transmitted vector   𝑇̅. We define an error vector 𝐸̅ which contains a “1” in 
each bit position in which an error occurs. The  received vector is therefore of the form:  
𝑅̅=𝑇̅+𝐸̅ 
 
transmitted vector  𝑇̅   عب رة عص الى message   مضٌ ف عليه  الى parity bits  وبنسمي  الى code word  
 𝑅̅ Received vector   ودا هو𝑇̅  بس بلد م  وص  للى Receiver  واللي وارد يشوص في  خطأاثن   االرس  . 
 𝐸̅ Error vector  هي مصفوفة تحتود علا اصف ر م  عداالمش ص اللي في  خطأ هن قي في  واحد و  لو الى 
transmission  صحي  الى 𝐸̅ قيمت  شله  اصف ر. 
الل قة بينهم هي اص  𝑅̅ Received vector   هي نفسه transmitted vector  𝑇̅   ولشص مُض ف علي  الى error  
 اللي حص , ومننس ش اص الجمع هن  هو جمع بدوص carry   يلني ش نن  بنلم XOR operation 
 
For Example  
[     1
1
1
0
1
1]     
=
[     1
1
1
1
1
1]     
+
[     0
0
0
1
0
0]     
 
 
 
 في المث   دا حص  خطأ في الىbit  رقم4   وبد  م  توص1  وصلت0  وب لت لي𝐸̅  شل  اصف ر م  عدا الىbit 
اللي حص  فيه  خطأ وهي bit  رقم4   
 
 في الى Transmitter    ضربن[𝐻]  في𝑇̅  وس وين  الن تج بصفر وب لت لي هنلم  نفس الش م في الى Receiver 
  هنضرب[𝐻]  في𝑅̅ 
 
[𝐻] 𝑅̅=[𝐻] [𝑇̅+𝐸̅]=[𝐻] 𝑇̅+[𝐻] 𝐸̅ 
∴[𝐻] 𝑅̅=[𝐻] 𝐸̅ 
Syndrome ( 𝑆̅) =[𝐻] 𝑅̅=[𝐻] 𝐸̅ 
 
The syndrome characterizes the specific bit error.  The result is a vector that is identical to 
one column of  𝐻, that column being the one corresponding to the bit  position in error.  
 
Syndrome ( 𝑺̅)   هو vector   بقدر اعم  بي error detection and correction   حي  اص ن تج الى
Syndrome  ين ظر احدأعمدة المصفوفة [𝐻]  ورقم اللمود دا هو رقم الىbit   اللي حص  فيه.  error 
 لو مفيشأد  error الى syndrome هيس ود صفر  𝑆̅=0 Decoding at the receiver  
𝑅̅ 𝑇̅ 𝐸̅ 
0 
 
[𝑯] defined as:  
[𝑯]=[𝟏𝟏𝟎𝟏𝟏𝟎𝟎
𝟏𝟎𝟏𝟏𝟎𝟏𝟎
𝟎𝟏𝟏𝟏𝟎𝟎𝟏] 
A message word 1010 is transmitted. A bit error occurs in the fourth bit position. 
Find the syndrome.  
Solution  
 عف ص نحسب الى Syndrome   هنجيب𝑅̅    وزد م  قولن  انه𝑇̅  ولشص مُض ف علي  الى error   اللي حص  وفي
المث   دا حدد اص الى error   حص  فيرابع bit . 
 𝑇̅ عب رة عص الى message word 1010  مُض ف عليه  الى parity bits  واللي ممشص نحسبه  ب ستخدام الل قة 
[𝐻] 𝑇̅=0 
[𝐻] 𝑇̅=[1101100
1011010
0111001] 
[      1
0
1
0
𝑐1
𝑐2
𝑐3]      
=0 
 
1+0+0+𝑐1=0→  𝑐1=1 
1+1+0+𝑐2=0→  𝑐2=0 
0+1+0+𝑐3=0→  𝑐3=1 
 
 اذاً الى code word  اللي هنبلت  هو 1010 101   وبم  ان  حدد اص حص error  في رابعbit   هيوص  للى
Receiver 1011101 
 
The transmitted code word is 1010101, and the received word is 1011101. We form the 
product [𝐻] 𝑅̅ to get  
[𝐻] 𝑅̅=[1101100
1011010
0111001] 
[      1
0
1
1
1
0
1]      
=[1
1
1] 
 Example  2 
 
 حسبن  الى Syndrome  بضرب ش  صف في المصفوفة[𝐻]  في اللمود بت ع المصفوفة𝑅̅   ومننس ش انن
بنجمع binary   بدوص carry 
1+0+0+1+1+0+0=1 
1+0+1+1+0+0+0=1 
0+0+1+1+0+0+1=1 
 
We note that the result is equal to the fourth column of [𝐻],thus identifying an error as 
having occurred in the fourth bit position.  
ن حظ اص ن تج  الى Syndrome  هو رابع عمود في المصفوفة[𝐻]   ودا ملن ه اص الى error  حص  في رابع
bit 
 
 
 لومقالش ان الخطأ حصل في الـ bit  رقم4  كنا هنعرف ازاي؟ 
 
مص خ   مق رنة قيمة الى Syndrome  معأعمدة الى  [𝐻]  اقدر اني اعرف مش ص الى error  .فيصودا مص 
خ   الل قة   
Syndrome (𝑆̅)=[𝐻] 𝑅̅=[𝐻] 𝐸̅  
 عرفن  اص المصفوفة𝐸̅   شله  اصف ر م  عدا المش ص اللي حص  في error   هنحط في  واحد فملنا اص
[𝐻] 𝑅̅   يس ود[1
1
1]    اذاً[𝐻] 𝐸̅   بردو يس ود[1
1
1]   ودا ملن ه اص𝐸̅   فيهbit  قيمت  تس ود1 وتحديدا في الى  
bit   الرابعالص الن تج فب  اللمود الرابع   
[𝐻] 𝐸̅=[1101100
1011010
0111001] 
[      0
0
0
1
0
0
0]      
=[1
1
1]   
 
 ولو ن تج الى Syndrome  طلع[0
0
0]  دا ملن ه اص مفيشأد error  وملن ه اص𝐸̅  شله  اصف ر 
 
في مسائل االمتحان هيبقى معانا معطى 𝑹̅ Received vector  و[𝑯]  ونحدد هل فيه خطأوال ال ولو   
فيه نحدد في أي bit. 
 
 
 الشرح اللي فات في حالة حصل error   واحد ولو الخطأ حصل في error 2 نحدده ازاي؟ 
 
قولن  لو  حص   خطأ في bit واحده الى Syndrome (𝑆̅)    هو عمودين ظر احد االعمدة  في المصفوفة  [𝐻] 
 ورقم اللمود هو رقم الىbit   اللي حص  فيه error 
 
لو الخطأ حص  في  bits 2  اذا الى Syndrome (𝑆̅)   هيطلععمود يس ود  مجموع عموديص  مص أعمدة  
المصفوفة [𝐻]  ورقم اللموديص هم رقمالى bits 2  اللي حص  فيهم الخطأ. 
 
 
الزم تصميم المصفوفة  [𝑯]   يكون بشكل معين النفيه حاالت ممكن النظام دا يفشل فيها  وهم:  
▪  لو مجموع عموديص مصالمصفوفة [𝐻] يس ود عمود في نفس المصفوفة وفي الح لة دد حص  2 
error ولشص الى Receiver  هيفسره ان  error  فيbit  واحده بس الن  دايم  بيرج  حدو  عدد األخط   
االق . 
 
▪ لو مجموع عموديص في نفس المصفوفة يس ود مجموع عموديص ت نييص وهن  مش هيقدر يحدد أد  bits 
2  اللي حص  فيهم error. 
 
▪  لو عندد خطأ في bits 2 واللموديص اللي فيهم الخطأ مجموع هم  يس ود صفر  و الى Syndrome (𝑆̅)  
طلع بصفر  شدا هيفسر اص محصلش error 
 
  الح  في انن  نليد تصميم المصفوفة[𝐻] .بحي  نت فا المف ش  الس بقة 
 
 
مالحظة مهم ة :  ممشص نلرف الى minimum  distance   بيص الى code  words ب ستخدام المصفوفة  [𝐻] 
 ودامص خ    ح س ب عدد الوح يد  في ش  صف  ونفوف الصف اللي في  اق  عدد مص الوح يد واللدد دا 
هو الى minimum  distance  
  
    [𝐻]=[11001000
10100100
01010010
00110001] 
 
 في المث   دا عدد الوح يد في ش  الصفوف هو3 اذا  𝐷𝑚𝑖𝑛=3 ←  
 فياالمتح ص الزم نحسب الى code words شلهم و الم حظة دد عف ص نت شد اص الى code words  و الى  
𝐷𝑚𝑖𝑛   اللي حسبن همص   
 
 
 
converts digital bits into digital signal.  
 هي عملية التحوي  مص digital data  الا digital signal  
 
 في الخطوات الس بقة حصلن  علا code word    ودا اللي نقصد بي digital data  ألنن لحد االص مل ن  شود  
عب رة عص اصف ر ووح يد وع يزيص نلبر عن  بقيم الى volts  اللي هنبلت  بيه  ودا اللي نقصد بي  الى digital 
signal  وشن  قولن  اصاإلف رة اللي بنحص  عليه  بلد عملية الى Line Coding  بنسميه  Baseband 
Signal  وهي دداإلف رة اللي هنلمله  modulation   في البلوك اللي اسم Carrier Modulator 
 
  يوجد طرن مختلفة المستخدمة في تمثي  البي ن ت الرقمية المرسلة ولش  منه  خص ئص ومميزات وعيوب
وفيم  يلي الطرن المستخدمة في الى Line Coding   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Line Coding  
Differential 
format  
Differential 
with Biphase  
 
Transmits +V volts for digital 1 and -V volts for digital 0.  
 
   في هذا النوع يتم تمثي binary 1  بى pulse  موجبة+V   وتمثي binary 0  بى pulse  س لبة-V  
 
 
 
 
 
 
1. When the data is static (no change from bit interval to bit interval) , there are no 
transition the transmitted waveform. This can cause serious timing problem when we 
try to establish bit synchronization. Establishing the proper clock signals requires 
transitions in the received waveform  
 
   لو بلت مث  عدد اصف ر شبير ورا بلض او عدد وح يد شبير مش هيحصىى transition    للىىى  signal   لفتىىرة
ودا هيسبب مف ش  في الى timing  وال ى  synchronization   الص بيشوص فيىى clock  فىىي transmitter  و clock 
  في receiver    االتنيص دو  الزم يشونوا synchronized    مع بلض فلحظ ت االنتق   دد هي اللىىي بتسىى عدن
في تحقيق ا  synchronization 
 
 
2. If the levels are reversed during transmission (i.e. +V is interpreted as -V at the 
receiver), the entire data train will be inverted,  and every bit will be in error  
 
  لو الد سبب مص األسب ب اإلف رة اتقلبت180   درجة ش  االصف ر هتبقا وح يد وش  الوح يد هتبقااصف ر 
والدات  شله  بقت غلط 
 
For these reasons we often choose differential forms of encoding. In such techniques, the 
data are represented as changes in levels rather than  the practical level signal.  
 
 ب ستخدام NRZ–L  هنحص  علاأنواع أخرى  نح  فيه  الليوب الس بقة وفي  أنواع هتح  عيب واحد فقط  
واخر نوع هيح  المفشلتيص.  
 
 
 
1. Non Return to Zero –L  (NRZ  – L) 
Disadvantages of NRZ  – L +V 
-V 
 
 
A data 1 is represented by a change in level between two consecutive bit times which a 
data 0 is represented by no change.  
 هنلبر عص1   بى change in level   يلني لو ش نتاإلف رة عند +V  هنلشسه  وتبقا-V    واللشس يلني لو
س لب هنقلبه  موجب ,ونلبر عص 0  بى change in level no  يلنياإلف رة هتشم  علا نفس الى level   مص
غير تغيير.  
 
 
 
في األو   بنحدد Level نبدأ بي  وممشص نبدأ مص  +V  زد المث   دا او نبدأ مص-V   
 
 
 
 
We start with NRZ–L and excl usive with the delayed NRZ–M  
 
  لو مل ناإلف رة NRZ–L  هنستخدمه  في الحصو  علا NRZ–M  :ب ستخدام الدايرة اللي في الصورة 
 
 
 
 T دد delay circuit   بتلم delay  بمقدار interval  .واحد 
 
بنلم  XOR  بيص NRZ–L  و NRZ–M delayed  والمقصود بى delayed  انه  مت خرة بى interval  واحده
وزد م  قولن  اص NRZ–M  بتبدأ بىlevel  مليصاحن  بنحدده وفي الغ لب بيشوص  +V   
 
2.  Non Return to Zero –M (NRZ –M) 
NRZ–M Encoder  +V 
 
 بنبدأ مث  بى+V   وهنلم XOR  بيص+V  اللي هنلبر عنه  بى1    وبيص اوlevel  في NRZ–L    واللي هو هن
+V   ,بلد شدا نلم XOR   بيص او NRZ–M  اللي حسبن ه في الخطوة اللي قبله  وبيص ت ني  level   في
NRZ–L وهشذا 
 
1          ⊕        1    =    0 
 
 
 
                                           0  ⊕   1    =    1 
 
 
مص خ   الرسم هن قي  مص خ   الدايرة الس بقة اقدر احص  علا NRZ–M  ب ستخدام NRZ–L 
 
 
 
 
 
NRZ–S is the same as NRZ–M except that the two outputs are reversed.  
 هي نفس فشرة NRZ–M  ولشص هنلشس وهنلبر عص1  بى change in level no  وهنلبر عص0   بى change 
in level  
 
 
 
 ش  الطريقتيص NRZ–S   و NRZ–M  بنسميهم Differential format  ألنن مبنلبرش عص الى Levels   بقيم
ث بت  ولشص بنلبر بى change  او no change 
 
NRZ–S and NRZ–M differential systems have solved the problem of waveform inversion 
but the not solved the problem of timing information.  
 
3.  Non Return to Zero –S (NRZ –S) NRZ–M delayed  NRZ–L 
NRZ–M delayed  NRZ–L NRZ–M  
NRZ–M  
+V 
 
 الطريقتيص NRZ–S  و NRZ–M  حلو المفشلة الت ني  وهي لواإلف رة حصله  phase shift  بى180   الدات
هتوص  ص .  
مث  لو جربن  وعشسن  رسمة الى NRZ–S   في المث   اللي ف ت وبردو+V    اللي بدأن  بيههتتلشس مل هم  
وب لت لي مص الرسمة اللي حصلن  عليه  هيطلع مل ن  نفس الشود الص 1  لس  يلبر عن  بى change in level 
no  و0   يلبر عن  بى change in level  وب لت لي لم  يوص  للى receiver  هيقرأ الدات  ص ,ولشص مفشلة   
الى Synchronization  .لس  موجودة 
 
 
 
 
الى binary 1  هلبرعن  بى +V  في النص األو  و  -V في النص الت ني والى binary 0  هلبرعن  بى -V  في
النص األو  و  +V  في النص الت ني 
 
 
 
ممشص اللشس والى binary 1  هلبرعن  بى -V  في النص األو  و +V   في النص الت ني المهم يشوص في
transition  في نص الى interval    وش  الطريقتيص ص  بس غ لب  بنستخدم الطريقة الت ني 
 
 
 
This format overcome this static data and timing problem.  
The problem of signal inversion during transmission or reception will still result in 
bit reversed  
 
 النوع دا  ح   ال مفشلة  االولا  ال ى   synchronization  الص الى  transitions  دايم  موجود في نص الى interval، ولشىىص 
المفشلة الت ني  لس  موجوده  وهي اص لو اإلف رة اتقلبت 180 درجة الدات  شله  هتقرأ بفش  خ طئ. 
 
 
4.  Biphase  – L 
 
 
  بيشوص مل ن– L  NRZ   وع يزيص نجيب منه Biphase – L     ودا مص خعم  XOR بىىيص – L  NRZ  والىىى 
clock  
 
 
 
 الى clock  بتشوص+ve  في النصاألو  و -ve  في النص الت ني خ   الى interval  وب لت لي لو binary 1   مىىث
وعملن   XOR  مع الى clock  هنحص  علا-ve  في النصاألو   و  +ve في النص الت ني 
1   ⊕  1    =  0      ,     1    ⊕    0    =   1 
 
 لو binary 0    وعملن XOR  مع الى clock  هنحص  علا+ve  في النصاألو   و  -ve في النص الت ني 
0    ⊕    1 =    1          ,          0   ⊕  0    =   0 
 
   اذاً مص خ   عم XOR بيص  – L  NRZ  والى clock  قدرن  نحص  علا Biphase – L  
 
 
 
 
 
 
 
 
 
 
 
 
Biphase  – L Encoder  
NRZ – L 
Biphase – L 
 
 
 
We can combine differential encoding with biphase encoding to solve both problems.  
 
In the Biphase –M & Biphase –S formats a transition occurs at the beginning of every bit 
period , in Biphase –M a data 1 is represented by an additional transition in the 
midperiod, while a 0 is represented by no period transition .  
 
in Biphase –S a data 1 results in no mid period transition, while data 0 results in a mid -
period transition  
 
  ش ص عندن  مفشلتيصاألولا  خ صة ب لى  Synchronization    وحلين ه  ب ستخدام Biphase–L     والمفشلة الت نيىى
ش نت في الى Phase shift 180  وحلين ه  ب ستخدام الى Differential format  ( NRZ–S NRZ–M and) 
 
 فىىي النىىوع دا هنىىدمج بىىيص الطىىريقتيص عفىى ص نقىىدر نتغلىىب علىىا المفىىشلتيص مىىص خىى   اسىىتخدام فشىىرة الىىى
Biphase–L    في انن  نلم transition  فيش  interval  وفشىىرة الىىى Differential format  فىىي اننىى  منلبىىرش
عص الى level  بقيمث بت ،  ولشص هنقدر ندمج م  بيص النوعيص ازاد؟ 
 
في بداية ش   bit     بنلم transition    وفي النوع Biphase –M     هنمث  في1    بى change    في نىىص الىىى interval 
   بينم0    هنمثل  فيbit    ش مل  يلني بلد م  نلم transition  في بدايىىة الىىىbit   لىىو لقينىى0   هنسىىيب القيمىىة ث بتىى
اللي جبن ه  بلد الى transition    مص غير م  نلم transition   في نص الى interval 
 
   اللشس ب لنسبة للنوع Biphase –S     هنمث  في0    بى change   فىىي نىىص الىىى interval   بينمىى1  هنمثلىى  فىىيbit 
 ش مل 
 
 
 
 
 
 
في النوعيص دو  بنخت ر بردو Level  نبدأ بي  وفي المث   اللي ف ت بدأ بى-V 
 
 
 
 
 
 5.  Biphase  – M  and  Biphase–S 
 
 
 لو الدخ  ش ص NRZ–M   الخرج Biphase –S  ولو الدخ  ش ص NRZ–S  الخرج Biphase –M    وبنلم
delay  للدخ  بمقدار نص الى Interval T/2 
 
 
 
NRZ–M →Biphase–S  
NRZ–S →Biphase–M  
 
  في المث   دا الدخ  NRZ–M   وعملن delay  لي  بمقدار نص interval   وعملن XOR  لي  مع الى clock  هن قي
اص الخرج اللي حصلن  عليه  هو  Biphase –S    
 
 الفشىىىرة اص NRZ–M   بتلمىىى change  عنىىىد1 
   وب لت لي لم  نلم delay    بىT/2    شدا الى change 
  هيحص  في نص الى interval     اللي فيه1  ولشىص
عف ص نحصى  علىا  Biphase –S    الزم يحصى
change     في بداية ش interval    ف لح  انن  نلمى
XOR  مىىع الىىى clock   الص زد مىى  قولنىى  حصىى
delay  قيمتىى  نىىص الىىى interval  ملنىىا شىىدا اص
ض مص اص في  قيم  ث بت   قب  بداية الىى  bit   بىنص
interval  وبلده  بنص interval 
  وب لت ليلم  يتلمله  XOR  مره مىع1  ومىره مىع
0    هضمص اصهيحصله   change    فىي بدايىة شى
bit  
   والقىيم اللىي ش نىت ث بتى  خى0 bit   هيحصىىله
change    في نص الى interval   والقيم اللي ش نىت
بيحصله   change      خ1  bit   هتثبتألنهى  لمى  
يتلملهى  XOR  مىىره مىىع واحىىد ومىىره مىىع صىىفر
وهي نفسه  بتتغير مىره بواحىد ومىره بصىفر او 
اللشس هتدد رقم ث بت في االخر 
 
 
 
شدا المح ضرة خلصت والجز  اللي جي دا جدعنة مص عندد  
Biphase  – M  and  Biphase  – S  Encoder  
 Encoder  
 
 
 ان  ع رف اص المح ضرة ملي نة رغي شتير ومش ع رف ميص فيص وتحفظ اي  ف  اخوك مش ن سيك  وج يبلك اسئلة مص  
امتح ن ت قديمة علا الجز  دا بحي  نلرف الش م دا بيتسأ  في  ازاد وشدا. 
 
 
 
An 8 -bit code word is formed by adding 4 parity bits to the 4 information bits. The parity bits are 
given by  
𝑪𝟏=𝒂𝟏 ⊕𝒂𝟐          𝑪𝟐=𝒂𝟏 ⊕𝒂𝟑             𝑪𝟑=𝒂𝟐 ⊕𝒂𝟒             𝑪𝟒=𝒂𝟑 ⊕𝒂𝟒  
 
i. find the minimum distance between code words.  
ii. How many errors can be detected? How many errors can be corrected?  
iii. If the received code word 10001111, find the transmitted code word?  
Solution  
 
i. find the minimum distance between code words.  
  لملرفة الى𝐷𝑚𝑖𝑛  هنجيب الى code words    مص خحس ب الى parity bits  مص المل دالت الملط ة في السؤا   
For message 0000  
𝐶1=0 ⊕0=0             𝐶2=0 ⊕0=0 
𝐶3=0 ⊕0=0             𝐶4=0 ⊕0=0 
 
 
For message 000 1 
𝐶1=0 ⊕0=0             𝐶2=0 ⊕0=0 
𝐶3=0 ⊕1=1             𝐶4=0 ⊕1=1 
 
For message 0010  
𝐶1=0 ⊕0=0             𝐶2=0 ⊕1=1 
𝐶3=0 ⊕0=0             𝐶4=1 ⊕0=1 
 
For message 0011  
𝐶1=0 ⊕0=0             𝐶2=0 ⊕1=1 
𝐶3=0 ⊕1=1             𝐶4=1 ⊕1=0 
 
 
  وهشذا في ب قي الى message words  حتا نحص  علا الى code words   في الجدو  االتي:  
  
  [Extra] Related Exam s Questions  
Question 1 (Final 2020)  
 Encoder  
 
Code Word s Message Word s 
0000  0000  0000  
0001  0011   0001  
0010  0101  0010  
0011  0110  0011  
0100  1010  0100  
0101  1001  0101  
0110  1111  0110  
0111  1100  0111  
1000  1100  1000  
1001  1111  1001  
1010  1001  1010  
1011  1010  1011  
1100  0110  1100  
1101  0101  1101  
1110  0011  1110  
1111  0000  1111  
 
∴𝑫𝒎𝒊𝒏=𝟑  
 مص االشواد اللي حصلن  عليه  هن قي ش  شود يختلف عص االخر علااألق  في 3 bits   
 عرفن  دا مص خ   مق رنة االشواد ببلض هن قي اص اخر اتنيص code word   يختلفوا في3-bit 
 
 
ii.  How many errors can be detected? How many errors can be corrected?  
 
error can be detected  =𝐷𝑚𝑖𝑛−1  →3−1  =2 bits  
error can be corrected =𝐷𝑚𝑖𝑛
2−1 →3
2−1
2   =1  bit  
 
iii. If the received code word 10001111, find the transmitted code word?  
 عف ص نجيب الشود اللي بلتن ه الزم نجيب الى syndrome   عف ص نلرف لو حص error   . او ال ولو في  نصلح 
Syndrome ( 𝑆̅) =[𝐻] 𝑅̅ 
 
 𝑅̅ ملطا  ولشص هنحت ج نجيب [𝐻]   ودا نقدر نجيب  مص خ   مل دالت الى parity bits   وعدد الى information bits    والى
parity bits  الملط ة فيالسؤا  
 
 
𝑚=4  (message bits) , 𝑛=4   (parity bits)  
Size of [𝐻]→𝑛 ×(𝑛+𝑚)→4 ×8  
  اذاً حجم المصفوفة [𝐻]    هو4 ×8    ,وشن  قولن  اص المصفوفة بتتشوص مص جزئيص الجز  الت ني عب رة عص unity 
matrix     حجمه𝑛 ×𝑛    وب لت لي في المث   دا حجمه4 ×4 
 
  ب قي شدا قيم الىℎ ودد هنجيبه  مص المل دالت الملط ة مث  او  مل دلة  𝐶1=𝑎1 ⊕𝑎2    وممشص نوصفه  بفش  ت ني
واص مجموع الى information bits  والى parity bit   جمع بدوص carry   الن تج يس ود صفر 𝑎1+𝑎2+𝐶1=0  
  اذاً مص خ   الل قة [𝐻]𝑇̅=0   هنحط قيمℎ اللي تن ظر القيم الموجوده في ش  مل دلة يلني مث  او  مل دلة هنحط  
او  قيمتيص في الصف األو   بواحد والقيمتيص الت نييص بصفر  عف ص لم  نضربه  في مصفوفة  𝑇̅     نجيب𝑎1   و𝑎2  , وهشذا
في ب قي المل دالت لحد م  نجيب ب قي قيم الى  ℎ   
[𝐻]𝑇̅=[ℎ11ℎ12ℎ13ℎ141000
ℎ21ℎ22ℎ23ℎ240100
ℎ31ℎ32ℎ33ℎ340010
ℎ41ℎ42ℎ43ℎ440001]
[       𝑎1
𝑎2
𝑎3
𝑎4
𝑐1
𝑐2
𝑐3
𝑐4]       
=0 
  مص المل دلة الس بقة هنقدر نفهم اشتر ازاد اخت رن  قيمℎ   
 
 
    [𝐻]=[11001000
10100100
01010010
00110001] 
 
 
 
𝑆̅=[𝐻] 𝑅̅=[11001000
10100100
01010010
00110001] 
[       1
0
0
0
1
1
1
1]       
=[0
0
1
1] 
 𝑆̅  هي نفس اللمود4   في المصفوفة[𝐻]  وب لت لي الخطأ حص  في الىbit  رقم4   وب لت لي نلد  قيمت  مص0  الا1 
 
∴transmitted  code= 100𝟏1111  
 
 𝑎1 𝑎2 𝑎3 𝑎4 𝑐1 𝑐2 𝑐3 𝑐4 
 
i. Explain the disadvantages of NRZ -L coding format.  
ii. State and explain a technique which overcomes these disadvantages.   
iii. Sketch an encoder circuit for the technique explain ed in (ii).  
Solution  
 
i.  Explain the disadvantages of NRZ -L coding format.  
1. When the data is static (no change from bit interval to bit interval), there are no transition the 
transmitted waveform. This can cause serious timing problem when we try to establish bit 
synchronization. Establishing the proper clock signals requires transitions in the received waveform  
 
2. If the levels are reversed during transmission (i.e. +V is interpreted as -V at the receiver), the entire 
data train will be inverted, and every bit will be in error.  
 
ii. State and explain a technique which overcomes these disadvantages.    
Biphase – M  or  Biphase –S can overcome these  disadvantages.  
 We can combine differential encoding with biphase encoding to solve both problems.  
 
In the Biphase –M & Biphase –S formats a transition occurs at the beginning of every bit period , in 
Biphase –M a data 1 is represented by an additional transition in the midperiod, while a 0 is 
represented by no period transition.  
in Biphase –S a data 1 results in no mid period transition, while data 0 results in a mid -period 
transition.  
 
iii. Sketch an encoder circuit for the technique explained in (ii).  
 
 
 
NRZ–M →Biphase–S  
 
 
NRZ–S →Biphase–M  
 
Question 2 (Final 20 18) 
 Encoder  
 
Given the algebraic code with 3 bit message words and 3 parity check bits  
[𝑯] is defined as  
[𝑯]=[𝟏𝟎𝟏𝟏𝟎𝟎
𝟎𝟏𝟏𝟎𝟏𝟎
𝟏𝟏𝟎𝟎𝟎𝟏] 
If the receiver receives 100011, determine the corresponding data word.  
Solution  
𝑆̅=[𝐻] × 𝑅̅=[101100
011010
110001]×
[     1
0
0
0
1
1]     
=[1
1
0] 
So there error and it's in bit 3. So the correct word is 101011  
 
 
 
 
Discuss the advantages and disadvantages of NRZ -L, NRZ -M and biphase -L formats?  
Solution  
NRZ-L 
Advantage: It’s simple.  
Disadvantages:  
1- When the data is static, timing (synchronization) is lost.  
2- Data inversion: If 180 phase shift occurred in the received data , it will be translated wrong where 0 will be 
1 , and 1 will be 0.  
 
NRZ-M 
Advantage: It solves the phase shift problem  
Disadvantage: It doesn’t solve the synchronization problem.  
 
Biphase -L 
Advantage: It solves the synchronization problem.  
Disadvantage: It doesn’t solve the phase shift problem . 
 
 Question 4 (Final 20 16) 
 Encoder  Question 3 (Final 20 16) 
 Encoder  
 
A 1-kHz, 2V Pk -Pk sine wave is sampled 5000 times each second and encoding using  
3-bit PCM.  
(a) Sketch the waveform for the first 2 msec assuming that the NRZ -L format is used.  
(b) Repeat part (a) for the NRZ -M differential format.  
(c) Repeat part (a) for the biphase -L format  
Solution  
 فشرةالسؤا  انن  هنرسم رسمة  sine wave   تردده 1KHz   ونلمله sampling  بتردد 𝑓𝑠=5000 𝐻𝑧   يلني ن خد sample    ش
0.2 ms    ومطلوب نلمله encoding  في 3 bits   يلني هنقسم الرسمة لى 8 Levels   ونحص  علا الشود بت عاإلف رة خ   فترة  
2ms   ونرسمه  بى NRZ-L   و NRZ-M  و biphase -L 
 
𝑓𝑚=1000 𝐻𝑧  
𝐴=1 𝑉 
𝑥(𝑡)=𝐴sin(2𝜋 𝑓𝑚 𝑡)=sin(2000𝜋𝑡) 
 
𝑛=3 
𝐿=2𝑛=23=8 
∆𝑠=𝐷𝑅
𝐿=2
8=0.25 
 هنجيب الشود مص الرسم عص طريق انن  نفوف ش  sample  تقع فيأد  range 
at t=0 → 100  ,  at t=0.2 ms → 111  ,   at t=0.4ms → 110  ,  at t=0.6ms → 001  ,  at t=0.8ms → 000 
at t=1ms → 100   
and it’s repeated for the second period  
at t=1.2 ms → 111  ,   at t=1.4ms → 110  ,  at t=1.6ms → 001  ,  at t=1.8ms → 000  ,  at t=2ms → 100 
 
so the binary code of the 2  msec from the signal is  
100 111 110 001 000 100 111 110 001 000 100 
 
 
 
 
 
Question 5 (Final 20 16) 
 Encoder  
000 001 010 011 100 101 110 111 
 
 
Output digital bits
 
 
 
 
 
 
هتفق معاك
 
ان 
ا
لكالم في المحاضرة دي كان كتير وكل شوية بنكرر حاجات بس صدقني.. 
 
 
 
 
 
 

 
 
 
 
 
 
 
  
 
©
 
Basem Hesham
Matched Filter & Amplitude Shift Keying
Lecture
 
10
 
 
 وصلنا المحاضرة اللي فاتت لحد Line Coding  وعرفناأي الطرق المختلفة اللي بنمثل بيها االصفار والوحايد وبنحصلل 
في الخطوة دي على اللل signal Baseband  وبعلد دلدا المفلروع نعملل modulation لإلشلارة دي عشلا  ندلدر نبعتهلا 
لمسافات طويلة وهندرس الطرق المختلفة لعمل الل modulation   ولداألول هنتعلم حاجة مهملة وهلي Matched Filter 
  ودا فلتر موجود في الل Receiver    وظيفته انه يحل مشدلة الل noise    الللي بتلرثر عللىاإلشلارة  الللي بعلد ملا عمللت ليهلا 
modulation  وبعتها على الل channel. 
 
المثال والرسم االتي توضيح بشدل مبسط لفدرة الل matched filter  
 
 
 
 
نفترع ا  عندنا اشارة 𝑠(𝑡)  هنندلها في channel  بتتعرع لل noise 𝑛(𝑡)   وهنفترع انهلا AWGN  يعنلي اللل noise 
 متساوية في قيمة الباور في جميع الترددات وتساوي𝑁𝑜
2 . 
 
  بعد ما تعدياإلشارة  𝑠(𝑡)    خالل الل channel    وبعد ما تتعرع لل noise    هتخلراإلشلارة 𝑠(𝑡)+𝑛(𝑡)   ومل  الرسلم
واضح ا  اإلشارة شدلها اتغير وصعب نفرق فيها بي   1  و0  وهنا ييجي دور اللل matched filter  وهلو ملب بيشليل اللل
noise    ولد  بيزود ترثير الل signal    مدارنة بالل noise    وبنحصل منه علىاإلشارة  𝑦(𝑡)    وهي ملب شلبه 𝑠(𝑡)   بالضلبط
ولد  مل  السلهل التعلرل عللى 1  و0  فيهلا للو دخلناهلا عللى decision circuit   وحلددناvolt threshold  للو زاد عنله
هنعتبره بل 1  ولو قل عنه هنعتبره بل0 , واحنا بنعملل threshold  بناخلد decision  دلل Time period T  والللي بتمثلل اللل
duration  بتاعة الل pulse. 
 
 في المحاضرة دي هنتعلممعادلة الل impulse response ℎ(𝑡)   لللل matched filter  وازاي نعملل implementation  ليله
وهنعرل ازاي بيزود ترثير الل signal  مدارنلة باللل noise,  وهنلتعلم ازاي نحسلب احتملال الخطلر Probability of error 
(𝑝𝑒) وم  خاللها نددر  نحسب دفاءة وأداء الل digital modulation techniques ( ASK , FSK , PSK). 
 
 اللي فات دا دا  مددمة م  عندي لتوضيح الفدرة قبل ما نبدأ في المحاضرة 
 
 
 
 
 
 
 
 
 
 
Matched Filter  
𝑠(𝑡) 𝑛(𝑡) 𝑠(𝑡)+𝑛(𝑡) ℎ(𝑡) 𝑦(𝑡) 𝑡=𝑇 
 
The matched filter is one technique for processing the received signal. The criterion for designing the 
filter is that the output at a sample T is to have maximum signal to noise ratio.  
 
 
Matched Filter هو فلتر موجود في الل Receiver ووظيفته هو زيلادة تلرثير اللل signal  بالنسلبة لللل noise  يعنلي زيلادة
SNR    ,والطريدة اللي بيتم االعتماد عليها في تصميم النوع دا هي اننا بناخد sample    بعد دل زمT  ويحدد الللي اتبعلت
دا واحد وال صفر. 
 
نفترع ا  اإلشارة اللي بعتناها 𝑠(𝑡)  مضال عليها noise 𝑛(𝑡)   ووصلت 𝑠(𝑡)+𝑛(𝑡)   عند اللل Receiver  ,هناخلد
اإلشارة بعد ما اتضال عليهلا noise  ونلدخلها عللى Matched Filter بلل impulse response ℎ(𝑡)   ,وهنفتلرع ا  اللل
noise  هي AWGN  يعني ليها power spectral density ثابته قيمتها 𝑁𝑜
2 ( وحدتها watt/Hz  ) 
 
Let us assume that 𝑛(𝑡) is white noise with power spectral density 𝑁𝑜
2 , the filter impulse response then 
becomes  
ℎ(𝑡)=1
2 𝐶 𝑁𝑜 𝑠(𝑇−𝑡) 
 𝐶  هي ثابت و𝑁𝑜
2  زي قولنا انها  power spectral densityقيمتها ثابته في دلل التلرددات  وبالتلالي ممدل  نعلوع عل  
الجزء 1
2𝐶𝑁𝑜  بثابت هنسميه 𝐾  
ℎ(𝑡)=𝐾 𝑠(𝑇−𝑡) 
the impulse response of the optimum filter, except for the factor K,  is a time reversed and delayed 
version of the input signal, that is, it is "matched" to the input signal.  
 
 م  المعادلة نالحظ ا  عالقة الل impulse response ℎ(𝑡)   بتاعة الللfilter  تشلبه جلدا معادللة اللل input signal   وممدل
ندول انها هي نفسها ولد  time reversed   الإشلارة 𝑡  بالسلالب و  delayed بمدلدار𝑇 ,  ودا الللي هيخللي الفلتلر يعملل
detect لإلشارة بشدل صحيح وبما انهم شبه بعع بندول بينهم matching  
 
 دا سبب تسميته Matched Filter  ال  الل impulse Response   بتاعته بتدو matching  مع الل input ألنها نفلس الشلدل 
ولد  time reversed  وفيهاshift  بمددارT 
 
 
 
 
 
 
 

 
 
 
 
العالقة بي  الخر  والدخل في أي فلتر في الل time domain  هي convolution  بلي  اللدخل 𝑠(𝑡)+𝑛(𝑡)   واللل
impulse response ℎ(𝑡)  
𝑠𝑜(𝑡)+𝑛𝑜(𝑡)=[𝑠(𝑡)+𝑛(𝑡)]∗ℎ(𝑡) 
=∫[𝑠(𝜏)+𝑛(𝜏)]𝑡
0ℎ(𝑡−𝜏) 𝑑𝜏 
 
ℎ(𝑡)=𝐾 𝑠(𝑇−𝑡) 
∴ℎ(𝑡−𝜏)=𝐾 𝑠(𝑇−𝑡+𝜏) 
 
𝑠𝑜(𝑡)+𝑛𝑜(𝑡)=𝑘∫[𝑠(𝜏)+𝑛(𝜏)]𝑡
0𝑠(𝑇−𝑡+𝜏) 𝑑𝜏 
 
 الل decision   بتاع الفلتر بيتم عند دل𝑇  فهنشيل دل 𝑡  في المعادلةونعوع عنها بل   𝑇 
At 𝑡=𝑇 
𝑠𝑜(𝑇)+𝑛𝑜(𝑇)=𝑘∫[𝑠(𝜏)+𝑛(𝜏)]𝑇
0𝑠(𝜏) 𝑑𝜏 
 
 
 
 
 
 
 
The operation being performed by the system is called correlation (i.e. multiply two time functions 
together and integrate the product).  For that reason, the matched filter is sometimes called correlator . 
 فدرة الفلتر انه بياخد الدخل المضال عليه noise  ويضربه فياإلشارة م  غير noise  ويداملل حاصلل الضلرب ودنلا
بندللول علللى تدامللل حاصللل ضللرب اشللارتي  هللو correlation  وعشللا  دللدا الللل matched filter  ليلله اسللم تللاني وهللو
correlator وهنشول قدام ازاي بنستفيد م  العالقة دي في حل مشدلة الل noise. 
 
أي فلتر الل operation  بتاعته هي convolution  وفلي اللل matched filter  لملا عملنلا Implementation  ليله لدينلا ا  اللل
convolution  اللي بيتم عنده يدافئ correlation . 
 
 
 
 Implementation of Matched Filter  
∫  𝑑𝑡𝑇
0 
 
 
 
 
 
 
 
 
 
 
 
 
Binary Matched 
Filter
 
  دا نوع م
أنواع
 
الل 
matched filter
 
 خاص باستدبال
اإلشلارات 
 
الرقميلة يعنلي 
اإلشلارة
 
عبلارة 
ع  اصفار ووحايد وبالتالي عندي نوعي  م  
𝑠
(
𝑡
)
 
 واحده للل
1
 
 هنسميها
𝑠
1
(
𝑡
)
  
وأخرى
 
للل 
0
 
 هنسميها
𝑠
2
(
𝑡
)
 
.
 
المثللال االتللي هيوضللح فدللرة العمللل
 
وفللي المثللال دا للتسللهيل هنفتللرع  اننللا معملنللاب 
modulation
 
 وبعتنللا
إشللارة
 
الللل 
Baseband
 
 
 وهتوصل الل
Receiver
   
 و
 
نفتللرع اننللا بنعبللر علل  
binary 1
 
بإشللارة
 
بالشللدل االتللي      
 
نتيجة الل 
noise
  
 
 وهتوصللللل الللللل
Receiver
 
 
  هنعبللر علل
binary 
0
 
بإشللارة
 
بالشللدل االتللي       
 
 
يعني عملنا 
Line Coding
 
 بل
Biphase
-
L
 
 والل
Receiver
 
 عارل نوع الل
Line Coding
 
 الللي شلغال بيله اللل
transmitter
 
 وبالتالي عارل شدل الل
binary 1
 
 والل
binary 0
 
 م  غير
noise
 
 
 الل
Receiver
 
وصلت
ل
ه الل 
noisy signal
 
 وهو مب عارل
اإلشارة
 
دي عبارة ع  
أي
 
وعشا  يحدد هي 
أي
 
هيضربها مره 
في 
𝑠
1
(
𝑡
)
 
 اللي هي
إشارة
 
binary 1
 
 م  غير
noise
 
 ويضربها في في
𝑠
2
(
𝑡
)
 
 اللي هي
إشارة
 
binary 0
 
 م  غيلر
noise
 
 ويدامل حاصل الضرب. ناتج الل
branch
 
 اللي فوق عبارة ع  مدى تشابة الل
noisy signal
 
 مع
إشلارة
 
اللل 
binary 1
 
  مل
غير 
noise
 
و
 
ناتج الل 
branch
 
 اللي تحت عبارة ع  مدى تشابة الل
noisy signal
 
 مع
إشارة
 
الل 
binary 0
 
 م  غير
noise
 
 .يعني دانه بيحسب الل
correlation
 
 لللل
noisy signal
 
 ملع
1
 
 وملع
0
 . بعلد دلدا بياخلد الديملة فلي اللل
branch
 
 الللي فلوق
يطرحها م 
 
في الل 
branch
 
 اللي تحت والناتج اللي طلع هندارنه بديمة
threshold 
𝑦
𝑜
 
 لو ناتج الطرح اعلى مل  قيملة اللل
threshold
  
 دا معناه ا  الل
correlation
 
 في الل
branch
 
 اللي فوق ادبر م  اللي تحت
 
يعني 
اإلشارة
 
شلدلها اقلرب للل 
1
 
 اذا
اللي 
noisy signal
 
 دي دانت
1
 
 والعدس لو دا  ناتج الطرح اقل م  قيمة الل
threshold
 
 دا معناه ا  اللل
correlation
 
 فلي
الل 
branch
 
 اللي تحت ادبر م  اللي فوق يعني
اإلشارة
 
شدلها اقرب لل 
0
 
 اذا اللي
noisy signal
 
 دي دانت
0
 
 ددا نجح في انه يدلل الل
noise
 
 جدا وبالتالي الل
SNR
  
هتزيد بشدل دبير
. 
 
 
 
 
 
 
Binary Matched Filter
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
∑
 
 
 قبل ما نبدأ في الل Digital modulation techniques  هنتعلم ازاي نحسب دفائتهم م  خالل معادللة اللل probability of 
error  وفي الجزء دا هنتعلم معادلة احتمال الخطر فيأي digital system  بشدل عام تساويأي وبعد دلدا نطبدهلا عللى 
دل نوع م  أنواع الل  Digital modulation .ونشول الناتج في دل حالة 
 
اإلشارة  اللي واصلة للل  receiver    سواء دانتإشارة  معبرة ع  الصفر او معبرة ع  الواحلد هلي  إشلارة  مضلال اليهلا 
noise ودنا فرضنا ا  الل  noise  هي AWGN وبالتالي الل probability density function  الخاصلةباإلشلارة المسلتدبلة 
هي Gaussian  لها mean  و variance .بديمة معينه 
 
الرسم االتي هو الل probability density function المعبرة ع  استدبال الواحد واستدبال الصفر و افترضنا ا  الل  
threshold = 0   وبالتالي الل receiver  هيستدبلأي قيمة ادبر م  صفر انها 1   ,واي قيمة اقل م  صفر انها.0   
 
 
 
 
   
 
 
 
 
 
 
 
 
 اثبات قانو  الل probability of bit error  مب علينا ولد  مهم نفهم الدانو  ونعرل دل parameter   فيه وترثيره على
احتمال الخطر  
 
The probability of  bit error (bit error rate) is given by :  
 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜 
 
𝑁𝑜→ power spectral density of noise (Watt/Hz)  
𝐸→ average energy per bit  
𝐸=𝐸1+𝐸2
2 
𝐸1→ Energy of binary 1                                                         𝐸2→ Energy of binary 0  
Probability of error (𝒑𝒆) 
 احتمالية ارسال0  غلط احتمالية ارسال 1  غلط Probability density 
function for zero 
binary received  Probability density 
function for one 
binary received  
Threshold  
𝑦𝑜 
 
 
𝐸
 
 هي
average
 
 الل
Energy
  
  و
𝑁
𝑜
 
 هي الل
PSD
 
 للل
noise
  
 وبالتالي ممد  نعتبر
 
ا  
  
𝐸
 
 
𝑁
𝑜
 
  هي
SNR
 
 
 م  المعادلة السابدة واضح ا  الل
probability of error
  
 تعتمد على
SNR
  
 ودلما زادت قل احتمال الخطر حيث ا
 
في 
  
𝑒𝑟𝑓𝑐
(
𝑥
)
 
  دلما زاد
𝑥
 
 دلما قل
 
𝑒𝑟𝑓𝑐
(
𝑥
)
  والعدس
 
 
𝜌
→
 
cross correlation between 
𝑠
1
(
𝑡
)
 
 
and  
𝑠
2
(
𝑡
)
 
 
𝑠
1
(
𝑡
)
→
 
signal express digit 1
 
𝑠
2
(
𝑡
)
→
 
signal express digit 0
 
 
 
 فدرة عمل الل
matched filter
 
 هو
إيجاد 
 
تشابه ما بي  الل 
noisy signal
 
و
اإلشارة
 
المعبرة ع  
1
 
binary
 
 م  غير
noise
 
   واالشارة المعبرة ع
0
  
binary
  
  م  غير
noise
  
 واللي قيمة التشلابه بتاعتهلا اعللى هلي دي اللل
bit
 
 الللي دانلت مبعوتله
,وبالتالي لو دانت االشارتي  
𝑠
1
(
𝑡
)
 
 و
𝑠
2
(
𝑡
)
 
 شبه بعع اللل
matched filter
 
 ملب هيدلدر يحلدد اللل
noisy signal
 
 دي
دا  اصلها 
أي
 
ال  قيمة التشابه معاهم تدريبا 
متساوية
 
 
نستنتج م  ددا ا   
شدل  
اإلشارات 
  
يفرق في معدل الخطر 
وبالتالي عشا  نحصل عللى معلدل خطلر اقلل الزم 
اإلشلارات 
 
تدو  مختلفة 
يعني
 
الل
 
cross correlation 
 
 بينهم اقل ما يدو
 
ددا فهمنا ليه موجوده في الدانو  
(
1
−
𝜌
)
 
 في البسط بحيلث دلل ملا اللل
cross correlation
 
 يبدلى اصلغر دلملا زادت
الديمة داخل 
𝑒𝑟𝑓𝑐
 
وبالتالي قل معدل الخطر
 
 
The probability of error decreases as the average energy increases, the correlation decreases, or the 
noise power decreases.
 
 هذه العالقة منطديه جدا, فم  المنطدي عند زيادة الل
power
 
 الموجودة في
اإلشارة 
  
وندص الل 
power
 
  الموجودة في الل
noise
 
 ا  تدل احتماالت الخطر, ودذلك عندما تدل الل
correlation
 
 ما بي  االشارتي  ا  يدل احتمال الخطر ال  ذلك
يعني ا  االشارتي  مختلفتي  في الشدل والل 
receiver
 
 .لم يخطئ في التمييز ما بينهم
 
 
threshold
 
دحالة عامه ال يشترط ا  يدو  صفر وقيمته بعد االثبا
ت 
: 
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
 
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
 
 
 
 
 
 
 
 
 
 
,
 
 
 
 
 
𝑇
𝑏
0
 
 
𝐸
2
=
∫
𝑠
2
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
 
 
𝜌
=
1
𝐸
∫
𝑠
1
(
𝑡
)
 
𝑠
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
 
 
𝜌
 
  تتراوح م
1
 
 الى
-
1
 
  حيث
1
 
هي ادبر قيمة وتدل على التشابه التام بي  االشارتي  و
-
1
  
  تدل ا  االشارتي  عدس بعع
 
 
 
 
 
 
 
 
  خطوات حل المسائل 
 
 
1.  رسم رسمة Binary Matched Filter  دامله 
 
2.   نجيب معادلة 𝑠1(𝑡)    و 𝑠2(𝑡)  ونحطهم على الرسم  او نسيبهم درموز ونحط المعادالت تحت الرسلم ونجيلب قيملة 
𝑇𝑏  ونحطها في حدود التدامل اللي على الرسم  
 
3.  نحسب𝐸1  و𝐸2  ومنها نجيب الل𝐸 
𝐸1=∫𝑠12(𝑡) 𝑑𝑡    ,       𝐸2=∫𝑠22(𝑡) 𝑑𝑡    ,    𝐸=𝐸1+𝐸2
2 
 
4.  نحسب الل𝜌 cross correlation  
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠2(𝑡) 𝑑𝑡 
 
5.  نحسب Probability of error  
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜 
 
 
 
 
 
Given the two -baseband signals shown in the following figure. Assume that white Gaussian 
noise of power spectral density =𝟏𝟎−𝟑 W/Hz  is added  
 
 
 
 
 
i. Design a binary matched filter to choose between these baseband signals.  
ii. Find the probability of error  
Solution  
Example  1 

 
 في
أي 
 
سؤال لو طلب 
design matched filter
 
   المطلوب هو رسم الرسمة دامله ودتابة المعادالت على الرسم او ممد
ندتب على الرسمة  
  
𝑠
1
(
𝑡
)
  و
𝑠
2
(
𝑡
)
  
 وتحت الرسمة ندتب المعادالت
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
1
 
 
 
 
 
 
 
 
0
≤
𝑡
≤
8
 
msec
 
𝑠
2
(
𝑡
)
=
{
1
 
 
 
 
 
 
 
 
 
 
0
≤
𝑡
≤
4
 
msec
−
1
 
 
 
 
4
msec
≤
𝑡
≤
8
 
msec
 
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
0
=
∫
1
2
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
0
=
8
×
1
0
−
3
 
Watt
.
sec
 
𝐸
2
=
∫
1
2
 
𝑑𝑡
4
 
𝑚𝑠𝑒𝑐
0
+
∫
(
−
1
)
2
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
4
 
𝑚𝑠𝑒𝑐
=
8
×
10
−
3
 
Watt
.
sec
 
𝐸
=
𝐸
1
+
𝐸
2
2
=
8
×
10
−
3
 
Watt
.
sec
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
=
0
 
𝜌
=
1
𝐸
∫
𝑠
1
(
𝑡
)
 
𝑠
2
(
𝑡
)
 
𝑑𝑡
=
1
8
×
10
−
3
 
[
∫
1
 
𝑑𝑡
4
 
𝑚𝑠𝑒𝑐
0
+
∫
−
1
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
4
 
𝑚𝑠𝑒𝑐
]
=
0
 
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐸
 
(
1
−
𝜌
)
2
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
√
8
×
10
−
3
2
 
×
10
−
3
=
0
.
5
 
𝑒𝑟𝑓𝑐
(
2
)
=
2
.
34
×
10
−
3
 
 
 
 
 
 
∫
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
0
 
∫
 
𝑑𝑡
8
 
𝑚𝑠𝑒𝑐
0
 
𝑦
<
0
 
 
 
 
 
 
 
 
 
0
 
𝑦
>
0
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
=
0
 
 
 
𝑇
𝑏
=
8
 
𝑚𝑠𝑒𝑐
 
∑
 
 
Design a 
binary matched filter 
detector for the two signals shown below. 
Assume that white 
Gaussian noise is added
 
𝑵
𝒐
=
𝟎
.
𝟏
 
𝐖𝐚𝐭𝐭
/
𝐇𝐳
. Find the probability of error.
 
 
 
 
 
 
 
 
Solution
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
sin
2
𝜋𝑡
 
𝑠
2
(
𝑡
)
=
{
1
 
 
 
 
 
 
 
 
 
 
0
≤
𝑡
≤
0
.
5
−
1
 
 
 
 
0
.
5
≤
𝑡
≤
1
 
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
1
0
=
∫
(
sin
2
𝜋𝑡
)
2
 
𝑑𝑡
1
0
 
=
1
2
∫
[
1
−
cos
4
𝜋𝑡
]
 
𝑑𝑡
1
0
=
1
2
[
∫
1
 
𝑑𝑡
1
0
−
∫
cos
4
𝜋𝑡
 
𝑑𝑡
1
0
]
 
=
1
2
[
 
𝑡
|
0
1
−
 
𝑠𝑖𝑛
(
4
𝜋𝑡
)
4
𝜋
|
0
1
]
 
=
1
2
[
1
−
0
]
=
1
2
 
Watt
.
sec
 
𝐸
2
=
∫
𝑠
2
2
(
𝑡
)
 
𝑑𝑡
1
0
=
∫
1
2
 
𝑑𝑡
0
.
5
0
+
∫
(
−
1
)
2
 
𝑑𝑡
1
0
.
5
=
1
 
 
Watt
.
sec
 
Example
 
2
 
𝒔
𝟐
(
𝒕
)
 
𝒔
𝟏
(
𝒕
)
 
∫
 
 
𝑑𝑡
1
0
 
∫
 
 
𝑑𝑡
1
0
 
𝑦
𝑜
=
−
0
.
25
 
 
 
T
b
=
1
 
𝑠𝑒𝑐
 
𝑦
<
−
0
.
25
 
 
 
 
 
 
 
 
0
 
𝑦
>
−
0
.
25
 
 
 
 
 
 
 
 
 
1
 
∑
 
𝐸=𝐸1+𝐸2
2=0.5+1
2=3
4 Watt .sec 
𝑦𝑜=0.5−1
2=−0.25 Watt .sec 
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠2(𝑡) 𝑑𝑡𝑇𝑏
0=4
3 [∫ 1×sin2𝜋𝑡 𝑑𝑡0.5
0+∫−1×sin2𝜋𝑡 𝑑𝑡1
0.5] 
=4
3 [− 𝑐𝑜𝑠(2𝜋𝑡)
2𝜋|
00.5
+ 𝑐𝑜𝑠(2𝜋𝑡)
2𝜋|
0.51
]=4
3×1
2𝜋 [−(−1−1)+ (1+1)]=4
6𝜋(4) 
=16
6𝜋=0.85 
 لو الحظنام  الرسم هنالقي ا    𝑠1(𝑡)   و 𝑠2(𝑡)     يشبهوا بعع ودا سبب ا𝜌   قيمتها قريبه م 1   وطبعا الل system  
دا غير دفئ.  
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √0.75 (1−0.85)
2× 0.1=0.5 𝑒𝑟𝑓𝑐 (0.75)=0.5×.28884 =0.14442  
 
 
 
 
Is one in which amplitude of carrier is modulated by digital signal  
الللل Baseband Signal   ديإشللارة بتللرددات منخفضللة ال تصلللح انهللا تتبعللت علللى الللل channel  والمفللروع نعمللل
modulation    م  خالل اننا نحملاإلشارة  على  carrier   بتردد عالي وبيلتم التعلديل عللى احلداللل Parameter  بتاعلةاللل 
carrier طبدا للل information signal. في النوع الل Parameter  اللي هيتم التعديل في الل Amplitude 
 
ASK   يشبه الل Amplitude Modulation   اللي درسناه قبل ددا حيث ا  فيASK  التعديل بيتم في الل amplitude   بتاع الل
carrier  تبعاً للل information signal. 
 
 عشا  نعمل ديزاي  الل matched filter  لللFSK     هنحتا  نجيب معادلتي 𝑠1(𝑡)    و 𝑠2(𝑡)    حيث ا 𝑠1(𝑡)   هي الل
modulated signal   المعبرة ع binary 1   و 𝑠2(𝑡)  هي الل modulated signal   المعبرة ع binary 0 
 
  نفترع ا  احنا عندنا 𝑓𝑐(𝑡) carrier signal   عبارة عإشارة  sinusoidal  : بتردد عالي 
 
Assume carrier signal   𝑓𝑐(𝑡)=𝐴
2cos(2𝜋𝑓𝑐𝑡) 
 Amplitude Shift Keying (ASK)  
 
𝑑(𝑡)  هي الل information signal  ودياإلشارة  اللي عايز احملها على carrier وليها قيمتي , قيمة   𝐵   عند1 digital 
  وقيمة−𝐵  عند0 digital   
Assume digital signal   𝑑(𝑡)={𝐵   digital  1
−𝐵   digital  0 
 
𝑆𝐴𝑆𝐾 =(𝐴
2+𝑑(𝑡))cos(2𝜋𝑓𝑐𝑡)=(𝐴
2±𝐵)cos(2𝜋𝑓𝑐𝑡) 
=𝐴
2(1±2𝐵
𝐴)cos(2𝜋𝑓𝑐𝑡)=𝐴
2(1±𝑚)cos(2𝜋𝑓𝑐𝑡) 
 
𝑠1(𝑡)=𝐴
2(1+𝑚)cos(2𝜋𝑓𝑐𝑡) 
𝑠2(𝑡)=𝐴
2(1−𝑚)cos(2𝜋𝑓𝑐𝑡) 
 
𝑚 (modulation index) =𝐵
𝐴2⁄=signal  amplitude  
carrier  amplitude   
 
 يوجد قيمتي  فدط للل amplitude , قيمة عند digital 0  وتساوي  𝐴
2(1−𝑚)   وقيمة عند digital 1   وتساوي𝐴
2(1+𝑚)  
 
 
 
 
 
digital  1 
digital  0 
 
 
Is special type of ASK in which 𝑚=1 
 
  حالة خاصة مASK    عندما يدو modulation index 𝑚=1   وبالتالي عند ارسال digital 1    تدو  قيمة الل
تساوي modulated signal 𝐴  ,وعند ارسال digital 0  تدو  قيمة الل modulated signal  تساوي صفر ودا سبب
تسميتها بالفتح والغلق  On-off Keying  ودرنه  بي دفل عند  digital  0 وبيفتح  عند  digital 1 
 
𝑠1(𝑡)=𝐴cos(2𝜋𝑓𝑐𝑡) 
𝑠2(𝑡)=0  
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
On-off Keying (OOK)  
1 1 1 0 
Carrier signal   𝒇𝒄 
Modulated  signal   𝑺𝑨𝑺𝑲 Modulating signal   𝒅(𝒕) 0 
 
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
)
 
𝑠
2
(
𝑡
)
=
0
 
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
∫
[
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
)
]
2
 
𝑑𝑡
𝑇
𝑏
0
 
=
𝐴
2
 
∫
1
2
[
1
+
𝑐𝑜𝑠
(
4
𝜋
𝑓
𝑐
𝑡
)
]
 
𝑑𝑡
𝑇
𝑏
0
=
𝐴
2
2
 
[
∫
1
 
𝑑𝑡
𝑇
𝑏
0
+
∫
𝑐𝑜𝑠
(
4
𝜋
𝑓
𝑐
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
]
 
𝐴
2
2
[
 
𝑡
|
0
𝑇
𝑏
+
 
𝑠𝑖𝑛
(
4
𝜋
𝑓
𝑐
𝑡
)
4
𝜋
𝑓
𝑐
|
0
𝑇
𝑏
]
=
𝐴
2
2
 
[
 
𝑇
𝑏
+
 
0
]
 
∴
𝐸
1
=
𝐴
2
2
 
𝑇
𝑏
 
 
𝐸
2
=
∫
𝑠
2
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
0
 
 
𝐸
=
𝐸
1
+
𝐸
2
2
=
 
[
𝐴
2
2
 
𝑇
𝑏
+
0
2
]
=
𝐴
2
4
 
𝑇
𝑏
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
=
[
𝐴
2
2
 
𝑇
𝑏
−
0
2
]
=
𝐴
2
4
 
𝑇
𝑏
 
 
 
𝒔
𝑨𝑺𝑲
(
𝒕
)
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
 
 
Design and Performance
 
for OOK
 
∑
 
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠2(𝑡) 𝑑𝑡=0 
 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐴2
4 𝑇𝑏 (1−0)
2 𝑁𝑜 
∴ 𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐴2 𝑇𝑏 
8 𝑁𝑜 
 
 
 
  المسائل في الجزء دا هتدو  على الحالة الخاصة OOK  وفي الحل هنرسم الرسمة ونحلط المعلادالت والدليم عليهلا زي
مثال نحسب  𝑇𝑏  ونحطها في حدود التدامل والمعادالت 𝑠1(𝑡)  و 𝑠2(𝑡)   ندتلبهم عللى الرسلم او نسليبهم درملوز ولدل
ندتبهم تحت الرسم 
دي الدواني  اللي استنتجناها وهنستخدمها في حل المسائل :  
 
𝑠1(𝑡)=𝐴𝑐𝑜𝑠(2𝜋𝑓𝑐𝑡) 
𝑠2(𝑡)=0 
𝐸1=𝐴2
2 𝑇𝑏 
𝐸2=0 
𝐸=𝐸1+𝐸2
2=𝐴2
4 𝑇𝑏 
𝑦𝑜=𝐸1−𝐸2
2=𝐴2
4 𝑇𝑏 
𝜌=0 
 𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐴2 𝑇𝑏 
8 𝑁𝑜 
 
 
 
 
 
 
 
 
 
Binary information is transmitted at 10 kbps using OOK. The carrier frequency is 10 MHz and 
the received carrier amplitude is 
𝟏𝟎
−
𝟐
 
volt. The additive noise power is 
𝟓
×
𝟏𝟎
−
𝟏𝟎
  
Watt/Hz. 
Design a matched filter detector and find the probability of error
 
(bit error rate)
.
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
)
=
10
−
2
 
𝑐𝑜𝑠
(
2
×
10
7
 
𝜋𝑡
)
 
𝑠
2
(
𝑡
)
=
0
 
 
𝑇
𝑏
=
1
𝑅
𝑏
=
1
10
×
10
3
=
0
.
1
 
msec
 
 
 
𝐸
1
=
𝐴
2
2
 
𝑇
𝑏
=
(
10
−
2
)
2
2
 
×
10
−
4
=
5
×
10
−
9
 
 
Watt
.
sec
 
𝐸
2
=
0
 
 
𝐸
=
𝐸
1
+
𝐸
2
2
=
2
.
5
 
×
10
−
9
 
 
Watt
.
sec
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
=
2
.
5
 
×
10
−
9
 
 
Watt
.
sec
 
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐴
2
 
𝑇
𝑏
 
8
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
(
10
−
2
)
2
 
×
10
−
4
 
8
 
×
 
5
×
10
−
10
=
0
.
5
 
𝑒𝑟𝑓𝑐
(
1
.
58
)
 
 
=
0
.
5
 
×
0
.
02545
=
0
.
013
 
 
Example
 
3
 
∫
 
 
𝑑𝑡
10
−
4
0
 
∫
 
 
𝑑𝑡
10
−
4
0
 
𝒔
𝑨𝑺𝑲
(
𝒕
)
 
𝑦
𝑜
=
2
.
5
×
10
−
9
 
 
 
T
b
=
0
.
1
 
𝑚𝑠𝑒𝑐
 
𝑦
<
2
.
5
×
10
−
9
 
 
 
 
 
 
 
0
 
𝑦
>
2
.
5
×
10
−
9
 
 
 
 
 
 
 
 
1
 
∑
 
 
 
 
 
Explain how you can implement a matched filter . 
 
Solution  
 
 
 
for matched filter  
ℎ(𝑡)=𝐾 𝑠(𝑇−𝑡) 
∴ℎ(𝑡−𝜏)=𝐾 𝑠(𝑇−𝑡+𝜏) 
 
𝑠𝑜(𝑡)+𝑛𝑜(𝑡)=[𝑠(𝑡)+𝑛(𝑡)]∗ℎ(𝑡) 
=∫[𝑠(𝜏)+𝑛(𝜏)]𝑡
0ℎ(𝑡−𝜏) 𝑑𝜏 
 
𝑠𝑜(𝑡)+𝑛𝑜(𝑡)=𝑘∫[𝑠(𝜏)+𝑛(𝜏)]𝑡
0𝑠(𝑇−𝑡+𝜏) 𝑑𝜏 
 
At 𝑡=𝑇 
𝑠𝑜(𝑇)+𝑛𝑜(𝑇)=𝑘∫[𝑠(𝜏)+𝑛(𝜏)]𝑇
0𝑠(𝜏) 𝑑𝜏 
 
 
 
 
 
 
 
 
 
 
 
  [Extra] Related Exams Questions  
Question 1 (Final 2016)  
 
∫  𝑑𝑡𝑇
0 
 
Given the two baseband signals shown in the figure. Assume that white Gaussian noise of 
power spectral density 
=
𝟏𝟎
−
𝟑
 
W/Hz is added 
 
 
 
 
 
 
i.
 
Design a binary matched filter detector to choose between these baseband signals.
 
ii.
 
Find the probability of bit 
error
.
 
iii.
 
Redesign the matched filter detector if 
𝒔
𝒐
(
𝒕
)
=
𝟎
 
, then find the probability of bit error.
 
Solution
 
 
i.
 
Design a binary matched filter detector to choose between these baseband signals.
 
 
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
{
1
 
 
 
 
 
 
0
≤
𝑡
≤
1
 
msec
−
1
 
 
 
 
1
 
msec
≤
𝑡
≤
2
 
msec
1
 
 
 
 
2
 
msec
≤
𝑡
≤
3
 
msec
      
    
    
   
𝑠
0
(
𝑡
)
=
{
−
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
≤
𝑡
≤
1
 
msec
 
 
1
 
 
 
 
 
 
 
 
1
 
msec
≤
𝑡
≤
2
 
msec
 
 
 
−
1
 
 
 
 
 
 
 
2
 
msec
≤
𝑡
≤
3
 
msec
 
 
ii.
 
Find the probability of bit error.
 
𝐸
1
=
∫
1
2
 
𝑑𝑡
1
 
𝑚𝑠𝑒𝑐
0
+
∫
(
−
1
)
2
 
𝑑𝑡
2
 
𝑚𝑠𝑒𝑐
1
 
𝑚𝑠𝑒𝑐
+
∫
1
2
 
𝑑𝑡
3
 
𝑚𝑠𝑒𝑐
2
 
𝑚𝑠𝑒𝑐
 
 
𝑡
|
0
1
 
𝑚𝑠𝑒𝑐
+
 
𝑡
|
1
 
𝑚𝑠𝑒𝑐
2
 
𝑚𝑠𝑒𝑐
+
𝑡
|
2
 
𝑚𝑠𝑒𝑐
3
 
𝑚𝑠𝑒𝑐
=
[
(
1
−
0
)
+
(
2
−
1
)
+
(
3
−
2
)
]
×
10
−
3
 
=
3
 
×
10
−
3
 
 
Watt
.
sec
 
 
∫
 
𝑑𝑡
3
 
𝑚𝑠𝑒𝑐
0
 
∫
 
𝑑𝑡
3
 
𝑚𝑠𝑒𝑐
0
 
𝑦
<
0
 
 
 
 
 
 
 
 
 
0
 
𝑦
>
0
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
=
0
 
 
 
𝑇
𝑏
=
3
 
𝑚𝑠𝑒𝑐
 
Question 2 
(Final 2018)
 
𝑠
0
(
𝑡
)
 
∑
 
 
𝐸𝑜=∫ (−1)2  𝑑𝑡1 𝑚𝑠𝑒𝑐
0+∫ 12 𝑑𝑡2 𝑚𝑠𝑒𝑐
1 𝑚𝑠𝑒𝑐+∫ (−1)2  𝑑𝑡3 𝑚𝑠𝑒𝑐
2 𝑚𝑠𝑒𝑐 
𝑡|01 𝑚𝑠𝑒𝑐+ 𝑡|1 𝑚𝑠𝑒𝑐2 𝑚𝑠𝑒𝑐+𝑡|2 𝑚𝑠𝑒𝑐3 𝑚𝑠𝑒𝑐=[(1−0)+(2−1)+(3−2)]×10−3 
=3 ×10−3  Watt .sec  
 
𝐸=𝐸1+𝐸0
2=3 ×10−3  Watt .sec 
𝑦𝑜=𝐸1−𝐸0
2=0 
 
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠0(𝑡) 𝑑𝑡=1
3×10−3 [∫ −1 𝑑𝑡1 𝑚𝑠𝑒𝑐
0+∫ −1 𝑑𝑡2 𝑚𝑠𝑒𝑐
1 𝑚𝑠𝑒𝑐+∫ −1 𝑑𝑡3 𝑚𝑠𝑒𝑐
2 𝑚𝑠𝑒𝑐] 
1
3×10−3[−𝑡|01 𝑚𝑠𝑒𝑐− 𝑡|1 𝑚𝑠𝑒𝑐2 𝑚𝑠𝑒𝑐−𝑡|2 𝑚𝑠𝑒𝑐3 𝑚𝑠𝑒𝑐]=10−3
3×10−3 [−(1−0)−(2−1)−(3−2)] 
∴𝜌=−3×10−3
3×10−3=−1 
  قيمة الل𝜌 cross correlation  تساوي-1    ودي اقل قيمة ممدنه ودا منطدي ال  االشارتيعدس بعع    
 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐 √3 ×10−3(1+1)
2 ×10−3=0.5 𝑒𝑟𝑓𝑐 (1.73)=0.5×0.01442   
=7.21 ×10−3 
 
 
 
 
 
 
 
 
 
 
i.
 
Redesign the matched filter detector if 
𝑠
𝑜
(
𝑡
)
=
0
 
, then find the probability of bit error.
 
 
 
 
 
 
 
 
 
 
𝑠
0
(
𝑡
)
=
0
 
 
𝐸
1
=
3
 
×
10
−
3
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
 
𝐸
𝑜
=
0
 
𝐸
=
𝐸
1
+
𝐸
0
2
=
1
.
5
 
×
10
−
3
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
𝑦
𝑜
=
𝐸
1
−
𝐸
0
2
=
1
.
5
 
×
10
−
3
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
 
𝜌
=
1
𝐸
∫
𝑠
1
(
𝑡
)
 
𝑠
0
(
𝑡
)
 
𝑑𝑡
=
0
 
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐸
 
(
1
−
𝜌
)
2
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
√
1
.
5
 
×
10
−
3
(
1
−
0
)
2
 
×
10
−
3
=
0
.
5
 
𝑒𝑟𝑓𝑐
(
0
.
87
)
=
0
.
5
×
0
.
21856
 
 
=
0
.
10928
 
 
 
 
 
 
 
 
 
∫
 
𝑑𝑡
3
 
𝑚𝑠𝑒𝑐
0
 
∫
 
𝑑𝑡
3
 
𝑚𝑠𝑒𝑐
0
 
𝑦
<
1
.
5
×
10
−
3
 
 
 
 
 
 
 
 
 
0
 
 
𝑦
>
1
.
5
×
10
−
3
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
 
=
1
.
5
×
10
−
3
 
  
 
𝑇
𝑏
=
3
 
𝑚𝑠𝑒𝑐
 
𝑠
0
(
𝑡
)
=
0
 
∑
 
Given the two baseband signals shown in the figure. Assume
.
  
𝑵
𝒐
=
𝟐
×
𝟏𝟎
−
𝟑
 
W/Hz is added 
 
 
 
 
                                                            
 
 
 
 
i.
 
Design a binary matched filter detector to choose between 
these baseband signals.
 
ii.
 
Find the probability of bit error.
 
Solution
 
 
Design a binary matched filter detector to choose between these baseband signals.
 
 
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
{
1
 
 
 
 
 
 
0
≤
𝑡
≤
1
 
−
2
 
 
 
 
1
 
≤
𝑡
≤
3
 
2
 
 
 
 
3
≤
𝑡
≤
4
 
      
    
    
   
𝑠
0
(
𝑡
)
=
{
1
 
 
 
 
 
 
 
 
0
≤
𝑡
≤
1
−
1
 
 
 
 
 
 
 
1
≤
𝑡
≤
2
1
 
 
 
 
 
 
 
 
 
2
≤
𝑡
≤
3
−
1
 
 
 
 
 
 
 
 
3
≤
𝑡
≤
4
 
 
Find the probability of bit error.
 
𝐸
1
=
∫
1
2
 
𝑑𝑡
1
0
+
∫
(
−
2
)
2
 
𝑑𝑡
2
1
+
∫
(
−
2
)
2
 
𝑑𝑡
3
 
2
 
+
∫
2
2
 
𝑑𝑡
4
3
 
 
𝑡
|
0
1
 
+
 
4
𝑡
|
1
 
2
 
+
4
𝑡
|
2
 
3
 
+
4
𝑡
|
3
 
4
 
=
[
(
1
−
0
)
+
(
8
−
4
)
+
(
12
−
8
)
+
+
(
16
−
12
)
]
 
=
13
 
 
Watt
.
sec
 
 
Question 3 
(Final 2020)
 
 
 
∫
 
𝑑𝑡
4
0
 
∫
 
𝑑𝑡
4
0
 
𝑦
<
4
.
5
 
 
 
 
 
 
 
 
 
0
 
𝑦
>
4
.
5
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
=
4
.
5
 
 
 
𝑇
𝑏
=
4
 
𝑠𝑒𝑐
 
𝑠
0
(
𝑡
)
 
∑
 
 
 
𝐸𝑜=∫12 𝑑𝑡1
0+∫(−1)2 𝑑𝑡2
1+∫12 𝑑𝑡3 
2 +∫(−1)2 𝑑𝑡4
3 
 𝑡|01 + 𝑡|1 2 +𝑡|2 3 +𝑡|3 4 =[(1−0)+(2−1)+(3−2)+(4−3)] 
=4  Watt .sec  
 
𝐸=𝐸1+𝐸0
2=13+4
2=8.5  Watt .sec 
𝑦𝑜=𝐸1−𝐸0
2=13−4
2=4.5  Watt .sec  
 
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠0(𝑡) 𝑑𝑡=1
8.5 [∫1 𝑑𝑡1
0+∫2 𝑑𝑡2
1+∫−2 𝑑𝑡3 
2 +∫−2 𝑑𝑡4
3] 
1
8.5 [𝑡|01 + 2𝑡|1 2 −2𝑡|2 3 −2𝑡|3 4 ]=[(1−0)+(4−2)−(6−4)−(8−6)] 
∴𝜌=1+2−2−2
8.5=−0.118 
 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐 √8.5 (1+0.118 )
2 ×2×10−3=0.5 𝑒𝑟𝑓𝑐 (48) 
 48  مب موجوده في الجدولفهنسيب النات ج بداللة   𝑒𝑟𝑓𝑐 
 
 
 
 
 
 
 
 
 
Binary information is transmitted at 100 kbps using OOK. The carrier frequency is 20
 
MHz and 
the received carrier amplitude is 
𝟏
𝟎
−
𝟑
 
volt. The additive noise power is 
𝟏
𝟎
−
𝟏𝟐
 
Watt/Hz. Design 
a matched filter detector and find the probability of error.
 
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
𝑓
𝑐
=
20
 
𝑀𝐻𝑧
 
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
 
 
 
 
𝐴
=
10
−
3
 
𝑣𝑜𝑙𝑡
 
 
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
 
𝑁
𝑜
=
10
−
12
 
𝑊𝑎𝑡𝑡
/
𝐻𝑧
 
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
)
=
10
−
3
 
𝑐𝑜𝑠
(
4
×
10
7
 
𝜋𝑡
)
 
𝑠
2
(
𝑡
)
=
0
 
 
𝑇
𝑏
=
1
𝑅
𝑏
=
1
100
×
10
3
=
10
−
5
 
𝑠𝑒𝑐
 
 
 
𝐸
1
=
𝐴
2
2
 
𝑇
𝑏
=
(
10
−
3
)
2
2
 
×
10
−
5
=
5
×
10
−
12
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
𝐸
2
=
0
 
 
𝐸
=
𝐸
1
+
𝐸
2
2
=
2
.
5
 
×
10
−
12
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
=
2
.
5
 
×
10
−
12
 
 
𝑊𝑎𝑡𝑡
.
𝑠𝑒𝑐
 
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐴
2
 
𝑇
𝑏
 
8
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
(
10
−
3
)
2
 
×
10
−
5
 
8
 
×
10
−
12
=
0
.
5
 
𝑒𝑟𝑓𝑐
(
1
.
12
)
 
 
=
0
.
5
 
×
0
.
11321
=
0
.
06
 
Question 
4
 
(Final 2016)
 
∫
 
 
𝑑𝑡
10
−
5
0
 
∫
 
 
𝑑𝑡
10
−
5
0
 
𝒔
𝑨𝑺𝑲
(
𝒕
)
 
𝑦
𝑜
=
2
.
5
×
10
−
9
 
 
 
T
b
=
0
.
1
 
𝑚𝑠𝑒𝑐
 
𝑦
<
2
.
5
×
10
−
12
 
 
 
 
 
 
 
0
 
𝑦
>
2
.
5
×
10
−
12
 
 
 
 
 
 
 
 
1
 
∑
 
 
 
 
 
 
   
 
©
 
Basem Hesham
Serial Quantizer  & Parallel Quantizer & 
Quantization noise & Delta Modulation 
Lecture
 
6
 
 
Design a counting  ADC to convert 𝒔(𝒕)=𝒔𝒊𝒏(𝟐𝝅𝒕 ) into a 4-bit signal.  
Choose appropriate parameter s for ADC.   
Solution  
 
 اإلشارة 𝑠(𝑡)  ليها قيمةmax  عند1  وقيمة min  عند-1  , و الـ Counting Quantizer   بيشتغل
عند قيم موجبة فقط فهنعمل shift لإلشارة بمقدار  1  والـ range  هيكون بالشكل االتي 
Amplitude range  0→2 𝑣𝑜𝑙𝑡  
𝑓𝑚=1 𝐻𝑧 
𝑓𝑠≥2 𝑓𝑚    ,   𝑓𝑠≥2 
 
Let us choose a rate 25% above the Nyquist  value   
𝑓𝑠=2+0.25×2=2.5  samples /sec 
𝑇𝑠=1
𝑓𝑠=1
2.5=0.4 sec/sample  
 
The ramp must reach the maximum value with one sampling period  
Assume 𝑇𝑟𝑎𝑚𝑝 =𝑇𝑠=0.4  
ramp  slope =2
0.4=5  𝑣𝑜𝑙𝑡 /𝑠𝑒𝑐 
 
0.4 𝑠                 24 
1  𝑠                     𝑓𝑐𝑙𝑜𝑐𝑘  
 
𝑓𝑐𝑙𝑜𝑐𝑘 =16
0.4=40 counts /sec 
 
 
 
 Example  1 
 
 
 
Serial  Quantizer  تاني نوع مــنأنــواع PCM Modulator  وهــو عبــارة عــن ADC  يحــولاإلشــارة 
التناظرية الى رقمية  ,و فكرة عمله  تعتمد على  تقسيم  اإلشــارة  التناظريــة الــى مســتويا  مــن  0  الــى  1 
  عن طريق عمل normalization  لإلشارة  (نقسم  على اكبر قيمة لإلشارة للحصول على range  من0 
الى 1)  ولو كان  قيم االشارة بالسالب هنعملshift  للحصول على قيم موجبة  من  0  الى1 
 عدد المستويا  يعتمد على عددالـ bits  المستخدمة (لــو اســتخدمنا3 bit سنحصــل علــى 8  مســتويا 
ولو استخدمنا  4 bit  سنحصل على 16 مستوى  وهكذا)  وكل مستوى ليه كود مختلف. 
وتتم العملية باستخدام  comparators    حيث يتم الحصول علىالـ  bits    المعبرة عناإلشارة  بدايــة مــن 
MSB  الىLSB  بشكل تسلسليكما سنوضح في المثال االتي.  
 
 
 
▪ The serial quantizer successively divides the ordinates into two regions.  
▪ It first divides the axis in half and observes whether the sample is in the upper or lower 
half. 
▪ The result of this observation generates the most significant bit in the code word.  
 
 
 
 سيتم تقسيم قيماإلشــارة الــى 8  مســتويا وســنحتاا الــى 3 comparator  فقــط عنــد1
2  و1
4  و1
8  ومــن
خاللهم هنقدر نعرف  قيمة  الـ sample   موجوده فيأي range بالضبط  
في البداية هيشوف قيمة الـ sample  اكبر من1
2  وال أل لو اقل هيحط0 في( 𝑏1) MSB  ولو اكبر هيحط
1    ويطرح1
2    وبعد كده هيشوف البــاقي بعــد طــرح الــنا هيكــون اكبــر مــن1
4   وال ألو لــو اقــل هــيحط 
𝑏2=0    ولو اكبر هيحط 𝑏2=1    ويطرح1
4   وفي النهاية هيشوف البــاقي هيكــون اكبــر مــن1
8  وال ال
لو اقل هيحط 0 في( 𝑏3) LSB  ولو اكبر هيحط 𝑏3=1  (عملية الطــرح تــتم لمــا يتحقــق الشــرط فقــط
يعني لما القيمة تبقى اكبر من لكن لو كان  اقل هنحط  0   ونعديمن غير ما نطرح)  
Serial Quantizer  
Serial 3 -bit Quantizer  
 
  الجدول االتي يوضح المكافئالـ binary  لكل range ولو ركزنا فــي قــيم الجــدول هــنالحظ ان األرقــام 
األكبر  من  1
2    قيمة MSB    تساوي1  واألرقام  األقل  من  1
2    قيمة MSB    تساوي0  ,واألرقام  األقل  من  1
4   او
األرقام اللي لو طرحنا منها 1
2  وقيمتها اقل من1
4  هنالقي تانيbit  ليها تساوي 0  وهكــذا فــي بــاقي القــيم
,ودا بيحقق فكرة عمل  Serial Quantizer اللي تكلمنا عنها 
 
 
 
من خالل عملية الطرح بعد تحقق الشرط بعد كل مقارنة نعرف نوصل  للـ  range   المطلوب الن مثال
لو كان  القيمة اكبر من  1
2   وبعد ما نطرح منها1
2  ونقارنها بـ  1
4  كاننا بنقارنها بـ 6
8 
(Value greater than 1
2 ) −1
2>1
4                     (Value greater than 1
2 ) >1
4+1
2=6
8  
 
 ولو كان  اكبر من1
4   هنطرح منها1
4   ونقارنهاب ـ 1
8   كاننا بنقارنهاب ـ 7
8 
(Value greater than 6
8 ) −1
4−1
2>1
8                (Value greater than 6
8 ) >1
4+1
2+1
8=7
8  
 
بالتالي مش  هنحتاا  نستخدم  comparator    عنــد6
8    و7
8  ألننــا  ممكــن نجيــب المكــافئ بطــرح  1
2    او1
4   مــن
القيمة اللي عندي وبالتالي هحصل على القيمة المطلوبة 
بالمثل لو عندنا Serial 4-bit Quantizer  هنحتاا 4 comparator  عند1
2  و1
4  و1
8  و1
16   ولــو لــو عنــدنا
Serial 5-bit Quantizer  هنحتاا 5 comparator  عند1
2  و1
4  و1
8  و1
16  و1
32  ولــو عنــدنا Serial 2-bit 
Quantizer   هنحتاا 2 comparator   عند1
2  و1
4 فقط 
Number of comparators = number of bits  
𝒃𝟏 
MSB  𝒃𝟐 𝒃𝟑 
LSB 
 
 
Illustrate the operation of the system of 3 -bit serial quantizer for the following 
two input sample values: 0.2 and 0.8  
Solution  
 
 
 
 
For 0.2 V  
0.2<1
2  
The first comparison with 1
2 would yield a NO answer. Therefor 𝑏1=0 
0.2<1
4  
The second comparison with 1
4 would yield a NO answer. Therefor 𝑏2=0 
0.2>1
8  
The third comparison with 1
8 would yield a YES answer. Therefor 𝑏3=1 
∴𝟎.𝟐→𝟎𝟎𝟏  
 
For 0. 8 V 
0.8>1
2  
The first comparison with 1
2 would yield a YES answer. Therefor 𝑏1=1 
0.8−1
2>1
4→0.3>1
4  
The second comparison with 1
4 would yield a YES answer. Therefor 𝑏2=1 
0.3−1
4<1
8→0.05<1
8  
The third comparison with 1
8 would yield a NO answer. Therefor 𝑏3=0 
∴𝟎.𝟖→𝟏𝟏𝟎  Example  2 
 
 
 
▪ The parallel quantizer is the fastest operation since it develops all bits of the 
code words simultaneously.  
▪ It is also the most complex, requiring a number of comparators that is only one 
less than  the number of levels of quantization.  
▪ The block labeled "coder" observes the output of seven comparators.  
 
 هو اسرعمن الـ serial  ألنه بيخرا كلالـ bits في وق  واحــد، ولكنــ ه اعقــد more complex  النــه
بيحتاا عدد comparators  اقل من levels of quantization   بمقدار1 
3-bit parallel quantizer  will use 23−1=7 comparators  
 
الـ  input    بيدخل على sampler    لعمل sampling  وبعد كده  كل قيمة تدخل على  comparator   عشــان
تشوف القيمــة دي اكبــر مــن القيمــة فــي الــ ـ comparator  وال ال, ولــو اكبــر هيخــرا1  ولــو اصــغر
هيخرا  0 وفي االخر االصفار والوحايد دي تدخل على  coder وتطلع  الـ code النهائي 
 
▪ If all seven outputs are 1,the coder output is 111 ,since the sample value had to be 
greater than 7/8  
 لو كان خرا كلالـ comparators  يساوي واحد دا معناه انالـ sample value  اكبــر مــن7/8  وتقــع 
في الـ range  من7/8 الى 1 وبالتالي الـ output code  يساوي111 
 
▪ If comparator outputs 1 through 6 are 1 ,and output 7 are 0, the coder output is 110 
, since the sample had to be between 6/8 and 7/8  
 لــو كــان خــراالــ ـ comparators  يســاوي واحــد مــن1  الــى6  ويســاوي0  عنــد7  دا معنــاه ان
الـ sample value  تقع فيالـ range من 6/8 الى 7/8 وبالتالي الـ output code  يساوي110 
Parallel  Quantizer  
3-bit parallel quantizer  
 
 
▪ The quantization noise is defined as:  
𝑒(𝑛𝑇𝑠)=𝑆(𝑛𝑇𝑠)−𝑆𝑞(𝑛𝑇𝑠) 
 
▪ where 𝑆(𝑛𝑇𝑠) is the original sample value and 𝑆𝑞(𝑛𝑇𝑠) is the quantized sample value.  
هو الفرق بين القيمة الفعلية للـ sample والقيمة اللي قرب  ليها.  
 
▪ The amplitude range of the signal is divided into L uniformly spaced intervals, each of 
width ∆𝑠 
▪ The maximum quantization error is ±∆𝑠
2 
▪ The quantization error lies in the range (−∆𝑠
2 ,+∆𝑠
2)  
 اقصى قيمة للخطــم ممكــن تحصــل انالــ ـ quantization error  تســاوي ±∆𝒔
𝟐  الن فــي الحالــة دي
القيمة هتكون موجوده في النا بين مستويين  ودا اقصى فرق ممكن يحصل ما بين القيمة الفعليــة 
للـ sample والقيمة اللي قرب  ليها. 
−∆𝒔
𝟐≤ 𝒆𝒓𝒓𝒐𝒓 ≤∆𝒔
𝟐  
 
Assuming that the error is equally likely to lie anywhere in this range.  
  نفترض ان احتمال وقوع الخطم متساوي فيأي  مكان فــي  الــ ـ  range  (−∆𝒔
𝟐 ,+∆𝒔
𝟐)    ودا معنــاه
ان probability density functio n is uniform   قيمتها بـ 1
∆𝒔  عشان تكون المساحة تح  المنحنــى
تساوي واحد 
 
 
 
 
 
 
 
 Quantization Noise   
𝒑(𝒆) 
𝒆 
−∆𝒔
𝟐 ∆𝒔
𝟐 𝟏
∆𝒔 
 
 وبالتــالي يمكــن ان نحصــل علــى عالقــة مهمــة وهــي Mean square error (mse)   وهــي 
الـ expected value  للـ error  تربيع اومتوسط الـ error  تربيع وهو دا الباور بتاعــةالــ ـ error  او
الـ  noise  ,وبالتالي  لو طبقنا المفهوم السابق ممكن نحصل علــى قيمــة بــاور  الــ ـ  noise   مــن خــالل
قانون  الـ expected value  . 
 
 فيالمثــال الســابق  الــ ـ random variable  هــو𝑒2  و probability density function هــي  𝒑(𝒆) 
 وليها قيمة ثابته وهي𝟏
∆𝒔 ألنها uniform distribution function  وحدود التكامل تقع فيالــ ـ range 
(−∆𝒔
𝟐 ,+∆𝒔
𝟐)  
𝐸{𝑒2}=∫ 𝑒2 𝑝(𝑒) 𝑑𝑒∆𝑠
2
−∆𝑠
2     , 𝑝(𝑒)=1
∆𝑠 
=1
∆𝑠∫ 𝑒2 𝑑𝑒∆𝑠
2
−∆𝑠
2=1
∆𝑠    𝑒3
3|
−∆𝑠
2∆𝑠
2
=1
3 ∆𝑠 [(∆𝑠
2)3
−(−∆𝑠
2)3
]  
=1
3 ∆𝑠×2(∆𝑠
2)3
=2
3 ∆𝑠×∆𝑠3
8=∆𝑠2
12 
   
∴𝑒2̅̅̅=𝑚𝑠𝑒 =∆𝑠2
12  
 
   زي ما عرفنا من المحاضراالسابقة ان الباور هو متوسط تكامل الدالة تربيع حيث تكامل الدالــة  
تربيع هو الـ Energy وعشان اجيب المتوسط هقسم على  الـ interval .اللي كامل  فيها 
 
لو طبقنا المفهوم دا هنوصل لنفس عالقة التكامل حيث اننا هنكامل الــ ـ  error تربيــع( 𝑒2)  فــيالــ ـ 
range (−∆𝒔
𝟐 ,+∆𝒔
𝟐)   ونقسم علىالـ interval  اللي كامل  فيها1
∆𝑠 
 
 
 
 
 
 
 
𝑺𝑵𝑹 =𝒑𝒐𝒘𝒆𝒓  𝒐𝒇 𝒔𝒊𝒈𝒏𝒂𝒍
𝒑𝒐𝒘𝒆𝒓  𝒐𝒇 𝒆𝒓𝒓𝒐𝒓=𝑺𝟐̅̅̅
𝒆𝟐̅̅̅=𝟏𝟐 𝑺𝟐̅̅̅
∆𝒔𝟐 
 
SNR   هو النسبة بين باوراإلشارة الى باور الـ noise 
  باوراإلشارة  𝑆2̅̅̅=𝐴2
2     لو كاناإلشــارة  sinusoidal  حيــث  𝐴    هــو amplitude  اإلشــارة  ,و  𝑒2̅̅̅ 
  هي باورالـ noise ( Mean square error)  
 
 المسائل في الجزء دا على uniform quantization ومطلوب فيه حساب  Signal to Noise Ratio 
(SNR)  والمقصود بالـ Noise هنا هي Quantization Noise 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Signal to noise ratio (SNR)  
 
Consider an audio signal comprised of the sinusoidal term  
𝒔(𝒕)=𝟑 𝐜𝐨𝐬 (𝟓𝟎𝟎𝒕 ) 
(a) Find the signal to quantization noise ratio if this is quantized using 10 -bit PCM  
(b) How many bits of quantization are needed to achieve a signal to quantization 
noise ratio of at least 40 dB?  
Solution  
(a) SNR  
the total swing of the signal is 6 V so the size of each interval equal  
∆𝒔=𝐷𝑅
2𝑛=6
210=5.86×10−3 
The signal power is  
𝑆2̅̅̅=𝐴2
2=32
2=4.5 
The signal to quantization noise ratio is then given by  
SNR =12 𝑆2̅̅̅
∆𝑠2=12 ×4.5
(5.86×10−3)2=1.57×106 
If we wish to express this in decibels, we take its log and multiply by 10. Therefore,  
SNR =10log(1.57×106)=62 𝑑𝐵 
 
(b)  
40 𝑑𝐵=10log(SNR )→SNR =104 
SNR ≥104 
12 ×4.5
∆𝑠2≥104 
∆𝑠≤7.35×10−2 
6
2𝑛≤7.35×10−2 
2𝑛≥81.6 
𝑛≥6.35 
∴𝑛𝑚𝑖𝑛 =7 𝑏𝑖𝑡𝑠 Example  3 
 
 
It's desired to set up control station for monitoring ECGs of 10 patients. The data 
from the room of the 10 patients are sampled, quantized and binary coded and time 
division multiplexed. The multiplexed data are now transmitted over wires to the 
monitoring station. The ECGs signal bandwidth is 100Hz . The maximum acceptance 
error in sample amplitude is 0.25% oof the peak signal amplitude. The sampling 
must be at least twice the Nyquist rate.  
Determine the data rate (bit/sec) transmitted over the cable.  
Solution  
  مطلوب عمل station    لمراقبةإشارا   القلب  ECGs  لـ  10    مرضى حيث يتم اخذ عينا  منإشارة  كــل 
مريض ونحولهــا الــى المكــافئ الــ ـ  binary    ونســتخدم TDM  حيــث  يــتم ارســالهم فــي  frame  مكــون مــن 
samples 10 ويتم ارسال الداتا عبر االسالك الى المكان الذي سيتم فيه مراقبة اإلشارة. 
اعلى تردد في إشارا  الـ ECGs  يساوي 100 Hz ,واعلى error  مسموح بيه فياإلشــارة هــو 0.25% 
 من قيمة اعلى amplitude  فياإلشارة. 
Nyquist rate  =200  𝐻𝑧=2𝑓𝑚 
Sampling rate =2 Nyquist rate  =4 𝑓𝑚=4×100 =400  𝐻𝑧 
Total number of samples for 10 signals =400 ×10=4000  samples /sec 
 
 اقصى خطم مسموح بيه فياإلشارة ونساويه ب ـ  ∆𝑠
2   هحصل على عدد المستوياL   ومنها نحصل
على عدد الـ  bits لكل sample 
Quantized error ≤0.25
100𝑚𝑝=𝑚𝑝
400 
Maximum quantization error =∆𝑠
2=  2𝑚𝑝
2𝐿=𝑚𝑝
𝐿      ,    ∆𝑠=2𝑚𝑝
𝐿 
𝑚𝑝
𝐿=𝑚𝑝
400 →𝐿=400 
𝑛=log 2400 ≅9 𝑏𝑖𝑡 
 
 فيه  4000  samples  وكل sample  معمولها coding في 9 bit وبالتالي الـ bit rate  :يساوي 
∴bit rate =9 ×4000 =36 KHz  (Kbit /sec)  
 
 
 Example  4 
 
 
 
▪ Delta modulation is a simple technique for reducing the dynamic range of the 
numbers to be coded.  
 
▪  Instead of sending each sample value, we send the difference between a sample 
and previous sample.  
 
▪ Delta modulation quantizes this difference using only one bit of quantization. Thus , a 
"1" is sent if the difference is positive, and a "0" is sent if the difference is negative.  
 
▪ We shall refer to these two possibilities as either +∆ or −∆. At every sample point, 
the quantized waveform can increase or decrease by ∆. 
 
 هي طريقة بقلل بيهاالـ  dynamic rangeلألرقام اللي عايز اعملها coding  الن بدل مــا ابعــ  المكــافئ
الـ binary للـ sample value ببع  كود الفرق بين  قيمة  الـ sample  والـ previous sample. 
 يتم التعبير عن الفرق دا بـ 1 bit   فقط, لو الفرق positive    ببع1  و لو الفرق  negative     ببع0. 
 
 لو الفرق positive   هيزيد بمقدار∆  ولو   negative   هيقل بمقدار∆ ودا هيوضح اكتر من الرسم. 
 
 
في البداية هناخد قيمة الـ  sample  (هنرمز ليه بالرمزa  ) مع خــراالــ ـ staircase  (هنرمــز ليــه بــالرمزb )
  وندخلهم على comparator    . لــو 𝒂>𝒃  اذا    الــ ـ  staircase  هيزيــد خطــوة لفــوق  (هيزيــد بمقــدار  ∆  ,)   لــو
𝒂<𝒃 اذا   الــ ـ staircase  هيقــل خطــوة لتحــ (هيقــل بمقــدار ∆  ), وهكــذا مــع كــل sample وفــي النهايــة 
هن حصل على شكل مقارب لشكل  اإلشارة األصلية. 
ودا سبب تسميته  ب ـ  Delta Modulation  ألن  الخرا بيقل ويزيد بمقدار  ∆ Delta. 
 
 
Delta Modulation  
 
Since the quantized waveform can only either increase or decrease by 𝞓 at each  
sample point, we shall attempt to fit a staircase approximation to the analog  waveform.  
 
If the staircase is below the analog sample value, the decision is to increment 
positively (an up step). If the staircase is above, we increment negatively (down step).  
 
 
 
 دولالـ bits  الليبتعبر عن المكافئ ال ـ binary لإلشارة حيث الزيــادة بمقــدار ∆  بيطلــع binary 1 
والنقصان بمقدار   ∆   بيطلع binary 0.  
 
The key to effective use of delta modulation is the intelligent choice of the two 
parameters, step size and sampling rate.  Increasing the sampling frequency means that 
the delta -modulated waveform requires larger bandwidth.  Increasing the step size 
increases the quantization error.  
 
 من الحاجا  المهمة اللي الزم اراعيها في الطريقة دي عشانإشارة الـ staircase الخارجة تكون 
قريبه من اإلشارة االصلية وهي الـ sampling rate  حيث ان زيادةالـ sampling rate  هيزود الـــ
Bandwidth  والـ step size   لو زاد بشكل كبير دا هيزود quantization error. 
 
 
 
 
 
 
  قيمةالـ  sample  اقل منالـ staircase  
  اذا الـ staircase   هيقل بمقدار∆ 
  قيمةالـ  sample  اعلى منالـ   
staircase   اذا الـ   
staircase   هيزيد بمقدار∆ 
  قيمةالـ  staircase   
 
 قيمةالـ  sample 
 
 
 
If a bit error occurs in delta modulation, the D/A converter in the receiver will go  up instead 
of down (or vise versa) and all later values will contain an error.  
 من عيوب Delta modulation  دي ان لو حصل خطم فيbit واحده كل اللي بعده هيكون خطم.  
 
If the steps are too small, we can experience a slope overload condition, where  the 
staircase cannot track rapid changes in the analog signal.  
 
 لوالـ step size  صغيره والتغير فياإلشارة كان كبير الـ staircase  مش
هتقدر توصل لشكل اإلشارة الـ analog  زي ما واضح من الصورة
وهيحصل خطم اسمه slope overload 
 
 
 
 
If the steps are too large, considerable overshoot will occur  during periods when the signal 
is not changing rapidly. We have significant  quantization noise, known as granular noise.  
 
 لوالـ step size  كبير والتغير فياإلشارة كان صغير الـ staircase 
 هيفضل يزيد ويقل بـ∆  ومش هتقدر توصل لشكلاإلشارة الـ analog  
 زي ما واضح من الصورة وهيحصل خطم اسمه granular noise 
 
 
 
 
  المعدل اللي بتتغير بيهاإلشارة هو اللي بيحكمني في اختيار  step size .مناسب 
 
Disadvantages  

 
 
  
 
 
 
 
  
 
©
 
Basem Hesham
Frequency Shift Keying (FSK)
 & Phase Shift Keying (PSK)
Lecture
 
11
 
 
 فيحالة FSK   يتم تغيير تردد الـ carrier  تبعا لقيمة الـ information signal  مع ثبات قيمة الـ amplitude   ,وبالتالي
عندي ترددين لإلرسال, األول  𝑓1  عندما نرسل binary 1    , والتردد الثاني𝑓2  عندما نرسل binary 0   
 عشان نعمل ديزاين الـ matched filter  للـFSK    هنحتاج نجيب معادلتين 𝑠1(𝑡)    و 𝑠2(𝑡)   حيث ان 𝑠1(𝑡)   هي الـ
modulated signal  المعبرة عن binary 1   و 𝑠2(𝑡)  هي الـ modulated signal  المعبرة عن binary 0 
 
▪ Digital signal     Sb(t)=±v 
نعبر عن binary 1  بـ  +v  وعن binary 0  بـ  −v 
▪ Carrier signal    𝑓𝑐(𝑡)=Acos2πfct 
 
▪ FSK signal      SFSK=Acos(2πfct+2π Kf ∫Sb(t)𝑑𝑡 )   
    
 عشان نفهم المعادلة اللي فاتت الزم نعرف عالقة مهمه بين التردد والـ phase  كنا درسناها قبل كدا وهي ان التردد
اللحظي instantaneous frequency 𝑓𝑖(𝑡)   يساوي تفاضل الـ phase   : 
𝑓𝑖(𝑡)=1
2𝜋 𝜃̇(𝑡)=1
2𝜋 𝑑𝜃(𝑡)
𝑑𝑡  
  عشانالتردد يتناسب مع  الـ information signal  يبقى الزم في معادلة الـ phase في الجزء اللي هنضيفه عشان نغير  
في التردد يكون حاجة مضروبة في  𝑡   عشان لما نفاضل ونجيب 𝑓𝑖(𝑡)    يكون عبارة عن الـ carrier  مُضاف عليهإشارة   
المعلومات مضروبة في ثابت التناسب  Kf  . 
 اخدنا المعادلة دي فيFM   وكنا بنتعامل مع عدد ال نهائي من القيم فيإشارة المعلومات وبالتالي التردد اللحظي ليه قيم  
كتير جدا ولكن في  FSK  عندنا قيمتين فقط فيإشارة  المعلومات   وفي الحالة دي هم  ±v 
 
SFSK=Acos(2π fct±2π Kf v t) 
 
 التردد والـ phase  االتنين بيتغيرو فيFSK   ولكن التردد يتناسب معالـ information signal  والـ phase   يتناسب مع
تكامل الـ information signal 
▪ Maximum  frequency deviation       ∆f=Kf v 
SFSK=Acos(2πfct±2π ∆f t)=Acos(2π(fc±∆f) t) 
∆f  هو اقصى زيادة او نقص ممكن تحصل عن تردد الـ carrier  وبما ان عندنا قيمتين فقط وبالتالي ترددين ,التردداألول   
هو قيمة الـ carrier   مُضاف عليها ∆f  وداالتردد المعبر عن ارسال  binary 1    ,والتردد الثاني هو قيمة الـ carrier  ناقص
∆f  ودا التردد المعبر عن ارسال binary 0   
 Frequency Shift Keying (FSK) 
 
𝑓𝑖(𝑡)=1
2𝜋 𝑑𝜃(𝑡)
𝑑𝑡 =2π(fc±∆f)
2𝜋=fc±∆f 
 
f1=fc+∆f 
f2=fc−∆f 
 
 بالتالي المعادلتين اللي هستخدمهم في التعبير عن binary 1  و binary 0 : 
 
s1(t)=Acos(2πf1t)        binary  1  
s2(t)=Acos(2πf2t)         binary  0  
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
s1(t)=Acos(2πf1t) 
s2(t)=Acos(2πf2t) 
 
 Energy  (E) 
 
𝐸1=∫𝑠12(𝑡) 𝑑𝑡𝑇𝑏
0=𝐴2
2 𝑇𝑏 
 
𝐸2=∫𝑠22(𝑡) 𝑑𝑡𝑇𝑏
0=𝐴2
2 𝑇𝑏 
 
E=E1+E2
2= [A2
2 Tb+A2
2 Tb
2]=A2
2 Tb 
 
 Threshold (𝒚𝟎) 
 
𝑦𝑜=𝐸1−𝐸2
2=[𝐴2
2 𝑇𝑏−𝐴2
2 𝑇𝑏
2]=0 
 
Design and Performance for FSK 
∫   𝑑𝑡𝑇𝑏
0 ∫   𝑑𝑡𝑇𝑏
0 
𝑠2(𝑡)=𝐴𝑐𝑜𝑠(2𝜋𝑓2𝑡) 𝑆𝐹𝑆𝐾 
𝑠1(𝑡)=𝐴𝑐𝑜𝑠(2𝜋𝑓1𝑡) ∑  
 
 Cross correlation  ( 𝝆 ) 
 
𝜌=1
E∫ s1(t) s2(t) dtTb
0=2
A2Tb∫ Acos(2πf1t) Acos(2πf2t) dtTb
0 
=2
A2Tb×A2
2∫ [cos(2π(f1−f2)t)+ cos(2π(f1+f2)t)] dtTb
0 
=1
Tb[∫ cos(2π(f1−f2)t) dtTb
0+∫ cos(2π(f1+f2)t) dtTb
0] 
f1=fc+∆f 
f2=fc−∆f 
∴f1−f2=2∆f                ,                      f1+f2=2fc  
𝜌=1
Tb[∫ cos(4π∆f t) dtTb
0+∫ cos(4πfct) dtTb
0] 
=1
Tb[  sin(4π∆f t)
4π∆f|
0Tb
+ sin(4πfct)
4πfc|
0Tb
]=1
Tb[  sin(4π∆f Tb)
4π∆f+ sin(4πfcTb)
4πfc] 
fc≫∆f 
 sin(4πfct)Tb
4πfc   تقريبا تساوي صفر الن𝑓𝑐  الموجودة في المقام قيمتها كبيره جدامقارنة بـ  ∆f 
 
∴ 𝜌=sin(4π∆f Tb)
4π∆f Tb 
 
  التشابه بين االشارتين s1(t)  و  s2(t)   يعتمد على∆f  وبالتالي يعتمد على التردداتf1    وf2 ,وايضا  يعتمد على  Tb 
ومقلوبه هو معدل االرسال  bit rate Rb  
 
 
 
 
 
 
 
 
 Probability of error  
 لحساب نسبة الخطأفيه  عندي 3   حاالت على حسب قيمة الـ   𝜌  : 
 الحالةاألولى  ( General Case )  في الحالة دي بنستخدم القانون العام للـ  𝜌  لو لم يتم في السؤال ذكر الحاالتاالخرى.  
الحالة الثانية  ( Maximum performance   ) ودي بنستخدم فيها𝜌  .اللي تحقق اعلى كفاءة 
الحالة الثالثة ( Orthogonal )   بنعوض فيها عن 𝜌=0. 
 General Case  
𝜌=sin(4π∆f Tb)
4π∆f Tb 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐 √𝐸 (1−𝜌)
2 𝑁𝑜 
 
 For Maximum performance  
Minimum  probability of error  𝑝𝑒 at minimum  𝜌 
اعلى كفاءة ممكن احصل عليها هي عند اقل قيمة للـ   𝜌   فهنجيب قيمة∆f   اللي تحقق اقل𝜌    من خالل اننا نفاضل معادلة
الـ 𝜌   بالنسبة لـ∆f   ونساوي ناتج التفاضل بصفر 
 
For minimum 𝜌 →  𝜕𝜌
𝜕∆f=0 
𝜕𝜌
𝜕∆f=4π∆f Tb×cos(4π∆f Tb)×4πTb−sin(4π∆f Tb)×4πTb
(4π∆f Tb)2=0 
 
4π∆f Tb×cos(4π∆f Tb)×4πTb−sin(4π∆f Tb)×4πTb=0 
4π∆f Tb×cos(4π∆f Tb)=sin(4π∆f Tb) 
 
4π∆f Tb×cos(4π∆f Tb)
cos(4π∆f Tb)=sin(4π∆f Tb)
cos(4π∆f Tb) 
 
4π∆f Tb=tan(4π∆f Tb) 
4π∆f Tb=0.715 ×2π   rad 
∆f=0.715 ×2π
4π Tb=0.715
2 Tb   
 حل المعادلة tan(𝑥)=x   ممكن نجيبهاباننا أوال نعمل االله على نظام الراديان ونكتـب المعادلـة ونعمـل  solve shift → 
على االله  هنالقي حل المعادلة 0 وطبعا الحل دا مرفوض  الن كدا ∆f هتكون بصفر فعشان نجيب الحل التاني هنعمل  solve 
shift →  تاني بس عند  solve for x  هنختار x بـ 1  هنالقي الحل بصفر بردو فهنجرب تاني عند x بـ 2 هنالقي الحل 4.493 
 واللي هو 0.715 ×2π   
 
 الرســـم البيـــاني االتـــي يوضـــد حلـــول المعادلـــة
tan(𝑥)=x   عنـــد تقـــاطع الخـــط𝑥  مـــع منحنـــى
𝑡𝑎𝑛(𝑥)  والحــلاألول عنــد 𝑥=0  ودا مرفــوض
والحل التاني عنـد  𝑥=4.493 =0.715 ×2π  
ودا الحــل اللــي هناخــده الن دا اللــي هيحقــق اقــل 𝜌 
وباقي الحلول   لو عوضنا بيها  هنالقي انها بتحقق   قيم 
اكبر للـ  𝜌 
1
 Tb=Rb 
∴∆f=0.715 ×Rb
2   
Substitution in 𝜌 
𝜌=sin(4π×0.715 ×Rb
2×Tb)
4π×0.715 ×Rb
2× Tb=sin(2π×0.715 )
2π×0.715=−0.22 
 واحنا بنعوض نضرب اللي داخل الـsin   في180
π  لو شغالين بنظام الـ degree    على االله 
 
Minimum probability of error at  𝜌=−0.22 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐 √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐 √1.22 𝐸
2 𝑁𝑜 
 
 عرفنا المحاضرة اللي فاتت ان قيمة𝜌  تتراوح من1  الى-1  ←   −1<𝜌<1   
 وعرفنا ان اعلى كفاءة نحصل عليها يعني اقل نسبة خطأ هي عند اقل قيمة للـ𝜌  عشان في معادلة𝑝𝑒  دا هيزود القيمـة
داخل 𝑒𝑟𝑓𝑐  وبالتالي نسبة الخطأ هتقل وفي حالةFSK  اقل قيمة𝜌  ممكن نوصلها هي −0.22 
 
 لكن لو هنتكلم عن minimum 𝜌   من مفهوم التشـابه بـيناإلشـارات , 𝜌=1  معناهـا ان االشـارتين متشـابهين تمامـا و
𝜌=−1  معناها ان االشارتين متشابهين بردو ولكن معكوسين يعني بينهم phase shift 180  فلـو عرفنـاإشـارة مـنهم 
هنعرف اإلشارة التانيه وبالتالي minimum 𝜌   واللي نقصد بيه هنا اقل تشابه بين االشارتينمش عنـد 𝜌=−1  ولكـن
لما يكونوا orthogonal  يعني 𝜌=0  
 
 
 Orthogonal Tone Spacing  
   
orthogonal   يعني تعامد والتعامد يعني اختالف معناها ان لو عندي اشارتين وقولنا انهم orthogonal  يبقى هم مختلفـين
تمام عن بعض وبالمعنى الرياضي يعني  تكامل  حاصل ضربهم بصفر وبالتالي الـ cross  correlation  بينهم يساوي صفر
 𝜌=0 
 

 
Tone Spacing   يعني الفرق بين ترددإشارة  𝑠1(𝑡)   وتردد اشارة 𝑠2(𝑡)     ← 𝑓1−𝑓2
2=∆f      وبالتالي المقصود بالـ
spacing  هنا هي  ∆f 
 
  اذا المقصود بـ  orthogonal tone spacing هي قيمة  ∆f  اللي تحقق الـ orthogonality   بين االشارتين يعني 𝜌=0  
 
For the two tones with frequencies 𝑓1 and 𝑓2 to be orthogonal (i.e. 𝜌=0 ) a condition must be satisfied  
∆f=𝑛 Rb
2 
𝑓1−𝑓2=2∆f=𝑛Rb 
 
 اذا  في حالة FSK   عشان نحصل على الـ orthogonality  الزم الفرق بين الترددين  𝑓1   و𝑓2  يساوي الـ bit rate   او
مضاعفاته  
 
بالتعويض بعالقة  ∆f   السابقة هنحصل على 𝜌=0   حيث𝑛 أي رقم صحيد وبالتالي اقل قيمة لـ  ∆f  تحققالـ  
orthogonality   لما تكون 𝑛=1 
The minimum value of ∆f that satisfies orthogonality is for 𝑛=1  
∆f=Rb
2 
Substitution in 𝜌 
𝜌=sin(4π∆f Tb)
4π∆f Tb=sin(4π×Rb
2 ×Tb)
4π×Rb
2 × Tb=sin(2π)
2π=0 
 
Larger ∆f  means wider separation between signaling frequencies and consequence larger transmission 
bandwidth. To minimize bandwidth, ∆f  should be as small as possible. The minimum value of ∆f that 
can be used for orthogonal signaling is Rb
2 
 
  الحالة السابقة اللي فيها∆f  تحقق الـ orthogonality   وايضا تكون اقل قيمة ممكنه لما نعوض بـ 𝑛=1     بنقول عليها
minimum shift keying (MSK)    وهي حالة خاصة منFSK   فيها قيمة الـ𝜌   , تساوي صفرو بحقق معاه اقل  ∆f  
 وبالتالياقل bandwidth 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐸
2 𝑁𝑜 
∴𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐴2 𝑇𝑏
4 𝑁𝑜 
 
 
 
 
 
F
ind the probability of error for the FSK system
,
 
where
 
𝒔
𝟏
(
𝒕
)
=
𝟏
.
𝟒𝟏𝟒
𝒄𝒐𝒔
(
𝟏𝟎𝟏𝟎𝒕
)
 
𝒔
𝟐
(
𝒕
)
=
𝟏
.
𝟒𝟏𝟒
𝒄𝒐𝒔
(
𝟏𝟎𝟎𝟎𝒕
)
 
Noise power spectral density  
𝑵
𝟎
𝟐
=
𝟎
.
𝟎𝟏
  
is added, and 
a
 
matched filter detector is used. Assume 
that the bit period is 1 sec. 
 
Solution
 
 
 
 
 
 
 
 
 
 
 مادام مقالش نشتغل
maximum performance
 
 او
orthogonal tone spacing
 
   يبقى هنشتغل على القانون العام للـ
𝜌
 
𝐸
1
=
𝐸
2
=
𝐴
2
2
 
𝑇
𝑏
=
1
.
414
2
2
 
×
1
=
1
 
 
Watt
.
sec
 
𝐸
=
1
 
 
 
Watt
.
sec
 
𝑦
𝑜
=
0
 
 
 
Watt
.
sec
 
f
1
−
f
2
=
2
∆
f
 
∴
∆
f
=
f
1
−
f
2
2
=
1010
2
𝜋
−
1000
2
𝜋
2
=
2
.
5
𝜋
 
Hz
 
𝜌
=
sin
(
4π
∆
f
 
T
b
)
4π
∆
f
 
T
b
=
sin
(
4π
×
2
.
5
𝜋
×
1
)
4π
×
2
.
5
𝜋
×
 
1
=
sin
(
10
×
180
𝜋
)
10
=
−
0
.
054
 
 
 مننساش ان اللي داخل الـ
sin
 
  بالراديان فضربنا في
180
𝜋
 
 عشان نحولها لـ
degree
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
1
 
(
1
+
0
.
054
)
2
 
×
0
.
02
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
(
5
.
1
)
 
 
5
.
1
 
 مش موجوده في الجدول فهنسيب الناتج بداللة الـ
erfc
 Example
 
1
 
∫
 
𝑑𝑡
1
0
 
∫
 
𝑑𝑡
1
0
 
𝑠
(
𝑡
)
 
𝑦
<
0
 
 
 
 
 
 
 
 
 
0
 
𝑦
>
0
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
=
0
 
 
 
𝑇
𝑏
=
1
 
𝑠𝑒𝑐
 
𝑆
𝐹𝑆𝐾
 
𝑠
1
(
𝑡
)
=
1
.
414
𝑐𝑜𝑠
(
1010
𝑡
)
 
𝑠
2
(
𝑡
)
=
1
.
414
𝑐𝑜𝑠
(
10
0
0
𝑡
)
 
∑
 
 
 فيحالة PSK  يتم تغيير phase اشارة الـ carrier  تبعـا لقيمـة الــ information signal  مـع ثبـات قيمـة الــ amplitude 
 ,وبالتالي عندي اتنين phase  ,لإلرسالاألول 𝜃1  عندما نرسل binary 1  , و الثاني𝜃2  عندما نرسل binary 0  
 عشان نعمل ديزاينالـ matched filter  للــPSK  هنحتـاج نجيـب معـادلتين 𝑠1(𝑡)  و 𝑠2(𝑡)  حيـث ان 𝑠1(𝑡)  هـي الــ
modulated signal   المعبرة عن binary 1  و 𝑠2(𝑡)  هي الـ modulated signal  المعبرة عن binary 0 
 
▪ Digital signal     Sb(t)=±v 
نعبر عن binary 1  بـ  +v   ونعبر عن binary 0  بـ  −v 
▪ Carrier signal    𝑓𝑐(𝑡)=Acos2πfct 
 
▪ PSK signal      SPSK =Acos(2πfct+Kp Sb(t) )   
    
 عشان نخلي جزء الـ phase  يتغير مع الـ information signal  هنضيف على الـ phase إشارة المعلومات  Sb(t) 
  مضروبة في ثابت التناسبKp    
 
  نعوض عن Sb(t)    بـ± v  اللي تمثل1 binary  و0 binary  وبالتالي نحصل على العالقة االتيه:  
SPSK =Acos(2πfct±Kp v) 
 التردد والـ phase  االتنين بيتغيرو مع الزمن فيPSK   ولكن التردد يتناسب مع تفاضلالـ information signal   والـ
phase  يتناسب معالـ information signal 
𝑓𝑖(𝑡)=1
2𝜋 𝑑𝜃(𝑡)
𝑑𝑡 =fc+Kp
2𝜋 Sb(t) 
 
▪ Max phase deviation       𝛽=∆θ=Kp v 
SPSK =Acos(2πfct±𝛽) 
▪ Assume   +𝛽=𝜃1    ,   −𝛽=𝜃2   
 
 
𝑠1(𝑡)=Acos(2πfct+θ1)        binary   1 
𝑠2(𝑡)=Acos(2πfct+θ2)         binary   0 
 
 
 
 Phase Shift Keying (PSK)  
 
 
 
 
 
 
 
 
 
 
 
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
+
𝜃
1
)
 
 
𝑠
2
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
+
𝜃
2
)
 
 

 
Energy
 
(E)
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
𝐴
2
2
 
𝑇
𝑏
 
 
𝐸
2
=
∫
𝑠
2
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
𝐴
2
2
 
𝑇
𝑏
 
 
𝐸
=
𝐸
1
+
𝐸
2
2
=
 
[
𝐴
2
2
 
𝑇
𝑏
+
𝐴
2
2
 
𝑇
𝑏
2
]
=
𝐴
2
2
 
𝑇
𝑏
 
 

 
Threshold 
(
𝒚
𝟎
)
 
 
𝑦
𝑜
=
𝐸
1
−
𝐸
0
2
=
[
𝐴
2
2
 
𝑇
𝑏
−
𝐴
2
2
 
𝑇
𝑏
2
]
=
0
 
 
 
 
Design and Performance for PSK
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
𝑠
2
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
+
𝜃
2
)
 
 
S
PSK
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
+
𝜃
1
)
 
 
∑
 
 Cross correlation  
 
𝜌=1
𝐸∫𝑠1(𝑡) 𝑠2(𝑡) 𝑑𝑡𝑇𝑏
0=2
𝐴2𝑇𝑏∫𝐴𝑐𝑜𝑠(2𝜋𝑓𝑐𝑡+𝜃1) 𝐴𝑐𝑜𝑠(2𝜋𝑓𝑐𝑡+𝜃2) 𝑑𝑡𝑇𝑏
0 
=2
𝐴2𝑇𝑏×𝐴2
2∫[𝑐𝑜𝑠(4𝜋𝑓𝑐𝑡+𝜃1+𝜃2)+ 𝑐𝑜𝑠(𝜃1−𝜃2)] 𝑑𝑡𝑇𝑏
0 
=1
𝑇𝑏[∫𝑐𝑜𝑠(4𝜋𝑓𝑐𝑡+𝜃1+𝜃2) 𝑑𝑡𝑇𝑏
0+∫𝑐𝑜𝑠(𝜃1−𝜃2) 𝑑𝑡𝑇𝑏
0] 
=1
𝑇𝑏[  𝑠𝑖𝑛(4𝜋𝑓𝑐𝑡+𝜃1+𝜃0)
4𝜋𝑓𝑐|
0𝑇𝑏
+𝑐𝑜𝑠(𝜃1−𝜃2) 𝑡 |0𝑇𝑏]≅1
𝑇𝑏[  0+𝑐𝑜𝑠(𝜃1−𝜃2) 𝑇𝑏] 
 
 𝑠𝑖𝑛(4𝜋𝑓𝑐𝑡+𝜃1+𝜃0)
4𝜋𝑓𝑐   تقريبا تساوي صفر الن𝑓𝑐   الموجودة في المقام قيمتها كبيره جدا 
 
∴ 𝜌=𝑐𝑜𝑠(𝜃1−𝜃2)  
 
 واضد من المعادلة ان الـ cross correlation   تعتمد على𝜃1   و𝜃2   ودا منطقي الن قيمهم لما تتغير دا بيغير شكل
إشارة  𝑠1(𝑡)   و  𝑠2(𝑡)  .وبالتالي التشابه بينهم هيتغير 
 
 Maximum performance  
maximum  performance at minimum 𝜌               
 
 اعلى كفاءة عندما تكون قيمة   𝜌  اقل قيمة الن داهيقلل قيمة احتمال الخطأ  𝑝𝑒  , واقل قيمة عند  𝜌=−1    ودي ممكن
نحصل عليها لما الـ phase shift    بين𝜃1   و𝜃2  يكون بـ180 
 
𝑐𝑜𝑠(𝜃1−𝜃2)=−1  
𝜃1−𝜃2=𝜋 
𝜃1=𝜋
2      ,     𝜃2=−𝜋
2 
 
 
 Probability of error (bit error rate)  
For maximum performance at 𝜌=−1 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1+1)
2 𝑁𝑜 
∴ 𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 
 𝑁𝑜 
 
For the same performance, the pulse energy in ASK must be twice that in PSK ,thus in coherent 
detection, PSK is always performable to ASK  
  فيASK   فيحالة OOK  كانت( 𝜌=0)  واحتمال الخطأ يساوي  (0.5 𝑒𝑟𝑓𝑐  √𝐸 
2 𝑁𝑜)   وبالتـالي احتمـال الخطـأ فـي
PSK    اقلوعشان نستخدم   ASK    بنفس الكفاءة الـ Energy    بتاعةاإلشارة  هتكون الضـعف  ,ودا منطقـي الن   ASK   اكثـر
تأثرا بالـ noise  الن الـ amplitude  بيتأثر بالـ noise  اكتر من الـ  phase وبالتاليكفاءة System PSK  اعلى من كفاءة
System ASK 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
A coherent matched filter detector is used to detect the following two signals 
 
𝐬
𝟏
(
𝐭
)
=
𝟎
.
𝟎𝟓
𝐜𝐨𝐬
(
𝟐𝛑
𝐟
𝐜
𝐭
+
𝟗𝟎
)
 
𝐬
𝟐
(
𝐭
)
=
𝟎
.
𝟎𝟓
𝐜𝐨𝐬
(
𝟐𝛑
𝐟
𝐜
𝐭
−
𝟗𝟎
)
 
bit period 
 
𝑻
𝒃
=
𝟎
.
𝟓
 
𝒔𝒆𝒄
 
and
 
power spectral noise
 
𝑵
𝟎
=
𝟏𝟎
−
𝟒
 
Find the 
probability of bit error
 
Solution
 
 
 
 
 
 
 
 
 
 
 في
المسالة
 
لم يتم ذكر نوع الـ 
modulation
 
 ولكن من المعادالت هنعرف انه
PSK
  
 الن التغير بيحصل في الـ
phase
 
𝐸
1
=
𝐸
2
=
𝐴
2
2
 
𝑇
𝑏
=
0
.
05
2
2
 
×
0
.
5
=
6
.
25
×
10
−
4
 
 
Watt
.
sec
 
𝐸
=
𝐴
2
2
 
𝑇
𝑏
=
6
.
25
×
10
−
4
 
 
Watt
.
sec
 
𝑦
𝑜
=
0
 
𝜌
=
𝑐𝑜𝑠
(
𝜃
1
−
𝜃
2
)
 
=
𝑐𝑜𝑠
(
90
+
90
)
=
−
1
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
6
.
25
×
10
−
4
 
10
−
4
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
(
2
.
5
)
=
0
.
5
×
0
.
00
041
=
2
.
05
×
10
−
4
 
 
 
 
 
 
 
 Example
 
2
 
∫
 
𝑑𝑡
0
.
5
0
 
∫
 
𝑑𝑡
0
.
5
0
 
𝑠
(
𝑡
)
 
𝑦
<
0
 
 
 
 
 
 
 
 
 
0
 
𝑦
>
0
 
 
 
 
 
 
 
 
 
1
 
𝑦
𝑜
=
0
 
 
 
𝑇
𝑏
=
0
.
5
 
𝑠𝑒𝑐
 
𝑠
2
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
−
90
)
 
S
PSK
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
𝑐
𝑡
+
90
)
 
∑
 
 
 
Matched 
filter can be
 
used as coherent detector for BFSK demodulation:
 
a.
 
Design a matched filter to demodulate BFSK signal.
 
b.
 
Deduce the probability of bit error rate
.
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
s
1
(
t
)
=
A
cos
(
2π
f
1
t
)
 
s
2
(
t
)
=
A
cos
(
2π
f
2
t
)
 
 

 
Energy
 
(E)
 
𝐸
1
=
∫
𝑠
1
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
𝐴
2
2
 
𝑇
𝑏
 
𝐸
2
=
∫
𝑠
2
2
(
𝑡
)
 
𝑑𝑡
𝑇
𝑏
0
=
𝐴
2
2
 
𝑇
𝑏
 
E
=
E
1
+
E
2
2
=
 
[
A
2
2
 
T
b
+
A
2
2
 
T
b
2
]
=
A
2
2
 
T
b
 
 

 
Threshold 
(
𝒚
𝟎
)
 
𝑦
𝑜
=
𝐸
1
−
𝐸
2
2
=
[
𝐴
2
2
 
𝑇
𝑏
−
𝐴
2
2
 
𝑇
𝑏
2
]
=
0
 
 
 
  
[Extra] 
Related Exams Questions
 
Question 
1
 
(Final 20
20
)
 
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
∫
 
 
𝑑𝑡
𝑇
𝑏
0
 
𝑠
1
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
1
𝑡
)
 
𝑠
2
(
𝑡
)
=
𝐴
𝑐𝑜𝑠
(
2
𝜋
𝑓
2
𝑡
)
 
𝑆
𝐹𝑆𝐾
 
∑
 
 Cross correlation ( 𝝆 ) 
 
𝜌=1
E∫ s1(t) s2(t) dtTb
0=2
A2Tb∫ Acos(2πf1t) Acos(2πf2t) dtTb
0 
=2
A2Tb×A2
2∫ [cos(2π(f1−f2)t)+ cos(2π(f1+f2)t)] dtTb
0 
=1
Tb[∫ cos(2π(f1−f2)t) dtTb
0+∫ cos(2π(f1+f2)t) dtTb
0] 
f1=fc+∆f 
f2=fc−∆f 
∴f1−f2=2∆f                ,                      f1+f2=2fc  
𝜌=1
Tb[∫ cos(4π∆f t) dtTb
0+∫ cos(4πfct) dtTb
0] 
=1
Tb[  sin(4π∆f t)
4π∆f|
0Tb
+ sin(4πfct)
4πfc|
0Tb
]=1
Tb[  sin(4π∆f Tb)
4π∆f+ sin(4πfcTb)
4πfc] 
fc≫∆f 
∴ 𝜌=sin(4π∆f Tb)
4π∆f Tb 
 
 Bit error rate  (𝒑𝒆)   
 
 General Case  
𝜌=sin(4π∆f Tb)
4π∆f Tb 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐 √𝐸 (1−𝜌)
2 𝑁𝑜 
 
 For Maximum performance  
Minimum probability of error 𝑝𝑒 at minimum  𝜌 
For minimum 𝜌 →  𝜕𝜌
𝜕∆f=0 
 
 
𝜕𝜌
𝜕∆f=4π∆f Tb×cos(4π∆f Tb)×4πTb−sin(4π∆f Tb)×4πTb
(4π∆f Tb)2=0 
4π∆f Tb×cos(4π∆f Tb)×4πTb−sin(4π∆f Tb)×4πTb=0 
4π∆f Tb×cos(4π∆f Tb)=sin(4π∆f Tb) 
 
4π∆f Tb=tan(4π∆f Tb) 
4π∆f Tb=0.715 ×2π   rad 
 
∆f=0.715 ×2π
4π Tb=0.715
2 Tb   
1
 Tb=Rb 
∴∆f=0.715 ×Rb
2   
Substitution in 𝜌 
𝜌=sin(4π×0.715 ×Rb
2×Tb)
4π×0.715 ×Rb
2× Tb=sin(2π×0.715 )
2π×0.715=−0.22 
 
Minimum probability of error at  𝜌=−0.22 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐 √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐 √1.22 𝐸
2 𝑁𝑜 
 
 Orthogonal Tone Spacing  
∆f=Rb
2 
𝑓1−𝑓2=2∆f=Rb 
Substitution in 𝜌 
𝜌=sin(4π∆f Tb)
4π∆f Tb=sin(4π×Rb
2 ×Tb)
4π×Rb
2 × Tb=sin(2π)
2π=0 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐸
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐴2 𝑇𝑏
4 𝑁𝑜 
 
 
 
A binary matched filter detector is used to demodulate BFSK received signal  
i. Find the value of the correlation coefficient 𝝆 when orthogonal tone spacing is used.  
ii. For the above case, calculate the probability of bit error.  
Solution  
 
For the two tones with frequencies 𝑓1 and 𝑓2 to be orthogonal (i.e. 𝜌=0 ) a condition must be 
satisfied. The minimum value of ∆f that satisfies orthogonality is for 𝑛=1  
∆f=𝑛 Rb
2 
∆f=Rb
2 
𝑓1−𝑓2=2∆f=Rb 
 
Substitution in 𝜌 
𝜌=sin(4π∆f Tb)
4π∆f Tb=sin(4π×Rb
2 ×Tb)
4π×Rb
2 × Tb=sin(2π)
2π=0 
 
 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐸 (1−𝜌)
2 𝑁𝑜=0.5 𝑒𝑟𝑓𝑐  √𝐸
2 𝑁𝑜 
𝐸=𝐴2
2 𝑇𝑏 
𝑝𝑒=0.5 𝑒𝑟𝑓𝑐  √𝐴2 𝑇𝑏
4 𝑁𝑜 
 
 
 
 
 
 
 
 
 
 
 
 Question 2 (Final 201 8) 
 
 
 
In binary phase shift keying (BPSK), the two signals used to transmit a 0 and a 1 are:
 
𝒔
𝟎
(
𝒕
)
=
𝑨
𝒄𝒐𝒔
(
𝟐𝝅
𝒇
𝒄
𝒕
+
𝜽
𝟎
)
 
𝒔
𝟏
(
𝒕
)
=
𝑨
𝒄𝒐𝒔
(
𝟐𝝅
𝒇
𝒄
𝒕
+
𝜽
𝟏
)
 
What is the best choice of phase angles that achieves a minimum bit error rate? Why?
 
Solution
 
 
The best choice achieved by
 
𝜃
1
−
𝜃
0
=
180
 
Because 
the maximum performance occurred 
at minimum
 
𝜌
 
𝜌
=
cos
(
θ
1
−
θ
0
)
 
=
cos
(
180
)
 
=
−
1
 
Then 
the bit error rate will be minimum
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐸
 
(
1
+
1
)
2
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐸
 
 
 
 
 
 
 
Find the probability of error of the coherent detector when used for FSK, the bit period is 2
 
msec and the signals have amplitude of 
0.4 volt. The two frequencies are 1KHZ and 2 KHZ .
 
The additive noise has power of 
𝟏
𝟎
−
𝟏𝟐
 
W/HZ
 
Solution
 
𝐴
=
0
.
4
 
volt
 
 
 
 
,
 
 
 
 
 
 
𝑇
𝑏
=
2
 
msec
 
 
 
,
 
 
 
 
 
 
 
𝑓
1
=
2
 
KHz
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
𝑓
2
=
1
 
KHz
 
 
 
 
 
 
 
𝐸
=
𝐴
2
2
𝑇
𝑏
=
0
.
4
2
2
×
2
×
10
−
3
=
1
.
6
×
10
−
4
 
 
Watt
.
sec
 
𝑅
𝑏
=
1
2
 
msec
=
0
.
5
 
KHz
 
∆
𝑓
=
𝑓
1
−
𝑓
2
2
=
2
−
1
2
=
0
.
5
 
KHz
 
 
 
 
 
 
𝜌
=
sin
(
4π
∆
f
 
T
b
)
4π
∆
f
 
T
b
=
sin
(
4π
×
0
.
5
×
10
3
 
×
2
×
10
−
3
)
4π
×
0
.
5
×
10
3
 
×
2
×
10
−
3
=
sin
(
4π
)
4π
=
0
 
 
so
 
i
t
′
s
 
orthogonal
 
∆
𝑓
 
 حققت الـ
orthogonality
 
 و
ممكن نعرف
 
ان  
𝜌
=
0
 
 من غير التعويض في القانون
من خالل العالقة    
∆
f
=
𝑛
 
R
b
2
   
  وفي الحالة دي
𝑛
=
2
 
𝑝
𝑒
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
𝐸
 
(
1
−
𝜌
)
2
 
𝑁
𝑜
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
√
1
.
6
×
10
−
4
2
 
×
1
0
−
12
=
0
.
5
 
𝑒𝑟𝑓𝑐
 
(
8944
.
27
)
 
Question 
3
 
(Final 2016)
 
 
Question 
4
 
(Final 201
6
)
 
𝑁
𝑜
 
 
 
 
The 
Minimum Shift Keying (MSK) is a special case of Frequency Shift Keying (FSK),
 
explain.
 
Solution
 
 
Minimum shift keying (MSK) is a special type of frequency shift keying (FSK) when the two FSK 
frequencies are orthogonal
 
and 
the 
frequency separation 
(
∆
f
)
 
is minimum
 
 
∆
f
=
𝑛
 
R
b
2
 
when the two frequencies are orthogonal 
,the minimum value of 
∆
f
 
that satisfies orthogonality is for 
𝑛
=
1
 
 
∆
f
=
R
b
2
 
The minimum frequency separation in MSK is chosen to be half the bit rate
 
 
 
 
 
 
 
 
 
 
بكدا
 
نكون وصلنا لنهاية الكورس  
يارب يكون ضاف ليكو حاجة جديده واكون فعال قدرت اقدم حاجة تفيد الناس. 
 
وارجو من هللا ان يكون هذا العمل في ميزان حسناتي وال تنسوني من صالح دعائكم.  
 
 
 
 
 
 
 
 
 
 
For any comments or feedback:
 
https://forms.gle/ouviBGLr4nae9pLC6
 
 
 
End of the course
 
Question 
5
 
(Final 201
6
)
 

 
 
 
 
 
 
   
 
©
 
Basem HeshamDelta PCM & Differential PCM & 
Convolution & AWGN
Lecture
 
7
 
 
▪ We approximate continuous waveform to a staircase signal at each sampling point we 
developed an error term (the difference between the signal and the staircase).  
▪ In case of delta modulation, the coding is done in units of 1 -bit. 
▪ In case of delta PCM modulation we code the difference into more than one bit.  
Delta PCM  نفس مبدأ Delta modulation وهي ان بدل ماان نب اام ااا   الاا ا samples  هنب اام الفااين باا ن
ا مة الا sample وا مة الا staircase  وبن بي عنه با bit  واحده بس ولكن في حنلة Delta PCM  بن بي عنه
بنكتي من  bit 
Delta modulation بنسااتمدمه لماان  كااون الاا ا sampling rate عاانلي وبنلتاانلي الفااين باا ن الاا ا samples 
 وب ضهن صغ ي وب نلتنلي bit  واحده كنف ه عشنن اعبي عن الفين,ولكاان لااو كاانن ا لاا ا sampling rate  ال اال
وبنلتنلي الفين ب ن الا  samples  كب ي  وبنلتاانلي  bit  واحااده ماا  ه  كااون كاانفي  فهنحتاان   اكتااي ماان  bit   فااي
الحنلة دي هنستمد   طي  قة  Delta PCM . 
 
 
 
 
 
▪ Is another technique for sending information about changes in the  samples rather than 
about the sample values themselves.  
▪ The modulator sends the difference between a sample and its  predicted value.  
 
  𝑆̂(𝑛𝑇𝑠) هي الا  predicted value.في الا Transmitter   ف ه دا ية اسمهن predictor  وبتحسب الق مة المتوا ة
بننء على الا  previous sample   الن في م ظاإلشنيام ف ه correlation من ب ن ا   الا sample   القي بة من
ب ضهن وا مة الا  sample  .الجد دة تسنوي الق مة اللي ابلهن بس مضيوبة في ا مة م  نه 
𝑆̂(𝑛𝑇𝑠)=𝐴[𝑆(𝑛−1)𝑇𝑠] 
 
  عشنن اعمل د زا ن بشكل صح ح الز  اج ب ا مااة البنباامA    اللااي تحقاال اااال error  الن  الاا ا predictor  عشاانن
تكون كفنئته  عنل ة الز  الفين ب ن الق مة االصل ة والق مة المتوا ة اال من  كون. 
 
فكية االببنم اننن هنحسب الا  𝑒(𝑛𝑇𝑠) error   وهو  سنوي الفين ب ن الق مةالحق ق ة للا sample  𝑆(𝑛𝑇𝑆)  
وا مة الا 𝑆̂(𝑛𝑇𝑠)  predicted value   وب دهن نج ب   𝑚𝑠𝑒 وزي من عيفنن المحنضية اللي فنتم انهن الا  
expected value  للا  error تيب ع وب د فك الم ندلة نحصل على  𝑚𝑠𝑒 بنستمدا  الا  correlation 
عشنن اج ب اال ا مة A اللي تحقل اال  𝑚𝑠𝑒 هفنضل الا  𝑚𝑠𝑒  بنلنسبةلا A  واسنويالتفنضل ب نلصفي. 
 Delta PCM  
Differential  PCM  
 
𝑆̂(𝑛𝑇𝑠)=𝐴𝑆([𝑛−1]𝑇𝑠) 
𝑒(𝑛𝑇𝑠)=𝑆(𝑛𝑇𝑆)−𝑆̂(𝑛𝑇𝑠)=𝑆(𝑛𝑇𝑆)−𝐴𝑆([𝑛−1]𝑇𝑠) 
𝑚𝑠𝑒 =𝐸{𝑒2(𝑛𝑇𝑠)}=𝐸{[𝑆(𝑛𝑇𝑆)−𝑆̂(𝑛𝑇𝑠)]2} 
=𝐸{[𝑆(𝑛𝑇𝑆)−𝐴𝑆([𝑛−1]𝑇𝑠)]2} 
 
𝑚𝑠𝑒 =𝐸{𝑆2(𝑛𝑇𝑆)−2𝐴𝑆(𝑛𝑇𝑠)𝑆([𝑛−1]𝑇𝑠)+𝐴2𝑆2([𝑛−1]𝑇𝑠)} 
=𝐸{𝑆2(𝑛𝑇𝑆)}−2𝐴 𝐸{𝑆(𝑛𝑇𝑠)𝑆([𝑛−1]𝑇𝑠)}+𝐴2 𝐸{𝑆2([𝑛−1]𝑇𝑠)} 
=𝑅(0)−2𝐴𝑅(𝑇𝑠)+𝐴2𝑅(0) 
=𝑅(0)[1+𝐴2]−2𝐴𝑅(𝑇𝑠) 
 
For min error  
𝑑(𝑚𝑠𝑒 )
𝑑𝐴=0 
∴ 𝑑(𝑚𝑠𝑒 )
𝑑𝐴=2𝐴𝑅(0)−2𝑅(𝑇𝑠)=0 
∴ 𝐴=𝑅(𝑇𝑠)
𝑅(0) 
 
الا  predictor  ممكن   تمد على اكتي من previous sample  .لو مبال كنن ب  تمد على ا مت ن previous 
sample 2 م ندلته هتكون بنلشكل دا وفي الحل هنحتن  نفنضل ميت ن  ميه بنلنسبة لا A  وميه بنلنسبةلا B 
𝑆̂(𝑛𝑇𝑠)=𝐴𝑆([𝑛−1]𝑇𝑠)+𝐵𝑆([𝑛−2]𝑇𝑠)  
 
 
 
 
 
 
 
 
 
▪ Correlation is a measurement of the similarity between two  signals/sequences.  
▪ Convolution is a measurement of the effect of one signal on the  other signal.  
▪ Convolution is the common operation a linear and time  invariant system can perform on a 
given input signal.  
▪ In convolution, there is some input -output relationship, so this  acts like a filtering operation.  
 
Convolution  
 
 
 
 
𝑦(𝑛)= 𝑥(𝑛)∗ℎ(𝑛)=∑ 𝑥(𝑛−𝑘) ℎ(𝑘)∞
𝑘 = 0 
 
 
 
 
= 𝑥(𝑛) ℎ(0)+𝑥(𝑛−1) ℎ(1)+𝑥(𝑛−2) ℎ(2)+𝑥(𝑛−3) ℎ(3) 
 
 𝑦(𝑛) إشنية  المي  output signal   و 𝑥(𝑛) إشنية الدمل input signal 
ℎ(𝑛)     استجنبة النظن response 
 
convolution  هي عمل ة ي نض ة بنج ب ف هن تأب ي إشنية على إشنية تنن ة وتستمد  في إ جااند ال الاااة 
ب ن مي   أي نظن   Linear Time Invariant (LTI)  ودمله. 
 
من  األمبلة  على  convolution    انلااو عناادنن  إشاانية  𝑥(𝑛)    وهنب ااماإلشاانية  دي فااي الهااوا وعاان ز ن 
ن يف ازي الوسط اللي هنب م ف  ه  اإلشنية  ه أبي عل هاا ن  ,ودا هن يفااه  مااالل اسااتجنبة النظاان   ℎ(𝑛) 
 واللي ب وضح تأب ي المسنفة والظيوف الب ئ ة علىاإلشنية و  لو طبقنن الا  convolution  هن يف تااأب ي
النظن  ℎ(𝑛)  علىاإلشنية 𝑥(𝑛)   وبنلتنلي ن يف المي 𝑦(𝑛) 
 
  لو النظن  عندنن كنن فلتي مبال وℎ (𝑛)    هي استجنبة النظن  والليبتوضح تأب ي الفلتي دا   على  اإلشاانية  
ولنفتيض انه LPF  وبنلتنلي لو طبقننالا  convolution   هنحصل على تنب ي الفلتي على الدمل وهنالاااي
ان التيددام ال نل ة الفلتي شنلهن. Difference Between Correlation and 
Convolution  
𝒉(𝒏) 
 𝒙(𝒏) 
 𝒚(𝒏) 
 
𝑥(𝑛) 
 𝑦(𝑛) 
 
𝒉(𝟎) 𝒉(𝟏) 𝒉(𝟐) 𝒉(𝟑) 
 
 
Given : 𝒙[𝒏]=[𝟏 ,𝟏 ,𝟎 ,𝟎 ,𝟏]   ,   𝒉[𝒏]=[𝟏 ,𝟎 ,𝟏]    
Calculate  𝒙[𝒏] convoluted with 𝒉[𝒏] 
Solution  
𝑦(𝑛)=∑ 𝑥(𝑛−𝑘) ℎ(𝑘)∞
𝑘 = 0 
 =  𝑥(𝑛) ℎ(0)+𝑥(𝑛−1) ℎ(1)+𝑥(𝑛−2) ℎ(2) 
 
 
              1    1    0    0    1  
                    0    0    0    0    0 
                          1    1    0    0    1 
 
𝒚(𝒏)=   1    1    1    1    1    0    1    
 
𝒚(𝒏)=[𝟏    𝟏    𝟏    𝟏    𝟏    𝟎    𝟏] 
 
 
 
Given : 𝒙[𝒏]=[𝟒 ,𝟏 ,𝟐 ,𝟓]   ,   𝒉[𝒏]=[𝟏  ,𝟐  ,−𝟏]    
Calculate  𝒙[𝒏] convoluted with 𝒉[𝒏] 
Solution  
𝑦(𝑛)=∑ 𝑥(𝑛−𝑘) ℎ(𝑘)∞
𝑘 = 0 
=𝑥(𝑛) ℎ(0)+𝑥(𝑛−1) ℎ(1)+𝑥(𝑛−2) ℎ(2) 
 
 
              4     1     2     5     
                     8     2     4     10     
                           -4    -1     -2     -5 
 
𝒚(𝒏)=   4     9     0     8     8      -5      
 
𝒚(𝒏)=[𝟒     𝟗     𝟎     𝟖       𝟖      −𝟓     ] 
 Example 1  
Example 2  
 
 
Given : 
𝒙
[
𝒏
]
=
[
𝟏
 
,
𝟐
 
,
𝟑
]
 
  
,   
𝒉
[
𝒏
]
=
[
𝟒
 
 
,
𝟓
 
]
 
  
 
Calculate  
𝒙
[
𝒏
]
 
convoluted with 
𝒉
[
𝒏
]
 
Solution
 
𝑦
(
𝑛
)
=
∑
𝑥
(
𝑛
−
𝑘
)
 
ℎ
(
𝑘
)
=
𝑥
(
𝑛
)
 
ℎ
(
0
)
+
𝑥
(
𝑛
−
1
)
 
ℎ
(
1
)
∞
𝑘
 
=
 
0
 
 
              
4      8      12         
 
                      
5      10      15         
 
                          
 
𝒚
(
𝒏
)
=
 
 
4      13     22     15     
 
 
𝒚
(
𝒏
)
=
[
𝟒
 
 
 
 
 
 
𝟏𝟑
 
 
 
 
 
𝟐𝟐
 
 
 
 
 
𝟏𝟓
]
 
 
 
Cross Correlation
 
  بضيب كل ا مة
ل
إلشنية
  
𝑥
[
𝑛
]
  
  في الق   المننظية ل هن في
اإلشنية
  
ℎ
[
𝑛
]
  
  وهنن بضيب
اإلشنية
  
كنمله م  
ا مة واحده بس  
ألننن
  
بنج ب التشنبه ب ن اشاانيت ن
  
فبنقاانين كاال نقطااة والمناانظي ل هاان
  
وبنحسااب  
باايدو  
عنااد 
حنالم حصل ف هن 
shift
  
لإلشنية
 
وبن يف منهن مدى 
التشنبه
 
ب ن  
اإلشنيام.
 
 
 
 
Given :
 
𝒉
[
𝒏
]
=
[
𝟏
,
𝟐
,
𝟑
]
 
  
,   
𝒙
[
𝒏
]
=
[
𝟒
 
,
𝟓
]
 
  
 
Calculate  
𝒙
[
𝒏
]
 
correlated with 
𝒉
[
𝒏
]
 
Solution
 
 
𝒉
[
𝒏
]
       
1       2       3
 
𝒙
[
𝒏
]
       
4       5       0
 
 
               
4      10      0   
=  14
 
 
 
 
 
Example 
3
 
 
Example 
4
 
 
 
𝒉[𝒏]              1       2       3  
𝒙[𝒏−𝟏]       0       4       5 
 
                       0       8      15       =  23  
 
 
𝒉[𝒏]              1       2       3        0     
𝒙[𝒏−𝟐]       0       0       4       5  
 
                       0       0       12      0      =  12  
 
 
𝒉[𝒏]              1       2       3       0       0   
𝒙[𝒏−𝟑]       0       0       0      4       5  
 
                       0       0       0      0        0        =  0  
 
 
 
𝒉[𝒏]               0      1        2       3  
𝒙[𝒏+𝟏]       4       5       0       0  
 
                       0       5       0        0       =  5  
 
 
𝒉[𝒏]              0        0       1       2       3  
𝒙[𝒏+𝟐]       4       5       0       0        0  
 
                       0       0       0        0            =  0  
 
 
𝒄𝒐𝒓𝒓 (𝒉,𝒙)=[𝟎     𝟓     𝟏𝟒    𝟐𝟑    𝟏𝟐   𝟎] 
 
 
 𝒉[𝒏] 𝒙[𝒏] 𝒉[𝒏] 𝒙[𝒏+𝟏] 𝒉[𝒏] 𝒙[𝒏+𝟐] 𝒉[𝒏] 𝒙[𝒏−𝟏] 𝒉[𝒏] 𝒙[𝒏−𝟐] 𝒉[𝒏] 𝒙[𝒏−𝟑] 
 
 
 
▪
 
The performance of a digital system is 
quantified by the probability of bit detection 
errors in the presence of thermal noise. Its main source is addition of random signals a 
rising from the vibration of atoms in the receiver electronics.
 
 
  كفنءة
ا
ل
ا
  
System
  
ن
م
ل
 
س
ن
ق
ُ
ت
  
نقدي نكتشف االمطنء
  
في ا
ل
ا
  
Receiver
 
 وهو
 
ن
ا
ا
ه
 
ل
ع
 
ف
ن
ُ
ض
م
noise
 
 
 
ة
د
ن
ا
ا
ع
و
بتكون 
thermal noise
 
 ,والمصدي
األسنسي
 
ل
 هن اهتزاز الذيام في ا
لاا 
ا
 
Receiver
 
 واللااي بتساابب تول ااد
إشنيام
 
عشوائ ة.
 
 
Additive
 
𝒓
(
𝒕
)
=
𝒔
(
𝒕
)
+
𝝎
(
𝒕
)
 
The noise is added to the signal and is 
statistically independent of the signal 
 
:
Additive
 
ا
 
ن
ا
 
ى
ل
ا
 
ي
 
ش
ُ
ت
ل
ا
 
noise
 
 ت  اضنفتهن الى ا
ل
ا
 
transmitted signal
 
 ح ث ان
𝑟
(
𝑡
)
 
عبنية عن ا
لاا 
ا
 
transmitted signal
 
𝑠
(
𝑡
)
 
ا
 
ن
ه
 
ل
ع
 
ف
ن
ض
ُ
م
ل
ا
 
noise
 
𝜔
(
𝑡
)
 
statistically independent
 
م ننهن ان
 
االشاانيت ن مب  تماادو  علااى 
ب ض   ني ا
ل
ا
 
cross correlation
 
ب نه   سنوي صفي
.
 
 
 
White 
 
Just like the white colour which is  composed of all frequencies in the visible spectrum
. 
White noise has uniform power across the whole frequency
 
bands.
 
White
 : ت ني أن
ا
ل
ا
  
noise
  
  تكون متسنو ة
في ا مة البنوي
  
في جم ع التيددام
.
  
لو يسمنن 
ا
ل
ا
  
(PSD)
  
power 
spectral density
 
  هنحصل على الشكل االتي
.
 
 
 
 
 
 
 
 
(PSD)
 
power spectral density
  
 عالاااة باا ن التاايدد علااى المحااوي االفقااي و الباانوي علااى المحااوي
اليأسي ووحدتهن  
watt/Hz
 
ولمن نكنمل المسنحة تحم المنحنى هنج ب ا مة البنوي
  
بن
ل
ا
 
watt
.
 
Additive White Gaussian Noise 
(AWGN)
 
𝒇
 
PSD
 
Constant
 
 
Gaussian
 
The probability distribution of the noise samples is Gaussian with zero mean.
 
The values close to zero 
have a higher chance to occurrence.
 
 
Gaussian
 
 :
  ني أن
 
ال
ا
 
 
probability distribution
 
function
لل
ا
 
noise
 
  
هي
 
Gaussian
 
  وا مة
المتوسط الحسنبي ل ه تسنوي صفي   ني الق   القي بة من الصفي ل هن 
احتمنل
 
اكبي في الحدوث
.  
 
 
The time domain average of large number of noise samples is equal to zero.
 
 دا م ننه اننن
لو 
  
حسبنن المتوسط 
 
ل
 دد كب ي من ع ننم 
ا
ل
ا
  
noise
 
 ،
ا مة 
المتوسط 
هتسنوي
 
صفي
 
و 
 
دا م ننه  
ان
 
 
mean = 0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
The following figure shows the transition probability diagram of binary symmetric 
channel.  
 
The probabilities of sending binary 0 and 1 are : 𝑷(𝑨𝟎)=𝑷𝟎 and 𝑷(𝑨𝟏)=𝑷𝟏 
1. Calculate the probability 𝑷(𝑩𝟎) of receiving symbol zero  and   𝑷(𝑩𝟏) of 
receiving symbol one.  
2.  𝑷(𝑨𝟏|𝑩𝟏)   and  𝑷(𝑨𝟎|𝑩𝟎)  
Solution  
 
𝑃(𝐵1|𝐴0)=𝑃(𝐵0|𝐴1)= 𝑝  ,  and    𝑃(𝐵0|𝐴0)=𝑃(𝐵1|𝐴1)= 1−𝑝   
 
1. 𝑷(𝑩𝟎)   and   𝑷(𝑩𝟏) 
 
𝑃(𝐵0)=𝑃(𝐵0|𝐴0) 𝑃(𝐴0)+𝑃(𝐵0|𝐴1) 𝑃(𝐴1)  
=(1−𝑃)𝑃0+𝑃 𝑃1 
 
𝑃(𝐵1)=𝑃(𝐵1|𝐴0) 𝑃(𝐴0)+𝑃(𝐵1|𝐴1) 𝑃(𝐴1)  
=𝑃 𝑃0+(1−𝑃) 𝑃1 
 
2. 𝑷(𝑨𝟎|𝑩𝟎)   and  𝑷(𝑨𝟏|𝑩𝟏)  
𝑃(𝐴0|𝐵0) : if a 0 was received what is the probability that a 0 was sent  
 𝑃(𝐴0|𝐵0)  م ننهن لو وصل0 في الا  Receiver    أي احتمنل ان اللي ت  ايسنله0 
 
𝑃(𝐴1|𝐵1) : if a 1 was received what is the probability that a 1 was sent  
 𝑃(𝐴1|𝐵1)   م ننهن لو وصل1   فيالا  Receiver   أي احتمنل ان اللي ت  ايسنله1 
 
 
Example  5 
 
 المطلوب اللي فنم بنسم ه Posterior probability  ببدأ بنللي استقبلته واحسب اللي ت  ايسنله واللي 
موجود في المطلوب األول  بنسم ه Prior probability     ودا التي ب ال ندي اللي بنشوفهنب م  
وهنستقبل كن .  
 
Posterior probability  بنحله بقننون اسمه Bay's Rule 
 
Bay's Rule  
𝑃(𝐴|𝐵)=𝑃(𝐵|𝐴)  𝑃(𝐴) 
𝑃(𝐵) 
 
𝑃(𝐴0|𝐵0)=𝑃(𝐵0|𝐴0)  𝑃(𝐴0) 
𝑃(𝐵0)=(1−𝑃)𝑃0
(1−𝑃)𝑃0+𝑃 𝑃1 
𝑃(𝐴1|𝐵1)=𝑃(𝐵1|𝐴1)  𝑃(𝐴1) 
𝑃(𝐵1)=(1−𝑃)𝑃1
𝑃 𝑃0+(1−𝑃) 𝑃1 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
©
 
Basem HeshamInformation Theory &  Entropy Coding 
& Channel Capacity
Lecture
 
8
 
 
The concept of information content is related to predictability. That is , the more likely a 
particular message, the less information is given by transmitting the message.  
 
𝐼𝑥=log 2(1
𝑃𝑥) 
 
𝐼𝑥  : information con tent of the message 𝑥   
𝑃𝑥  : probability of occurrence of the message  
 
  𝐼𝑥  هوعدد الـ bits المعبرة عن الـ 𝑥 message 
𝑃𝑥  هو احتمال حدوث الـ 𝑥 message 
 
الـ message ذات االحتماالت العالية تحمل قيم قليله من المعلومات (عدد اقل من الـ bits   ) والعكس صحيح 
 
من المفاهيم المهمة اللي الزم نتعلمها ربط محتــوا الــ ـ message باحتمــال حــدووها وزــي  ــز  الــ ـ source 
coding    كناب نستخدم عدد  bits    وابت لكل message   ولكــن عمليــا مبنســتخدم  نفــس العــدد للتعبيــر عــن كــل 
message  ومــن الطــرل اللــي هنتعلمهــا ان  عــدد الــ ـ bits اللــي بنســتخدمه يعتمــد علــ  احتمــال حــدوث الــ ـ 
message  .لو احتمال حدوث الـ message  كبير هنستخدم عــددbits  اقــل للتعبيــر عنــه ولــو احتمــال حدووــه
ضــعيه هنســتخدم عــدد اكبــر مــن الــ ـ bits  وبكــدا هنــوزر زــي ا لــ ـ Bandwidth  النلــو بنبعــت عــدد مــن ا لــ ـ 
messages  زي المتوسطهنستخدم  عدد  bits اقل. 
 
تعريه ادل للـ 𝐼𝑥  : 
Information  content  𝐼𝑥 can be interpreted as the minimum number of binary digits required to 
encode the message.  
 𝐼𝑥  هواقل عدد ممكن  من الـ bits   مطلوب لعمل coding للـ message 
 
 
 
 
 
 
 
 Information Theory  
bits  
 
 
Is defined as the average information per message.  
 
𝐻=∑𝑃𝑥𝑖  𝐼𝑥𝑖𝑛
𝑖=1=∑𝑃𝑥𝑖 log 2(1
𝑃𝑥𝑖)𝑛
𝑖=1 
 
Entropy  هو متوسط ما تحمله م موعة من الـ  messages   من معلومات وعشان نحسبه بنضرب محتوا
كل  message 𝐼𝑥   زي احتمال حدووه𝑃𝑥 .ون مع النواتج 
واضح من القانون ان الـ Entropy قيمتها تعتمد عل  الـ probability. 
  𝑛هي عدد الـ messages 
 
تعريه ادل للـ Entropy : 
Entropy is equal to the minimum number of digits per message required on the average 
for encoding  
𝐻 هو اقل عدد من الـ bits   مطلوب زي المتوسط لعمل coding لم موعة من الـ message 
 
 
 
 
A communication system consists of six  possible messages  with probabilities  𝟏
𝟒 , 𝟏
𝟒 , 
𝟏
𝟖 , 𝟏
𝟖 , 𝟏
𝟖 , 𝟏
𝟖  .Find the Entropy  
 
Solution  
𝐻=∑𝑃𝑥𝑖  𝐼𝑥𝑖𝑛
𝑖=1=∑𝑃𝑥𝑖 log 2(1
𝑃𝑥𝑖)𝑛
𝑖=1 
=1
4× log 2(4)+1
4× log 2(4)+1
8× log 2(8)+1
8× log 2(8)+1
8× log 2(8)
+1
8× log 2(8)=2.5 bits/message  
 يعني محتاج زي المتوسط 2.5 bits   لكل message  عشان اقدر اعمل coding   باقل عدد ممكن منالـ bits 
 
 Example 1  Entropy  
bits / message  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
∑
𝑃
𝑥𝑖
 
 
𝐼
𝑥𝑖
2
𝑖
=
1
=
𝑃
𝑥
1
 
log
2
(
1
𝑃
𝑥
1
)
+
𝑃
𝑥
2
 
log
2
(
1
𝑃
𝑥
2
)
 
=
𝑃
𝑥
1
 
log
2
(
1
𝑃
𝑥
1
)
+
(
1
−
𝑃
𝑥
1
)
log
2
(
1
1
−
𝑃
𝑥
1
)
 
 
 اسمها
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
 
ألننا
  
بنبعت  
message
 
2
 
زقط
 
بالتعويا زي القانون السابق عند قيم مختلفة
 
At 
𝑃
𝑥
1
=
0
 
⟹
   
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
0
+
(
1
)
log
2
(
1
1
)
=
0
 
At 
𝑃
𝑥
1
=
1
4
 
⟹
   
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
1
4
 
log
2
(
4
)
+
(
3
4
)
log
2
(
4
3
)
=
0
.
8
 
At 
𝑃
𝑥
1
=
1
2
 
⟹
   
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
1
2
 
log
2
(
2
)
+
1
2
 
log
2
(
2
)
=
1
 
At 
𝑃
𝑥
1
=
3
4
 
⟹
   
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
(
3
4
)
log
2
(
4
3
)
+
1
4
 
log
2
(
4
)
=
0
.
8
 
At 
𝑃
𝑥
1
=
1
 
⟹
   
𝐻
𝑏𝑖𝑛𝑎𝑟𝑦
=
(
1
)
log
2
(
1
1
)
+
0
=
0
 
 
The result is sketched as a function of 
𝑃
𝑥
1
 
 
Note that as either of the two messages becomes more likely , the entropy decreases. 
When either message has probability 1, the entropy goes to zero. This is reasonable, since
,
 
at these points
, the outcome 
is certain. If 
𝑃
𝑥
1
=
1
 
we know that message 
𝑥
1
 
will be sent
 
all the time. No information is transmitted by sending the message.
 
Proof 
of
 
Maximum 
Entropy
 
  
𝑃
𝑥
2
 
=
 
1
 
−
 
𝑃
𝑥
1
:نحصل عل  االتي
 
Entropy
 
,وباستخدام قانون الـ
 
𝑃
𝑥
1
 
هي واحد ناقص
 
𝑃
𝑥
2
 
يساوي واحد اذا احتمال
وبمــا ان م مــوح االحتمــاالت
 
𝑥
2
 
و
 
𝑥
1
 
زقــط هــم
 
2
 
message
 
بيبعــت
 
digital 
 
system
 
نفتــرا ان عنــدنا
Consider a communication scheme made up of two  possible message
 
𝑥
1 
and
 
𝑥
2
 
Maximum entropy occurs in the case of equally possible messages
.
 
نستنتج ان اعل  قيمة ل
ل
ـ
  
entropy
 
  بتحصل لما احتماالت حدووهم تكون متساوية ودا منطقي
الن 
لو احد ا
ل
ـ
 
messages
 
  احتمال حدووها اكيد زي ان
𝑃
𝑥
1
=
1
 
  زدا معناه اننا دايما بنبعت
𝑥
1
 
message
 
 زقط وعرزنا
ان كل ما احتمال االرسال زاد كل ما 
ا
ل
ـ
 
information
 
 قلت
وبالتالي لما االحتمال يكون اكيد ا
ل
ـ
 
Entropy
 
  هيساوي صفر وبالمول عند
𝑃
𝑥
1
=
0
 
 دا معناه ان
𝑃
𝑥
2
=
1
 
 
 كل ما ابعد عن نقط
التأكيد
 
احتمالية ارسال احد ا
ل
ـ
 
messages
 
  بتعل  والتانيه بتقل وبالتالي زيه معلومة
بتتنقل
 
لحد ما نوصل ألعل  قيمة ل
ل
ـ
 
entropy
 
 عند
1
2
 
 
 .لما تكون احتماالتهم متساوية
 
 
 
 
 
Given M possible message, we wish to convert 
them
 
into M possible code words.
 
The code 
words can be selected to achieve objectives such as efficiency, error
 
detection and
 
correction 
,
security.
 
 
  زي ال ز  دا هنتعلم طرل مختلفة لعمل
Coding
  
لم موعة من ا
ل
ـ
  
messages
  
وممكن يكون الهده من ا
ل
ـ
 
Coding
  
  دا
تحقيق  
efficiency
  
عالية
  
يعني ابعت عدد  
bits
  
  اقل
او 
  
ممكن يكون الهده ان الداتا توصل صح 
وبالتالي الهده عندي زيادة
 
error correction
 
capability
 
زي ا
ل
ـ
  
System
 
  من خالل
إضازة
  
Redundant 
bits
  
  عشان نقدر نكتشه ونصحح
األخطا 
  
,وممكن يكون الهده هو تحقيق  
ا
ل
ـ
  
Security
  
زــي ا
لــ 
ـ
 
System
 
زي ا
ل
ـ
Encryption
 
We discuss codes to achieve efficiency under the 
heading entropy coding. Such codes 
attempt to send the information using the minimum number of bits.
 
 
هندرس زي المحاضرة دي طرل ا
ل
ـ
  
Coding
  
اللــي بحقــق مــن خاللهــا ا
لــ 
ـ
  
efficiency
  
 و
المقصــود بالكفــا ة 
efficiency
 
هو النقل باقل عدد من ا
ل
ـ
 
bits
 
وهندرس طريق
تين بحقق
 
مــن خاللهــم دا وهمــا 
 
ا
لــ 
ـ
 
Huffman 
coding
 
 و
Shannon coding
 
  اللي يندر وا تحت مسم
entropy coding
 
 (كال الطريقتين من
أنــواح
 
ا
ل
ـ
 
lossless compression
 
)
 
 
We illustrate error detection and correction using block 
coding.
 
 زيه طريقة هنتعلمها
المحاضرة ال ايه 
خاصة ب 
الــ 
ـ
 
error detection and correction
 
 اســمها
block coding
 
  هنتعلم زيها ازاي نعمل
coding
 
ل
ـ
 
message
 
  بحيث نقدر نكتشه ونصحح
األخطا 
  
زيها.
 
 
قبل ما ندرس طرل ا
ل
ـ
 
Coding
 
 الزم نعره خصائص االكواد اللي
الزم تتحقق.
 
 
Coding
 
 
 
▪ Uniquely decodable  
معناها ان االكواد تُفسر بطريقة واحدة زقط زي الـ   Receiver   يعني كل كود يُعبر عن message  واحده بــس
م   أي message .تانيه او اتنين مع بعا 
 
موال : لو عندنا  4 messages :بالشكل االتي 
𝑀1=1   ,   𝑀2=10  ,   𝑀3=01  ,   𝑀4=101 
If 101 is received ⇒ 𝑀4 or  𝑀2 𝑀1 or 𝑀1𝑀3 
 لو وصل101 للـ Receiver  هتتفســر بــاكتر مــن طريقــة وبالتــالي االكــواد دي ليســت uniquely decodable 
 ومتنفع  زي الـ Coding 
 
▪ Prefix property  
No code word forms the starting sequence (prefix) of any other code word.  
الخاصية دي معناها ان االكواد ال تمول بداية لكود اخر, ولو الخاصــية دي تحققــت بنقــول علــ  االكــواد دي  
instantaneous  
 
  موال : لو عندنا 4 messages :بالشكل االتي 
𝑀1=1   ,   𝑀2=01  ,   𝑀3=001   ,   𝑀4=0001  
  مفيأي كود يعتبر بداية لكود تاني وبالتالي خاصية الـ prefix  تحققت 
 
The prefix restriction is sufficient but not necessary for uniquely decodable  
خاصية الـ prefix  كازيه اننا نعره ان الكود uniquely decodable  النالــ ـ prefix  لمــا تت حقــق هتضــمن ان 
االكــواد uniquely decodable ,  ولكــنلــو الــ ـ prefix  متحققــت  دا مــ  معنــاه انهــا ليســت uniquely  
decodable زي الموال االتي 
 
𝑀1=1   ,   𝑀2=10  ,   𝑀3=100   ,   𝑀4=1000  
الـ  prefix    هنا غير محققه الن𝑀1    هي prefix    او بداية ل𝑀2  , و𝑀2  هي prefix  ل𝑀3 , و𝑀3  هــي بدايــة
𝑀4  ,ولكنبالرغم ان ال ـ prefix  هنا غيــر محققــه لكــناالكــواد uniquely decodable   الن كــل كــود يفُســر
بطريقة مختلفة زي الـ Receiver 
 
 
 
 
 Properties of codes  
 
 
▪ It is of interest to find uniquely decodable codes of minimum length  by assigning the 
shorter code word to the most probable messages.  
▪ Different messages are coded into words of different length.  
▪ When talking about length of a code, we therefore must refer to the average length of the 
code words. This average is computed by taking the probabilities of each message into 
account.  
▪ We want to get code words with minimum average length for code word  
 
 من المهم زيأي digital system  هي الحصول عل  اكواد لها minimum length  ويتم عمــل coding  بحيــث
ان كل  message تاخد كود مختله زي الـ Length عل  حسب احتماليه حدوث الـ message 
  بما اننا بنبعت زيأي  system  م موعة من الـ  messages   زهنتعامــل مــع متوســط الـــ  Length   ودا بنحصــل
عليه من خالل  ضرب  طول كل  message .زي احتمال حدووه ون مع النواتج 
 
الهــده هنــا هــو الحصــول علــ  code words with minimum average length  ودا مــن خــالل ان الـــ 
messages  اللي احتمال حدووها كبير تاخد عدد اقل من الـ  bits  و الـ  messages   اللي احتمال حــدووهاقليــ ل 
تاخد عدد اكبر من الـ bits وبالتالي زي المتوسط الـ average length هيكون اقل وبالتالي الكفائة هتزيد. 
 
A fundamental theorem exists in noiseless coding theorem. The theorem states that :  
For binary coding alphabets, the average code word length is greater than or equal to the 
entropy  
Entropy H  هي ازضل قيمة يمكن الحصول عليهــا عنــد عمــل coding  للـــ messages  وقيمــة الـــ average 
length اكبر من او تساوي قيمة الـ  Entropy   وبالتالي هدزنا هو الحصول عل code يكون زيــه متوســط عــدد 
الـ bits يصل ال  الـ Entropy أي اقل ما يكون وبالتالي نحقق اعل  كفا ة. 
 
𝐿̅=∑𝑃𝑖 𝐿𝑖𝑛
𝑖=1 
𝐿̅≥𝐻 
𝜂=𝐻
𝐿̅×100 
𝐿̅→ Average length (average bits/message)          𝜂→  Efficiency  
𝐿→ bits /message   
 𝐿هي عدد الـ bits الفعلي ( actual) المو ودة زي الـ message  ولكن𝐼𝑥 ( هي اقل عدد ممكن perfect ) 
 Entropy Coding  Techniques  
 
Maximum efficiency occurs at 𝐿̅=𝐻 and requires that the probability of every message be 
inverse power of 2  (𝑃𝑖=1
2𝑛) 
 
  اعل  كفا  ممكن نحصل عليها عند 𝐿̅=𝐻  ودا ممكن يتحقق لما احتماالتالـ message   تكون
inverse power of two   يعني لو قيم االحتماالت كانت… ,1
16 ,1
8 ,1
4 ,1
2   اذاً 𝐿̅=𝐻   وبالتالي هحصل
عل  اعل  كفا ة 100%. 
 
 
 
 
𝐿̅=𝐻 
∑𝑃𝑖 𝐿𝑖𝑛
𝑖=1=∑𝑃𝑖  𝐼𝑖𝑛
𝑖=1 
∑𝑃𝑖 𝐿𝑖𝑛
𝑖=1=∑𝑃𝑖 log 2(1
𝑃𝑖)𝑛
𝑖=1 
∑ 𝐿𝑖𝑛
𝑖=1=∑log 2(1
𝑃𝑖)𝑛
𝑖=1 
𝐿=log 2(1
𝑃) 
𝐿 هي عدد الـ bits المو ودة زي الـ message زالزم تطلع عدد صحيح والحالة الوحيدة اللي تحقق دا ان 
𝑃 تكون inverse power of two   
∴ 𝑃=1
2𝑛 
 
 
هندرس طريقتين من الـ  Entropy Coding    وهما Huffman coding    و Shannon coding    واسمهم Entropy 
Coding ألن زي الطرل دي بنحاول نوصل الـ length بتاح الـ message للـ  Entropy   قدراإلمكان. 
 
 
 
 Proof of maximum efficiency  
 
 
Provide an organized technique for finding efficient variable length codes for a given set of 
messages  
 هيطريقة من طرل الـ Entropy Coding بعره او د بيها اكواد بأطوال مختلفة لم موعة من الـ  
messages ,زي المسألة بيكون معط  م موعة من الـ messages   واحتماالت حدووهم ومطلوب او د كود
كل message  ونتأكد ان االكواد uniquely dec odable   وليها minimum length 
 
 الخطوات 
 
1.  نتأكد ان م موح االحتماالتالمعطاة تساوي واحد.  
2. نرتب الـ messages  .ترتيباً تنازلياً حسب قيم احتماالت حدووها  
3.  ن مع اخر قيمتين زي االحتماالت ونعيد ترتيب هذه االحتماالتبحيث نحازظ عل  الترتيب التنازلي.   
4.  نكرر العملية رقم2   و3   حت  نحصل عل  عددين زقط 
5.  العدداألول  نفترا له كود 0 والعدد التاني 1   
6. نقوم بالر وح باألسهم وعليها االكواد لو السهم مُفرد الكود ير ع زي ما هو  
7.  لوالسهم مزدوج  نر ع الكود ونضيه عل  ال ز  اللي زول 0  وال ز  اللي تحت1   
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
𝑺𝟏  𝟏
𝟏𝟔   ,  𝑺𝟐  𝟏
𝟖   ,  𝑺𝟑  𝟏
𝟒   ,  𝑺𝟒  𝟏
𝟏𝟔  ,  𝑺𝟓  𝟏
𝟐 
Construct the Huffman cod es and calculate the efficiency of the code  
Solution  
 
 
Huffman coding  
Example 2  
 
codes  
𝑺𝟏    1110  
𝑺𝟐    110  
𝑺𝟑    10   
𝑺𝟒    1111   
𝑺𝟓    0    
 االكواد تحقق زيها خاصية uniquely decodable حيث ان كل كود مختله عن االخر وبالتالي يُفسر زي الـ   
Receiver  .بطريقة واحده, واالكواد ليها اطوال مختلفة وتعتمد عل  احتماالت الحدوث 
 
Average Length  
𝐿̅=∑𝑃𝑖 𝐿𝑖=4×1
16+3× 1
8+2× 1
4+4× 1
16+1× 1
25
𝑖=1 
=15
8  bits/message   
 
Entropy  
𝐻=∑𝑃𝑖 log 2(1
𝑃𝑖)5
𝑖=1 
=1
16× log 2(16)+1
8× log 2(8)+1
4× log 2(4)+1
16× log 2(16)+1
2× log 2(2) 
=15
8  bits/message   
 
Efficiency  
𝜂=𝐻
𝐿̅×100 =15
8
15
8×100 =100%  
 نالحظ ان قيم االحتماالت هي inverse power of two  وبالتالي 𝐻=𝐿̅  وبالتالي نحصل عل  اعل  كفا ة
ألننا حصلنا عل  اكواد باقل عدد ممكن من ال ـ bits 
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
𝑺𝟏  𝟎.𝟎𝟓   ,  𝑺𝟐   𝟎.𝟏𝟓   ,  𝑺𝟑  𝟎.𝟐   ,  𝑺𝟒  𝟎.𝟎𝟓  ,  𝑺𝟓  𝟎.𝟏𝟓 ,  𝑺𝟔  𝟎.𝟑  ,  𝑺𝟕  𝟎.𝟏 
Construct the Huffman cod es and calculate the efficiency of the code  
Solution  
 
 
 
 
codes  
𝑺𝟏    1110  
𝑺𝟐    010  
𝑺𝟑    10   
𝑺𝟒    1111   
𝑺𝟓    011    
𝑺𝟔    00   
𝑺𝟕    110    
 
Example 3  
 
Average Length  
𝐿̅=∑𝑃𝑖 𝐿𝑖7
𝑖=1 
=4×0.05+3× 0.15+2× 0.2+4× 0.05+3× 0.15+2×0.3+3×0.1 
=2.6  bits/message   
 
Entropy  
𝐻=∑𝑃𝑖 log 2(1
𝑃𝑖)7
𝑖=1 
=0.05×log 2(1
0.05)+0.15×log 2(1
0.15)+0.2×log 2(1
0.2) 
+0.05×log 2(1
0.05)+0.15×log 2(1
0.15)+0.3×log 2(1
0.3)+0.1×log 2(1
0.1)  
 
=2.57  bits/message   
 
Efficiency  
𝜂=𝐻
𝐿̅×100 =2.57
2.6×100 =98.85%  
 
 
 
 
 
 
 
 
 
 
 
 
 
Shannon Coding  is similar to the Huffman, a major being that the operations are performed 
in forward rather than backward direction.  
تاني  طريقة من طرل الـ  Entropy Coding  وزكرتها مقاربة لفكرة Huffman  Coding   وتقريبا بتدي نفس
الكفا ة ولكن االختاله زي الـ operation  حيث ان زي Huffman  Coding  كنا بنبدأ من االخر لحد ماأوصل  
للبداية ( backward ) لكن زي Shannon Coding  بيتم بشكل forward  زي ما هنشوه 
 
الخطوات  
 
1. .نتأكد ان م موح االحتماالت المعطاة تساوي واحد 
2. نرتب الـ messages  .ترتيباً تنازلياً حسب قيم احتماالت حدووها  
3.   نقوم بتقسيم االحتماالت ال  نصفين(قدر اإلمكان) بخط زاصل    
4.  ما زولالخط نعطيه الكود  0  وما تحت الخط نعطيه الكود1 
5.  نكرر الخطوتين السابقتين مع كل االحتماالت زول الخط وتحت الخط حت  يكون زول كل خط احتمال واحد 
 
الفكرة باختصار اننا  بنقسم بالخط بحيث يكون م موح اللي زول الخط  يساوي اللي تحت الخط  بقدر اإلمكان.  
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
𝑺𝟏  𝟏
𝟏𝟔   ,  𝑺𝟐  𝟏
𝟖   ,  𝑺𝟑  𝟏
𝟒   ,  𝑺𝟒  𝟏
𝟏𝟔  ,  𝑺𝟓  𝟏
𝟐 
Construct the Shannon  codes and calculate the efficiency of the code  
Solution  
 
 
Shannon Coding  
Example 4  
 
حل تفصيلي بالخطوات
  
:
 
 
 
 
 
1
.
 
هنقسم االحتماالت
  
بخط زاصل بحيــث نحصــل علــ  قيمــة نــص زــول الخــط 
الفاصل (باللون 
األحمر
) وم مــوح االحتمــاالت تحــت الخــط قيمتهــا تســاوي 
نص وهنحط القيم زول الخط ب 
ـ
 
0
 
 وكل القيم تحت الخط ب
ـ
 
1
 
 
 
 
 
 
2
.
 
 ال ز  اللي زول ليه قيمــة واحــده بــنص مــا هنقســمه وال ــز  اللــي تحــت
هنقسمه بخط زاصل
 
بعد 
𝑆
3
 
 (باللون
األخضر
)
 
بحيث تكون القيم زــول الخــط 
بربع ونحط عندها 
0
 
 وم موح القيم تحت الخط بربع ونحط عندها
1
 
 
 
 
 
 
 
3
.
 
  بعد كدا هنعمل خط زاصل بعد
𝑆
2
  
  (باللون
االزرق
) ل زئين قيمة كــل  ــز  
منهم  
1
8
  
  وزول الخط نحط
0
 
  وتحت الخط نحط
1
 
 
 
 
 
 
 
4
.
 
 اخر خطوة هنقسم ال ز  الباقي بخط زاصل بعد
𝑆
1
 
 (باللون
االسوو 
)
 
بحيــث 
نقسمهم ل زئين كل  ز  قيمته  
1
16
 
  وزول الخط نحط
0
 
  وتحت الخط نحط
1
 
 
 
 
 
الخطوات
  
واال
ل
وان
  
للتو 
ضيح زقط لكن زي حل السؤال هنرسم  
الرسمة النهائية
.
 
 

 
codes  
𝑺𝟏    1110           
𝑺𝟐    110            
𝑺𝟑    10             
𝑺𝟒    1111          
𝑺𝟓    0    
 
 
Average Length  
𝐿̅=∑𝑃𝑖 𝐿𝑖=4×1
16+3× 1
8+2× 1
4+4× 1
16+1× 1
25
𝑖=1 
=15
8  bits/message   
 
Entropy  
𝐻=∑𝑃𝑖 log 2(1
𝑃𝑖)5
𝑖=1 
=1
16× log 2(16)+1
8× log 2(8)+1
4× log 2(4)+1
16× log 2(16)+1
2× log 2(2) 
=15
8  bits/message   
 
 
Efficiency  
𝜂=𝐻
𝐿̅×100 =15
8
15
8×100 =100%  
 
 
 
 
A 
communication system consists of five possible messages. The probability of 
them is as follows :
 
𝑺
𝟏
 
 
𝟎
.
𝟎𝟓
   
,  
𝑺
𝟐
 
 
 
𝟎
.
𝟏𝟓
   
,  
𝑺
𝟑
 
 
𝟎
.
𝟐
   
,  
𝑺
𝟒
 
 
𝟎
.
𝟎𝟓
  
,  
𝑺
𝟓
 
 
𝟎
.
𝟏𝟓
 
,  
𝑺
𝟔
 
 
𝟎
.
𝟑
  
,  
𝑺
𝟕
 
 
𝟎
.
𝟏
 
Construct the 
Shannon
 
cod
es and calculate the efficiency of the code 
 
Solution
 
 
 
 
حل تفصيلي بالخطوات
  
:
 
 
 
1
.
 
 بعد ترتيب االحتماالت بشكل تنازلي
هنقسم االحتماالت
 
بخط زاصــل
 
بعــد 
𝑆
3
 
  (باللون
األحمر
)
  
بحيث  
م موح احتماالت
  
يســاوي
  
نــص زــول الخــط الفاصــل
 
وم موح االحتماالت تحت الخط قيمتها تساوي نص
  
وهنحط القيم زول الخط 
ب 
ـ
 
0
 
 وكل القيم تحت الخط ب
ـ
  
1
 
 
 
 
Example 
5
 
 
 
 
 
 
2
.
 
  نقسم بخط زاصل من بعد
𝑆
6
  
  (باللون
األخضر
)
  
بحيث يكون اللي زول الخط 
0.3
  
  واللي تحت الخط
0.2
  
القيمتين قريبين من بعا الي حد ما
.
  
و 
بعــد
  
كــدا 
نقســم الــنص اللــي تحــت الــ   ــزئين متســاويين 
بخــط زاصــل مــن بعــد 
𝑆
5
 
  (باللون
األخضر
)
  
بحيث يكون اللي زول الخــط م مــوعهم 
0.3
 
 واللــي تحــت
الخط
 
م موعهم
  
0.2
 
 وهنحط القيم زول الخط ب
ـ
 
0
 
 و
القيم تحت الخط ب 
ـ
 
1
 
 
 
 
 
 
3
.
 
 ونقسم بخط زاصل من بعد
𝑆
7
 
 (باللون
االزرق
) 
بحيث اللي زول الخــط 
0.
1
 
واللي تحت الخط
 
م موعــة
 
0.
1
 
 ونقســم بخــط زاصــل مــن بعــد
𝑆
2
 
 (بــاللون
االزرق
)  
بحيث اللي زول الخط  
0.
1
5
  
واللي تحت الخط
  
0.
1
5
  
 وهنحط القــيم
زول الخط ب 
ـ
  
0
 
 و
القيم تحت الخط ب 
ـ
 
1
 
 
 
 
 
 
 
 
4
.
 
  اخر خطوة هنقســم ال ــز  البــاقي بخــط زاصــل بعــد
𝑆
1
  
  (بــاللون
االسوو 
)
 
بحيث نقسمهم ل زئين كل  ز  قيمته  
0
.
05
  
 وزــول الخــط نحــط
0
 
 وتحــت
الخط نحط  
1
 
 
 
 
 
 
 
 
 
  الزم ازصل بين
أي
  
قيمتين بيساووا بعا بخــط 
زي مــا 
حطيــت 
خــط بــين 
0.15
 
 و
0.15
 
 وكــذلب بــين
0.
05
 
 و
0.05
 
عشان االتنين مياخدوا نفس الكود وكم
ان الزم يبق  تحت كل خط احتمال  
 
 
 
code s  
𝑺𝟏    1110             
𝑺𝟐    100  
𝑺𝟑    01                  
𝑺𝟒    1111   
𝑺𝟓    101                
𝑺𝟔    00   
𝑺𝟕    110    
 
 
Average Length  
𝐿̅=∑𝑃𝑖 𝐿𝑖7
𝑖=1 
=4×0.05+3× 0.15+2× 0.2+4× 0.05+3× 0.15+2×0.3+3×0.1 
=2.6  bits/message   
Entropy  
𝐻=∑𝑃𝑖 log 2(1
𝑃𝑖)7
𝑖=1 
=0.05×log 2(1
0.05)+0.15×log 2(1
0.15)+0.2×log 2(1
0.2) 
+0.05×log 2(1
0.05)+0.15×log 2(1
0.15)+0.3×log 2(1
0.3)+0.1×log 2(1
0.1)  
 
=2.57  bits/message   
 
Efficiency  
𝜂=𝐻
𝐿̅×100 =2.57
2.6×100 =98.85%  
 
 
 
 
Shannon has shown that a given communication channel has a maximum rate of 
information C shown as the channel capacity.  If the information rate R, is less than C, one 
can approach arbitrarily small error probabilities by intelligent choice techniques.  
If the information rate R is greater than the channel capacity C, errors cannot be avoided 
regardless of the coding technique employed.  
 
 العالم Shannon  وضح ان زيهrate معين مقدر  ات اوزه زي ارسال الداتا زي الـ channel   واقصrate  
  اقدر ابعت بيه الداتا علالـ channel  هوC Channel Capacity وطول ما الـ information rate R   اقل من
C Channel Capacity   هنقدر نعمل coding techniques  يكتشهاألخطا  ويصححها، ولكن لو  R > C    م
هقدر ات اوز األخطا  مهما استخدمت من طرل لتصحيح واكتشاه األخطا  الن الـ rate بق  عالي  دا 
 
We consider the bandlimited channel operation at the presence of additive white Gaussian 
noise. The channel capacity is given by  
 
𝐶=𝐵log 2(1+𝑆
𝑁) 
 
C  is the capacity in bits/sec  
B  is the bandwidth of the channel in Hz  
𝑆
𝑁  Signal to noise ratio  
 
  السرعة القصوا لنقل البيانات عل  القناة يعتمد عل channel bandwidth  B   و قيمة Signal to noise ratio   
  حيثتم ازتراا و ود   additive white Gaussian noise زي الـ channel. 
 
If the bandwidth approaches infinity, the capacity also approaches infinity. this is not correct  
Since the noise is assumed to be white, the wider the bandwidth the more noise is admitted 
to the system. Thus, as B increase ,  𝑆
𝑁 decrease  
واضح من المعادلة ان بزيادة الـ Bandwidth  بيزيد معدل االرسالوأيضا بيزيد تأوير الـ noise عل  الـ  
system   يعني𝑆
𝑁  هتقلوبالتالي لو ازترضنا ان  الـ  Bandwidth ال نهائي دا م  معناه اننا نقدر نبعت bit rate  
ال نهائي  الن الـ noise بتأور عل  الـ rate اللي ببعت بيه الداتا. 
 
 Channel Capacity  
 
 
 
𝐶
=
𝐵
log
2
(
1
+
𝑆
𝑁
)
 
𝐶
=
𝐵
log
2
(
1
+
𝑆
𝑁
o
𝐵
)
 
Note that B is in hertz and that 
𝑁
o
 
is the power spectral density in watts per hertz. We now 
find the value of channel capacity approaches as B goes to infinity:
 
lim
𝐵
→
∞
𝐶
=
lim
𝐵
→
∞
𝐵
log
2
(
1
+
𝑆
𝑁
o
𝐵
)
 
 
 هنستخدم العالقة الرياضية
األتية
 
زي االوبات ونعدل شكل المعادلة بحيث نوصل لشكل العالقة دي:
 
lim
𝑥
→
∞
𝑥
 
log
2
(
1
+
1
𝑥
)
=
log
2
(
𝑒
)
=
1
.
44
 
 
lim
𝐵
→
∞
𝐶
=
lim
𝐵
→
∞
𝑆
𝑁
o
 
[
𝑁
o
 
𝐵
𝑆
 
log
2
(
1
+
𝑆
𝑁
o
𝐵
)
]
 
∴
𝐶
𝑚𝑎𝑥
 
=
𝑆
𝑁
o
 
log
2
(
𝑒
)
=
1
.
44
 
𝑆
𝑁
o
 
The equation show the maximum possible channel capacity as a function of signal power 
and noise spectral density 
 
 
 
 
Proof of capacity C for infinity Bandwidth
 
 
 
 
If the SNR is 20 dB 
,and the bandwidth available is 4KHz ,which is approach for 
telephone communications, find the maximum rate of information that could be 
transmitted across the channel.
 
Solution
 
20
 
𝑑𝐵
=
10
log
10
(
𝑆𝑁𝑅
)
 
⇒
𝑆𝑁𝑅
=
100
 
𝐶
=
𝐵
log
2
(
1
+
𝑆
𝑁
)
=
4
×
10
3
 
×
log
2
(
1
+
100
)
=
26
.
6
 
Kbit
/
sec
 
 
 
 
 
 
If the requirement to transmit at 50 Kbit/sec and a bandwidth of 1MHz is used, find 
the minimum 
𝑺
𝑵
 
required.
 
Solution
 
𝐶
=
𝐵
log
2
(
1
+
𝑆
𝑁
)
 
50
×
 
10
3
=
1
×
10
6
log
2
(
1
+
𝑆
𝑁
)
 
0
.
05
=
log
2
(
1
+
𝑆
𝑁
)
 
2
0
.
05
=
1
+
𝑆
𝑁
 
𝑆
𝑁
=
0
.
035
=
−
14
.
5
 
dB
 
Example 
6
 
 
Example 
7
  

SYSTEMS

4th Edition

Simon Hayhin

 

~S¥STEMS

 

4th ed 4th Edition

Simon Hayhin

  

| COMMUNICATION SYSTEMS

 

 

 

478 EDITION

| COMMUNICATION SYSTEMS

 

 

 

 

 

 

Simon Haykin
McMaster University

W

JOHN WILEY & Sons, INC.
New York # Chichester # Weinheim Brisbane # Singapore

4-m EDITION

@ Toronto

 

 

 

Editor Bill Zobrist

Marketing Manager Katherine Hepburn
Associate Production Director Lucifle Buonocore
Senior Production Editor Monique Calello
Cover Designer Madelyn Lesure
Illustration Coordinator Gene Aiello
Illustration Studio Wellington Studios

Cover Photo NASA/Photo Researchers, Inc.

This book was set in 10/12 Times Roman by UG / GGS Information Services, Inc. and printed and bound
by Hamilton Printing Company. The cover was printed by Phoenix Color Corporation.

This book is printed on acid-free paper.

The paper in this book was manufactured by a mill whose forest management programs include sustained
yield harvesting of its timberlands, Sustained yield harvesting principles ensure that the numbers of trees
cut each year does not exceed the amount of new growth.

Copyright © 2001, John Wiley & Sons, Inc. All rights Reserved.

No part of this publication may be reproduced, stored in a retrieval system or transmitted
in any form or by any means, electronic, mechanical, photocopying, recording, scanning
or otherwise, except as permitted under Sections 107 or 1089 of the 1976 United States
Copyright Act, without either the prior written permission of the Publisher, or
authorization through payment of the appropriate per-copy fee to the Copyright
Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (508) 750-8400, fax

(508) 750-4470. Requests to the Publisher for permission should be addressed to the
Permissions Department, John Wiley & Sons, Inc., 605 Third Avenue, New York, NY
10158-0012, (212) 850-6011, fax (212) 850-6008, E-Mail: PERMREQ@WILEY.COM.

To order books or for customer service call 1-800-CALL-WILEY (225-5945).

Library of Congress Cataloging-in-Publication Data
Haykin, Simon
Communication systems / Simon Haykin.—4th ed.
p. cm,
ISBN 0-471-17869-1 (cloth : alk. paper)
1. Telecommunication. 2. Signal theory (Telecommunication) I. Title.

TK5101 .H37 2000
621,382—de21 99-042977

Printed in the United States of America

1098765432

Tn loving memory of Vera

 

 

Electrical engineering education has undergone some radical changes during the past cou-
ple of decades and continues to do so. A modern undergraduate program in electrical
engineering includes the following two introductory courses:

& Signals and Systems, which provides a balanced and integrated treatment of contin-
uous-time and discrete-time forms of signals and systems. The Fourier transform (in
its different forms), Laplace transform, and z-transform are treated in detail. Typi-
cally, the course also includes an elementary treatment of communication systems.

® Probability and Random Processes, which develops an intuitive grasp of discrete and
continuous random variables and then introduces the notion of a random process
and its characteristics.

Typically, these two introductory courses lead to a senior-level course on communication
systems.

The fourth edition of this book has been written with this background and primary
objective in mind. Simply put, the book provides a modern treatment of communication
systems at a level suitable for a one- or two-semester senior undergraduate course. The
emphasis is on the statistical underpinnings of communication theory with applications.

The material is presented in a logical manner, and it is illustrated with examples,
with the overall aim being that of helping the student develop an intuitive grasp of the
theory under discussion. Except for the Background and Preview chapter, each chapter
ends with numerous problems designed not only to help the students test their understand-
ing of the material covered in the chapter but also to challenge them to extend this material.
Every chapter includes notes and references that provide suggestions for further reading.
Sections or subsections that can be bypassed without loss of continuity are identified with
a footnote. :

A distinctive feature of the book is the inclusion of eight computer experiments using
MATLAB. This set of experiments provides the basis of a “Software Laboratory”, with
each experiment being designed to extend the material covered in the pertinent chapter.
Most important, the experiments exploit the unique capabilities of MATLAB in an instruc-
tive manner. The MATLAB codes for all these experiments are available on the Wiley Web
site: http://www.wiley.com/college/haykin/.

The Background and Preview chapter presents introductory and motivational ma-
terial, paving the way for detailed treatment of the many facets of communication systems
in the subsequent 10 chapters. The material in these chapters is organized as follows:

» Chapter 1 develops a detailed treatment of random, or stochastic, processes, with
particular emphasis on their partial characterization (i.e., second-order statistics). In
effect, the discussion is restricted to wide-sense stationary processes. The correlation

vil

Vi

fais

fate

PREFACE

¥

properties and power spectra of random processes are described in detail. Gaussian
processes and narrowband noise feature prominently in the study of communication
systems, hence their treatment in the latter part of the chapter. This treatment nat-
urally leads to the consideration of the Rayleigh and Rician distributions that arise
in a communications environment.

Chapter 2 presents an integrated treatment of continuous-wave (CW) modulation
(i.e., analog communications) and their different types, as outlined here:

(i) Amplitude modulation, which itself can assume one of the following forms (de-
pending on how the spectral characteristics of the modulated wave are specified):

Full amplitude modulation

Double sideband-suppressed carrier modulation

Quadrature amplitude modulation

Single sideband modulation

Vestigial sideband modulation

(ii) Angle modulation, which itself can assume one of two interrelated forms:
» Phase modulation
® Frequency modulation

The time-domain and spectral characteristics of these modulated waves, methods for
their generation and detection, and the effects of channel noise on their performances
are discussed.

¥¥Y ¥ ¥ ¥

Chapter 3 covers pulse modulation and discusses the processes of sampling, quan-
tization, and coding that are fundamental to the digital transmission of analog sig-
nals. This chapter may be viewed as the transition from analog to digital commu-
nications. Specifically, the following types of pulse modulation are discussed:
(i) Analog pulse modulation, where only time is represented in discrete form; it
embodies the following special forms:
® Pulse amplitude modulation
» Pulse width (duration) modulation
® Pule position modulation
The characteristics of pulse amplitude modulation are discussed in detail, as it is
basic to all forms of pulse modulation, be they of the analog or digital type.
(ii) Digital pulse modulation, in which both time and signal amplitude are repre-
sented in discrete form; it embodies the following special forms:

& Pulse-code modulation

» Delta modulation

» Differential pulse-code modulation
In delta modulation, the sampling rate is increased far in excess of that used in pulse-
code modulation so as to simplify implementation of the system. In contrast, in
differential pulse-code modulation, the sampling rate is reduced through the use of
a predictor that exploits the correlation properties of the information-bearing signal.
(iii) MPEG/audio coding standard, which includes a psychoacoustic model as a key

element in the design of the encoder.

Chapter 4 covers baseband pulse transmission, which deals with the transmission of
pulse-amplitude modulated signals in their baseband form. Two important issues are
discussed: the effects of channel noise and limited channel bandwidth on the perfor-
mance of a digital communication system. Assuming that the channel noise is additive

PREFACE ix

and white, this effect is minimized by using a matched filter, which is basic to the
design of communication receivers. As for limited channel bandwidth, it manifests
itself in the form of a phenomenon known as intersymbol interference. To combat
the degrading effects of this signal-dependent interference, we may use either a pulse-
shaping filter or correlative encoder/decoder; both of these approaches are discussed.
The chapter includes a discussion of digital subscriber lines for direct communication
between a subscriber and an Internet service provider. This is followed by a deriva-
tion of the optimum linear receiver for combatting the combined effects of channel
noise and intersymbol interference, which, in turn, leads to an introductory treatment
of adaptive equalization.

Chapter S$ discusses signal-space analysis for an additive white Gaussian noise chan-
nel. In particular, the foundations for the geometric representation of signals with
finite energy are established. The correlation receiver is derived, and its equivalence
with the matched filter receiver is demonstrated. The chapter finishes with a discus-
sion of the probability of error and its approximate calculation.

Chapter 6 discusses passband data transmission, where a sinusoidal carrier wave is
employed to facilitate the transmission of the digitally modulated wave over a band-
pass channel, This chapter builds on the geometric interpretation of signals presented
in Chapter 5. In particular, the effect of channel noise on the performance of digital

communication systems is evaluated, using the following modulation techniques:
(i) Phase-shift keying, which is the digital counterpart to phase modulation with
the phase of the carrier wave taking on one of a prescribed set of discrete values.

(ii) Hybrid amplitude/phase modulation schemes including quadrature-amplitude
modulation (QAM), and carrierless amplitude/phase modulation (CAP).

(iii) Frequency-shift keying, which is the digital counterpart of frequency modulation
with the frequency of the carrier wave taking on one of a prescribed set of discrete
values, ;

(iv) Generic multichannel modulation, followed by discrete multitone, the use of
which has been standardized in asymmetric digital subscriber lines.

In a digital communication system, timing is everything, which means that the re-

ceiver must be synchronized to the transmitter. In this context, we speak of the

receiver being coherent or noncoherent. In a coherent receiver, provisions are made
for the recovery of both the carrier phase and symbol timing. In a noncoherent
receiver the carrier.phase is ignored and provision is only’‘made for symbol timing.

Such a strategy is dictated by the fact that the carrier phase may be random, making

phase recovery a costly proposition. Synchronization techniques are discussed in the

latter part of the chapter, with particular emphasis on discrete-time signal processing.

Chapter 7 introduces spread-spectrum modulation. Unlike traditional forms of mod-

ulation discussed in earlier chapters, channel bandwidth is purposely sacrificed in

spread-spectrum modulation for the sake of security or protection against interfering
signals, The direct-sequence and frequency-hop forms of spread-spectrum modula-
tion are discussed.

Chapter 8 deals with multiuser radio communications, where a multitude of users

have access to a common radio channel. This type of communication channel is well

represented in satellite and wireless communication systems, both of which are dis-
cussed, The chapter includes a presentation of link budget analysis, emphasizing the
related antenna and propagation concepts, and noise calculations.

Chapter 9 develops the fundamental limits in information theory, which are embod-

ied in Shannon’s theorems for data compaction, data compression, and data trans-

xX PREFACE

mission. These theorems provide upper bounds on the performance of information
sources and communication channels. Two concepts, basic to formulation of the
theorems, are (1) the entropy of a source (whose definition is analogous to that of
entropy in thermodynamics), and (2) channel capacity.

» Chapter 10 deals with error-contro! coding, which encompasses techniques for the
encoding and decoding of digital data streams for their reliable transmission over
noisy channels. Four types of error-control coding are discussed:

(i) Linear block codes, which are completely described by sets of linearly indepen-
dent code words, each of which consists of message bits and parity-check bits.
The parity-check bits are included for the purpose of error control.

(ii) Cyclic codes, which form a subclass of linear block codes.

(iti) Convolutional codes, which involve operating on the message sequence contin-
uously in a serial manner.

(iv) Turbo codes, which provide a novel method of constructing good codes that
approach Shannon’s channel capacity in a physically realizable manner.

Methods for the generation of these codes and their decoding are discussed.

The book also includes supplementary material in the form of six appendices as
follows:

¥

Appendix 1 reviews probability theory.

Appendix 2, on the representation of signals and systems, reviews the Fourier trans-
form and its properties, the various definitions of bandwidth, the Hilbert transform,
and the low-pass equivalents of narrowband signals and systems.

Appendix 3 presents an introductory treatment of the Bessel function and its modified
form. Bessel functions arise in the study of frequency modulation, noncoherent de-
tection of signals in noise, and symbol timing synchronization.

Appendix 4 introduces the confluent hypergeometric function, the need for which
arises in the envelope detection of amplitude-modulated signals in noise.

Appendix 5 provides an introduction to cryptography, which is basic to secure
communications.

» Appendix 6 includes 12 useful tables of various kinds.

¥

a

a

¥

As mentioned previously, the primary purpose of this book is to provide a modern
treatment of communication systems suitable for use in a one- or. two-semester under- ~
graduate course at the senior level.’The make-up of the material for the course is naturally
determined by the background of the students and the interests of the teachers involved.
The material covered in the book is both broad and deep enough to satisfy a variety of
backgrounds and interests, thereby providing considerable flexibility in the choice of
course material. As an aid to the teacher of the course, a detailed solutions manual for all
the problems in the book is available from the publisher.

a Acknowledgments

I wish to express my deep gratitude to Dr. Gregory J. Pottie (University of California, Los
Angeles), Dr. Santosh Venkatesh (University of Pennsylvania), Dr. Stephen G. Wilson (Uni-
versity of Virginia), Dr. Gordon Stiiber (Georgia Institute of Technology), Dr. Venugopal
Veeraralli (Cornell University), and Dr. Granville E. Ott (University of Texas at Austin)

PREFACE xi

for critical reviews of an earlier version of the manuscript and for making numerous sug-
gestions that have helped me shape the book into its present form. The treatment of the
effect of noise on envelope detection presented in Chapter 2 is based on course notes made
available to me by Dr. Santosh Venkatesh, for which I am grateful. I am grateful to Dr.
Gordon Stiiber for giving permission to reproduce Figure 6.32.

I am indebted to Dr. Michael Moher (Communications Research Centre, Ottawa)
for reading five chapters of an earlier version of the manuscript and for making many
constructive comments on turbo codes. I am equally indebted to Dr. Brendan Frey (Uni-
versity of Waterloo, Ontario) for his invaluable help in refining the material on turbo
codes, comments on low-density parity-check codes, for providing the software to plot
Fig. 9.18, and giving me the permission to reproduce Figures 10.27 and 10.33. I am grate-
ful to Dr. David Conn (McMaster University, Ontario) for his critical reading of the Back-
ground and Preview Chapter and for making suggestions on how to improve the presen-
tation of the material therein.

I also wish to thank Dr. Jean-Jacque Werner (Lucent Technologies, Holmdel), Dr.
James Mazo (Lucent Technologies, Murray Hill), Dr. Andrew Viterbi (Qualcom, San Di-
ego), Dr. Radford Neal (University of Toronto, Ontario), Dr. Yitzhak (Irwin) Kalet (Tech-
nion, Israel), Dr. Walter Chen (Motorola), Dr. John Cioffi (Stanford University), Dr. Jon
Mark (University of Waterloo, Ontario), and Dr. Robert Dony (University of Guelph,
Ontario); I thank them all for their helpful comments on selected sections in the book.
Corrections and suggestions for improvements to the book made by Dr. Donald Wunsch
II (University of Missouri) are also appreciated.

Iam grateful to my graduate student Mathini Sellathurai (McMaster University) for
performing the computer experiments in the book, and Hugh Pasika (McMaster Univer-
sity) for many useful comments on the Background and Preview Chapter and for doing
the computations on some graphical plots in the book. Proofreading of the page proofs
by Mathini Sellathurai and Alpesh Patel is much appreciated.

Iam particularly grateful to my editor at Wiley, Bill Zobrist, for his strong support
and help throughout the writing of the book. I am indebted to Monique Calello, Senior
Production Editor at Wiley, for her tireless effort in overseeing the production of the book
in its various stages. I thank Katherine Hepburn for advertising and marketing the book.
I thank Karen Tongish for her careful copyediting of the manuscript, Katrina Avery for
her careful proofreading of the page proofs, and-Kristen Maus for composing the index
of the book.

Last but by no means least, as always, I am grateful to my Technical Coordinator,
Lola Brooks, for her tireless effort in typing the manuscript of the book. I also wish to
record my gratitude to Brigitte Maier, Assistant Librarian, and Regina Bendig, Reference
Librarian, at McMaster University, for helping.me on numerous occasions in tracing ref-
erences for the bibliography,

Simon Haykin
Ancaster, Ontario
January, 2000

 

i BACKGROUND AND PREVIEW ; 1

wp EnNnanpawnrp

nn
=

{| CHAPTER I

The Communication Process 1

Primary Communication Resources 3
Sources of Information 3

Communication Networks 10
Communication Channels 15

Modulation Process 19

Analog and Digital Types of Communication 21
Shannon’s Information Capacity Theorem 23
A Digital Communication Problem 24
Historical Notes 26

Notes and References 29

Random Processes 31

 

 

1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
1.10
1.11

1.13
1.14

Introduction 31

Mathematical Definition of a Random Process 32

Stationary Processes 33

Mean, Correlation, and Covariance Functions 35

Ergodic Processes 41

Transmission of a Random Process Through a Linear Time-Invariant Filter 42
Power Spectral Density 44

Gaussian Process 54

Noise 58

Narrowband Noise 64

Representation of Narrowband Noise in Terms of In-phase and Quadrature
Components 64 ;
Representation of Narrowband Noise in Terms of Envelope and Phase
Components 67

Sine Wave Plus Narrowband Noise 69
Computer Experiments: Flat-Fading Channel 71

xiv CONTENTS

1,15

CHAPTER 2

Summary and Discussion 75
Notes and References 77
Problems 78

Continuous-Wave Modulation

 

2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11
2.12
2.13
2.14
2.15

Introduction 88
Amplitude Modulation 90

_ Linear Modulation Schemes 93

Frequency Translation 103

Frequency-Division Multiplexing 105

Angle Modulation 107

Frequency Modulation 109

Nonlinear Effects in FM Systems 126
Superheterodyne Receiver 128

Noise in CW Modulation Systems 130

Noise in Linear Receivers using Coherent Detection 132
Noise in AM Receivers using Envelope Detection 135
Noise in FM Receivers 142

Computer Experiments: Phase-locked Loop 157
Summary and Discussion 162

Notes and References 165

Problems 166

| CHAPTER 3 Pulse Modulation

3.1
3,2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15

Introduction 183

Sampling Process 184

Pulse-Amplitude Modulation 188

Other Forms of Pulse Modulation 191
Bandwidth—Noise Trade-off 193

Quantization Process 193

Pulse-Code Modulation 201

Noise Considerations in PCM Systems 209
Time-Division Multiplexing 211

Digital Multiplexers 214

Virtues, Limitations, and Modifications of PCM | 217
Delta Modulation 218

Linear Prediction 223

Differential Pulse-Code Modulation 227
Adaptive Differential Pulse-Code Modulation 229

88

183

CONTENTS xv

 

 

 

 

3.16 Computer Experiment: Adaptive Delta Modulation 232
3.17 MPEG Audio Coding Standard 234
3.18 Summary and Discussion 236
Notes and References 238
Problems 239
| CHAPTER 4 Baseband Pulse Transmission 247
4.1 Introduction 247
4.2 Matched Filter 248
4.3. Error Rate Due to Noise 253
4.4 Intersymbol Interference 259
4.5 Nyquist’s Criterion for Distortionless Baseband Binary Transmission 261
4.6 Correlative-Level Coding 267
4.7 Baseband M-ary PAM Transmission 275 .
4.8 . Digital Subscriber Lines 277
4.9 Optimum Linear Receiver 282
4.10 Adaptive Equalization 287

Signal-Space Analysis 309

 

 

Conversion of the Continuous AWGN Channel into a Vector Channel 318

Coherent Detection of Signals in Noise: Maximum Likelihood Decoding 322

Passband Digital Transmission 344

 

 

4.11 Computer Experiments: Eye Patterns 293
4.12 Summary and Discussion 296
Notes and References 297
Problems 300
{ CHapTer 5
5.1. Introduction 309
5.2 Geometric Representation of Signals 311
5.3
5.4 Likelihood Functions 322
5.5
5.6 Correlation Receiver 326
5.7 Probability of Error 328
5.8 Summary and Discussion 337
Notes and References 337
Problems 338
7 CuaPrer 6
6.1 Introduction 344
6.2 Passband Transmission Model 348
6.3 Coherent Phase-Shift Keying 349

xvi CONTENTS

6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11

6.12.

6.13
6.14
6.15
6.16

i CHaPrer 7

Hybrid Amplitude/Phase Modulation Schemes 368

Coherent Frequency-Shift Keying 380

Detection of Signals with Unknown Phase 403

Noncoherent Orthogonal Modulation 407

Noncoherent Binary Frequency-Shift Keying 413

Differential Phase-Shift Keying 414

Comparison of Digital Modulation Schemes Using a Single Carrier 417
Voiceband Modems 420

Multichannel Modulation 431

Discrete Multitone 440

Synchronization 448

Computer Experiments: Carrier Recovery and Symbol Timing 458
Summary and Discussion 464

Notes and References 465

Problems 468

Spread-Spectrum Modulation 479

 

 

7.1
7.2
7.3
74
7.5
7.6
7.7
7.8
7.9

Introduction 479

Pseudo-Noise Sequences 480

A Notion of Spread Spectrum 488

Direct-Sequence Spread Spectrum with Coherent Binary Phase-Shift Keying 490
Signal-Space Dimensionality and Processing Gain 493
Probability of Error 497

Frequency-Hop Spread Spectrum 499

Computer Experiments: Maximal-Length and Gold Codes 505
Summary and Discussion 508

Notes and References 509

Problems 509

Multiuser Radio Communications 512

 

 

 

 

i CHAPTER 8

8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
8.9

Introduction 512

Multiple-Access Techniques 513

Satellite Communications 514

Radio Link Analysis 517

Wireless Communications 529. _

Statistical Characterization of Multipath Channels 535
Binary Signaling over a Rayleigh Fading Channel 542
TDMA and CDMA Wireless Communication Systems 547
Source Coding of Speech for Wireless Communications 550

CONTENTS wii

8.10 Adaptive Antenna Arrays for Wireless Communications $53
8.11 Summary and Discussion 559

Notes and References 560

Problems 562

§ CHAPTER 9 Fundamental Limits in Information Theory 567

 

 

 

 

9.1 Introduction 567
9.2 Uncertainty, Information, and Entropy 568
9.3 Source-Coding Theorem 574
9.4 Data Compaction 575
9.5 Discrete Memoryless Channels 581
9.6  MautualInformation 584
9.7 Channel Capacity 587
9.8 Channel-Coding Theorem 589 -- :
9.9 Differential Entropy and Mutual Information for Continuous Ensembles 593
9.10 Information Capacity Theorem 597
9,11 Implications of the Information Capacity Theorem 601
9,12 Information Capacity of Colored Noise Channel 607
9,13 Rate Distortion Theory 611 .
9.14 Data Compression 614
9.15 Summary and Discussion 616
Notes and References 617
Problems 618

| Cuaprer 10 Error-Control Coding 626

10.1. Introduction 626
10.2 Discrete-Memoryless Channels 629
10.3. Linear Block Codes 632
10.4 Cyclic Codes 641
10.5 Convolutional Codes 654
10.6 Maximum Likelihood Decoding of Convolutional Codes, 660
10.7 Trellis-Coded Modulation 668 ‘
10.8 Turbo Codes 674
10.9 Computer Experiment: Turbo Decoding 682
10.10 Low-Density Parity-Check Codes 683
10.11 Irregular Codes 691
10,12 Summary and Discussion 693
Notes and References 694
Problems 696

 

xviii CONTENTS

APPENDIX 1 Probability Theory 703

APPENDIX 2 Representation of Signals and Systems 715

APPENDIX 3 Bessel Functions 735

APPENDIX 4 Confluent Hypergeometric Functions 740

APPENDIX 5 Cryptography 742

APPENDIX 6 Tables 761

GLOSSARY 77%
BIBLIOGRAPHY 777

INDEX 792

 

BACKGROUND
AND PREVIEW

The background and preview material presented herein sets the stage for a statistical
treatment of communication systems in subsequent chapters. In particular, we describe the
following:

» The communication process.

> Primary communication resources, namely, transmitted power and channel bandwidth.
> Sources of information.

» The two primary types of switching: circuit switching and packet switching.

» Communication channels for the transportation of information-bearing signals from the
transmitter to the receiver.

> The modulation process, which is basic to communication systems.
» Analog and digital types of communication systems.
> Shannon's information capacity theorem.

> A digital communications problem.

The chapter concludes with some historical notes, as a source of motivation for the
reader.

i The Conumunication Process

Today, communication enters our daily lives in so many different ways that it is very easy
to overlook the multitude of its facets, The telephones at our hands, the radios and tele-
visions in our living rooms, the computer terminals with access to the Internet in our offices
and homes, and our newspapers are all capable of providing rapid communications from
every corner of the globe. Communication provides the senses for ships on the high seas,
aircraft in flight, and rockets and satellites in space. Communication through a wireless
telephone keeps a car driver in touch with the office or home miles away. Communication
keeps a weather forecaster informed of conditions measured by a multitude of sensors.
Indeed, the list of applications involving the use of communication in one way or another
is almost endless.

2 & BACKGROUND AND PREVIEW

In the most fundamental sense, communication involves implicitly the transmission
of information from one point to another through a succession of processes, as described
here:

1. The generation of a message signal: voice, music, picture, or computer data. -

2. The description of that message signal with a certain measure of precision, by a set
of symbols: electrical, aural, or visual.

3. The encoding of these symbols in a form that is suitable for transmission over a
physical medium of interest.

4. The transmission of the encoded symbols to the desired destination.

5. The decoding and reproduction of the original symbols.

6. The re-creation of the original message signal, with a definable degradation in qual-
ity; the degradation is caused by imperfections in the system.

There are, of course, many other forms of communication that do not directly involve
the human mind in real time. For example, in computer communications involving com-
munication between two or more computers, human decisions may enter only in setting
up the programs or commands for the computer, or in monitoring the results.

Irrespective of the form of communication process being considered, there are three
basic elements to every communication system, namely, iransmitter, channel, and receiver,
as depicted in Figure 1. The transmitter is located at one point in space, the receiver is
located at some other point separate from the transmitter, and the channel is the physical
medium that connects them. The purpose of the transmitter is to convert the message signal
produced by the source of information into a form suitable for transmission over the
channel. However, as the transmitted signal propagates along the channel, it is distorted
due to channel imperfections. Moreover, noise and interfering signals (originating from
other sources) are added to the channel output, with the result that the received signal is
a corrupted version of the transmitted signal. The receiver has the task of operating on
the received signal so as to reconstruct a recognizable form of the original message signal
for a user.

There are two basic modes of communication:

1. Broadcasting, which involves the use of a single powerful transmitter and numerous
receivers that are relatively inexpensive to build. Here information-bearing signals
flow only in one direction. —

2. Point-to-point communication, in which the communication process takes place over
a link between a single transmitter and a receiver. In this case, there is usually a
bidirectional flow of information-bearing signals, which requires the use of a trans-
mitter and receiver at each end of the link.

Communication System

t ~ 7~ |

 

 

 

 

 

 

 

 

 

 

 

 

| |
Source of Transmitter Receiver i User of
information Message | Estimate of | information
signal | message |
signal |
]
| i
| Channel -
l Transmitted Received |
signal signal |

 

Figure 1 Elements of a communication system.

Sources of Information 3

The broadcasting mode of communication is exemplified by radio and television, and the
ubiquitous telephone provides the means for one form of point-to-point communication.
Another example of point-to-point communication is the link between an Earth station
and a robot navigating the surface of a distant planet.

All these different communication systems as well as others not mentioned here share
a common feature: The underlying communication process in each and every one of them
is statistical in nature. Indeed, it is for this important reason that much of this book is
devoted to the statistical underpinnings of communication systems. In so doing, we develop
an exposition of the fundamental issues involved in the study of different communication
methodologies and thereby provide a natural forum for their comparative evaluations.

i Primary Communication Resources

 

 

 

In a communication system, two primary resources are employed: transmitted power and
channel bandwidth, The transmitted power is the average power of the transmitted signal.
The channel bandwidth is defined as the band of frequencies allocated for the transmission
of the message signal. A general system design objective is to use these two resources as
efficiently as possible. In most communication channels, one resource may be considered
more important than the other. We may therefore classify communication channels as
power limited or band limited. For example, the telephone circuit is a typical band-limited
channel, whereas a space communication link or satellite channel is typically power
limited.

When the spectrum of a message signal extends down to zero or low frequencies, we
define the bandwidth of the signal as that upper frequency above which the spectral content
of the signal is negligible and therefore unnecessary for transmitting information. For
example, the average voice spectrum extends well beyond 10 kHz, though most of the
average power is concentrated in the range of 100 to 600 Hz, and a band from 300 to
3100 Hz gives good articulation. Accordingly, we find that telephone circuits that respond
well to this latter range of frequencies give quite satisfactory commercial telephone service.

Another-important point that we have to keep in mind is the unavoidable presence
of noise in a communication system. Noise refers to unwanted waves that tend to disturb
the transmission and processing of message signals in a communication system. The
sources of noise may be internal or external to the system.

A quantitative way to account for the effect of noise is to introduce signal-to-noise
ratio (SNR) as a system parameter. For example, we may define the SNR at the receiver
input as the ratio of the average signal power to the average noise power, both being
measured at the same point. The customary practice is to express the SNR in decibels
(dBs), defined as 10 times the logarithm (to base 10) of the power ratio, For example,
signal-to-noise ratios of 10, 100, and 1,000 correspond to 10, 20, and 30 dBs, respectively.

i Sources of Information

The telecommunications environment is dominated by four important sources of infor-
mation: speech, music, pictures, and computer data. A source of information may be
characterized in terms of the signal that carries the information. A signal is defined as a
single-valued function of time that plays the role of the independent variable; at every
instant of time, the function has a unique value. The signal can be one-dimensional, as in
the case of speech, music, or computer data; two-dimensional, as in the case of pictures;

4 & BACKGROUND AND PREVIEW

three-dimensional, as in the case of video data; and four-dimensional, as in the case of
volume data over time. In the sequel, we elaborate on different sources of information.

(i)

Speech is the primary method of human communication. Specifically, the speech
communication process involves the transfer of information from a speaker to a
listener, which takes place in three successive stages:

» Production. An intended message in the speaker’s mind is represented by a speech
signal that consists of sounds (i.e., pressure waves) generated inside the vocal tract
and whose arrangement is governed by the rules of language.

> Propagation. The sound waves propagate through the air at a speed of 300 m/s,
reaching the listener’s ears,

» Perception, The incoming sounds are deciphered by the listener into a received
message, thereby completing the chain of events that culminate in the transfer of
information from the speaker to the listener.

The speech-production process may be viewed as a form of filtering, in which a sound
source excites a vocal tract filter. The vocal tract consists of a tube of nonuniform
cross-sectional area, beginning at the glottis (i.e., the opening between the vocal
cords) and ending at the lip. As the sound propagates along the vocal tract, the
spectrum (i.e., frequency content) is shaped by the frequency selectivity of the vocal
tract; this effect is somewhat similar to the resonance phenomenon observed in organ
pipes. The important point to note here is that the power spectrum (i.e., the distri-
bution of long-term average power versus frequency) of speech approaches zero for
zero frequency and reaches a peak in the neighborhood of a few hundred hertz. To
put matters into proper perspective, however, we have to keep in mind that the
hearing mechanism is very sensitive to frequency. Moreover, the type of communi-
cation system being considered has an important bearing on the band of frequencies
considered to be “essential” for the communication process. For example, as men-
tioned previously, a bandwidth of 300 to 3100 Hz is considered adequate for com-
mercial telephonic communication.

The second source of information, music, originates from instruments such as the
piano, violin, and flute. The note made by a musical instrument may last for a short
time interval as in the’pressing of a key on a piano, or it may be sustained for a long
time interval as in the example of a flute player holding a prolonged note. Typically,
music has two structures: a melodic structure consisting of a time sequence of sounds,
and a harmonic structure consisting of a set of simultaneous sounds. Like a speech
signal, a musical signal is bipolar. However, a musical signal differs from a speech
signal in that its spectrum occupies a much wider band of frequencies that may extend
up to about 15 kHz. Accordingly, musical signals demand a much wider channel
bandwidth than speech signals for their transmission.

The third source of information, pictures, relies on the human visual system for its
perception. The picture can be dynamic, as in television, or static, as in facsimile.
Taking the case of television first, the pictures in motion are converted into electrical
signals to facilitate their transport from the transmitter to the receiver. To do so,
each complete picture is sequentially scanned. The scanning process is carried out in
a TV camera. In a black-and-white TV, the camera contains optics designed to focus
an image on a photocathode consisting of a large number of photosensitive elements.
The charge pattern so generated on the photosensitive surface is scanned by an elec-
tron beam, thereby producing an output current that varies temporally with the way
in which the brightness of the original picture varies spatially from one point to
another. The resulting output current is called a video signal. The type of scanning

Sources of Information 5

used in television is a form of spatial sampling called raster scanning, which converts
a two-dimensional image intensity into a one-dimensional waveform; it is somewhat
analogous to the manner in which we read a printed paper in that the scanning is
performed from left to right on a line-by-line basis. In North American analog tele-
vision, a picture is divided into 525 lines, which constitute a frame. Each frame is
decomposed into. two interlaced fields, each of which consists of 262.5 lines. For
convenience of presentation, we will refer to the two fields as I and II. The scanning
procedure is illustrated in Figure 2. The lines of field I are depicted as solid lines, and
those of field II are depicted as dashed lines. The start and end of each field are also
included in the figure. Field I is scanned first. The scanning spot of the TV camera
moves with constant velocity across each line of the field from left to right, and the
image intensity at the center of the spot is measured; the scanning spot itself is partly
responsible for local spatial averaging of the image. When the end of a particular
line is reached, the scanning spot quickly flies back (in a horizontal direction) to the
start of the next line down in the field. This flyback is called the horizontal retrace.
The scanning process described here is continued until the whole field has been ac-
counted for. When this condition is reached, the scanning spot moves quickly (in a
vertical direction) from the end of field I to the start of field II. This second flyback
is called the vertical retrace. Field II is treated in the same fashion as field I. The time
taken for each field to be scanned is 1/60 s. Correspondingly, the time taken for a
frame or a complete picture to be scanned is 1/30 s. With 525 lines in a frame, the
line-scanning frequency equals 15.75 kHz. Thus, by flashing 30 still pictures per
second on the display tube of the TV receiver, the human eye perceives them to be
moving pictures. This effect is due to a phenomenon known as the persistence of
vision. During the horizontal- and vertical-retrace intervals, the picture tube is made
inoperative by means of blanking pulses that are generated at the transmitter. More-
over, synchronization between the various scanning operations at both transmitter
and receiver is accomplished by means of special pulses that are transmitted during
the blanking periods; thus, the synchronizing pulses do not show on the reproduced
picture. The reproduction quality of a TV picture is limited by. two basic factors:

1. The number of lines available in a raster scan, which limits resolution of the

picture in the vertical direction.
2. The channel bandwidth available for transmitting the video signal, which limits
resolution of the picture in the horizontal direction.

Start of field |

Start of field II

 
  

A

nine

 

  

 

 

 

End of field | End of field II

FiGuRE 2 Interlaced raster scan.

6

BACKGROUND AND PREVIEW

For each direction, resolution is expressed in terms of the maximum number of lines
alternating between black and white that can be resolved in the TV image along the
pertinent direction by a human observer. In the NTSC (National Television System
Committee) system, which is the North American standard, the parameter values
used result in a video bandwidth of 4.2 MHz, which extends down to zero frequency.
This bandwidth is orders of magnitude larger than that of a speech signal. Note also
that whereas a speech signal is bipolar, a video (television) signal is inherently positive
(i.e., unipolar).

In color TV, the perception of color is based on the three types of color recep-
tors (cones) in the human eye: red, green, and blue, whose wavelengths are 570 nm,
535 nm, and 445 nm, respectively. These three colors are referred to as primary
colors because any other color found in nature can be approximated by an additive
mixture of them. This physical reality is indeed the basis for the transmission of color
in commercial TV broadcasting. The three primary colors are represented by the
video signals mtp(t), #t¢(t), and mg(t), respectively. To conserve bandwidth and pro-
duce a picture that can be viewed on a conventional black-and-white (monochrome)
television receiver, the transmission of these three primary colors is accomplished by
observing that they can be uniquely represented by any three signals that are inde-
pendent linear combinations of w(t), m¢(t), and mp(t). The three signals are as
follows:

® A luminance signal, m;,(t), which produces a black-and-white version of the color
picture when it is received on a conventional monochrome television receiver.

> A pair of signals, #1;(t) and mo(t), called the chrominance signals, which indicate
the way the color of the picture departs from shades of gray.

The luminance signal *;(f) is assigned the entire 4.2 MHz bandwidth. Owing to
certain properties of human vision, tests show that if the nominal bandwidths of the
chrominance signals #;(£) and mg(t) are 1.6 MHz and 0.6 MHz, respectively, sat-
isfactory color reproduction is possible.

Turning next to a facsimile (fax) machine, the purpose of this machine is to
transmit still pictures over a communication channel (most notably, a telephone
channel). Such a machine provides a highly popular facility for the transmission of .
handwritten or printed text from one point to another; transmitting text by facsimile
is treated simply like transmitting a picture. The basic principle employed for signal
generation in a facsimile machine is to scan an original document (picture) and use
an image sensor to convert the light to an electrical signal.

Finally, personal computers (PCs) have become an integral part of our daily lives.
We use them for electronic mail, exchange of software, and sharing of resources. The
text transmitted by a PC is usually encoded using the American Standard Code for
Information Interchange (ASCII), which is the first code developed specifically for
computer communications. Each character in ASCII is represented by seven data bits
constituting a unique binary pattern made up of Os and 1s; bit is acronym for binary
digit. Thus a total of 2” = 128 different characters can be represented in ASCII. The
characters are various lowercase and uppercase letters, numbers, special control sym-
bols, and punctuation symbols commonly used such as @, $, and %. Some of the
special “control” symbols, such as BS (backspace) and CR (carriage return), are used
to control the printing of characters on a page. Other symbols, such as ENQ (enquiry)
and ETB (end of transmission block), are used for communication purposes. (A com-
plete listing of ASCII characters is given in Table A6.1.) The seven data bits are
ordered starting with the most significant bit b; down to the least significant bit b,,

Sources of Information 7

 

 

 

Idle

High ——
i
(e} bg 1
Low | |
Start Data bits | Parity Stop |
; bit ; bit | bit

FIGURE 3 The bit format for sending asynchronous serial data used in the RS-232
standard.

as illustrated in Figure 3. At the end of the data bits, an extra bit bs is appended for
the purpose of error detection. This error-detection bit is called a parity bit. A se-
quence of eight bits is referred to as a-byte, or an octet. The parity bit is set in such
a way that the total number of 1s in each byte is odd for odd parity and even for
even parity. Suppose, for example, the communicators agree to use even parity; then
the parity bit will be a 0 when the number of 1s in the data bit is even and a 1 when
it is odd. Hence, if a single bit in a byte is received in error and thereby violates the
even parity rule, it can be detected and then corrected through retransmission. Per-
sonal computers are often connected via their RS (recommended standard)-232 ports.
When ASCII data (in fact, all character data) are transmitted through these ports, a
start bit, set to 0, and one or more stop bits, set to 1, as shown in Figure 3, are added
to provide character framing. When the transmission is idle, a long series of 1s is
sent so as to keep the circuit connection alive. In Figure 3, symbols 0 and 1 are
designated as “low” and “high,” respectively. They are also sometimes referred to

“space” and “mark,”’ respectively; the latter terminology comes from the days of
telegraphy. The text prepared on a PC is usually stored and then transmitted over a
communication channel (e.g., a telephone channel} with a single character being sent
at a time. This form of data transmission is called asynchronous transmission, as
opposed to synchronous transmission in which a whole sequence of encoded char-
acters is sent over the channel in one long transmission. Encoded characters produced
by a mixture of asynchronous and synchronous terminals are combined by means
of data multiplexers. The multiplexed stream of data so formed is then applied to a
device called a modem (modulator-demodulator) for the purpose of transmission
over the channel.

In summary, computer-generated data and television signals are both wide-
band signals, in that their power content occupies a wide range of frequencies. An-
other important characteristic of data communication between personal computers
is burstiness, which means that information is usually transmitted from one terminal
to another in bursts with silent periods between bursts. Indeed, data traffic involving
computers in one form or another tends to be of a bursty nature. This is to be
contrasted with traffic in a digital transmission network due to voice or interactive
video, which, relatively speaking, is continuous.

Another way in which we use the computer is to download compressed forms
of text, audio, and video data from a service provider at a remote location. Data
compression provides a practical means for the efficient storage and transmission of
these kinds of data. A data compression system consists of an encoder and a decoder,
where the compression of an incoming data stream and its reconstruction are per-
formed, respectively. Basically, there are two forms of data compression:

1. Lossless compression operates by removing the redundant information contained
in the data of interest. The compression is said to be lossless because it is com-

8

& BACKGROUND AND PREVIEW

pletely reversible in that the original data can be reconstructed exactly. Lossless
compression is also referred to as data compaction.

2. Lossy compression involves the loss of information in a controlled manner; the
compression may therefore not be completely reversible. Lossy compression is,
however, capable of achieving a compression ratio higher than that attainable
with lossless methods.

For digital text, lossless compression is required. In this context, we mention the
Lempel-Ziv algorithm, which is intrinsically adaptive and capable of encoding
groups of source symbols that occur frequently. It achieves a compression of ap-
proximately 55 percent on ordinary English text, which, loosely speaking, corre-
sponds to the compression that would be achieved by encoding pairs of letters. The
Lempel-Ziv algorithm is a form of entropic coding, or source coding, which is dis-
cussed in Chapter 9.

In many other applications, lossy compression is usually the preferred approach
as its use can substantially reduce the data size without significantly altering the
perceptual quality of an image or audio signal. For such applications, this form of
data compression is acceptable, and in high-throughput data-transmission applica-
tions such as the Internet, it is a necessity. But in some other applications such as a
clinical setting, the quality of a medical image (e.g., digital x-ray radiograph) must
not be degraded on reconstruction,

For digital audio and video applications involving storage or transmission to
be viable in today’s marketplace, we need standard compression algorithms that
enable the interoperability of equipment produced by different manufacturers. In this
context, we mention three prominent standard compression algorithms that cater to
different needs:

» The JPEG image coding standard? is designed to compress full-color or grayscale
images of natural, real-world scenes by exploiting known limitations of the human
visual system; JPEG stands for Joint Photographic Experts Group. At the input to
the encoder, picture elements, or pixels, are grouped into 8 X 8 blocks, which are
applied to a relative of the Fourier transform known as the discrete cosine trans-
form (DCT)*. The DCT decomposes each block of pixels into a set of 64 coeffi-
cients that closely satisfy two related objectives:

1. The coefficients should be as uncorrelated as possible.

2. The energy of the input signal should be packed into the smallest number of
coefficients possible.
The next operation in the encoder is that of quantization, where each of the 64
DCT coefficients is rounded off. In JPEG, quantization is performed in conjunction
with a quantization table supplied by the user as an input to the encoder. Each
element of the table is an integer from 1 to 255 that specifies the step size of the
DCT coefficients, which, in turn, permits the representation of each quantized
DCT coefficient by an 8-bit code word. Basically, the purpose of quantization is
to discard information that is not perceptually discernible. Quantization is a many-
to-one mapping and therefore the principal source of lossiness in the encoder. The
final operation in the encoder is that of Huffman coding, which is a form of
entropic (source) coding also discussed in Chapter 9, Huffman coding achieves
additional data compression in a lossless manner by encoding the quantized DCT
coefficients in accordance with their statistical characteristics. At the decoder, data
reconstruction is performed through a sequence of operations that are the inverse

Vv

Sources of Information 9

of those-in the encoder, namely, Huffman decoding, dequantization in accordance

with the quantization table, and finally the inverse DCT.

The MPEG-1/video coding standard* is designed primarily to compress video sig-

nals at 30 frames per second (fps) into bit streams running at the rate of 1.5

megabits per second (Mb/s); MPEG stands for Motion Photographic Experts

Group. The MPEG-1 video coding standard achieves this design goal by exploiting

four basic forms of redundancy inherently present in video data:

1. Interframe (temporal) redundancy.

2. Interpixel redundancy within a frame.

3. Psychovisual redundancy.

4. Entropic coding redundancy.

It is the exploitation of interframe redundancy that distinguishes MPEG-1 from

JPEG. In principle, neighboring frames in typical video sequences are highly cor-

related. The meaning of this high correlation is that, in an average sense, a video

signal does not change rapidly from one frame to the next, and as a result, the
difference between adjacent frames has a variance (i.¢., average power) that is
much smaller than the variance of the video signal itself. Accordingly, the inter-
frame redundancy can be significantly reduced to produce a more efficiently com-
pressed video signal. This reduction is achieved through the use of prediction to
estimate each frame from its neighbors; the resulting prediction error is transmitted
for motion estimation and compensation. The prediction is nonlinear by virtue of
the nature of the problem. As with-JPEG, the interpixel redundancy is reduced
through the combined use of the DCT, quantization, and lossless entropic coding.

The net result is that full-motion.video becomes a 1.5 Mb/s stream of computer

data that can be stored on compact discs or integrated with texts and graphics.

Most important, the full-motion video and associated audio can be delivered over

existing computer and telecommunication networks, which, in turn, makes it pos-

sible to fulfill the need for video-on-demand on the Internet.

The MPEG-1/audio coding standard’ is based on perceptual coding, which is a

waveform-preserving process; that is, the amplitude-time waveform of the decoded

audio signal closely approximates that of the original audio signal. In basic terms,
the encoding process encompasses four distinct operations:

1. Time-frequency mapping, whereby the input audio signal is decomposed into
multiple subbands.

2. Psychoacoustic modeling, which simultaneously operates on the input audio
signal to compute certain thresholds using known rules from the psychoacous-
tic behavior of the human auditory system.

3. Quantization and coding, which, in conjunction with the psychoacoustic
model, works on the output of the time-frequency mapper so as to maintain
the noise resulting from quantization process at an inaudible level.

4. Frame-packing, which is used to format the quantized audio samples into a
decodable bit stream.

The psychoacoustic model builds on a perceptual phenomenon known as auditory

masking. Specifically, the human ear does not perceive quantization noise in a

given frequency band if the average noise power lies below the masking threshold

(ie., the threshold of just noticeable distortion). For a given frequency band of

interest, the masking threshold varies with frequency across that band. The min-

10 & BACKGROUND AND PREVIEW

imum masking threshold is the one that is employed in the psychoacoustic model
on a band-by-band basis. For example, the net result of using the MPEG-1 stan-
dard on the two audio channels of a stereo program is that each digitized audio
signal, coming in at the rate of 768 kilobits per second (kb/s), is compressed to a
rate as low as 16 kb/s. (The incoming data rate of 768 kb/s corresponds to a
sampling rate of 48 kHz, with each sample being represented by a 16-bit code
word.) Thus the MPEG-1/audio coding standard is suitable for the storage of
audio signals in inexpensive media or their transmission over channels with limited
bandwidth, while at the same time maintaining perceptual quality.

i Communication Networks®

A communication network (or simply network), illustrated in Figure 4, consists of an
interconnection of a number of routers made up of intelligent processors (e.g., microproc-
essors). The primary purpose of these processors is to route data through the network,
hence the name. Each router has one or more hosts attached to it; hosts are devices that
communicate with one another. The network is designed to serve as a shared resource for
moving data exchanged between hosts in an efficient manner and to provide a framework
to support new applications and services.

The telephone network is an example of a communication network in which circuit
switching is used to provide a dedicated communication path, or circuit, between two
hosts. The circuit consists of a connected sequence of links from source to destination. For
example, the links may consist of time slots for which a common channel is available for
access by a multitude of users. The circuit, once in place, remains uninterrupted for the
duration of transmission. Circuit switching is usually controlled by a centralized hierar-
chical control mechanism with knowledge of the network’s organization, To establish a
circuit-switched connection, an available path through the network is seized and then
dedicated to the exclusive use of the two hosts wishing to communicate. In particular, a
call-request signal must propagate all the way to the destination and be acknowledged
before transmission can begin. Then, the network is effectively transparent to the users.
This means that during the connection time, the bandwidth and resources allocated to the
circuit are essentially “owned” by the two hosts until the circuit is disconnected. The circuit

 

 

   

Boundary
of subnet

 

Hosts

FIGURE 4 Communication network.

~ Communication Networks il

thus represents an efficient use of resources only to the extent that the allocated bandwidth
is properly used. Although the telephone network is used to transmit data, voice constitutes
the bulk of the network’s traffic. Indeed, circuit switching is well suited to the transmission
of voice signals, since voice gives rise to a stream traffic and voice conversations tend to
be of long duration (about 2 minutes on the average) compared to the time required for
setting up the circuit (about 0.1 to 0.5 seconds). Moreover, in most voice conversations,
there is information flow for a relatively large percentage of the connection time, which
makes circuit switching all the more suitable for voice conversations.

In circuit switching, a communication link is shared between the different sessions
using that link on a fixed allocation basis. In packet switching, on the other hand, the
sharing is done on a demand basis, so it has an advantage over circuit switching in that
when a link has traffic to send, the link may be more fully utilized.

The network principle of packet switching is “store and forward.” Specifically, in a
packet-switched network, any message larger.than a specified size is subdivided prior to
transmission into segments not exceeding the specified size. The segments are commonly
referred to as packets. The original message is reassembled at the destination on a packet-
by-packet basis. The network may be viewed as a distributed pool of network resources
(ie., channel bandwidth, buffers, and switching processors) whose capacity is shared dy-
namically by a community of competing hosts wishing to communicate. In contrast, in a
citcuit-switched network, resources are dedicated to a pair of hosts for the entire period
they are in session. Accordingly, packet switching is far better suited to a computer-
communication environment in which bursts of data are exchanged between hosts on an
occasional basis. The use of packet switching, however, requires that careful control be
exercised on user demands; otherwise, the network may be seriously abused.

The design of a data network (i.e., a network in which the hosts are all made up of
computers and terminals) may proceed in an orderly way by looking at the network in
terms of a layered architecture, regarded as a hierarchy of nested layers. Layer refers to a
process or device inside a computer system, designed to perform a specific function. Nat-
urally, the designers of a layer will be intimately familiar with its internal details and
operation. At the system level, however, a user views the layer merely as a “black box”
that is described in terms of inputs, outputs, and the functional relation between outputs
and inputs. In a layered architecture, each layer regards the next lower layer as one or
more black boxes with some given functional specification to be used by the given higher
layer. Thus, the highly complex communication problem in data networks is resolved as
a manageable set of well-defined interlocking functions. It is this line of reasoning that has
led to the development of the open systems interconnection (OSI)’ reference model by a
subcommittee of the International Organization for Standardization. The term open refers
to the ability of any two systems conforming to the reference model and its associated
standards to interconnect.

In the OSI reference model, the communications and related-connection functions
are organized as a series of layers, or levels, with well-defined interfaces, and with each
layer built on its predecessor. In particular, each layer performs a related subset of primitive
functions, and it relies on the next lower layer to perform additional primitive functions.
Moreover, each layer offers certain services to the next higher layer and shields the latter
from the implementation details of those services. Between each pair of layers, there is an
interface. It is the interface that defines the services offered by the lower layer to the upper
layer.

The OSI model is composed of seven layers, as illustrated in Figure 5; this figure also
includes a description of the functions of the individual layers of the model. Layer & on
system A, say, communicates with layer k on some other system B in accordance with a

"JosuOD YuY vIDP JOF spuLys eB oup Jo s[ppru aya uy QTC wWAUOIOE oy] {[apow [SQ ¢ TUNA

“J@UUBYD BY} SSadde 0} sjUaWaJInbas jeinpedoid pue
*yeUOIOUNY “2d JJOaIa ‘jeo|UeYIeW ay) UYIM S|zap JAE] Si}
tyouUeyo jeaysXyd @ JeA0 eJep Jo S}iq Med Jo UOIssWSued]

 

“jauueys au ssou9e
UO}JEWOJU! Jo 4aJSUAd] 21geI[a1 BY} JO} }O4]U0D 101g

“asnpaooud suijno2 ayy Aq punoy Yul}
Uo!JeI/UNWLWOD B J9A0 BoUBWOLad poos aajuesend 0} pausisep
[O.jU09 MOj} PUP YOAJ@U BL} YBnoIyy sjayoed Jo Buynoy

‘siasN Usemjeq pesueyoxe sadessauw
8U} JO fosjUOS (UO!eUNSap-0}-B04NOs ‘‘3'1) puds-0}-PUq

“Way] UaaMmjeq angdojeip ayy yo
quaweseuew AjJapio ay} pue ‘ssasn Buijeladooo om] UdaMteq
UOJJEHUNWWOD JO} BAN JONAS |01]U09 By} JO UOISIADLY

“AyaNDes aptaoid 0] uoldAuaua st
Uo}]eULOjsued] BJep Jo aldwere ue ‘axe; uolEaIjdde au Aq
pajogjas seaies apiaosd 0} B]ep yndu} 94} Jo UO|yeuoJsued |,

 

“S19SN-PUA JO} JUBWUOJIAUS |$Q Bl} 0} S599" Jo UO|SIADLY

uoiaun

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

q Wwaysks @pou Jeuqns yw waysks
ul] Jeoskyd mur yeatsAud
t
[eoiskud jeaistyg | eoiskud JRIISAU
[
Jouju09 _ ma na oe jOuU09
uly eed jooozosd joso}oid sun eed
Z 40ke] \ f Z Jake] |
yIOMJAN) sats T Toe (ome [-—-—-——-—----—- IOMIaN
Jooojoid joaoyoid
© Jake] € Jake] |
yodsuedL —- ~ yodsuel|
josojoid p Jake]
ugIssas --- -- —-- uOGIssaS

 

uolje]Uesald

os0}0id g sake]

 

 

 

uoneaddy

 

on0}01d 9 1842]

 

 

 

 

4 desn-puq

o20}0Id / 42AE]

|

 

 

Uole]UaSa1q

 

|

 

Lolealddy

 

 

 

X 4asn-pug

Jake]

12

Commeunication Networks 13

set of rules and conventions, collectively constituting the layer k protocol, where k = 1,
2,...7. (The term protocol has been borrowed from common usage, describing conven-
tional social behavior between human beings.) The entities that comprise the correspond-
ing layers on different systems are referred.to as peer processes. In other words, commu-
nication is achieved by having the peer processes in two different systems communicate
via a protocol, with the protocol itself being defined by a set of rules of procedure. Physical
communication between peer processes exists only at layer 1. On the other hand, layers 2
through 7 are in virtual communication with their distant peers. However, each of these
six layers can exchange data and control information with its neighboring layers (below
and above} through layer-to-layer interfaces. In Figure 5, physical communication is shown
by solid lines and virtual communication by dashed lines,
Our primary interest in this book is in the physical layer of the OSI model.

@ INTERNET

Any discussion of computer networks naturally leads to the Internet. In the Internet par-
adigm, the underlying network technology is decoupled from the applications at hand by
adopting an abstract definition of network service. In more specific terms, we may say the
following:

» The applications are carried out independently of the technology employed to con-
struct the network.

> By the same token, the network technology is capable of evolving without affecting
the applications.

The Internet architecture, depicted in Figure 6, has three functional blocks: hosts,
subnets, and routers. The hosts constitute nodes of the network, where data originate or
where they are delivered. The routers constitute intermediate nodes that are used to cross
subnet boundaries. Within a subnet, all the hosts belonging to that subnet exchange data
directly; see, for example, subnets 1 and 3 in Figure 6.

Like other computer networks, the Internet has a layered set of protocols. In partic-
ular, the exchange of data between the hosts and routers is accomplished by means of the
Internet protocol (IP), as illustrated in Figure 7. The IP is a universal protocol that resides
in the network layer (i.e., layer 3 of the OSI reference model}. It is simple, defining an
addressing plan with a built-in capability to transport data in the form of packets from
node to node. In crossing a subnetwork boundary, the routers make the decisions as to
how the packets addressed for a specified destination should be routed. This is done on
the basis of routing tables that are developed through the use of custom protocols for

 

 

Hosts Hosts

Ficure 6 An interconnected network of subnets.

14

& BACKGROUND AND PREVIEW

 

 

 

AP AP AP AP
TCP/UDP TCP/UDP TCP/UDP TCP/UDP
IP IP IP iP

 

 

 

 

 

 

 

 

 

 

 

 

AP: Application protocol UDP: User datagram protocol
TCP: Transmission control protocol _‘IP: Internet protocol

Ficure 7 Illustrating the network architecture of the Internet.

exchanging pertinent information with other routers. The net result of using the layered
set of protocols is the delivery of best effort service. That is, the Internet offers to deliver
each packet of data, but there are no guarantees on the transit time experienced in delivery
or even whether the packets will be delivered to the intended recipient.

& BroaDBAND NETWORKS

With the ever-increasing demand for new services (e.g., video on demand, multimedia
communications) and the availability of key enabling technologies (e.g., optical fibers,
digital switches), the telephone network is evolving into an all-purpose broadband network
known as the broadband integrated services digital network (B-ISDN). The underlying
technology that makes B-ISDN possible is a user-network interface protocol called the
asynchronous transfer mode (ATM). ATM is a high-bandwidth, low-delay, packet-like
technique used for switching and multiplexing; it is independent of the physical means of
transport. The low-delay feature of the technique is needed to support real-time services
such as voice. The high-bandwidth feature is required to handle video on demand. Simply
put, ATM is both a technology that is hidden from the users and a connection-oriented
service that is visible to the users.

As the name implies, ATM is not synchronous (i.e., tied to a master clock). It allows
for the transport of digital information in the form of small, fixed-size packets called cells.
The key feature of ATM to note here is that the connection-oriented service preserves call
sequencing, which means that no reassembly of cells is needed prior to presenting the traffic
stream to the destination host. The deployment of a cell-switching technology in B-ISDN
is a gigantic break with the traditional use of circuit switching in the telephone network.

The primary purpose of ATM is to allocate network resources (i.e., bandwidth, buf-
fers, and processing horsepower) efficiently so as to guarantee the expected quality of
service (QoS) for each connection. QoS is measured in terms of three parameters:

® Cell loss ratio, defined as the ratio of the number of cells lost in transport across the
network to the total number of cells pumped into the network.

» Cell delay, defined as the time taken for a cell of a particular connection to transit
across the network.

» Cell delay variation, defined as the dispersion or jitter about the mean cell delay.

Quality of service offered in B-ISDN is to be contrasted with best effort service offered by
the Internet.

Communication Channels 15

Taste 1 Hierarchy
of SONET data rates

 

 

Level* Data Rate (Mb/s)
OC-1 51.84
OC-3 155.52
Oc-9 466.56
OC-12 622.08
OC-18 933.12
OC-24 1,244.16
OC-36 1,866.24
OC-+48 2,488.32

 

OC stands for optical carrier level.

After their generation, the ATM cells are structured for transport across the network.
The cells in B-ISDN are placed on an optical transmission system called the synchronous
optical network (SONET);® optical fibers are discussed in the next section. SONET uses
time-division multiplexing, whereby the entire bandwidth of an optical fiber is devoted to
different incoming data streams on a time-shared basis, hence the need for a synchronous
operation. SONET is controlled by a master clock with an accuracy of about 1 part in
10°. Thus bits of data are sent on a SONET line at extremely precise intervals, controlled
by the master clock. Nevertheless, SONET permits the irregular time arrivals of ATM
cells. :

The basic SONET frame is a block of 810 bytes put out every 125 ys for an overall
data rate of $1.84 Mb/s. Having 8000 frames every second exactly matches the sampling
rate of 8 kHz, which is the standard sampling rate for the digital transmission of voice
signals across the telephone network. The basic data rates of 51.84 Mb/s are synchronously
byte-interleaved to generate a hierarchy of data rates, as summarized in Table 1.

| Communication Channels

 

The transmission of information across a communication network is accomplished in the
physical layer by means of a communication channel. Depending on the mode of trans-
mission used, we may distinguish two basic groups of communication channels: channels
based on guided propagation and those based on free propagation. The first group includes
telephone channels, coaxial cables, and optical fibers. The second group includes wireless
broadcast channels, mobile radio channels, and satellite channels. These six channels are
described in what follows.

(i) As mentioned earlier, a typical telephone network uses circuit switching to establish
an end-to-end communication link on a temporary basis, The primary purpose of
the network is to ensure that the telephone transmission between a speaker at one
end of the link and a listener at the other end is an acceptable substitute for
face-to-face conversation. In this form of communication, the message source is the
sound produced by the speaker’s voice, and the ultimate destination is the listener’s
ear. The telephone channel, however, supports only the transmission of electrical
signals. Accordingly, appropriate transducers are used at the transmitting and re-
ceiving ends of the system. Specifically, a microphone is placed near the speaker’s

16

Insertion loss (dB)

20

e
a

e
Oo

 

% BACKGROUND AND PREVIEW

1

mouth to convert sound waves into an electrical signal, and the electrical signal is
converted back into acoustic form by means of a moving-coil receiver placed near
the listener’s ear. Present-day designs of these two transducers have been perfected
so as to respond well to frequencies ranging from 20 to 8000 Hz; moreover, a pair
of them can be compactly packaged inside a single telephone set that is easy to speak
into or listen from. The telephone channel is a bandwidth-limited channel. The re-
striction on bandwidth arises from the requirement of sharing the channel among a
multitude of users at any one time. A practical solution to the telephonic commu-
nication problem must therefore minimize the channel bandwidth requirement, sub-
ject to a satisfactory transmission of human voice. To meet this requirement, the
transducers and channel specifications must conform to standards based on subjec-
tive tests that are performed on the intelligibility, or articulation, of telephone signals
by representative. male and female speakers. A speech signal (male or female) is es-
sentially limited to a band from 300 to 3100 Hz in the sense that frequencies outside
this band do not contribute much to articulation efficiency. This frequency band may
therefore be viewed as a rough guideline for the passband of a telephone channel
that provides a satisfactory service, as illustrated in Figure 8 for a typical toll con-
nection. Figure 8a shows the insertion loss of the channel plotted versus frequency;
insertion loss (in dB) is defined as 10 logio(Po/P;,), where P, is the power delivered
to a load from a source via the channel and Py, is the power delivered to the same
load when it is connected directly to the source. Figure 8b shows the corresponding
plot of the envelope (group) delay (in milliseconds) versus frequency; envelope delay
is defined as the negative of the derivative of the phase response with respect to the
angular frequency w = 27f. The plots of Figure 8 clearly illustrate the dispersive
nature of the telephone channel.

The telephone channel is built using twisted pairs for signal transmission. A
twisted pair consists of two solid copper conductors, each of which is encased in a
polyvinylchloride (PVC) sheath. Typically, each pair has a twist rate of 2 to 12 twists
per foot, and a characteristic impedance of 90 to 110 ohms, Twisted pairs are usually
made up into cables, with each cable consisting of many pairs in close proximity to

 

 

Envelope delay (ms)

 

 

 

ell
2 3 4 5
Frequency {kHz} Frequency (kHz)

{a) (h)

Ficune 8 Characteristics of typical telephone connection: (a) Insertion loss. (b) Envelope delay.
(Adapted from Bellamy, 1991.)

(iv)

Communication Channels 17

each other. Twisted pairs are naturally susceptible to electromagnetic interference
(EMI), the effects of which are mitigated through twisting the wires.
A coaxial cable consists of an inner conductor and an outer conductor, separated by
a dielectric insulating material. The inner conductor is made of a copper wire encased
inside the dielectric material. As for the outer conductor, it is made of copper, tinned
copper, or copper-coated steel. Typically, a coaxial cable has a characteristic imped-
ance of 50 or 75 ohms. Compared to a twisted-pair cable, a coaxial cable offers a
greater degree of immunity to EMI. Moreover, because of their much higher band-
width, coaxial cables can support the transmission of digital data at much higher bit
rates than twisted pairs. Rates up to 20 Mb/s are feasible using coaxial cables, with
10 Mb/s being the standard.

Whereas the use of a twisted pair has been confined mainly to point-to-point
service, a coaxial cable can operate as a multiple-access medium by using high-
impedance taps. A common application of coaxial cables is as the transmission me-

dium for local area networks in an office environment.

Another common application of coaxial cables is in cable-television systems,

also known as community-antenna television (CATV) systems. In this application
coaxial cables are used to distribute television, audio, and data signals from the head
end to the subscribers. The bead end is the central originating unit of the CATV
system, where all signals are carried and processed.
An optical fiber is a dielectric wave guide that transports light signals from one place
to another just as a twisted-wire pair or a coaxial cable transports electrical signals.
It consists of a central core within which the propagating electromagnetic field is
confined and which is surrounded by a cladding layer, which is itself surrounded by
a thin protective jacket.° The core and cladding are both made of pure silica glass,
whereas the jacket is made of plastic. Optical fibers have unique characteristics that
make them highly attractive as a transmission medium. In particular, they offer the
following unique characteristics:

» Enormous potential bandwidth, resulting from the use of optical carrier frequen-
cies around 2 * 10'* Hz; with such a high carrier frequency and a bandwidth
roughly equal to 10 percent of the carrier frequency, the theoretical bandwidth of
a lightwave system is around 2 X 10‘? Hz, which is very large indeed.

® Low transmission losses, as low as 0.1-dB/km.

» Immunity to electromagnetic interference, which is an inherent characteristic of
an optical fiber viewed as a dielectric waveguide.

& Small size and weight, characterized by a diameter no greater than that of a human

hair.

Ruggedness and flexibility, exemplified by very high tensile strengths and the pos-

sibility of being bent or twisted without damage.

¥"

Last, but by no means least, optical fibers offer the potential for low-cost line com-
munications since they are fabricated from sand, which, unlike the copper used in
metallic conductors, is not a scarce resource. The unique properties of optical fibers
have fuelled phenomenal advances in lightwave systems technology, which have, in
turn, revolutionized long-distance communications and continue to do so.

Wireless broadcast channels support the transmission of radio and television signals.
The information-bearing signal, representing speech, music, or pictures, is modulated
onto a carrier frequency that identifies the transmitting station; modulation is de-
scribed in the next section. The transmission originates from an antenna that acts as
the transition or matching unit between the source of the modulated signal and

18

& BACKGROUND AND PREVIEW

(vi)

electromagnetic waves in free space. The objective in designing the antenna is to
excite the waves in the required direction or directions, as efficiently as possible.
Typically, the transmitting antenna is mounted on a tower to provide an unob-
structed view of the surrounding area, as far afield as possible. By virtue of the
phenomenon of diffraction, which is a fundamental property of wave motion, radio
waves are bent around the earth’s surface. Propagation beyond the line of sight is
thereby made possible, albeit with somewhat greater loss than is incurred in free
space,

At the receiving end, an antenna is used to pick up-the radiated waves, estab-
lishing a communication link to the transmitter. Most radio receivers are of the
superheterodyne type. This technique consists of down-converting the received signal
to some convenient frequency band, called the intermediate frequency (IF) band, and
then recovering the original information-bearing signal by means of an appropriate
detector.

A mobile radio channel extends the capability of the public telecommunications net-
work by introducing mobility into the network by virtue of its ability to broadcast.
The term mobile radio is usually meant to encompass terrestrial situations where a
radio transmitter or receiver is capable of being moved, regardless of whether it
actually moves or not. The major propagation effects encountered in the use of a
mobile radio in built-up areas are due to the fact that the antenna of the mobile unit
may lie well below the surrounding buildings. Simply put, there is no “line-of-sight”
path for communication; rather, radio propagation takes place mainly by way of
scattering from the surfaces of the surrounding buildings and by diffraction over and/
or around them. The end result is that energy reaches the receiving antenna via more
than one path. In a mobile radio environment, we thus speak of a multipath phe-
nomenon in that the various incoming radio waves reach their destination from
different directions and with different time delays. Indeed, there may be a multitude
of propagation paths with different electrical lengths, and their contributions to the
received signal could combine in a variety of ways. Consequently, the received signal
strength varies with location in a very complicated fashion, and so a mobile radio
channel may be viewed as a linear time-varying channel that is statistical in nature.

Finally, a satellite channel adds another invaluable dimension to the public telecom-
munications network by providing broad-area coverage in both a continental and
an intercontinental sense. Moreover, access to remote areas not covered by conven-
tional cable or fiber communications is also a distinct feature of satellites. In almost
all satellite communication systems, the satellites are placed in geostationary orbit.
For the orbit to be geostationary, it has to satisfy two requirements. First, the orbit
is geosynchronous, which requires the satellite to be at an altitude of 22,300 miles;
a geosynchronous satellite orbits the Earth in 24 hours (i.e., the satellite is synchro-
nous with the Earth’s rotation). Second, the satellite is placed in orbit directly above
the equator on an eastward heading (1.c., it has zero inclination). Viewed from Earth,
a satellite in geostationary orbit appears to be stationary in the sky. Consequently,
an Earth station does not have to track the satellite; rather, it merely has to point its
antenna along a fixed direction, pointing toward the satellite. By so doing, the system
design is simplified considerably. Communications satellites in geostationary orbit
offer the following unique system capabilities:

» Broad-area coverage.
® Reliable transmission links.
® Wide transmission bandwidths.

Modulation Process 19

In terms of services, satellites can provide fixed point-to-point links extending over
long distances and into remote areas, Communication to mobile platforms (e.g., air-
craft, ships), or broadcast capabilities. Indeed, communications satellites play a key
role in the notion of the whole world being viewed as a “global village.” In a typical
satellite communication system, a message signal is transmitted from an Earth station
via an uplink to a satellite, amplified in a transponder (i.e., electronic circuitry) on
board the satellite, and then retransmitted from the satellite via a downlink to another
Earth station. With the satellite positioned in geostationary orbit, it is always visible
to all the Earth stations located inside the satellite antenna’s coverage zones on the
Earth’s surface. In effect, the satellite acts as a powerful repeater in the sky. The most
popular frequency band for satellite communications is 6 GHz for the uplink and
4 GHz for the downlink. The use of this frequency band offers the following
attributes:
» Relatively inexpensive microwave equipment.
» Low attenuation due to rainfall; rainfall is a primary atmospheric cause of signal
loss.
® Insignificant sky background noise; the sky background noise (due to random
noise emissions from galactic, solar, and terrestrial sources) reaches its lowest level
between 1 and 10 GHz.
In the 6/4-GHz band, a typical satellite is assigned a 500 MHz bandwidth that is
divided among 12 transponders on board the satellite. Each transponder, using ap-
proximately 36 MHz of the satellite bandwidth, corresponds to a specific radio chan-
nel. A single transponder can carry at least one color television signal, 1200 voice
circuits, or digital data at a rate of 50 Mb/s.

To summarize, a communication channel is central to the operation of a com-
munication system. Its properties determine both the information-carrying capacity
of the system and the quality of service offered by the system. We may classify com-
munication channels in different ways:

» A channel may be linear or nonlinear; a wireless radio channel is linear, whereas
a satellite channel is usually (but not always) nonlinear.

® A channel may be time invariant or time varying; an optical fiber is time invariant,
whereas a mobile radio channel is typically time varying.

® A channel may be bandwidth limited or power limited (i.e., limited in the available
transmitted power); a telephone channel is bandwidth limited, whereas an optical
fiber link and a satellite channel are both power limited.

Now that we have some understanding of sources of information and com-
munication channels, we may return to the block diagram of a communication sys-
tem shown in Figure 1.

| Modulation Process

 

The purpose of a communication system is to deliver a message signal from an information
source in recognizable form to a user destination, with the source and the user being
physically separated from each other. To do this, the transmitter modifies the message
signal into a form suitable for transmission over the channel. This modification is achieved
by means of a process known as modulation, which involves varying some parameter of
a carrier. wave in accordance with the message signal. The receiver re-creates the original

20

& BACKGROUND AND PREVEEW

message signal from a degraded version of the transmitted signal after propagation through
the channel. This re-creation is accomplished by using a process known as demodulation,
which is the reverse of the modulation process used in the transmitter. However, owing
to the unavoidable presence of noise and distortion in the received signal, we find that the
receiver cannot re-create the original message signal exactly. The resulting degradation in
overall system performance is influenced by the type of modulation scheme used. Specifi-
cally, we find that some modulation schemes are less sensitive to the effects of noise and
distortion than others.

We may classify the modulation process into continuous-wave modulation and pulse
modulation. In continuous-wave (CW) modulation, a sinusoidal wave is used as the car-
tier. When the amplitude of the carrier is varied in accordance with the message signal,
we have amplitude modulation (AM), and when the angle of the carrier is varied, we have
angle modulation. The latter form of CW modulation may be further subdivided into
frequency modulation (FM) and phase modulation (PM), in which the instantaneous fre-
quency and phase of the carrier, respectively, are varied in accordance with the message
signal.

In pulse modulation, on the other hand, the carrier consists of a periodic sequence
of rectangular pulses. Pulse modulation can itself be of an analog or digital type. In analog
pulse modulation, the amplitude, duration, or position of a pulse is varied in accordance
with sample values of the message signal. In such a case, we speak of pulse-amplitude
modulation (PAM), pulse-duration modulation (PDM), and pulse-position modulation
(PPM). -

The standard digital form of pulse modulation is known as pulse-code modulation
(PCM) that has no CW counterpart. PCM starts out essentially as PAM, but with an
important modification: The amplitude of each modulated pulse (i.e., sample of the original
message signal) is quantized or rounded off to the nearest value in a prescribed set of
discrete amplitude levels and then coded into a corresponding sequence of binary symbols.
The binary symbols 0 and 1 are themselves represented by pulse signals that are suitably
shaped for transmission over the channel. In any event, as a result of the quantization
process, some information is always lost and the original message signal cannot therefore
be reconstructed exactly. However, provided that the number of quantizing (discrete am-
plitude) levels is large enough, the distortion produced by the quantization process is not
discernible to the human ear in the case of a speech signal or the human eye in the case of
a two-dimensional image. Among all the different modulation schemes, pulse-code mod-
ulation has emerged as the preferred method of modulation for the transmission of analog
message signals for the following reasons:

» Robustness in noisy environments by regenerating the transmitted signal at regular
intervals.

» Flexible operation.

® Integration of diverse sources of information into a common format.

» Security of information in its transmission from source to destination.

In introducing the idea of modulation, we stressed its importance as a process that
ensures the transmission of a message signal over a prescribed channel. There is another
important benefit, namely, multiplexing, that results from the use of modulation. Multi-
plexing is the process of combining several message signals for their simultaneous trans-
mission over the same channel. Three commonly used methods of multiplexing are as
follows:

» Frequency-division multiplexing (FDM), in which CW modulation is used to trans-
late each message signal to reside in a specific frequency slot inside the passband of

Analog and Digital Types of Communication 21

the channel by assigning it a distinct carrier frequency; at the receiver, a bank of
filters is used to separate the different modulated signals and prepare them individ-
ually for demodulation.

> Time-division multiplexing (TDM), in which pulse modulation is used to position
samples of the different message signals in nonoverlapping time slots.

» Code-division multiplexing (CDM), in which each message ® signal is identified by a
distinctive code.

In FDM the message signals overlap with each other at the channel input; hence the system
may suffer from crosstalk (i.e., interaction between message signals) if the channel is non-
linear. In TDM the message signals use the full passband of the channel, but on a time-
shared basis. In CDM the message signals are permitted to overlap in both time and
frequency across the channel.

Mention should also be made of wavelength-division multiplexing (WDM), which
is special to optical fibers. In WDM, wavelength is used as a new degree of freedom by
concurrently operating distinct portions of the wavelength spectrum (i.e., distinct colors)
that are accessible within the optical fiber. However, recognizing the reciprocal relation-
ship that exists between the wavelength and frequency of an electromagnetic wave, we
may say that WDM is a form of FDM.

i Analog and Digital Types of Communication

Typically, in the design of a communication system the information source, communica-
tion channel, and information sink (end user) are all specified. The challenge is to design
the transmitter and the receiver with the following guidelines in mind:

 

 

 

® Encode/modulate the message signal generated by the source of information, transmit
it over the channel, and produce an “estimate” of it at the receiver output that satisfies
the requirements of the end user.

®» Do all of this at an affordable cost.

We have the option of using a digital or analog communication system.

Consider first the case of a digital communication system represented by the block
diagram of Figure 9, the rationale for which is rooted in information theory. The functional
blocks of the transmitter and the receiver, starting from the far end of the channel, are
paired as follows:

» Source encoder-decoder.
» Channel encoder-decoder.
®» Modulator-demodulator.

The source encoder removes redundant information from the message signal and is re-
sponsible for the efficient use of the channel. The resulting sequence of symbols is called
the source code word. The data stream is processed next by the channel encoder, which
produces a new sequence of symbols called the channel code word. The channel code word
is longer than the source code word by virtue of the co#trolled redundancy built into its
construction. Finally, the modulator represents each symbol of the channel code word by
a corresponding analog symbol, appropriately selected from a finite set of possible analog
symbols. The sequence of analog symbols produced by the modulator is called a waveform,
which is suitable for transmission over the channel. At the receiver, the channel output
(received signal) is processed in reverse order to that in the transmitter, thereby recon-

22

& BACKGROUND AND PREVIEW

 

   
  

    
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  
  

 

  
  
 

 

 

 

 

 

 

 

 

 

 

Estimate of
Source of Message signal message signa User of
information information
ee ee 1
Source Source 1
encoder ; } decoder
i Koa |
Source | Estimate of
code word Source
code word
Transmitter Channel Channel Receiver
encoder decoder
i
i Estimate of
| Channel channel
' code word code word
Modulator Demodulator
| 1
I
| en J L--____f _____ J
Received
Waveform signal
| Channel

 

  

 

 

 

Ficure 9 Block diagram of digital communication system.

structing a recognizable version of the original message signal. The reconstructed message
signal is finally delivered to the user of information at the destination. From this description
it is apparent that the design of a digital communication system is rather complex in
conceptual terms but easy to build. Moreover, the system is robust, offering greater tol-
erance of physical effects (e.g., temperature variations, aging, mechanical vibrations) than
its analog counterpart.

In contrast, the design of an analog communication system is simple in conceptual
terms but difficult to build because of stringent requirements on linearity and system ad-
justment. For example, voice communication requires nonlinear distortion products at
least 40 dB below the wanted message signal. In signal-processing terms, the transmitter
consists of a modulator and the receiver consists of a demodulator, the details of which
are determined by the type of CW modulation used.

The conceptual simplicity of analog communications is due to the fact that analog
modulation techniques, exemplified by their wide use in radio and television, make rela-
tively superficial changes to the message signal in order to prepare it for transmission over
the channel. More specifically, there is no significant effort made by the system designer
to tailor the waveform of the transmitted signal to suit the channel at any deeper level.
On the other hand, digital communication theory endeavors to find a finite set of wave-
forms that are closely matched to the characteristics of the channel and which are therefore
more tolerant of channel impairments. In so doing, reliable communication is established
over the channel. In the selection of good waveforms for digital communication over a
noisy channel, the design is influenced solely by the channel characteristics. However, once
the appropriate set of waveforms for transmission over the channel has been selected, the
source information can be encoded into the channel waveforms, and the efficient trans-

Shannon's Information Capacity Theerem 23

mission of information from the source to the user is thereby ensured. In summary, the
use of digital communications provides the capability for information transmission that is
both efficient and reliable.

From this discussion, it is apparent that the use of digital communications requires
a considerable amount of electronic circuitry, but nowadays electronics are inexpensive,
due to the ever-increasing availability of very-large-scale integrated (VLSI) circuits in the
form of silicon chips. Thus although cost considerations used to be a factor in selecting
analog communications over digital communications in the past, that is no longer the case.

Despite the trend toward the ever-increasing use of digital communications, a strong
case can be made for the study of analog communications for two important reasons:

1. As long as we hear and see analog communications around us via radio and televi-
sion, we need to understand how these communications systems work. Moreover,
the study of analog modulation motivates other digital modulation schemes.

2. Analog devices and circuits have a natural affinity for operating at very high speeds
and they consume very little power compared to their digital counterparts. Accord-
ingly, the implementation of very high-speed or very low-power communication sys-
tems dictates the use of an analog approach,

 

 

| Shannon's Information Capacity Theorem

The goal of a communication system designer is to configure a system that transports a
message signal from a source of interest across a noisy channel to a user at the other end
of the channel with the following objective:

The message signal is delivered to the user both efficiently and reliably, subject to
certain design constraints: allowable transmit power, available channel bandwidth,
and affordable cost of building the system.

In the case of a digital communication system, reliability is commonly expressed in terms
of bit error rate (BER) or probability of bit error measured at the receiver output. Clearly,
the smaller the BER, the more reliable the communication system is. A question that comes
to mind in this context is whether it is possible to design a communication system that
operates with zero BER even through the channel is noisy. In an ideal setting, the answer
to this question is an emphatic yes. The answer is embodied in one of Shannon’s celebrated
theorems,’° which is called the information capacity theorem.

Let B denote the channel bandwidth, and let SNR denote the received signal-to-noise
ratio. The information capacity theorem states that ideally these two parameters are related
as

C = B log,(1 + SNR) b/s (1)

where C is the information capacity of the channel. The information capacity is defined
as the maximum rate at which information can be transmitted across the channel without
error; it is measured in bits per second (b/s). For a prescribed channel bandwidth B and
received SNR, the information capacity theorem tells us that a message signal can be
transmitted through the system without error even when the channel is noisy, provided
that the actual signaling rate R in bits per second, at which data are transmitted through
the channel, is less than the information capacity C.

24

BACKGROUND AND PREVIEW

Unfortunately, Shannon’s information capacity theorem does not tell us how to de-
sign the system. Nevertheless, from a design point of view, the theorem is very valuable
for the following reasons:

1. The information capacity theorem provides a bound on what rate of data transmis-
sion is theoretically attainable for prescribed values of channel bandwidth B and
received SNR, On this basis, we may use the ratio

aC
as a measure of the efficiency of the digital communication system under study. The
closer 7 is to unity, the more efficient the system is.

2. Equation (1) provides a basis for the trade-off between channel bandwidth B and
received SNR, In particular, for a prescribed signaling rate R, we may reduce the
required SNR by increasing the channel bandwidth B, hence the motivation for using
a wideband modulated scheme (e.g., pulse-code modulation) for improved noise
performance.

3. Equation (1) provides an idealized framework for comparing the noise performance
of one modulation scheme against another.

i A Digital Communication Problem

 

When we speak of a digital communication system having a low bit error rate, say, the
implication is that only a small fraction in a long stream of binary symbols is decoded in
error by the receiver. The issue of the receiver determining whether a binary symbol sent
over a noisy channel is decoded in error or not is of fundamental importance to the design
of digital communication systems. It is therefore appropriate briefly to discuss this basic
issue so as to motivate the study of communication systems.

Suppose we have a random binary signal, 7(t), consisting of symbols 1 and 0, that
are equally likely. Symbol 1 is represented by a constant level +1, and symbol 0 is rep-
resented by a constant level —1, each of which lasts for a duration T. Such a signal may
represent the output of a digital computer or the digitized version of a speech signal. To
facilitate the transmission of this signal over a communication channel, we employ a simple
modulation scheme known as phase-shift keying. Specifically, the information bearing
signal 1(£) is multiplied by a sinusoidal carrier wave A, cos(27f,t), where A, is the carrier
amplitude, f. is the carrier frequency, and t is time. Figure 10a shows.a block diagram of
the transmitter, the output of which is defined by

s(t) = {* cos(27f,t) for symbol 1

2
—A, cos(27f.t) for symbol 0 (2)

where 0 =¢ = T. The carrier frequency f, is a multiple of 1/T.
The channel is assumed to be distortionless but noisy, as depicted in Figure 10b. The
received signal x(t) is thus defined by

x(t) = s(t) + w(t) (3)

where w(t)-is the additive channel noise.
The receiver consists of a correlator followed by a decision-making device, as de-
picted in Figure 10c. The correlator multiplies the received signal x(t) by a locally generated

A Digital Communication Problem 25

 

Message Transmitted Transmitted + Channel! output
signal m(s) signal s() signal s(2 2 {received signal) 0
~ +
Carrier wave Noise
A, cos (21rf,2) w(t)
{a (b)
Correiator
Received Decision [> Say ify > 0
signal x0) id

 

 

device }—S» Otherwise, say 0

 

 

Local carrier
cos (2rf,t) Threshold = 0

{c)

Ficure 10 Elements of a digital communication system. (a) Block diagram of transmitter.
(b) Block diagram of channel. (c) Block diagram of receiver.

carrier cos(27f.t) and then integrates the product over the symbol interval 0 = ¢ = T,
producing the output

T
Yr = I x(t) cos(2af.t) dt (4)

QO

Substituting Equations (2) and (3) into (4) and invoking the assumption that the carrier
frequency f, is a multiple of 1/T, we obtain (after the simplification of terms)

+ +wr for symbol 1
v= A (5)
“3 +w, for symbol 0

where wy is the contribution of the correlator output due to the channel noise w(t). To
reconstruct the original binary signal s(t), the correlator output y; is compared against a
threshold of zero volts by the decision-making device, the operation of which is based on
the following rule:

If the correlator output yr is greater than zero, the receiver outputs symbol 1;
otherwise, it outputs symbol 0.

With this background, we may now discuss/raise some basic issues. First, from Fou-
rier analysis we find that the time-bandwidth product of a pulse signal is constant. This
means that the bandwidth of a rectangular pulse of duration T is inversely proportional
to T, The transmitted signal in Figure 10a consists of the product of this rectangular signal
and the sinusoidal carrier A, cos(2af.t). The multiplication of a signal by a sinusoid has
the effect of shifting the Fourier transform of the signal to the right by f, and to the left
by an equal amount, except for the scaling factor of 1/2. It follows therefore that the
bandwidth of the transmitted signal m(¢), and therefore the required channel bandwidth,
is inversely proportional to the reciprocal of the symbol duration T. For the problem at
hand, the reciprocal of T is also the signaling rate of the system in b/s.

26

BACKGROUND AND PREVIEW

There are, however, some other issues that require theoretical considerations:

1. What is the justification for the receiver structure of Figure 10c?

2. The noise contribution wis the value of a random variable W produced by sampling
a certain realization w(t) of the channel noise at time ¢ = T in accordance with
Equations (3) and (4). How do we relate the statistics of the random variable W to
the statistical characteristics of the channel noise?

3. The receiver of Figure 10¢ makes occasional errors due to the random nature of the
correlator output. That is, the receiver decides in favor of symbol 0 given that symbol
1 was actually transmitted, and vice versa. What is the probability of decision errors?

Moreover, there are some important practical issues that need attention:

1. Channel bandwidth is a highly valuable resource. How do we choose a modulation
scheme that conserves bandwidth in a cost-effective manner? .

2. The binary signal m(t) may include redundant symbols introduced into it through
the use of channel encoding so as to provide protection against channel noise. How
do we design the channel encoder in the transmitter and the channel decoder in the
receiver so as to come very close to Shannon’s information capacity theorem in a
physically realizable manner?

3. The locally generated carrier in the receiver of Figure 10c is physically separate from
the carrier source used for modulation in the transmitter. How do we synchronize
the receiver to the transmitter with respect to both the carrier phase and symbol
timing so as to justify the use of Equation (4) as the basis of decision-making in the
reconstruction of the original binary signal?

The theoretical and practical issues raised here in the context of the simple digital
communication system of Figure 10 are addressed in the following chapters of the book.

| Historical Notes"

 

A preview of communications would be incomplete without a history of the subject. In
this final section of this introductory chapter we present some historical notes on com-
munications; each paragraph focuses on some important and related events. It is hoped
that this material will provide a sense of inspiration and motivation for the reader.

In 1837, the telegraph was perfected by Samuel Morse, a painter. With the words
“What hath God wrought,” transmitted by Morse’s electric telegraph between Washing-
ton, D.C., and Baltimore, Maryland, in 1844, a completely revolutionary means of real-
time, long-distance communications was triggered. The telegraph is the forerunner of dig-
ital communications in that the Morse code is a variable-length ternary code using an
alphabet of four symbols: a dot, a dash, a letter space, and a word space; short sequences
represent frequent letters, whereas long sequences represent infrequent letters. This type
of signaling is ideal for manual keying. Subsequently, Emile Baudot developed a fixed-
length binary code for telegraphy in 1875. In Baudot’s telegraphic code, well-suited for
use with teletypewriters, each code word consists of five equal-length code elements, and
each element is assigned one of two possible states: a mark or a space (i.e., symbol 1 or 0
in today’s terminology}.

In 1864, James Clerk Maxwell formulated the electromagnetic theory of light and
predicted the existence of radio waves; the underlying set of equations bears his name. The

Historical Notes 27

existence of radio waves was established experimentally by Heinrich Hertz in 1887. In
1894, Oliver Lodge demonstrated wireless communication over a relatively short distance
(150 yards). Then, on December 12, 1901, Guglielmo Marconi received a radio signal at
Signal Hill in Newfoundland; the radio signal had originated in Cornwall, England, 1700
miles away across the Atlantic. The way was thereby opened toward a tremendous broad-
eming of the scope of communications. In 1906, Reginald Fessenden, a self-educated aca-
demic, made history by conducting the first radio broadcast.

In 1875, the telephone was invented by Alexander Graham Bell, a teacher of the
deaf. The telephone made real-time transmission of speech by electrical encoding and
replication of sound a practical reality. The first version of the telephone was crude and
weak, enabling people to talk over short distances only. When telephone service was only
a few years old, interest developed in automating it. Notably, in 1897, A. B. Strowger, an
undertaker from Kansas City, Missouri, devised the automatic step-by-step switch that
bears his name; of all the electromechanical switches devised over the years, the Strowger
switch was the most popular and widely used.

In 1904, John Ambrose Fleming invented the vacuum-tube diode, which paved the
way for the invention of the vacuum-tube triode by Lee de Forest in 1906. The discovery
of the triode was instrumental in the development of transcontinental telephony in 1913
and signaled the dawn of wireless voice communications. Indeed, until the invention and
perfection of the transistor, the triode was the supreme device for the design of electronic
amplifiers. ;

In 1918, Edwin H. Armstrong invented the superheterodyne radio receiver; to this
day, almost all radio receivers are of this type. In 1933, Armstrong demonstrated another
revolutionary concept, namely, a modulation scheme that he called frequency modulation
(FM); Armstrong’s paper making the case for FM radio was published in 1936.

The first all-electronic television system was demonstrated by Philo T. Farnsworth
in 1928, and then by Vladimir K. Zworykin in 1929. By 1939, the British Broadcasting
Corporation (BBC) was broadcasting television on a commercial basis.

In 1928, Harry Nyquist published a classic paper on the theory of signal transmission
in telegraphy. In particular, Nyquist developed criteria for the correct reception of tele-
graph signals transmitted over dispersive channels in the absence of noise. Much of Ny-
quist’s early work was applied later to the transmission of digital data over dispersive
channels.

In 1937, Alec Reeves invented pulse-code modulation (PCM) for the digital encoding
of speech signals. The technique was developed during World War II to enable the en-
cryption of speech signals; indeed, a full-scale, 24-channel system was used in the field by
the United States military at the end of the war. However, PCM had to await the discovery
of the transistor and the subsequent development of large-scale integration of circuits for
its commercial exploitation.

In 1943, D. O. North devised the matched filter for the optimum detection of a
known signal in additive white noise. A similar result was obtained in 1946 independently
by J. H. Van Vleck and D. Middleton, who coined the term matched filter.

In 1947, the geometric representation of signals was developed by V. A. Kotel’nikov
in a doctoral dissertation presented before the Academic Council of the Molotov Energy
Institute in Moscow. This method was subsequently brought to full fruition by John M.
Wozencraft and Irwin M. Jacobs in a landmark textbook published in 1965.

In 1948, the theoretical foundations of digital communications were laid by Claude
Shannon in a paper entitled “A Mathematical Theory of Communication.” Shannon’s
paper was received with immediate and enthusiastic acclaim. It was perhaps this response

28

BACKGROUND AND PREVIEW

that emboldened Shannon to amend the title of his paper to ““The Mathematical Theory
of Communication” when it was reprinted a year later in a book co-authored with Warren
Weaver. It is noteworthy that prior to the publication of Shannon’s 1948 classic paper, it
was believed that increasing the rate of information transmission over a channel would
increase the probability of error; the communication theory community was taken by
surprise when Shannon proved that this was not true, provided that the transmission rate
was below the channel capacity. Shannon’s 1948 paper was followed by some significant
advances in coding theory, which include the following:

®* Development of the first nontrivial error-correcting codes by M. J. E. Golay in 1949
and Richard W. Hamming in 1950.

® Development of turbo codes by C. Berrou, A. Glavieux, and P. Thitimajshima in
1993; turbo codes provide near-optimum error-correcting coding and decoding per-
formance in the Shannon sense.

The transistor was invented in 1948 by Walter H. Brattain, John Bardeen, and Wil-
liam Shockley at Bell Laboratories. The first silicon integrated circuit (IC) was produced
by Robert Noyce in 1958. These landmark innovations in solid-state devices and integrated
circuits led to the development of very-large-scale integrated (VLSI) circuits and single-
chip microprocessors, and with them the nature of signal processing and the telecommu-
nications industry changed forever.

The invention of the transistor in 1948 spurred the application of electronics to
switching and digital communications. The motivation was to improve reliability, increase
capacity, and reduce cost. The first call through a stored-program system was placed in
March 1958 at Bell Laboratories, and the first commercial] telephone service with digital
switching began in Morris, Illinois, in June 1960. The first T-1 carrier system transmission
was installed in 1962 by Bell Laboratories.

During the period 1943 to 1946, the first electronic digital computer, called the
ENIAG, was built at the Moore School of Electrical Engineering of the University of Penn-
sylvania under the technical direction of J. Presper Eckert, Jr., and John W. Mauchly.
However, John von Neumann’s contributions were among the earliest and most funda-
mental to the theory, design, and application of digital computers, which go back to the
first draft of a report written in 1945. Computers and terminals started communicating
with each other over long distances in the early 1950s. The links used were initially voice-
grade telephone channels operating at low speeds (300 to 1200 b/s). Various factors have
contributed to a dramatic increase in data transmission rates; notable among them are the
idea of adaptive equalization, pioneered by Robert Lucky in 1965, and efficient modula-
tion techniques, pioneered by G. Ungerboeck in 1982. Another idea widely employed in
computer communications is that of automatic repeat-request (ARQ). The ARQ method
was originally devised by H. C. A. van Duuren during World War II and published in
1946. It was used to improve radio-telephony for telex transmission over long distances.

From 1950 to 1970, various studies were made on computer networks. However,
the most significant of them in terms of impact on computer communications was the
Advanced Research Project Agency Network (ARPANET), first put into service in 1971.
The development of ARPANET was sponsored by the Advanced Research Projects Agency
of the U.S. Department of Defense. The pioneering work in packet switching was done on
ARPANET. In 1985, ARPANET was renamed the Internet. The turning point in the evo-
lution of the Internet occurred in 1990 when Tim Berners-Lee proposed a hypermedia
software interface to the Internet, which he named the World Wide Web.” Thereupon, in

Notes and References 29

the space of only about two years, the Web went from nonexistence to worldwide popu-
larity, culminating in its commercialization in 1994. How do we explain the explosive
growth of the Internet? We may answer this question by offering these reasons:"

» Before the Web exploded into existence, the ingredients for its creation were already
in place. In particular, thanks to VLSI, personal computers (PCs) had already become
ubiquitous in homes throughout the world, and they were increasingly equipped with
modems for interconnectivity to the outside world.

® For about two decades, the Internet had grown steadily (albeit within a confined
community of users), reaching a critical threshold of user-value based electronic mail
and file transfer.

» Standards for document description and transfer, hypertext markup language
(HTML), and hypertext transfer protocol (HTTP) had been adopted.

Thus, everything needed for creating the Web was already in place except for two critical
ingredients: a simple user interface and a brilliant service concept.

In 1955, John R. Pierce proposed the use of satellites for communications. This
proposal was preceded, however, by an earlier paper by Arthur C. Clark that was pub-
lished in 1945, also proposing the idea of using an Earth-orbiting satellite as a relay point
for communication between two Earth stations. In 1957, the Soviet Union launched Sput-
nik I, which transmitted telemetry signals for 21 days. This was followed shortly by the
launching of Explorer I by the United States in 1958, which transmitted telemetry signals
for about five months. A major experimental step in communications satellite technology
was taken with the launching of Telstar I from Cape Canaveral on July 10, 1962. The
Telstar satellite was built by Bell Laboratories, which had acquired considerable knowl-
edge from pioneering work by Pierce. The satellite was capable of relaying TV programs
across the Atlantic; this was made possible only through the user of maser receivers and
large antennas.

The use of optical means (e.g., smoke and fire signals) for the transmission of infor-
mation dates back to prehistoric times. However, no major breakthrough in optical com-
munications was made until 1966, when K. C. Kao and G. A. Hockham of Standard
Telephone Laboratories, U.K., proposed the use of a clad glass fiber as a dielectric wave-
guide. The /aser (an acronym for light amplification by stimulated emission of radiation)
had been invented and developed in 1959 and 1960. Kao and Hockham pointed out that
(1) the attenuation in an optical fiber was due to impurities in the glass, and (2) the intrinsic
loss, determined by Rayleigh scattering, is very low. Indeed, they predicted that a loss of
20 dB/km should be attainable. This remarkable prediction, made at a time when the
power loss in a glass fiber was about 1000 dB/km, was to be demonstrated later. Nowa-
days, transmission losses as low as 0.1 dB/km are achievable.

The spectacular advances in microelectronics, digital computers, and lightwave sys-
tems that we have witnessed to date, and that will continue into the future, are all re-
sponsible for dramatic changes in the telecommunications environment; many of these
changes are already in place, and more changes will evolve as time goes on.

i NOTES AND REFERENCES

 

1. For essays on an early account of communications and other related disciplines (e.g., elec-
tronics, computers, radar, radio astronomy, satellites), see Overhage (1962); in particular,
see the chapter on “Communications” by L. V. Berkner, pp. 35—50.

30

© BACKGROUND AND PREVIEW

2.

10.

11.
12.
13.

The JPEG image coding standard is discussed in the papers by Wallace (1991); see also the
article by T. A. Ramstad in the handbook edited by Madisetti and Williams (1998).

. The discrete cosine transform (DCT) and its inverse for a block of 8 X 8 source image

samples are respectively defined by

Cll P [, Se sy) 00 ‘es cos( 2 al

4
7
f(x, y) = ; 2 > C(u)C(v) Flu, v) cos( 28 | cos 22 al

 

Flu, v) =

 

where

1 .
Clu), Ce) = 475 for 4 = 0 andv = 0
1 otherwise

For a full treatment of the DCT, see Rao and Yip (1990).

. The MPEG-1 video coding standard is discussed in the paper by Gall (1991); see also the

article by A. M. Tekalp in the handbook edited by Madisetti and Williams (1998), which
discusses the follow-up versions of the MPEG video coding standard.

. The MPEG-1 audio coding standard is discussed in the papers by Brandenburg and Stoll

(1994) and Pan (1993); see also the article by P. Noll in the handbook edited by Madisetti
and Williams (1998), which also discusses the follow-up versions of the MPEG audio
coding standard. In particular, the widespread use of the more current standard, MPEG-3
audio, is resulting in a level of piracy that may dwarf the earlier problems of “bootleg”
cassette tapes.

. For a detailed discussion of communication networks, see Tanenbaum (1996).

. The OSI reference model was developed by a subcommittee of the International Organi-

zation for Standardization (ISO) in 1977. For a discussion of the principles involved in
arriving at the seven layers of the OSI model and a description of the layers themselves,
see Tanenbaum (1996).

. SONET was originally proposed by Telcordia Technologies Inc. (then known as Bellcore)

and standardized by the American National Standards Institute (ANSI). Later, CCITT
approved a SONET standard and issued a set of parallel recommendations called synchro-
nous digital hierarchy (SDH). The differences between SONET and SDH are of a minor
nature.

. For a thorough and precise analysis of the propagation of light waves in an optical fiber,

we need to treat it as a dielectric waveguide and use Maxwell’s equations to carry out the
analysis; such an analysis is highly mathematical in nature. For a readable account of the
analysis, see Chapter 3 of Green, Jr. (1993).

For a semitechnical overview of Shannon’s theorems on information theory presented in a
highly readable fashion, see the book entitled Silicon Dreams by Lucky (1989).

For a readable account of the history of communications, see Lebow (1995).
For a historical account of the development of the Internet, see Leiner et al. (1997).

For an insightful essay on new telecommunications services and how society reacts to their
development, see Lucky (1997). This paper points to Metcalf’s law, according to which it
seems as if any new telecommunications service must take a long time for it to build to
universal acceptance. Lucky cites the World Wide Web as a startling counterexample to
Metcalf’s law and gives the reasons why.

 

RANDOM PROCESSES

This chapter presents an introductory treatment of stationary random processes with
emphasis on second-order statistics. In particular, it discusses the following issues:

b> The notion of a.random process.
b> The requirement that has to be satisfied for a random process to be stationary.

> The partial description of a random process in terms of its mean, correlation, and
covariance functions.

» The conditions that bave to be satisfied for a stationary random process to be ergodic, a
property that enables us to substitute time averages for ensemble averages.

» What happens to a stationary random process when it is transmitted through a linear
time-invariant filter?

> The frequency-domain description of a random process in terms of power spectral density.
> The characteristics of an important type of random process known as a Gaussian process.
» Sources of noise and their narrowband form.

> Rayleigh and Rician distributions, which represent two special probability distributions
that arise in the study of communication systems.

i 1.1L Eetroduction

The idea of a mathematical model used to describe a physical phenomenon is well estab-
lished in the physical sciences and engineering. In this context, we may distinguish two
classes of mathematical models: deterministic and stochastic. A model is said to be deter-
ministic if there is no uncertainty about its time-dependent behavior at any instant of time.
However, in many real-world problems the use of a deterministic model is inappropriate
because the physical phenomenon of interest involves too many unknown factors. Nev-
ertheless, it may be possible to consider a model described in probabilistic terms in that
we speak of the probability of a future value lying between two specified limits. In such a
case, the model is said to be stochastic or random. A brief review of probability theory is
presented in Appendix 1.

Consider, for example, a radio communication system. The received signal in such
a system usually consists of an information-bearing signal component, a random interfer-
ence component, and channel noise. The information-bearing signal component may rep-
resent, for example, a voice signal that, typically, consists of randomly spaced bursts of
energy of random duration. The interference component may represent spurious electro-
magnetic waves produced by other communication systems operating in the vicinity of the

31

32

CHAPTER 1 RANDOM PROCESSES

tadio receiver. A major source of channel noise is thermal noise, which is caused by the
random motion of the electrons in conductors and devices at the front end of the receiver.
We thus find that the received signal is random in nature. Although it is not possible to
predict the exact value of the signal in advance, it is possible to describe the signal in terms
of statistical parameters such as average power and power spectral density, as discussed
in this chapter.

1.2. Mathematical Definition
of a Random Process

 

 

In light of these introductory remarks, it is apparent that random processes have two
properties. First, they are functions of time. Second, they are random in the sense that
before conducting an experiment, it is not possible to exactly define the waveforms that
will be observed in the future.

In describing a random experiment it is convenient to think in terms of a sample
space. Specifically, each outcome of the experiment is associated with a sample point. The
totality of sample points corresponding to the aggregate of all possible outcomes of the
experiment is called the sample space. Each sample point of the sample space is a function
of time. The sample space or ensemble composed of functions of time is called a random
or stochastic process.’ As an integral part of this notion, we assume the existence of a
probability distribution defined over an appropriate class of sets in the sample space, so
that we may speak with confidence of the probability of various events.

Consider, then, a random experiment specified by the outcomes s from some sample
space S, by the events defined on the sample space S, and by the probabilities of these

Sample
space
$s
x(t)
x(t)
0 i Outcome of the
first trial of
the experiment
xglt
2h) xalt)
Outcome of the
second trial of
0 the experiment

 

Outcome of the
ath trial of
+T the experiment

   

 

 

t—

Ficure 1.1 An ensemble of sample functions.

1.3 Stationary Processes 33

events, Suppose that we assign to each sample point s a function of time in accordance
with the rule:

X(t, 5), -T=rsT (1.1)

where 2T is the total observation interval. For a fixed sample point s;, the graph of the
function X(t, s;) versus time ¢ is called a realization or sample function of the random
process. To simplify the notation, we denote this sample function as

x(t) = X(Z, s;) (1.2)

Figure 1.1 illustrates a set of sample functions {x,(t)|j = 1, 2,..., 2}. From this figure,
we note that for a fixed time #, inside the observation interval, the set of numbers

{1 (th), X2(te)s -- + Xaltad} = {Xtes 81), X(ta, $2), -- +, X tes Sud}

constitutes a random variable. Thus we have an indexed ensemble (family) of random
variables {X(¢, s)}, which is called a random process. To simplify the notation, the custom-
ary practice is to suppress the s and simply use X(z) to denote a random process. We may
now formally define a random process X(t) as an ensemble of time functions together with
a probability rule that assigns a probability to any meaningful event associated with an
observation of one of the sample functions of the random process. Moreover, we may
distinguish between a random variable and a random process as follows:

» For a random variable, the outcome of a random experiment is mapped into a
number.

» For a random process, the outcome of a random experiment is mapped into a wave-
form that is a function of time.

| 1.3 Stationary Processes

 

In dealing with random processes encountered in the real world, we often find that the
statistical characterization of a process is independent of the time at which observation of
the process is initiated. That is, if such a process is divided into a number of time intervals,
the various sections of the process exhibit essentially the same statistical properties. Such
a process is said to be stationary. Otherwise, it is said to be nonstationary. Generally
speaking, a stationary process arises from a stable physical phenomenon that has evolved
into a steady-state mode of behavior, whereas a nonstationary process arises from an
unstable phenomenon.

To be more precise, consider a random process X(t) that is initiated at t = —%. Let
X(t,), X(t2),..., X(t,) denote the random variables obtained by observing the random
process X(t) at times t), h,..., tz, respectively. The joint distribution function of this set
of random variables is Fxie,),..., xie)(%1» « - + » Xp). Suppose next we shift all the observation
times by a fixed amount 7, thereby obtaining a new set of random variables X(t, + 7),
X(t, + 7),..., X(t, + 7). The joint distribution function of this latter set of random
variables is Fyi,+1),..., x(e+7)(%1s + + + 5 X) The random process X(t) is said to be stationary
in the strict sense or strictly stationary if the following condition holds:

Fyn... Xtq+9(%1, ey Xe) = Pye, Xie) (X15 sey Xe) (1.3)
for all time shifts 7, all &, and all possible choices of observation times t,,... , f,. In other
words, a random process X(t), initiated at time t = —%, is strictly stationary if the joint

distribution of any set of random variables obtained by observing the random process X(t)
is invariant with respect to the location of the origin t = 0. Note that the finite-dimensional

34

CuHapTeR 1 & RANDOM PROCESSES

distributions in Equation (1.3) depend on the relative time separation between random
variables but not on their absolute time. That is, the random process has the same prob-
abilistic behavior through all time.

Similarly, we may say that two random processes X(t) and Y(t) are jointly strictly
stationary if the joint finite-dimensional distributions of the two sets of random variables
X(ty),..., X(t) and Y(e4),..., Y(t}) are invariant with respect to the origin t = 0 for all
k and j and all choices of observation times t),...,t, and Z1,..., t}.

Returning to Equation (1.3), we may distinguish two situations of special interest:

1. For k = 1, we have
Fey) = Fxtern(x) = Fx(x) for all ¢ and + (1.4)

That is, the first-order distribution function of a stationary random process is inde-
pendent of time.
2. For k = 2 and 7 = —t,, we have

Fy xc (*15 x2) = Pro, x2) (%15 X) for all #, and 4 (1.5)

That is, the second-order distribution function of a stationary random process de-
pends only on the time difference between the observation times and not on the
particular times at which the random process is observed.

These two properties have profound implications for the statistical parameterization of a
stationary random process; this issue is discussed in Section 1.4.

& EXAMPLE 1.1

Consider Figure 1.2, depicting three spatial windows located at times t,, t, t;. We wish to
evaluate the probability of obtaining a sample function x(t) of a random process X(t) that
passes through this set of windows, that is, the probability of the joint event

A={a<X) <b}, #¢=1,2,3
In terms of the joint distribution function, this probability equals
P(A) = F ye), xte),Xtt3)( Pas bs, b3) — Fees) Xt.) (415 2, a3)

Suppose now the random process X(z) is known to be strictly stationary. An implication
of strict stationarity is that the probability of the set of sample functions of this process passing
through the windows of Figure 1.32 is equal to the probability of the set of sample functions
passing through the corresponding time-shifted windows of Figure 1.3b. Note, however, that
it is not necessary that these two sets consist of the same sample functions. a

--=~ by

ww yas, lens ;
\ ,. ~. A posible

AS 793 sample

function

 

So

1*

Ficure 1.2 Illustrating the probability of a joint event.

 

 

1.4 Mean, Correlation, and Covariance Functions 35

bo I,

ay
: a3
8g
t
cay bo ta

I”

 

{a)

| Lal |
aq bg
a
tg +t
itr bo tgtt

1”

 

()
FIGURE 1.3 Ilustrating the concept of stationarity in Example 1.1.

and Covariance Functions

| 1.4 Mean, Correlation,

 

Consider a strictly stationary random process X(t). We define the sean of the process X(t)
as the expectation of the random variable obtained by observing the process at some time
t, as shown by

bex(t) = ELX(e)]

. (1.6)
= | . xfrxy(x) dx

where f.(.)(x) is the first-order probability density function of the process. From Equation
(1.4) we deduce that for a strictly stationary random process, fy;(x) is independent of
time ¢. Consequently, the mean of a strictly stationary process is a constant, as shown by

bex(t) = bx — for all t (1.7)

We define the autocorrelation function of the process X(t) as the expectation of the product
of two random variables, X(z) and X(t), obtained by observing the process X(t) at times
t, and f,, respectively. Specifically, we write

Ryx(t1, 2) = E[X(é)X(é)]
© po (1.8)
= in in 1X2 fixie x(e,)(X19 X2) dx dx,

where fixi,),xie,)(X1) X2) is the second-order probability density function of the process.
From Equation (1.5), we deduce that for a strictly stationary random process,
Fexie,),xte)(%1, X2) depends only on the difference between the observation times ¢, and #3.

36

CHAPTER 1 ®& RANDOM PROCESSES

This, in turn, implies that the autocorrelation function of a strictly stationary process
depends only on the time difference t, — t,, as shown by

Rx, 2) = Rxf(t2 — t,) for all t, and (1.9)
Similarly, the autocovariance function of a strictly stationary process X(t) is written as

Cx(ty, to) = El(X(t) — bex)(X(2) — ex)]
= Rxlt — th) - pk
Equation (1.10) shows that, like the autocorrelation function, the autocovariance function
of a strictly stationary process X(t} depends only on the time difference t, — 4. This
equation also shows that if we know the mean and autocorrelation function of the process,
we can uniquely determine the autocovariance function. The mean and autocorrelation
function are therefore sufficient to describe the first two moments of the process.
However, two important points should be carefully noted:

(1.10)

1. The mean and autocorrelation function only provide a partial description of the
distribution of a random process X(t).

2. The conditions of Equations (1.7) and (1.9), involving the mean and autocorrelation
function, respectively, are not sufficient to guarantee that the random process X(t)
is strictly stationary.

Nevertheless, practical considerations often dictate that we simply limit ourselves to a
partial description of the process given by the mean and autocorrelation function. The
class of random processes that satisfy Equations (1.7) and (1.9) has been given various
names, such as second-order stationary, wide-sense stationary, or weakly stationary pro-
cesses. Henceforth, we shall simply refer to them as stationary processes.”

A stationary process is not necessarily strictly stationary because Equations (1.7) and
(1.9) obviously do not imply the invariance of the joint (k-dimensional) distribution of
Equation (1.3) with respect to the time shift 7 for all &. On the other hand, a strictly
stationary process does not necessarily satisfy Equations (1.7) and (1.9) as the first- and
second-order moments may not exist. Clearly, however, the class of strictly stationary
processes with finite second-order moments forms a subclass of the class of all stationary
processes.

PROPERTIES OF THE AUTOCORRELATION FUNCTION

For convenience of notation, we redefine the autocorrelation function of a stationary pro-
cess X(t) as

Ry(t) = E[X(t + X(¢)]_ for allt (1.11)
This autocorrelation function has several important properties:

1. The mean-square value of the process may be obtained from R,(7) simply by putting
7 = 0 in Equation (1.11), as shown by

Ry(0) = E[X?(2)] (1.12)
2. The autocorrelation function Rx(7)-is an even function of 7, that is,
Rx(t) = Rx(~7) (1.13)

This property follows directly from the defining equation (1.11). Accordingly, we
may also define the autocorrelation function R(t) as

Rx(z) = E[X(e)X(t — 7)]

14 Mean, Correlation, and Covariance Functions 37

Ry) Slowly fluctuating

random process

  
  
 
  

Rapidly fluctuating
random process

 

t

ie)

Ficure 1.4 Illustrating the autocorrelation functions of slowly and rapidly fluctuating random
processes,

3. The autocorrelation function Ry(7) has its maximum magnitude at 7 = 0, that is,
|Rx(7)| = Rx(0) (1.14)

To prove this property, consider the nonnegative quantity

E[(X(¢ + 7) + X(2))?] = 0
Expanding terms and taking their individual expectations, we readily find that

E[X2(t + 7)] + 2E[X(t + X(t] + ELX2(2)] = 0
which, in light of Equations (1.11) and (1.12), reduces to
2Rx(0) + 2Rx(7) = 0

Equivalently, we may write

Rx(0) S Rx(t) S Rx(0)
from which Equation (1.14) follows directly.

The physical significance of the autocorrelation function Rx(7) is that it provides a
means of describing the interdependence of two random variables obtained by observing
a random process X(t) at times 7 seconds apart. It is therefore apparent that the more
rapidly the random process X(t) changes with time, the more rapidly will the autocorre-
lation function Rx(z7) decrease from its maximum Rx(0) as 7 increases, as illustrated in
Figure 1.4. This decrease may be characterized by a decorrelation time 7, such that for
T > 7%, the magnitude of the autocorrelation function Rx(7) remains below some prescribed
value. We may thus define the decorrelation time 7) of a stationary process X(t) of zero
mean as the time taken for the magnitude of the autocorrelation function Rx(7) to decrease
to 1 percent, say, of its maximum value R(0).

© EXAMPLE 1.2 Sinusoidal Wave with Random Phase
Consider a sinusoidal signal with random phase, defined by
X(t) = A cos(2rft + ©) (1.15)

where A and f, are constants and is a random variable that is uniformly distributed over
the interval [~7, a], that is,

1
fl0) 42m? 7S OS™ (1.16)

0, elsewhere

38

CuHaprer 1. & RANDOM PROCESSES

 

Figure 1.5 Autocorrelation function of a sine wave with random phase.

This means that the random variable © is equally likely to have any value @ in the interval
[-a, a]. Each value of @ corresponds to a sample in the sample space of the random process
X(@).

The process X(z) defined by Equations (1.15) and (1.16) may represent a locally gen-
erated carrier in the receiver of a communication system, which is used in demodulation of
the received signal. In particular, the random variable @ denotes the phase difference between
this locally generated carrier and the sinusoidal carrier wave used to modulate the message
signal in the transmitter.

The autocorrelation function of X(2) is

Rx(7) = EEX + 1) X(2)]
= E[A? cos(2af¢ + 2afr + ©) cos(2aft + @)]

 

 

At At
aw Elcos(4arf,t + 2af.r + 20)] + a E[cos(27f,7)]

A? {” 1 At
=> su cos(4afi.t + 2af.r + 28) dé + — cos(2af-7)
2 Jew lor 2

The first term integrates to zero, and so we get

Rx(n = a cos(2-1f,7) (1.17)

which is plotted in Figure 1.5. We see therefore that the autocorrelation function of a sinu-
soidal wave with random phase is another sinusoid at the same frequency in the “7 domain”
rather than the original time domain. <

» EXAMPLE 1.3. Random Binary Wave

Figure 1.6 shows the sample function x(t) of a process X(t) consisting of a random sequence
of binary symbols 1 and 0. The following assumptions are made:

1. The symbols 1 and 0 are represented by pulses of amplitude +A and —A volts, respec-
tively, and duration T seconds.

2, The pulses are not synchronized, so the starting time £, of the first complete pulse for
positive time is equally likely to lie anywhere between zero and T seconds, That is, ty
is the sample value of a uniformly distributed random variable T,, with its probability
density function defined by .

1
frltd) =y7? OS HST
0, elsewhere

3. During any time interval (x - 1)T <t — #, < nT, where 7 is an integer, the presence
of a 1 or a 0 is determined by tossing a fair coin; specifically, if the outcome is heads,

1.4 Mean, Correlation, and Covariance Functions 39

xt)

 

oak ode

FiGuRE 1.6 Sample function of random binary wave.

 

we have a 1 and if the outcome is tails, we have a 0. These two symbols are thus equally
likely, and the presence of a 1 or 0 in any one interval is independent of all other
intervals.

Since the amplitude levels —A and +A occur with equal probability, it follows imme-
diately that E[X(t}] = 0 for all t, and the mean of the process is therefore zero.

To find the autocorrelation function R(t, £), we have to evaluate E[X(z,)X(z))], where
X(#,) and_X(t,) are random variables obtained by observing the random process X(t) at times
t, and #;, respectively.

Consider first the case when |#, — t;| > T. Under this condition the random variables
X(t,) and X(t, occur in different pulse intervals and are therefore independent. We thus have

E(X(t,)X(4)] = ELX(e,)ELX(4)] = 0, [te — &| > T

Consider next the case when | #, — i;| < T, with t, = 0 and t; < f,. In such a situation
we observe from Figure 1.6 that the random variables X(t,) and X(¢,) occur in the same pulse
interval if and only if the delay tz satisfies the condition tz < T — |t, — #;|. We thus obtain
the conditional expectation:

A? ty<T— | - &|
E[X(t)X(e)ltal = 4,” ‘
0, elsewhere
Averaging this result over all possible values of tz, we get

T— | tytit

6 A’f. TAta) dty

T- [tet] 42
= = dt,
j pa

= a(t | te = al), [te t,| <T

ELX(t)X(4)] = [

 

By similar reasoning for any other value of t,, we conclude that the autocorrelation function
of a random binary wave, represented by the sample function shown in Figure 1.6, is only a
function of the time difference rT = t, — t,, as shown by

in|
Rx() = a(t - lal), In]<T (1.18)

0, |r] =T

This result is plotted in Figure 1.7. <

CHAPTER 1 RANDOM PROCESSES

Ryfr)

Ae

T
-T ie] T

FIGURE 1.7 Autocorrelation function of random binary wave.

s Cross-CORRELATION FUNCTIONS

Consider next the more general case of two random processes X(t) and Y(¢) with auto-
correlation functions Ry(é, u) and Ry(t, ), respectively. The two cross-correlation func-

tions of X(t) and Y(t) are defined by

Ryy(t, 4) = ELX(t)¥(w)] (1.19)
and

Ryx(t, 4) = E[Y(é)X(w)] (1.20)

where ¢ and w denote two values of time at which the processes are observed. In this case,
the correlation properties of the two random processes X(t) and Y(t) may be displayed
conveniently in matrix form as follows:

Ryxlt, uw) Ryle, °)

Rem) = an u) —Rylt, #)

which is called the correlation matrix of the random processes X(t) and Y(z). If the random
processes X(t) and Y(¢) are each stationary and, in addition, they are jointly stationary,
then the correlation matrix can be written as

ee |

Re) = I eix(t) Rolo)

(1,21)

where T= t — u.
The cross-correlation function is not generally an even function of 7 as was true for
the autocorrelation function, nor does it have a maximum at the origin. However, it does

‘obey a certain symmetry relationship as follows (see Problem 1.9):

Rxy(7) = Ryx(-7) (1.22)

& ExaMpLe 1.4 Quadrature-Modulated Processes

Consider a pair of quadrature-modulated processes X,(t) and X,(t) that are related to a sta-
tionary process X(t) as follows:

X(t) = X(t) cos(2rft + O)

X,(t) = X(t) sin(27f.t + @)

1.5. Ergodic Processes 41

where f, is a carrier frequency, and the random variable © is uniformly distributed over the
interval [0, 27]. Moreover, @ is independent of X(#). One cross-correlation function of X,(z)
and X,(t) is given by

Rial) = BI ()X2(t — 7)
E[X(t)X(t — 1) cos(2arf.t + ©) sin(2afit ~ 2af.7 + 0)]
E[X()X(t — AIE[cos(2af,t + @) sin(2aft — 2rfr + O)] (1.23)
= 4Rx(DE|sin(4af.t — 2af.r +20) — sin(2af,7)]
—3Rx(7) sin(2af-7)

u

where, in the last line, we have made use of the uniform distribution of the random variable
® representing phase. Note that at 7 = 0, the factor sin(27f,7) is zero and therefore

Ri2(0) = ELX(t)X2(2)]
=0

This shows that the random variables obtained by simultaneously observing the quadrature-
modulated processes X,(¢) and X(t) at some fixed value of time ¢ are orthogonal to each
other. <

| 1.5 Ergodic Processes

The expectations or ensemble averages of a random process X(¢) are averages “‘across the
process.” For example, the mean of a random process X(t) at some fixed time t, is the
expectation of the random variable X(t,) that describes all possible values of the sample
functions of the process observed at time t = ¢,. Naturally, we may also define long-term
sample averages, or time averages that are averages “along the process.” We are therefore
interested in relating ensemble averages to time averages, for time averages represent a
practical means available to us for the estimation of ensemble averages of a random pro-
cess. The key question, of course, is: When can we substitute time averages for ensemble
averages? To explore this issue, consider the sample function x(t) of a stationary process
X(t), with the observation interval defined as -T = t = T. The DC value of x(t) is defined
by the time average

1 T

ot I x(t) dt (1.24)

HAT) =

Clearly, the time average y,(T) is a random variable, as its value depends on the obser-
vation interval and which particular sample function of the random process X(t) is picked
for use in Equation (1.24). Since the process X(t) is assumed to be stationary, the mean of
the time average y2,,(T) is given by (after interchanging the operation of expectation and
integration):

ElyAT -if) E[x(t)] de
1/7
~ 2T Jor

= Mx

bx dt (1.25)

42

CHAPTER 1 RANDOM PROCESSES

where jtx is the mean of the process X(t). Accordingly, the time average 2,(T) represents
an unbiased estimate of the ensemble-averaged mean x. We say that the process X(t) is
ergodic in the mean if two conditions are satisfied:

» The time average y,(T) approaches the ensemble average jx in the limit as the
observation interval T approaches infinity; that is,
lim BAT) = bx

> The variance of ,(T), treated as a random variable, approaches zero in the limit as
the observation interval T approaches infinity; that is,

lim var[y,(T)] = 0
T$00

The other time average of particular interest is the autocorrelation function R,(7, T)
defined in terms of the sample function x(t) observed over the interval -T = t = T.
Following Equation (1.24), we may formally define the time-averaged autocorrelation
function of a sample function x(£) as follows:

1 (7
R. == I 1,2

x(T, T) oT Jer x(t + r)x(t) dé (1.26)
This second time-average should also be viewed as a random variable with a mean and
variance of its own. In a manner similar to ergodicity of the mean, we say that the process
x(t) is ergodic in the autocorrelation function if the following two limiting conditions are
satisfied:

lim R,(7, T) = Rx(7}

Too

lim var[R,(7, T)] = 0
Toe

We could, of course, go on in a similar way to define ergodicity in the most general
sense by considering higher-order statistics of the process X(t). In practice, however, er-
godicity in the mean and ergodicity in the autocorrelation function, as described here, are
often (but not always) considered to be adequate. Note also that the use of Equations
(1.24) and (1.26) to compute the time averages yz,(T) and R,(t, T) requires that the process
X(t) be stationary. In other words, for a random process to be ergodic, it has to be sta-
tionary; however, the converse is not necessarily true.

-6 Transmission of a Random Process

[Tiros a

Through a Linear Time-Invariant Filter

 

Suppose that a random process X(t) is applied as input to a linear time-invariant filter of
impulse response b(t), producing a new random process Y(t) at the filter output, as in
Figure 1.8. In general, it is difficult to describe the probability distribution of the output
random process Y(t), even when the probability distribution of the input random process
X(t) is completely specified for —-% <¢< «,

In this section, we determine the time-domain form of the input-output relations of
the filter for defining the mean and autocorrelation functions of the output random process
¥(£) in terms of those of the input X(t), assuming that X(t) is a stationary process.

1.6 Transmission of a Random Process Through a Linear Time-Invariant Filter 43

 

impulse
xX()—>) so response >> F(s)
we)

 

 

 

Ficure 1.8 Transmission of a random process through a linear time-invariant filter.

The transmission of a process through a linear-time-invariant filter is governed by
the convolution integral; for a review of this operation, see Appendix 2. For the problem
at hand, we may thus express the output random process Y(t) in terms of the input random
process X(t) as

Y(t) = if b(m)X(t — 14) dry
where 1, is the integration variable. Hence, the mean of Y(é) is
By(t) = ELY(é)]
co (1.27)
= a [ b(7,)X(t — 74) an

Provided that the expectation E[X(t)] is finite for all t and the system is stable, we may
interchange the order of expectation and integration in Equation (1.27) and so write

v(t) = | birdBIXte ~ m1 dn
(1.28)

= in b(t )ux(t — 1) dr

When the input random process X(t) is stationary, the mean yx(t) is a constant px, so
that we may simplify Equation (1.28) as follows:

by = px [ h(1,) dr
= pxH(0)

(1.29)

where H(0) is the zero-frequency (DC) response of the system. Equation (1.29) states that
the mean of the random process Y(t) produced at the output of a linear time-invariant
system in response to X(t) acting as the input process is equal to the mean of X(t) multiplied
by the DC response of the system, which is intuitively satisfying.

Consider next the autocorrelation function of the output random process Y(t). By
definition, we have

Ryt, u) = E[Y(£)¥(w)]

where ¢ and # denote two values of the time at which the output process is observed. We
may therefore use the convolution integral to write

Ry{t, #) = elf b(r)X(t — 1%) dry ia b(1)X(u — 1) ar] (1.30)

44 CHAPTER 1 & RANDOM PROCESSES

Here again, provided that the mean-square value E[X7(z)] is finite for all t and the system
is stable, we may interchange the order of the expectation and the integrations with respect
to 7, and 7, in Equation (1.30), obtaining

Rut, =| dnbin) | droblnJELXte — 1)Xtu ~ nl
. : (1.31)
= [ dr h(7,) ia dth(m%)Rx(t — %, # — 7)

When the input X(f) is a stationary process, the autocorrelation function of X(t) is only a
function of the difference between the observation times t — 7 and # — 75. Thus, putting
7 = t— u in Equation (1.31), we may write

Ry{t) = in i: h(t, )b(m)Rx(t — 1%] + %) dy dm (1.32)

On combining this result with that involving the mean py, we see that if the input to a
stable linear time-invariant filter is a stationary process, then the output of the filter is also
a stationary process.

Since Ry(0) = E[Y?(£)}, it follows that the mean-square value of the output random
Process Y(t) is obtained by putting rT = 0 in Equation (1.32). We thus get the result

eye = [fo atrvbladRadna — 1) dry dry (1.33)

which is a constant.

| 1.7 Power Spectral Density

 

Thus far we have considered the characterization of stationary processes in linear systems
in the time domain. We turn next to the characterization of random processes in linear
systems by using frequency-domain ideas. In particular, we wish to derive the frequency-
domain equivalent to the result of Equation (1.33) defining the mean-square value of the
filter output.

By definition, the impulse response of a linear time-invariant filter is equal to the
inverse Fourier transform of the frequency response of the system; a review of the Fourier
transform is presented in Appendix 2. Using H(f) to denote the frequency response of the
system, we may thus write

b(t) = i: A(f) exp(j2afn) df (1.34)

Substituting this expression for /(7) into Equation (1.33), we get

evan = [|] | [HUN expl npn) af Porat — 1) dr, dr,
(1.35)

=] apni { deabirs) [Rule ~ 1) explj2afn) dx

1.7 Power Spectral Density 45

In the last integral on the right-hand side of Equation (1.35), define a new variable
T=m-% ,

Then we may rewrite Equation (1.35) in the form

Ey = {_ dptaie) | drabln) expli2afn) {| Rule) expl-i2mfn) dr (1.36)

However, the middle integral on the right-hand side in Equation (1.36) is simply H*(f),
the complex conjugate of the frequency response of the filter, and so we may simplify this
equation as

E[Y*(t)] = [. df |H(f)|? [. Ryx(7) exp(—j2af7) dr (1.37)

where | H(f)| is the magnitude response of the filter. We may further simplify Equation
(1.37) by recognizing that the last integral is simply the Fourier transform of the auto-
correlation function Rx(r) of the input random process X(t). This prompts us to introduce
the definition of a new parameter

Sx(f) = [. Rx(1) exp(—j2af7) dr (1.38)

The function Sy(f) is called the power spectral density, or power spectrum, of the station-
ary process X(t). Thus substituting Equation (1.38) into (1.37), we obtain the desired
relation:

Evo] = [LAU Se(f) df (1.39)

Equation (1.39) states that the mean-square value of the output of a stable linear time-
invariant filter in response to a stationary process is equal to the integral over all frequen-
cies of the power spectral density of the input process multiplied by the squared magnitude
response of the filter. This is the desired frequency-domain equivalent to the time-domain
relation of Equation (1.33).

To investigate the physical significance of the power spectral density, suppose that
the random process X(t) is passed through an ideal narrowband filter with a magnitude
response centered about the frequency f., as shown in Figure 1.9; that is,

14, If + fl <34f

1.40
0, [f+ fl>af (1-40)

LHUP)| -{

 

we 0 I. te
Af af

FicURE 1.9 Magnitude response of ideal narrowband filter.

46

CHAPTER 1 RANDOM PROCESSES

where Af is the bandwidth of the filter. Then from Equation (1.39) we find that if the
filter bandwidth Af is sufficiently small compared to the midband frequency f, and Sx(f)
is a continuous function, the mean-square value of the filter output is approximately

E[Y*(z)] ~ (2Af)Sx(f-) (1.41)

The filter, however, passes only those frequency components of the input random process
X(t) that lie inside a narrow frequency band of width Af centered about the frequency
+f.. Thus Sx(f,) represents the frequency density of the average power in the random
process X(z), evaluated at the frequency f = f,. The dimensions of the power spectral
density are therefore in watts per Hertz (W/Hz).

a PROPERTIES OF THE POWER SPECTRAL DENSITY

The power spectral density S,{f) and the autocorrelation function Ry(7) of a stationary
process X(t) form a Fourier-transform pair with 7 and f as the variables of interest, as
shown by the pair of-relations

2

Sx(f) = i . Rx(7) exp(—j2afr) dr (1.42)

Relr) = | Self) exp( mfr) af (1.43)

Equations (1.42) and (1.43) are:basic relations in the theory of spectral analysis of random
processes, and together they constitute what are usually called the Einstein—Wiener—
Khintchine relations?

The Einstein~Wiener-Khintchine relations show that if either the autocorrelation
function or power spectral density of a randoin process is known, the other can be found
exactly. But these functions display different aspects of the correlation information about
the process, It is commonly accepted that for practical purposes, the power spectral density
is the more useful “‘parameter.”

We now wish to use this pair of relations to derive some general properties of the
power spectral density of a stationary process.

Property 1

The zero-frequency value of the power spectral density of a stationary process equals the
total area under the graph of the autocorrelation function; that is,

Sx(0) = [- Rx(1) dr (1.44)

This property follows directly from Equation (1.42) by putting f = 0.

Property 2
The mean-square value of a stationary process equals the total area under the graph of
the power spectral density; that is,

E[X?(t)] = in Sx(f) df (1.45)

This property follows directly from Equation (1.43) by putting r = 0 and noting that
Ry (0) = E[X*(2)].

1.7 Pewer Spectral Density 47

Property 3
The power spectral density of a stationary process is always nonnegative; that is,
Sx(f} = 0 for all f (1.46)
This property is an immediate consequence of the fact that, in Equation (1.41), the
mean-square value E[Y?(t)] must always be nonnegative.
Property 4

The power spectral density of a real-valued random process is an even function of fre-
quency; that is,

Sxl-f) = Sx(f) (1.47)

This property is readily obtained by substituting —f for f in Equation (1.42):
Sx(—f) = [ Rule) exp(2afr) dr
Next, substituting —7 for 1, and recognizing that Ryx(—7) = Rx(7), we get

Sx(-f) = in Rx(7) exp(—j2a7fr) dr = Sx(f)

which is the desired result.

Property 5

The power spectral density, appropriately normalized, has the properties usually associated
with a probability density function.

The normalization we have in mind here is with respect to the total area under the
graph of the power spectral density (i.e., the mean-square value of the process). Consider
then the function

bx(f) = a (1.48)
FF sein ag
In light of Properties 2 and 3, we note that px(f) = 0 for all f. Moreover, the total area

under the function px(f) is unity. Hence, the normalized form of the power spectral den-
sity, as defined in Equation (1.48), behaves similar to a probability density function.

Sx(f)
Sx

® ExamMPLe 1.5 Sinusoidal Wave with Random Phase (continued)

Consider the random process X(t) = A cos(2af.¢ + ©), where @ is a uniformly distributed
random variable over the interval [—7, 7]. The autocorrelation function of this random pro-
cess is given by Equation (1.17), which is reproduced here for convenience:

2
Rx(7) = < cos(2 af,7)

48

Craprer | & RANDOM PROCESSES

Self)

ae Ae
y OF +f) “7 Af -f.)

 

he 0 fi t

FiGuRE 1.10 Power spectral density of sine wave with random phase; 6(f) denotes the delta
function at f = 0.

Let 8(f) denote the delta function at f = 0; for the definition of the delta function and its
properties, see Appendix 2. Taking the Fourier transform of both sides of the relation defining
Rx(7), we find that the power spectral density of the sinusoidal process X(z) is

Sx(f) = = [Sf — f) + 6 + #3] (1.49)

which consists of a pair of delta functions weighted by the factor A7/4 and located at +f, as
illustrated in Figure 1.10. We note that the total area under a delta function is one. Hence,
the total area under Sx(f) is equal to A?/2, as expected. <

& EXAMPLE 1.6 Random Binary Wave (continued)

Consider again a random binary wave consisting of a sequence of 1s and Os represented by
the values +A and —A, respectively. In Example 1.3 we showed that the autocorrelation
function of this random process has a triangular waveform, as shown by

;
x(n) = a(s - i) ri< 7
0, |r| =T

The power spectral density of the process is therefore

[|

Sx(f) = { ai - 4) exp(—j2afr) dr

-T
Using the Fourier transform of a triangular function (see Table A6.3), we obtain
Sx(f) = A°*T sinc?(fT) (1.50)

which is plotted in Figure 1.11. Here again we see that the power spectral density is nonneg-
ative for all f and that it is an even function of f. Noting that Rx(0) = A? and using Property
2, we find that the total area under Sx(f), or the average power of the random binary wave
described here, is A?, which is intuitively satisfying. 4

The result of Equation (1.50) may be generalized as follows. We note that the energy
spectral density (i.e., the squared magnitude of the Fourier transform) of a rectangular
pulse g(é) of amplitude.A and duration T is given by

€,(f) = A*T? sinc*(fT) (1.51)
We may therefore rewrite Equation (1.50) in terms of €,(f) simply as
€,(f)
Sx(f) = = (1.52)

T

1.7 Power Spectral Density 49

Sef}

 

 

2 1 0 1 2

T T T T

FiGure 1,11 Power spectral density of random binary wave.

Equation (1.52) states that for a random binary wave in which binary symbols 1 and 0
are represented by pulses g(t) and —g(t), respectively, the power spectral density Sx(f) is
equal to the energy spectral density &,(f) of the symbol shaping pulse g(t), divided by the
symbol duration T.

® ExaMe_e 1.7 Mixing of a Random Process with a Sinusoidal Process

A situation that often arises in practice is that of mixing (i.e., multiplication) of a stationary
process X(t) with a sinusoidal wave cos(2 aft + ©), where the phase @ is a random variable
that is uniformly distributed over the interval [0, 27]. The addition of the random phase ®
in this manner merely recognizes the fact that the time origin is arbitrarily chosen when
X(t) and cos(2af,t + @) come from physically independent sources, as is usually the case. We
are interested in determining the power spectral density of the random process Y(t), defined
by

Y(t) = X(t) cos(2aft + O) (1.53)

Using the definition of autocorrelation function of a stationary process and noting that the
random variable © is independent of X(t), we find that the autocorrelation function of Y(t) is
given by
Ry(7) = ELY(e + 7) ¥(2)]

= E[X(t + 7) cos(Qaf.t + 2af.7 + O)X(t) cos(2af.t + O)]

= E[X(t + X(t) ]Elcos(2af,t + 2af,7 + ©) cos(2aft + 0)] (1.54)

= $Rx(r)E[cos(2rf,r) + cos(4aft + 2rf.7r + 20)]
$Rx(7) cos(2af.7)

Because the power spectral density is the Fourier transform of the autocorrelation function,
we find that the power spectral densities of the random processes X(t) and Y(t) are related as
follows:

Sof) = USF - f) + Sxlf + fl (1.55)

According to Equation (1.55), the power spectral density of the random process Y(t) defined
in Equation (1.53) is obtained as follows: We shift the given power spectral density Sy({f) of
random process X(t) to the right by f., shift it to the left by fa add the two shifted power
spectra, and divide the result by 4. <

50

CHAPTER 1 RANDOM PROCESSES

& RELATION AMONG THE POWER SPECTRAL DENSITIES
OF THE INPUT AND OUTPUT RANDOM PROCESSES

Let Sx(f) denote the power spectral density of the output random process Y(t) obtained
by passing the random process X(z)} through a linear filter of frequency response H(f).
Then, recognizing by definition that the power spectral density of a random process is
equal to the Fourier transform of its autocorrelation function and using Equation (1.32),
we obtain

Sup) = [_ Ryln) expl—faaf) dr
(1.56)

= in in in h(7)h(m)Rx(t — 1% + 2) exp(—j2mfr) dr dry dr

Let tr — 7 + 72 = 7%, or, equivalently, r = 1) + 7, — 7. Then by making this substitution
in Equation (1.56), we find that S,(f) may be expressed as the product of three terms: the
frequency response H(f) of the filter, the complex conjugate of H(f), and the power spec-
tral density Sx(f) of the input random process X(t): We may thus simplify Equation (1.56)
as

Sy(f) = HUP\A*(PSx(f) (1.57)

Finally, since | H(f)|* = H(f)H*(f), we find that the relationship among the power spectral
densities of the input and output random processes is expressed in the frequency domain
by writing

Sy(f) = |H(f)|?Sx(f) (1.58)

Equation (1.58) states that the power spectral density of the output process Y(t) equals
the power spectral density of the input process X(t) multiplied by the squared magnitude
response of the filter. By using this relation, we can therefore determine the effect of passing
a random process through a stable, linear, time-invariant, filter. In computational terms,
Equation (1.58) is usually easier to handle than its time-domain counterpart of Equation
(1.32), involving the autocorrelation function.

= RELATION AMONG THE POWER SPECTRAL DENSITY
AND THE MAGNITUDE SPECTRUM OF A SAMPLE FUNCTION

We now wish to relate the power spectral density 5(f) directly to the spectral properties
of a sample function x(t) of a stationary process X(z) that is ergodic. For the sample
function x(t) to be Fourier transformable, however, it must be absolutely integrable; that
is :

[ |x(t)| dt < 0 (1.59)
This condition can never be satisfied by any stationary sample function x(t) of infinite
duration. In order to use the Fourier transform technique, we consider a truncated segment

of x(t), defined over the observation interval -T <= t < T, say. Thus, using X(f, T) to
denote the Fourier transform of the truncated sample function so defined, we may write

T
X(f, T) = [. x(t) exp(—j2aft) dt (1.60)

1.7 Power Spectral Density 51

Assuming that the process x(t) is also ergodic, we may evaluate the autocorrelation
function Rx(7) of X(¢) using the time-average formula (see Section 1.5)
T

Rx(t) = lim = x(t + r)x(t) dt (1.61)

It is customary to view the sample function x(f) as a power signal (i.e., a signal with finite
average power). Hence, we may formulate the following Fourier-transform pair:

1 {7 1
aT [. x(t + r)x(t) dt = oT |X(f, T)|? (1.62)

The parameter on the left-hand side is a time-averaged autocorrelation function. The pa-
rameter on the right-hand side is called the periodogram, whose dimensions are the same
as those of the power spectral density. This terminology is a misnomer, however, since the
periodogram is a function of frequency, not period. Nevertheless, it has wide usage. The
quantity was first used by statisticians to look for periodicities such as seasonal trends in
data.

Using the formula for the inverse Fourier transform in the Fourier-transform pair of
Equation (1.62), we may express the time-averaged autocorrelation function of the sample
function x(¢) in terms of the periodogram as

T

“1
aT Sop x(t + t)x(t) dt = in oT |X(f, T)|* exp(j2afT) df (1.63)

Hence, substituting Equation (1.63) into | 1.61), we get

Rx(1) = lim J oT
For a fixed value of the frequency f, the periodogram is a random variable in that
its value varies in a random manner from one sample function of the random process to
another. Thus, for a given sample function x(t), the periodogram does not converge in any
statistical sense to a limiting value as T tends to infinity. As such, it would be incorrect to
interchange the order of the integration and limiting operations in Equation (1.64). Sup-
pose, however, that we take the expectation of both sides of Equation (1.64) over the
ensemble of all sample functions of the random process and recognize that for an ergodic
process the autocorrelation function Rx(7) is unchanged by such an operation. Then, since
each sample function of an ergodic process eventually takes on néarly all the modes of
behavior of each other sample function, we may thus write

|X(f, T)|* exp(j2mfr) df (1.64)

oe

Rx(7) = lim Al |X(f, T)|?] exp(j2aft) df (1.65)

T-20 Yoo

Now we may interchange the order of the integration and limiting operations and so obtain

ret = fo ftim Zetixy, NP} explana dp (1.66

Hence, comparing Equations (1.66) and (1.43), we obtain the desired relation between
the power spectral density S,(f) of an ergodic process and the squared magnitude spectrum
|X(f, T)|? of a truncated sample function of the process:

1
Sx(f) = lim ap FLIX, T)/7]
7 (1.67)

T 2
= lim a 2| [. x(t) exp(—j2aft) dt

Too 2T

 

 

52

CuHarTerR 1 & RANDOM PROCESSES

It is important to note that in Equation (1.67) it is not possible to let T— © before taking
the expectation. Equation (1.67) provides the mathematical basis for estimating‘ the power
spectral density of an ergodic random process, given a sample function x(t) of the process
observed over the interval [—T, T].

= CRross-SPECTRAL DENSITIES

Just as the power spectral density provides a measure of the frequency distribution of a
single random process, cross-spectral densities provide a measure of the frequency inter-
relationship between two random processes. In particular, let X(t) and Y(t) be two jointly
stationary processes with their cross-correlation functions denoted by Rxy(7) and Ry (7).
We then define the cross-spectral densities Syy(f) and Syx(f) of this pair of random pro-
cesses to be the Fourier transforms of their respective cross-correlation functions, as
shown by

Sxy(f) = if Ryy(1) exp(—j2afr) dr (1.68)

and

Syx(f) = in Ryx(1) exp(—j2afr) dr (1.69)

The cross-correlation functions and cross-spectral densities thus form Fourier-transform
pairs. Accordingly, using the formula for inverse Fourier transformation we may also
write

Rxyylt) = if Sxy(f) exp( j2afr) df (1.70)

and

Ryx(7) = in Syx(f) exp(j2aft) af (1.71)

The cross-spectral densities Syy(f) and Syx(f) are not necessarily real functions of
the frequency #. However, substituting the relationship

Ryy(7) = Ryx(—7}

into Equation (1.68) and then using Equation (1.69) we find that Sxy(f) and Syx(f) are
related by

Sxy(f) = Syx(-f) = Syx(f) (1.72)

& EXAMPLE 1.8

Suppose that the random processes X(t) and Y(z) have zero mean, and they are individually
stationary. Consider the sum random process

Z(t) = X(t) + Y(n)

The problem is to determine the power spectral density of Z(t).

1.7 Power Spectral Density 53

The autocorrelation function of Z() is given by
Rz{t, u) = E[Z()Z(u)]
= Ef(X(t) + Y(#))(X(#) + Y(z))]
. = ElX(é)X(z)] + ELX(t)¥(w)] + E[Y(2)X(w)] + ELY(2)Y¥(x)]
= Rx(t, 4) + Ryylt, 4) + Ryx(t, 4) + Rylt, #)
Defining 7 = t ~ u, we may therefore write
Rz(t) = Rx(z) + Rxylt) + Ryx(7) + Ry(7) (1.73)
when the random processes X(t) and Y(¢) are also jointly stationary. Accordingly, taking the
Fourier transform of both sides of Equation (1.73), we get
S2{f) = Sx(f) + Sxv(f) + Syx(f) + Sy(f) (1.74)

We thus see that the cross-spectral densities Sxy(f) and Syx(f) represent the spectral compo-
nents that must be added to the individual power spectral densities of a pair of correlated
random processes in order to obtain the power spectral density of their sum.

When the stationary processes X(t) and Y(t) are uncorrelated, the cross-spectral densities
Sxy(f) and Syx(f) are zero, and so Equation (1.74) reduces as follows:

Salf) = Sx{f} + Sy(f) (1.75)

We may generalize this latter result by stating that when there is a multiplicity of zero-mean
stationary processes that are uncorrelated with each other, the power spectral density of their
sum is equal to the sum of their individual power spectral densities. <

 

&® EXAMPLE 1.9

Consider next the problem of passing two jointly stationary processes through a pair of sep-
arate, stable, linear, time-invariant filters, as shown in Figure 1.12. In particular, suppose that
the random process X(¢) is the input to the filter of impulse response /7,(t) and that the random
process Y(f) is the input to the filter of impulse response h(t). Let V(z) and Z(t) denote the
random processes at the respective filter outputs. The cross-correlation function of V(¢) and
Z(t) is therefore

Ryalt, w) = E[V(a)Z{u)]

= elf hy(7) X(t — %) dr ia h(m)¥(u — 7) ar]

ope (1.76)
= i if hy(t)bo( m)E[X(t — 71) Y(t — m)] dn dr

= in [. b(t )holtm)Revlt — Ty # — %) dry dry
where Rxy(#, u) is the cross-correlation function of X(t) and Y(z). Because the input random
processes are jointly stationary (by hypothesis), we may set r= ¢ — # and so rewrite Equation

(1.76) as follows:

Ryz(t) = [. [. by(t)ha(m)Raylt — 1 + 2) dry dr (1.77)

wo hO Lev YQ 0 LZ)

Ficure 1.12 A pair of separate linear time-invariant filters.

 

 

 

 

 

54 CHAPTER 1 # RANDOM PROCESSES

Taking the Fourier transform of both sides of Equation (1.77) and using a procedure
similar to that which led to the development of Equation (1.39), we finally get

Sv2lf) = Ai f)Hi(f)Sxv(f) (1.78)

where H,(f) and H,(f) are the frequency responses of the respective filters in Figure 1.12, and
H3(f) is the complex conjugate of H,(f). This is the desired relationship berween the cross-
spectral density of the output processes and that of the input processes. <4

i 1.8 Gaussian Process

The material we have presented on random processes up to this point in the discussion
has been of a fairly general nature. In this section, we consider an important family of
random processes known as Gaussian processes.*

Let us suppose that we observe a random process X(f) for an interval that starts at
time t = 0 and lasts until t = T. Suppose also that we weight the random process X(t) by
some function g(t) and then integrate the product g(t)X(t) over this observation interval,
thereby obtaining a random variable Y defined by

Y= I gle)X(t) dt (1.79)

We refer to Y as a linear functional of X(t). The distinction between a function and a
functional should be carefully noted. For example, the sum Y = =“,a;X;, where the a; are
constants and the X; are random variables, is a linear fection of the X;; for each observed
set of values for the random variables X;, we have a corresponding value for the random
variable Y. On the other hand, in Equation (1.79) the value of the random variable Y
depends on the course of the argument function g(t)X(t) over the entire observation in-
terval from 0 to T. Thus a functional is a quantity that depends on the entire course of
one or more functions rather than on a number of discrete variables. In other words, the
domain of a functional is a set or space of admissible functions rather than a region of a
coordinate space.

If in Equation (1.79) the weighting function g(t) is such that the mean-square value
of the random variable Y is finite, and if the random variable Y is a Gaussian-distributed
random variable for every g(£) in this class of functions, then the process X(£) is said to be
a Gaussian process. In other words, the process X(t) is a Gaussian process if every linear
functional of X(t) is a Gaussian random variable.

We say that the random variable Y has a Gaussian distribution if its probability
density function has the form

 

 

1 (y = By)?
frly) Vines op Ja3 (1.80)

where jy is the mean and o¥ is the variance of the random variable Y. A plot of this
probability density function is given in Figure 1.13 for the special case when the Gaussian
random variable Y is normalized to have a mean py of zero and a variance a? of one, as
shown by

 

_1 _¥
fr(y) ™ Feo ")

Such a normalized Gaussian distribution is commonly written as N(0, 1).

L8 Gaussian Process 55

FY)

0,6 --

 

 

 

Ficure 1.13 Normalized Gaussian distribution.

A Gaussian process has two main virtues. First, the Gaussian process has many
properties that make analytic results possible; we will discuss these properties later in the
section. Second, the random processes produced by physical phenomena are often such
that a Gaussian model is appropriate. Furthermore, the use of a Gaussian model to describe
the physical phenomena is usually confirmed by experiments. Thus the frequent occurrence
of physical phenomena for which a Gaussian model is appropriate, together with the ease
with which a Gaussian process is handled mathematically, make the Gaussian process very
important in the study of communication systems.

@ CENTRAL Limir THEOREM

The central limit theorem provides the mathematical justification for using a Gaussian
process as a model for a large number of different physical phenomena in which the
observed random variable, at a particular instant of time, is the result of a large number
of individual random events. To formulate this important theorem, let X,,i = 1,2,...,
N, be a set of random variables that satisfies the following requirements:

1. The X; are statistically independent.

2. The X; have the same probability distribution with mean px and variance 7%.

The X; so described are said to constitute a set of independently and identically distributed
(i.i.d.) random variables. Let these random variables be normalized as follows:

¥,=+(%)- px), i=1,2,...,N
ox
so that we have
EY] = 0

and

Define the random variable

56

CHAPTER 1 3 RANDOM PROCESSES

The central limit theorem states that the probability distribution of Vx approaches a nor-
malized Gaussian distribution N(0, 1) in the limit as the number of random variables N
approaches infinity.

It is important to realize, however, that the central limit theorem gives only the
“limiting” form of the probability distribution of the normalized random variable Vx as
N approaches infinity. When N is finite, it is sometimes found that the Gaussian limit gives
a relatively poor approximation for the actual probability distribution of Vx even though
N may be quite large.

PROPERTIES OF A GAUSSIAN PROCESS

A Gaussian process has some useful properties that are described in the sequel.

Property 1
If a Gaussian process X(t) is applied to a stable linear filter, then the random process Y(t)
developed at the output of the filter is also Gaussian.

This property is readily derived by using the definition of a Gaussian process based
on Equation (1.79). Consider the situation depicted in Figure 1.8, where we have a linear
time-invariant filter of impulse response 4(t), with the random process X(t) as input and
the random process Y(t) as output. We assume that X(t) is a Gaussian process. The random
processes Y(t) and X(t) are related by the convolution integral

n= [pie nx 1) dt, 0O=<t< (1.81)

We assume that the impulse response /(¢) is such that the mean-square value of the output
random process Y(¢) is finite for all ¢ in the range 0 = ¢ < » for which Y(z) is defined. To
demonstrate that the output process Y(t} is Gaussian, we must show that any linear func-
tional of it is a Gaussian random variable. That is, if we define the random variable

Z= [ev [pe ~ ox 1) dr dt (1.82)

then Z must be a Gaussian random variable for every function gy(t), such that the mean-
square value of Z is finite. Interchanging the order of integration in Equation (1.82), we
get

Z= [. etoxi X(7) (1.83)

where

= if gy(t)b(t — 1) dr (1.84)

Since X(t) is a Gaussian process by hypothesis, it follows from Equation (1.83) that Z
must be a Gaussian random variable. We have thus shown that if the input X(t) to a linear
filter is a Gaussian process, then the output Y(t) is also a Gaussian process. Note, however,

1.8 Gaussian Process 57

that although our proof was carried out assuming a time-invariant linear filter, this prop-
erty is true for any arbitrary stable linear system.

Property 2

Consider the set of random variables or samples X(t,), X(t2),..., X(t,), obtained by
observing a random process X(t) at times ty, th,..., t,. If the process X(t) is Gaussian,
then this set of random variables is jointly Gaussian for any n, with their n-fold joint
probability density function being completely determined by specifying the set of means

Bx) = E[X(@)], i= 1,2,...,0

and the set of covariance functions
Cyx(tz, t) = EU(X(t) — Bx) X(t) — xe), k,i=1,2,...,0

Let the #-by-1 vector X denote the set of random variables X(t,), ..., X(t,,) derived from
the Gaussian process X(t) by sampling it at times t,,..., t,. Let x denote a value of X.
According to Property 2, the random vector X has a multivariate Gaussian distribution
defined in matrix form as

 

 

1 1
Foxit) os X()(¥1y + + + » Xa) (2m)"2ar2 exo 3 (x — pyZix - ») (1.85)

where the superscript T denotes transposition and
p = mean vector
_ T
a [M1 May +++ Mn
Y = covariance matrix
= {Cx(te, teins
x1 = invetse of covariance matrix

A = determinant of covariance matrix ¥

Property 2 is frequently used as the definition of a Gaussian process. However, this
definition is more difficult to use than that based on Equation (1.79) for evaluating the
effects of filtering on a Gaussian process.

We may extend Property 2 to two (or more) random processes as follows. Consider
the composite set of tandom variables X(t), X(t),..., X(t), Y(#1), Y(u2),...5 Y(4,)
obtained by observing a random process X(t) at times {t,, i= 1,2,..., 7}, and a second
random process Y(z) at times {#,, k = 1, 2,..., mm}. We say that the processes X(t) and
Y(t) are jointly Gaussian if this composite set of random variables is jointly Gaussian for
any # and m, Note that in addition to the mean and correlation functions of the random
processes X(t) and Y(t) individually, we must also know the cross-covariance function

EU(X(t) — bextep) (Y(t) — Myegs)] = Ryuvltis te) — Moxiepb vey)
for any pair of observation instants (t;, #,). This additional knowledge is embodied in the
cross-correlation function, Ryy(t;, 4,), of the two processes X(t) and Y(t).
Property 3

If a Gaussian process is stationary, then the process is also strictly stationary.

This follows directly from Property 2.

58 CHAPTER 1 @ RANDOM PROCESSES

Property 4

If the random variables X(t,), X(to),..., X(t,), obtained by sampling a Gaussian process
X(t) at times ty, tz, ..., t,, are uncorrelated, that is,

E((X(t) — pxiy)(X(E) — exuy)] = 9, i#Xk
then these random variables are statistically independent.

The uncorrelatedness of X(t,),..., X(t,,) means that the covariance matrix & is a
diagonal matrix as shown by

of te)

where °
of = E[(X(t) - E[X(t)))", 7 = 1,2,...50

Under this condition, the multivariate Gaussian distribution of Equation (1.85) simplifies
to

where X; = X(¢,) and

 

fale) = a= exp( - &
xA%; Ino, ‘P 2a?

In words, if the Gaussian random variables X(¢,),..., X(t,,) are uncorrelated, then they
are statistically independent, which, in turn, means that the joint probability density func-
tion of this set of random variables can be expressed as the product of the probability
density functions of the individual random variables in the set.

i 1.9 Noise

The term noise is used customarily to designate unwanted signals that tend to disturb the
transmission and processing of signals in communication systems and over which we have
incomplete control. In practice, we find that there are many potential sources of noise in
a communication system. The sources of noise may be. external to the system (e.g., at-
mospheric noise, galactic noise, man-made noise), or internal to the system. The second
category includes an important type of noise that arises from spontaneous fluctuations of
current or voltage in electrical circuits.® This type of noise represents a basic limitation on
the transmission or detection of signals in communication systems involving the use of
electronic devices. The two most common examples of spontaneous fluctuations in elec-
trical circuits are shot noise and thermal noise, which are described in the sequel.

@ SHOT NOISE

Shot noise arises in electronic devices such as diodes and transistors because of the discrete
nature of current flow in these devices. For example, in a photodetector circuit a current

19 Noise 59

pulse is generated every time an electron is emitted by the cathode due to incident light
from a source of constant intensity. The electrons are naturally emitted at random times
denoted by 7, where ~% < k < &, It is assumed that the random emissions of electrons
have been going on for a long time. Thus, the total current flowing through the photo-
detector may be modeled as an infinite sum of current pulses, as shown by

X= S bt - x) (1.86)
ae
where A(t — 7) is the current pulse generated at time 7. The process X(t) defined by
Equation (1.86) is a stationary process called shot noise.

The number of electrons, N(t), emitted in the time interval [0, ¢] constitutes a discrete
stochastic process, the value of which increases by one each time an electron is emitted.
Figure 1.14 shows a sample function of such a process, Let the mean value of the number
of electrons, v, emitted between times ¢ and t + fo be defined by

E[v] = Ato (1.87)

The parameter A is a constant called the rate of the process. The total number of electrons
emitted in the interval [t, ¢ + ty], that is,
p= N(t + to) — N(t)
follows a Poisson distribution with a mean value equal to Afy. In particular, the probability
that & electrons are emitted in the interval [¢, t + fo] is defined by
1)
P(v = k) = re eM k=O, 1... (1.88)

Unfortunately, a detailed statistical characterization of the shot-noise process X(t)
defined in Equation (1.86) is a difficult mathematical task. Here we simply quote the results
pertaining to the first two moments of the process:

» The mean of X(z) is
By =A [ h(t) dt (1.89)

where A is the rate of the process and /(t) is the waveform of a current pulse.

 

 

 

N(t)
6k —
oo at =
4 —
) |
| ———!
3 i 1 |
i |
i
2k cS |
| \
| i
1+ oo | i
hod i
hbo i
a

0 T% «% Ts T] Ts t¢

Ficure 1.14 Sample function of a Poisson counting process.

60

CHAPTER 1 8 RANDOM PROCESSES

» The autocovariance function of X(t) is
Cxy(7}) =A [ h(t)b(t + 7) dt (1.90)

This second result is known as Campbell’s theorem.

For the special case of a waveform h(f) consisting of a rectangular pulse of amplitude
A and duration T, the mean of the shot-noise process X(z) is AAT, and its autocovariance
function is

MAXT— |r|), [7] <T

Cal) = {0 |r| =T

which has a triangular form similar to that shown in Figure 1.7.

THERMAL NOISE

Thermal noise is the name given to the electrical noise arising from the random motion of
electrons in a conductor. The mean-square value of the thermal noise voltage Vr ap-
pearing across the terminals of a resistor, measured in a bandwidth of Af Hertz, is, for all
practical purposes, given by

E[V3nx] = 4&TR Af volts? (1.91)

where k is Boltzmann’s constant equal to 1.38 X 1077? joules per degree Kelvin, T is the
absolute temperature in degrees Kelvin, and R is the resistance in ohms. We may thus
model a noisy resistor by the Thévenin equivalent circuit consisting of a noise voltage
generator of mean-square value E[V#y] in series with a noiseless resistor, as in Figure
1.15¢. Alternatively, we may use the Norton equivalent circuit consisting of a noise current
generator in parallel with a noiseless conductance, as in Figure 1.156. The mean-square
value of the noise current generator is

2 = 1 2.
Ellin] = 3 ElVin (1.92)
= 4kTG Af amps”
where G = 1/R is the conductance. It is also of interest to note that because the number
of electrons in a resistor is very large and their random motions inside the resistor are
statistically independent of each other, the central limit theorem indicates that thermal
noise is Gaussian distributed with zero mean.

ELtRy] G

ElVin

(a) (b)

Figure 1.15 Models of a noisy resistor. (2) Thévenin equivalent circuit. (b) Norton equivalent
circuit.

1.9 Noise 61

Noise calculations involve the transfer of power, and so we find that the use of the
maximum-power transfer theorem is applicable to such calculations. This theorem states
that the maximum possible power is transferred from a source of internal resistance R to
a load of resistance R; when R; = R. Under this matched condition, the power produced
by the source is divided equally between the internal resistance of the source and the load
resistance, and the power delivered to the load is referred to as the available power. Ap-
plying the maximum-power transfer theorem to the Thévenin equivalent circuit of Figure
1.154 or the Norton equivalent circuit of Figure 1.156, we find that a noisy resistor pro-
duces an available noise power equal to kT Af watts.

e Waite NOISE

The noise analysis of communication systems is customarily based on an idealized form
of noise called white noise, the power spectral density of which is independent of the
operating frequency. The adjective white is used in the sense that white light contains equal
amounts of all frequencies within the visible band of electromagnetic radiation. We express
the power spectral density of white noise, with a sample function denoted by w(t), as

N
Sw(f) = > (1.93)
which is illustrated in Figure 1.164. The dimensions of No are in watts per Hertz. The
parameter N, is usually referenced to the input stage of the receiver of a communication
system. Jt may be expressed as

No = kT, (1.94)

where & is Boltzmann’s constant and T, is the equivalent noise temperature of the receiver.”
The equivalent noise temperature of a system is defined as the temperature at which a
noisy resistor has to be maintained such that, by connecting the resistor to the input of a
noiseless version of the system, it produces the same available noise power at the output
of the system as that produced by all the sources of noise in the actual system. The im-
portant feature of the equivalent noise temperature is that it depends only on the param-
eters of the system.

Since the autocorrelation function is the inverse Fourier transform of the power
spectral density, it follows that for white noise

= No

Rela) = 5

&(7) (1.95)

Swf) Ry)

N,
Ng 3 &(r)

 

 

ie} . ie}
(a) ()

Figure 1.16 Characteristics of white noise. (2) Power spectral density. (b) Autocorrelation
function.

62

CHAPTER 1 RANDOM PROCESSES

That is, the autocorrelation function of white noise consists of a delta function weighted
by the factor No/2 and occurring at t = 0, as in Figure 1.166. We note that Ry(7) is zero
for + # 0. Accordingly, any two different samples of white noise, no matter how closely
together in time they are taken, are uncorrelated. If the white noise w(t) is also Gaussian,
then the two samples are statistically independent. In a sense, white Gaussian noise rep-
resents the ultimate in “randomness.”

Strictly speaking, white noise has infinite average power and, as such, it is not phys-
ically realizable. Nevertheless, white noise has simple mathematical properties exemplified
by Equations (1.93) and (1.95), which make it useful in statistical system analysis.

The utility of a white noise process is parallel to that of an impulse function or delta
function in the analysis of linear systems. Just as we may observe the effect of an impulse
only after it has been passed through a system with a finite bandwidth, so it is with white
noise whose effect is observed only after passing through a similar system. We may state,
therefore, that as long as the bandwidth of a noise process at the input of a system is
appreciably larger than that of the system itself, then we may model the noise process as
white noise.

® EXAMPLE 1.10 Ideal Low-Pass Filtered White Noise

Suppose that a white Gaussian noise w(t) of zero mean and power spectral density No/2 is
applied to an ideal low-pass filter of bandwidth B and passband magnitude response of one.
The power spectral density of the noise (£) appearing at the filter output is therefore (see

Figure 1.17)
No
Sif)= 4? “BSF <8 (1.96)
0, |f|>B

The autocorrelation function of v(t) is the inverse Fourier transform of the power spectral
density shown in Figure 1.174:

B
No .
Rat) = “= exp(j2af7) df
J “B2 (1.97)
= NoB sinc(2Br)

This autocorrelation function is plotted in Figure 1.17. We see that Rx(7) has its maximum
value of NoB at the origin, and it passes through zero at 7 = +k/2B, where k = 1, 2, 3,---.

Sw) Ry(s)
2 NoB

    

 

-B 0 B ‘1. 3
B 2B

 

{a) 0)

Ficure 1.17 Characteristics of low-pass filtered white noise. (2) Power spectral density. (b) Auto-
correlation function.

1.9 Noise 63

Since the input noise w(t) is Gaussian (by hypothesis), it follows that the band-limited
noise 7(t) at the filter output is also Gaussian. Suppose now that 7(t) is sampled at the rate of
2B times per second. From Figure 1.176, we see that the resulting noise samples are uncor-
related and, being Gaussian, they are statistically independent. Accordingly, the joint proba-
bility density function of a set of noise samples obtained in this way is equal to the product
of the individual probability density functions. Note that each such noise sample has a mean
of zero and variance of NoB. <j

m Examece 1.11 Correlation of White Noise with a Sinusoidal Wave

Consider the sample function

+

w'(t) = a) w(t) cos(2f-t) dt (1.98)
which is the output of a correlator with white Gaussian noise w(t) and sinusoidal wave
V2/T cos(27f.t) as inputs; the scaling factor V2/T is included here to make the sinusoidal
wave input have unit energy over the interval 0 <= ¢ = T. (This problem was encountered in
the Background and Preview chapter but was not elaborated on at that time.) With the noise

w(t) having zero mean, it immediately follows that the correlator output tw’ (t) has zero mean,
too. The variance of the correlator output is defined by

2 (T(t
G=E [25 [ w{t,) cos(2zf,t,)w(t2) cos(Zaf.to) dt, at, |
=2 i ‘ i "E t 2 2 dt, d
=F Io Jo [vo(ty)w(t2)] cos(2rf-t,) cos(2af.t,) dt, dt,

2 T rT
= =[ [ Ryty, t2)-cos(27f,t)) cos(2aft,) dt, dt,

where Ryw(t,, t2) is the autocorrelation function of the white noise w(t). But from Equation
(1.95):

N,
Rwlt, b) = > Alt, — t3)

where No/2 is the power spectral density of the white noise w(t). Accordingly, we may simplify
the expression for the variance o” as

aN f' S's — t) cos(2af,t,) cos(2af,t2) dt, dt.
= 3 Po Jo i 2) cos(2af,t1) cos(2af.tz) dt, dt,
We now invoke the sifting property of the delta function, namely,

in g(t) 5(t) dt = g(0)

where g(t) is a continuous function of time, assuming the value g(0) at time t = 0. Hence, we
may further simplify o” as

N 2,7,
= 2.5 d
a 7 Fd, 98 (Qaf.t) dt (1.99)
No
2

where it is assumed that the frequency f, of the sinusoidal wave input is an integer multiple
of the reciprocal of T.

64 CHAPTER I RANDOM PROCESSES

Sy(f)

 

Sf.-B fe Sf. +B 0 F.-B fp fp +B

 

(a)

FIGURE 1.18 (a) Power spectral density of narrowband noise. (b) Sample function of narrow-
band noise.

: 1.10 Narrowband Noise

The receiver of a communication system usually includes some provision for preprocessing
the received signal. The preprocessing may take the form of a narrowband filter whose
bandwidth is just large enough to pass the modulated component of the received signal
essentially undistorted but not so large as to admit excessive noise through the receiver.
The noise process appearing at the output of such a filter is called narrowband noise. With
the spectral components of narrowband noise concentrated about some midband fre-
quency +f, as in Figure 1.182, we find that a sample function n(¢) of such a process appears
somewhat similar to a sine wave of frequency f,, which undulates slowly in both amplitude
and phase, as illustrated in Figure 1.185.

To analyze the effects of narrowband noise on the performance of a communication
system, we need‘a mathematical representation of it. Depending on the application of
interest, there are two specific representations of narrowband noise:

1. The narrowband noise is defined in terms of a pair of components called the in-phase
and quadrature components.

2. The narrowband noise is defined in terms of two other components called the en-
velope and phase.

These two representations are described in what follows. For now it suffices to say that
given the in-phase and quadrature components, we may determine the envelope and phase
components, and vice versa: Moreover, in their own individual ways, the two represen-
tations are not only basic to the noise analysis of communication systems but also to the
characterization of narrowband noise itself.

1.11 Representation of Narrowband Noise
in Terms of In-Phase and Quadrature Components

 

 

Consider a narrowband noise n(z) of bandwidth 2B centered on frequency f,, as illustrated
in Figure 1.18. In light of the theory of band-pass signals and systems presented in Ap-
pendix 2, we may represent 7(¢) in the canonical (standard) form:

n(t) = n,(t) cos(2mf.t) — o(t) sin(2af,t) (1.100)

1.11 Ie-Phase and Quadrature Components 65

where x,(t) is called the in-phase component of n(t), and no(t) is called the quadrature
component of n(t). Both m,(t) and mo(t) are low-pass signals. Except for the midband
frequency f,, these two components are fully representative of the narrowband noise n(t}.

Given the narrowband noise x(t), we may extract its in-phase and quadrature com-
ponents using the scheme shown in Figure 1.194. It is assumed that the two low-pass filters
used in this scheme are ideal, each having a bandwidth equal to B (i.e., one-half the band-
width of the narrowband noise n(t}). The scheme of Figure 1.194 follows from the rep-
resentation of Equation (1.100). We may, of course, use this equation directly to generate
the narrowband noise x(¢), given its in-phase and quadrature components, as shown in
Figure 1.196, The schemes of Figures 1.194 and 1.196 may thus be viewed as narrowband
noise analyzer and synthesizer, respectively.

The in-phase and quadrature components of a narrowband noise have important
properties that are summarized here:

1, The in-phase component m,(t) and quadrature component no(t) of narrowband noise
n(t) have zero mean.

2. If the narrowband noise »(t) is Gaussian, then its in-phase component n;(t) and
quadrature component Q(t) are jointly Gaussian.

3. If the narrowband noise x(t) is stationary, then its in-phase component n;(t) and
quadrature component Q(t) are jointly stationary.

4. Both the in-phase component #,(¢) and quadrature component 79(t) have the same
power spectral density, which is related to the power spectral density Sy(f) of the
narrowband noise a(t) as

 

Suif ~ fe} + Swf + fs B=f=B

1.101
0, otherwise ( )

Suf) = Syolf) = {
where it is assumed that Sx(f) occupies the frequency interval f. - B=|f|<f,+B,
and f, > B.

5. The in-phase component #;(t) and quadrature component 79(t) have the same vari-
ance as the narrowband noise n(t).

6. The cross-spectral density of the in-phase and quadrature components of narrow-
band noise x(t) is purely imaginary, as shown by

Sunglf) = —Snon(f) (1.102)
ISMf +f.) -SNf-—f), —-BsfsB

0, otherwise

+

 
 

Low-pass
filter

  

nytt) y(t)

  
  
 
     
 

nt) 2 cos (2mf, 1) cos (27rf.2} nd)
Low-pass t ®
fitter ng) agit
~2 sin (2nf,i) sin (2af..2)
(a) {b)

Figure 1.19 (a) Extraction of in-phase and quadrature components of a narrowband process.
(b) Generation of a narrowband process from its in-phase and quadrature components.

66 CHAPTER 1 © RANDOM PROCESSES

7, If the narrowband noise v(t) is Gaussian and its power spectral density Sy(?) is sym-
metric about the mid-band frequency f,, then the in-phase component #;(t) and
quadrature component 7o(t) ate statistically independent.

For further discussions of these properties, the reader is referred to Problems 1.28 and
1.29.

® EXAMPLE 1.12 Ideal Band-Pass Filtered White Noise

Consider a white Gaussian noise of zero mean and power spectral density No/2, which is
passed through an ideal band-pass filter of passband magnitude response equal to one, mid-
band frequency f., and bandwidth 2B. The power spectral density characteristic of the filtered
noise #(t) will therefore be as shown in Figure 1.20¢. The problem is to determine the auto-
correlation functions of (t) and its in-phase and quadrature components.

The autocorrelation function of (¢) is the inverse Fourier transform of the power spec-
tral density characteristic shown in Figure 1.20a:

ft B No . ff tB No .

Ry(7) = Lo. 2 exp(j2afr) df + Ios > exp(j2afr) df
= NpB sine(2B1)[exp(—j2-af-7) + exp(i2nf7 (1.103)
== 2NoB sinc(2Br) cos(27f,7)

which is plotted in Figure 1.206.

The spectral density characteristic of Figure 1.20a is symmetric about +f.. Therefore,
we find that the corresponding spectral density characteristic of the in-phase noise component

 

  
     

Tt 0 fe m
2B

(@)

Sy, (f= Sy ff)

No

 

 

 

-B O B

 

(b) {ec}

FIGURE 1.20 Characteristics of ideal band-pass filtered white noise. (2) Power spectral density.
(b) Autocorrelation function. (c) Power spectral density of in-phase and quadrature components.

1.12 Envelope and Phase Components 67

n,(t) or the quadrature noise component #o(¢) is as shown in Figure 1.21c. The autocorrelation
function of 7;() or not) is therefore (see Example 1.10):

Rut) = Ry, (7) = 2NoB sinc(2B7) (1.104)

4
1.12 Representation of Narrowband Noise
| in Terms of Envelope and Phase Components

In Section 1.11 we considered the representation of a narrowband noise n(£) in terms of
its in-phase and quadrature components. We may also represent the noise m(t) in terms of
its envelope and phase components as follows:

n(t) = r(t}) cos[2af.t + ¥(2)] (1.105)
where
r(t) = [pi (t) + np(e)]’? (1.106)
and
= tan-1| Za)
y(t) = tan | ale | (1.107)

The function r(t) is called the envelope of n(t), and the function s(t) is called the phase of
nit).

The envelope r(t) and phase y(¢) are both sample functions of low-pass random
processes. As illustrated in Figure 1.18), the time interval between two successive peaks
of the envelope r(t) is approximately 1/B, where 2B is the bandwidth of the narrowband
noise n(t).

The probability distributions of 7(t) and #(t) may be obtained from those of 7,(t)
and no(t) as follows. Let N; and Ng denote the random variables obtained by observing
(at some fixed time) the random processes represented by the sample functions n;(t) and
no(t), respectively. We note that N; and No are independent Gaussian random variables
of zero mean and variance o*, and so we may express their joint probability density func-
tion by

 

1 nt + nh
fryng(tr Ng) = Imot exo( - 32 (1.108)
Accordingly, the probability of the joint event that N; lies between 2, and n, + dn; and
that Ng lies between mg and #g + dig (i.e., the pair of random variables N, and Ng lies
jointly inside the shaded area of Figure 1.212) is given by

1 ne + ne
Fix,nol"» MQ) dn; dng = Ino® en 8) dn; dng (1.109)
Define the transformation (see Figure 1.212)
ny, = r cosy (1.110)
Ao =r sings - (1.111)

In a limiting sense, we may equate the two incremental areas shown shaded in Figures
1.21¢@ and 1.21b and thus write

dn, dng = 1 dr di (1.112)

68

CHAPTER 1 @ RANDOM PROCESSES

 

 

 

 

 

—>| day; Na
im dr
ang | a

i) nna” + 0 eens eae”
|

| |

” i rool

i

]

aby i

% #\,
0 nr ie] Ay

{a) (b)

FIGURE 1.21 Illustrating the coordinate system for representation of narrowband noise: (a) in
terms of in-phase and quadrature components, and (b) in terms of envelope and phase.

Now, let R and ¥ denote the random variables obtained by observing (at some time f) the
random processes represented by the envelope 7(¢) and phase #/(¢), respectively. Then,
substituting Equations (1.110)-(1.112) into (1.109), we find that the probability of the
random variables R and lying jointly inside the shaded area of Figure 1.21b is equal to

(ex dr d
Imo Pag} ¥
That is, the joint probability density function of R and ¥ is

r r?
faovlt, ¥) = yO exp(- 5) (1.113)

This probability density function is independent of the angle #, which means that the
tandom variables R and W are statistically independent. We may thus express fa,y(7, ¥)
as the product of fg(r) and fy(ip). In particular, the random variable V representing phase
is uniformly distributed inside the range 0 to 27, as shown by

1

fol) 412g? OS USA (1.114)
0, elsewhere
This leaves the probability density function of the random variable R as
r 7 =0
felr) = Jer OP aot (1.115)
0, elsewhere

where a” is the variance of the original narrowband noise n(¢). A random variable having
the probability density function of Equation (1.115) is said to be Rayleigh distributed.’
For convenience of graphical presentation, let

v= (1.116)

;
tog
fulv) = ofglr) (1.117)

1.13 Sine Wave Plus Narrowband Noise 69

 

 

0 1 2 3

FicureE 1.22 Normalized Rayleigh distribution.

Then we may rewrite the Rayleigh distribution of Equation (1.115) in the normalized form

vy
fo(v) = ver(-5), veo (1.118)
0, elsewhere

Equation (1.118) is plotted in Figure 1.22. The peak value of the distribution fy(v) occurs
at v=1 and is equal to 0.607. Note also that, unlike the Gaussian distribution, the Rayleigh
distribution is zero for negative values of v. This is because the envelope 7(t) can assume
only nonnegative values.

i 1.13 Sine Wave Plus Narrowband Noise

Suppose next that we add the sinusoidal wave A cos(27f,t) to the narrowband noise n(t),
where A and f, are both constants. We assume that the frequency of the sinusoidal wave
is the same as the nominal carrier frequency of the noise. A sample function of the sinu-
soidal wave plus noise is then expressed by

x(t) = A cos(2mf.t) + v(t) (1.119)

Representing the narrowband noise x(t) in terms of its in-phase and quadrature compo-
nents, we may write

 

 

 

x(t) = n}(t) cos(2rf.t) — molt) sin(27f,2) (1.120)
where
ni(t) = A + n,(t) (1.121)
We assume that a(t) is Gaussian with zero mean and variance o*. Accordingly, we may
state the following:
1. Both 7;(t) and Q(t) are Gaussian and statistically independent.
2. The mean of #;(t) is A and that of #9(t) is zero.
3. The variance of both #/(t) and no(t) is o°.

We may therefore express the joint probability density function of the random variables
Ni and No, corresponding to #}(t) and #o(t), as follows:

1 ni ~ AP + n2
fri.nglth 9) = 5 exp| (ni aT 8 (1.122)

 

 

70

CHAPTER 1 & RANDOM PROCESSES

Let r(#) denote the envelope of x(t) and y(t) denote its phase. From Equation (1.120),
we thus find that

r(t) = {[ni(e)P + m(e)y? (1.123)
and
w(t) = en | 20 | (1.124)
y(t)

Following a procedure similar to that described in Section 1.12 for the derivation of the
Rayleigh distribution, we find that the joint probability density function of the random
variables R and’, corresponding to r(t) and y(t) for some fixed time #, is given by

2+ A? — 2A
fault, ¥) = i exp( PS Brent)

(1.125)
We see that in this case, however, we cannot express the joint probability density function
fr vlr, #) asa product fa(7)fu(W). This is because we now have a term involving the values
of both random variables multiplied together as 7 cos ¥. Hence, R and W are dependent
random variables for nonzero values of the amplitude A of the sinusoidal wave component.

We are interested, in particular, in the probability density function of R. To determine
this probability density function, we integrate Equation (1.125) over all possible values of
Ww obtaining the marginal density

ar

falr) = 0 frawlt, W) dp

24 A2\ 2" A
one °xP (-" 2 ) j &xP (4 £os v) ay

The integral in the right-hand side of Equation (1.126) can be identified in terms of the
defining integral for the modified Bessel function of the first kind of zero order (see Ap-
pendix 3); that is,

(1.126)

 

 

1 ag
I(x) = on [, exp(x cosy) di (1.127)

Thus, letting x = Ar/o”, we may rewrite Equation (1.126) in the compact form:

felt) =, exp ae *Yio(45) (1.128)

This relation is called the Rician distribution.”
As with the Rayleigh distribution, the graphical presentation of the Rician distribu-
tion is simplified by putting

 

 

var (1.129)
a= A (1.130)

ren
folv) = ofzl(r) (1.131)

1.14 Computer Experiments: Flat-Fading Channel 71

0.6 a=0

Fy)

 

 

 

 

FIGURE 1,23 Normalized Rician distribution.

Then we may express the Rician distribution of Equation (1.128) in the normalized form
2

folv) = ver(-¥ ; 2 Yiotaw (1.132)

 

which is plotted in Figure 1.23 for the values 0, 1, 2, 3, 5, of the parameter a. Based on
these curves, we may make the following observations:

1, When a is zero, the Rician distribution reduces to the Rayleigh distribution.

2. The envelope distribution is approximately Gaussian in the vicinity of v= a when a
is large, that is, when the sine-wave amplitude A is large compared with o, the square
root of the average power of the noise m(¢).

1.14 Computer Experiments:

Flat-Fading Channel
In this section we use computer simulations to study a multipath channel characterized by
Rayleigh fading, examples of which arise in wireless communications and long-range radio
transmission via the ionosphere. Fading occurs because of interference between different
versions of the transmitted signal, which reach the receiver at correspondingly different
times. The net result is that the received signal can vary widely in both amplitude and
phase. Under certain conditions, the statistical time-varying nature of the received signal’s
envelope is closely described by a Rayleigh distribution as demonstrated herein.

Figure 1.24 presents a model of a multipath channel. It consists of a large collection
of scatterers randomly positioned in space, whereby a single incident beam is converted
into a correspondingly large number of scattered beams at the receiving antenna. The
transmitted signal is set equal to A cos(27f-t). It is assumed that all the scattered beams
travel at the same mean velocity. However, they differ from each other in amplitude and
phase by virtue of differences in path loss and path delay. Thus the Ath scattered beam is

>given by A, cos(27f,t + @,), where the amplitude A, and phase @, are random variables
that vary slowly with time. Moreover, the ©, are all independent of one another and

 

 

 

 

 

72

CHapTer 1 RANDOM PROCESSES

      
 

    
   

 
 

Random
medium

Scattered
beams

Incident

 

Transmitting Receiving
antenna ° antenna

Ficure 1.24 Model of a multipath channel.

uniformly distributed inside the interval [0, 277]. The type of fading exhibited by the mul-
tipath channel described herein is referred to as “flat fading” because the spectral char-
acteristics of the transmitted signal are completely preserved at the channel output. How-
ever, the strength of the channel output changes with time due to random fluctuations in
the gain of the channel! caused by the multipath phenomenon.

Summing the contributions of all the scatterers, assumed to be N in number, we may
express the random process representing the received signal as

N
X(t) = >) A, cos(2af.t + @,) (1.33)
k=l
which may be rewriten in the equivalent form
X(t) = X; cos(2af,t) — Xg sin(2afZ) (1.134)
where the X; and Xq are respectively defined by
N
X, = >, A, cos Oy (1.135)
kat
and
N
Xo= 3D) A, sin @® (1.136)
é=1

For convenience of presentation and without loss of generality, we may assume that A,
lies in the closed interval [~—1, 1] for all z.

Experiment 1. Gaussian Distributions

From the central limit theorem we note that as the number of scatterers, N, approaches
infinity, both X; and Xg should approach Gaussian random variables. To test the validity
of this statement, the probability distributions of the in-phase component X; and quad-
rature component Xg are computed for N = 10, 100, 1000, and 10,000. To test the
validity of the central limit theorem, we need a measure of the goodness-of-fit that tests
the equivalence of the measured probability distribution of the sampled data for varying

1.14 Computer Experiments: Flat-Fading Channel 73

N to the theoretical Gaussian distribution. One way of performing such a test is to use
central moments of a distribution (up to order 4) to define the following two parameters:

2
U3
= 1.137
Pr ua ( )
and
f= (1.138)
He

where fz, Hs, and 14 are the second, third, and fourth central moments, respectively. The
parameters f, and > together provide a measure of the skezwness of the distribution under
test. The closer the values 8, and B, for the measured distribution are to the corresponding
ones for the theoretical distribution, the better is the goodness-of-fit. For a Gaussian ran-
dom variable X of mean px and variance o% we have

He = 0%
3 = 0
Ha = 30%
which yield
8, = 0
and
B= 3

Table 1.1 presents the values of 8, and 8, computed for both the in-phase component X;
and quadrature component Xg for varying N. Comparing these values with the corre-
sponding ones for a Gaussian distribution, we clearly see that as the number of scatterers,
N, increases the distributions of both X; and Xg do approach a zero-mean Gaussian
distribution in accordance with the central limit theorem.

| Taste 1.1 B Values for in-phase and quadrature components

(a) Measured Distribution

 

 

 

 

 

 

 

 

 

 

Number of Scatterers, N

 

 

 

 

10 100 1000 10,000
In-phase component X; Ba 0.2443 0.0255 0.0065 0.0003
fe 2.1567 2.8759 2.8587 3.0075

 

 

0.0874 0.0017 0.0004 0.0000

Bi
fo 1.9621 2.7109 3.1663 3.0135 |

(b) Theoretical Distribution: Gaussian
B, =0
B, = 3

 

Quadrature component Xo

 

 

 

 

 

 

 

 

 

74

CHAPTER 1 = RANDOM PROCESSES

Experiment 2. Rayleigh Distribution

In Equation (1.134) the random process X(t) is expressed in terms of its in-phase and
quadrature components. Equivalently, we may express X(¢) in terms of its envelope and
phase as

X(t) = R cos(2af,t + V) (1.139)
where
R= Vx? + XB (1.140)
and
Xx
= tan72{ (2
WY = tan (32) (1.141)

Note that in the experiments considered here the in-phase component X;, quadrature
component Xo, envelope R, and phase ¥ are all independent of time.

If X; and Xp approach Gaussian random variables for increasing N, then from the
theory presented in Section 1.12 we note that the envelope R will approach a Rayleigh
distribution, and the phase VW will approach a uniform distribution. In Figure 1.25 we
present the actual probability density function of r for data generated for the case of
N = 10,000, with 100 histograms and 100 ensemble averages being computed. This figure
also includes the theoretical curve. There is close agreement between these two curves,
substantiating the assertion that the envelope R of the received signal approaches a Ray-
leigh distribution.

Figure 1.26 illustrates the effect of Rayleigh fading on the waveform of the received
signal x(t}, a sample function of X(t), for the case of a sinusoidal transmitted signal with
unit amplitude (i.e., A = 1) and frequency f, = 1 MHz. Specifically, the transmitted signal
and the corresponding received signal are shown in parts a and b of Figure 1.26, respec-
tively. Comparing these two waveforms, we see that transmission through the multipath

 

9.774 Tt 4

0.6

 

 

 

 

FIGURE 1.25 Probability density function of the envelope of random process X(¢): comparing
theory and experiment.

1.15 Summary and Discussion 75

 

Amplitude
lo}

  

 

 

 

0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Time(s) x 10&

(a)

 

Amplitude

 

 

 

al
a
oa

=i
0 0.5 1 1.5 2 2.5 3 3.5
Time(s) x 108
(4)

FIGURE 1.26 Effect of Rayleigh fading on a sinusoidal wave. (a) Input sinusoidal wave.
(b) Waveform of the resulting signal.

channel of Figure 1,24 has resulted in a received signal whose amplitude and phase com-
ponents vary randomly with time, as expected.

[1.15 Summary and Discussion

Much of the material presented in this chapter has dealt with the characterization of a
particular class of random processes known to be stationary and ergodic. The implication
of (wide-sense) stationarity is that we may develop a partial description of a random
process in terms of two ensemble-averaged parameters: (1) a mean that is independent of
time, and (2) an autocorrelation function that depends only on the difference between the
times at which two observations of the process are made.’° Ergodicity enables us to use
time averages as “estimates” of these parameters. The time averages are computed using
a sample function (i.e., single realization) of the random process.

Another important parameter of a random process is the power spectral density. The
autocorrelation function and the power spectral density constitute a Fourier-transform
pair. The formulas that define the power spectral density in terms of the autocorrelation
function and vice versa are known as the Einstein-Wiener-Khintchine relations.

In Table 1.2 we present a graphical summary of the autocorrelation functions and
power spectral densities of important random processes. All the processes described in this
table are assumed to have zero mean and unit variance. This table should give the reader
a feeling for (1) the interplay between the autocorrelation function and power spectral
density of a random process, and (2) the role of linear filtering in shaping the autocorre-
lation function or, equivalently, the power spectral density of a white noise process.

The latter part of the chapter dealt with a noise process that is Gaussian and nar-
rowband, which is the kind of filtered noise encountered at the front end of an idealized
form of communication receiver. Gaussianity means that the random variable obtained by

76 CHAPTER | & RANDOM PROCESSES

densities of random processes of zero mean and unit variance

 

 

 

 

TABLE 1.2. Graphical summary of autocorrelation functions and power spectral

 

Type of Process, X(t)
Sinusoidal process of unit

frequency and random
phase

Random binary wave of unit
symbol-duration

RC low-pass filtered white
noise

Ideal low-pass filtered white
noise

Ideal band-pass filtered
white noise

RLC-filtered white noise

 

 

 

 

Autocorrelation Function, Rx(t)

Power Spectral Density, Sx(f)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2.9
~0.5 a 0.5
f
L
-0.5 0 05 f
0.5F
kL
i
“1.0 0 10. Ff

 

 

 

 

 

 

 

Notes and References 77

observing the output of the filter at some fixed time has a Gaussian distribution. The
narrowband nature of the noise means that it may be represented in terms of an in-phase
and a quadrature component. These two components are both low-pass, Gaussian pro-
cesses, each with zero mean and a variance equal to that of the original narrowband noise.
Alternatively, a Gaussian narrowband noise may be represented in terms of a Rayleigh-
distributed envelope and a uniformly distributed phase. Each of these representations has
its own specific area of application, as shown in subsequent chapters of the book.

i NoTES AND REFERENCES

1. For a rigorous treatment of random processes, sce the classic books of Doob (1953), Loéve
(1963), and Cramér and Leadbetter (1967).

2. There is another important class of random processes commonly encountered in practice,
the mean and autocorrelation function of which exhibit periodicity, as in

Hx(ty + T) = px(ty)
Rx(ty + T, th + T) = Rx(ty, tr)

for all t; and t,. A random process X(t) satisfying this pair of conditions is said to be
cyclostationary (in the wide sense). Modeling the process X(z) as cyclostationary adds a
new dimension, namely, period T to the partial description of the process. Examples of
cyclostationary processes include a television signal obtained by raster-scanning a random
video field, and a modulated process obtained by varying the amplitude, phase, or fre-
quency of a sinusoidal carrier. For detailed discussion of cyclostationary processes, see
Pranks (1969), pp. 204-214, and the paper by Gardner and Franks (1975).

3. Traditionally, Equations (1.42) and (1.43) have been referred to in the literature as the
Wiener-Khintchine relations in recognition of pioneering work done by Norbert Wiener
and A. I. Khintchine; for their original papers, see Wiener (1930) and Khintchine (1934).
A discovery of a forgotten paper by Albert Einstein on time-series analysis (delivered at the
Swiss Physical Society’s February 1914 meeting in Basel) reveals that Einstein had discussed
the autocorrelation function and its relationship to the spectral content of a time series
many years before Wiener and Khintchine. An English translation of Einstein’s paper is
reproduced in the IEEE ASSP Magazine, vol. 4, October 1987. This particular issue also
contains articles by W. A. Gardner and A. M. Yaglom, which elaborate on Einstein’s
original work.

4. For further details of power spectrum estimation, see Blackman and Tukey (1958), Box
and Jenkins (1976), Marple (1987), and Kay (1988).

5. The Gaussian distribution and associated Gaussian process are named after the great math-
ematician C. F. Gauss. At age 18, Gauss invented the method of least squares for finding
the best value of a sequence of measurements of some quantity. Gauss later used the method
of least squares in fitting orbits of planets to data measurements, a procedure that was
published in 1809 in his book entitled Theory of Motion of the Heavenly Bodies. In con-
nection with the error of observation, he developed thé Gaussian distribution. This distri-
bution is also known as the normal distribution. Partly for historical reasons, mathemati-
cians commonly use the term normal, while engineers and physicists commonly use the
term Gaussian.

6. For a detailed treatment of electrical noise, see Van der Ziel (1970) and the collection of
papers edited by Gupta (1977).

An introductory treatment of shot noise is presented in Helstrom (1990). For a more de-
tailed treatment, see the paper by Yue, Luganani, and Rice (1978).

78 CHAPTER 1 & RANDOM PROCESSES

10.

i PROBLEMS

Thermal noise was first studied experimentally by J. B. Johnson in 1928, and for this reason
it is sometimes referred to as the Johnson noise. Johnson’s experiments were confirmed
theoretically by Nyquist (1928).

. The noisiness of a receiver may also be measured in terms of the so-called noise figure. The

relationship between the noise figure and the equivalent noise temperature is developed in
Chapter 8.

. The Rayleigh distribution is named after the English physicist J. W. Strutt, Lord Rayleigh.
. The Rician distribution is named in honor of Stephen O. Rice for the original contribution

reported in a pair of papers published in 1944 and 1945, which are reproduced in Wax
(1954). ;

The statistical characterization of communication systems presented in this book is con-
fined to the first two moments, mean and autocorrelation function (equivalently, autoco-
variance function) of the pertinent random process. However, when a random process is
transmitted through a nonlinear system, valuable information is contained in higher-order
moments of the resulting output process. The parameters used to characterize higher-order
moments in the time domain are called cumulants, and their multidimensional Fourier
transforms are called polyspectra. For a discussion of higher-order cumulants and polys-
pectra and their estimation, see the paper by Nikias and Raghuveer (1987).

Stationarity and Ergodicity
1.1 Consider a random process X(t) defined by

X(t) = sin(27f,t)

in which the frequency f, is a random variable uniformly distributed over the interval
[0, W]. Show that X(t) is nonstationary. Hint: Examine specific sample functions of the
random process X(z) for the frequency f = W/4, W/2, and W, say.

1.2 Consider the sinusoidal process

1.3

X(t) = A cos(27rf.t)
where the frequency f, is constant and the amplitude A is uniformly distributed:
— fy Osa=l
fala) = {3 otherwise

Determine whether or not this process is strictly stationary.
A random process X(t) is defined by

X(t) = A cos(27rf,t)
where A is a Gaussian-distributed random variable of zero mean and variance 04. This
random process is applied to an ideal integrator, producing the output

Y(t) = [ xa dr

(a) Determine the probability density function of the output ¥(#) at a particular time t,.
(b) Determine whether or not Y(t) is stationary.
(c) Determine whether or not Y(t) is ergodic.

1.4 Let X and Y be statistically independent Gaussian-distributed random variables, each with

zero mean and unit variance. Define the Gaussian process
Z(t) = X cos(2at) + ¥ sin(27)

Problems 79

(a) Determine the joint probability density function of the random variables Z(t,) and
Z(t) obtained by observing Z(£) at times t, and t,, respectively.
(b) Is the process Z(t) stationary? Why?

Correlation and Spectral Density Functions

1.5 Prove the following two properties of the autocorrelation function Rx(r) of a random
process X(t):
(a) If X(t) contains a DC component equal to A, then Rx(7) will contain a constant
component equal to A”.
(b) If X(t) contains a sinusoidal component, then Rx(7) will also contain a sinusoidal
component of the same frequency.
1.6 The square wave x(t) of Figure P1.6 of constant amplitude A, period To, and delay tz,
represents the sample function of a random process X(t). The delay is random, described
by the probability density function

1
= Fy == 57
frAta) = 4 °° .
0, otherwise

(a) Determine the probability density function of the random variable X(r,) obtained by
observing the random process X(t) at time t,.

(b) Determine the mean and autocorrelation function of X(t) using ensemble-averaging.

(c) Determine the mean and autocorrelation function of X(t) using time-averaging.

(d) Establish whether or not X(¢) is stationary. In what sense is it ergodic?

xt)

tule

FIcure P1.6

1.7 A binary wave consists of a random sequence of symbols 1 and 0, similar to that described
in Example 1.3, with one basic difference: symbol 1 is now represented by a pulse of
amplitude A volts and symbol 0 is represented by zero volts. All other parameters are the
same as before. Show that for this new random binary wave X(z):

(a) The autocorrelation function is

A? A? |7|
fy+S (1-40 <
4° 4 ( pT) l<t
A?
4°?

Rx(7) =
|7|=T

(b) The power spectral density is
A? AT.
Sx(f) = ve af) + es sinc?(fT)

What is the percentage power contained in the DC component of the binary wave?

80 CuapTeR 1 & RANDOM PROCESSES

1.8 A random process Y(t) consists of a DC component of 3/2 volts, a periodic component
g(t), and a random component X(t). The autocorrelation function of Y(t) is shown in
Figure P1.8.

(a) What is the average power of the periodic component g(t}?
(b) What is the average power of the random component X(#)}?

Ry (7)
Wolts)?

 

 

 

 

 

FIGURE P1.8

1.9 Consider a pair of stationary processes X(t) and Y(t). Show that the cross-correlations
Rxy(r) and Ryx(z) of these processes have the following properties:
(a) Rxy(7) = Ryx(—7)
(b) | Rxv{7) | = Z[Rx(0) + Ry(0)]
where Ry(7) and Ry(7) are the autocorrelation functions of X(t) and Y(t), respectively.
1.10 Consider two linear filters connected in cascade as in Figure P1.10. Let X(t) be a stationary
process with autocorrelation function Rx(r). The random process appearing at the first
filter output is V(t) and that at the second filter output is Y(t).
(a) Find the autocorrelation function of Y(t).
(b) Find the cross-correlation function Ryy(r) of V(t) and Y(t).

Vv
XQ— — Aft) 0

FIGURE P1.10

 

 

 

 

1.11 A stationary process: X(¢) is applied to a linear time-invariant filter of impulse response
A(t), producing an output Y(¢).
(a) Show that the cross-correlation function Ryx(7) of the output Y(t) and the input X(t)
is equal to the impulse response 4(r) convolved with the autocorrelation function
Rx(7) of the input, as shown by

Ryx(1) = if h(u)Rx(t — u) du
Show that the second cross-correlation function Rx (7) equals
Ryy(t} = if h(—u)Rx(1 — 4) du

(b) Find the cross-spectral densities Syx(f) and Sxy(f)}.

Problems 81

(c) Assuming that X(t) is a white noise process with zero mean and power spectral density
N,/2, show that

Ryx(a) = 32 b(n)

Comment on the practical significance of this result.
1.12 The power spectral density of a random process X(t) is shown in Figure P1.12. It consists

of a delta function at f = 0 and a triangular component.

(a) Determine and sketch the autocorrelation function Rx(1) of X(t).

(b) What is the DC power contained in X(t)?

(c) What is the AC power contained in X(t)?

(d) What sampling rates will give uncorrelated samples of X(t)? Are the samples statis-
tically independent?

Sf)
6(f)

1.0

fo 0 to
Figure P1.12

1.13 A pair of noise processes 1,(t) and ,(t) are related by
ni(t) = n,(t) cos(2afit + 6) ~ 2,(t) sin(2af,t + 0)
where f, is a constant, and @ is the value of a random variable © whose probability density
function is defined by

i
fol®) = 422°
0, otherwise

Os¢@527

The noise process ,(t) is stationary and its power spectral density is as shown in Figure
P1.13. Find and plot the corresponding power spectral density of 72(t).

Sy ,f)

 

 

-W Q w
Figure P1.13

1.14 A random telegraph signal X(t), characterized by the autocorrelation function
Rx(z) = exp(—2v|7|)

82

Cuaprer 1 & RANDOM PROCESSES

where vis a constant, is applied to the low-pass RC filter of Figure P1.14. Determine the
power spectral density and autocorrelation function of the random process at the filter
output.

R
Input Cc f Output

Figure P1.14

1.15 A running integrator is defined by

where x(t) is the input, y(t) is the output, and T is the integration period. Both x(#) and
y(t) are sample functions of stationary processes X(t) and Y(i), respectively. Show that
the power spectral density of the integrator output is related to that of the integrator input
as

Sof) =-T? sinc(fT)Sx(f)

1.16 A zero-mean stationary process X(t) is applied to a linear filter whose impulse response
is defined by a truncated exponential:

ae~™. Osr1sT
b(t) = ;
) {é otherwise

Show that the power spectral density of the filter output Y(t) is defined by

ae
Sy(f) a + 4p f? (1

 

 

2 exp(—aT) cos(2mfT) + exp(—2aT))Sx(f)

where Sx(f) is the power spectral density of the filter input.
1.17 The output of an oscillator is described by

X(t) = A cos(2nft — ©)

where A is a constant, and f and © are independent random variables. The probability
density function of @ is defined by

1
fol®) = 420”
0, otherwise

Os 0527

Find the power spectral density of X(t) in terms of the probability density function of the
frequency f. What happens to this power spectral density when the frequency f assumes
a constant value?

Problems 83

Gaussian Processes

1.18 A stationary, Gaussian process X(t) has zero mean and power spectral density Sx(f).
Determine the probability density function of a random variable obtained by observing
the process X(t) at some time t,.

1.19 A Gaussian process X(t) of zero mean and variance oX is passed through a full-wave
rectifier, which is described by the input-output relation of Figure P1.19. Show that the
probability density function of the random variable Y(t,), obtained by observing the ran-
dom process Y(t) at the rectifier output at time f,, is as follows:

21. a =0
Fryugly) = T Ox P 2% I? y=

0, y<0

 

 

Ficure P1.19

1.20 Let X(t) be a zero-mean, stationary, Gaussian process with autocorrelation function
Rx(7). This process is applied to a square-law device, which is defined by the input-output
relation

Y(t) = X?(2)
where Y(t) is the output.
(a) Show that the mean of Y(t) is Rx(0).
(b) Show that the autocovariance function of Y(t) is 2R3(r).

1.21 A stationary, Gaussian process X(t) with mean px and variance 7% is passed through two
linear filters with impulse responses /,(t) and /2(t), yielding processes Y(t) and Z(t), as
shown in Figure P1.21.

(a) Determine the joint probability density function of the random variables Y(t,) and
Z(t).

(b) What conditions are necessary and sufficient to ensure that Y(t,) and Z(t,) are statis-
tically independent?

 

AO > yi)

 

X(1)

hl) pe 20)

 

 

 

Ficure P1.21

84 Cwaprer 1 & RANDOM PROCESSES

1.22 A stationary, Gaussian process X(t) with zero mean and power spectral density Sx(f) is
applied to a linear filter whose impulse response 4(¢) is shown in Figure P1,22. A sample
Y is taken of the random process at the filter output at time T.

(a) Determine the mean and variance of Y.
(b) What is the probability density function of Y?

A(t)

me

Q T
Figure P1.22

Noise
1.23 Consider a white Gaussian noise process of zero mean and power spectral density No/2
that is applied to the input of the high-pass RL filter shown in Figure P1.23.

(a) Find the autocorrelation function and power spectral density of the random process
at the output of the filter.

(b) What are the mean and variance of this output?

Input L Output

FiGure P1.23

1.24 A white noise w(t) of power spectral density No/2 is applied to a Butterworth low-pass
filter of order n, whose magnitude response is defined by

1
© [L + (f/f)?
(a) Determine the noise equivalent bandwidth for this low-pass filter. (See Appendix 2
for the definition of noise equivalent bandwidth.)
(b) What is the limiting value of the noise equivalent bandwidth as # approaches infinity?
1.25 The shot-noise process X(t) defined by Equation (1.86) is stationary. Why?
1.26 White Gaussian noise of zero mean and power spectral density N,/2 is applied to the

filtering scheme shown in Figure P1.26a. The frequency responses of these two filters are
shown in Figure P1.26b. The noise at the low-pass filter output is denoted by n(¢).

(a) Find the power spectral density and the autocorrelation function of n(t).
(b) Find the mean and variance of x(t).

|Hif)|

White
noise

Problems 85

(c) What is the rate at which w(t) can be sampled so that the resulting samples are essen-
tially uncorrelated?

 

 

 

 

HAC f
Band-pass Low-pass 1.0
filter filter Output
Hf) HAfY n)
f
cos (27rf,t) fe 0 Jf . i 0 . f
2B 2B
(a) )

Ficure P1.26

1.27 Let X(t) be a stationary process with zero mean, autocorrelation function Ry(7), and
power spectral density $x(f). We are required to find a linear filter with impulse response
h(t), such that the filter output has the same statistical characteristics as X(t) when the
input is white noise of power spectral density N,/2.
(a) Determine the condition which the impulse response /(t) must satisfy to achieve this
requirement.
(b) What is the corresponding condition on the frequency response H(f) of the filter?

Narrowband Noise

1.28 In the noise analyzer of Figure 1.192, the low-pass filters are ideal with a bandwidth equal
to one-half that of the narrowband noise x(t) applied to the input. Using this scheme,
derive the following results:

(a) Equation (1.101), defining the power spectral densities of the in-phase noise com-
ponent #;(¢) and quadrature noise component (tf) in terms of the power spectral
density of n(t).

(b) Equation (1.102), defining the cross-spectral densities of 2;(t) and no(t).

1.29 Assume that the narrowband noise n(t) is Gaussian and its power spectral density Sy(f)
is symmetric about the midband frequency f.. Show that the in-phase and quadrature
components of x(t) are statistically independent.

1.30 The power spectral density of a narrowband noise n(t) is as shown in Figure P1.30. The
carrier frequency is 5 Hz.

(a) Find the power spectral densities of the in-phase and quadrature components of n(t).

(b) Find their cross-spectral densities.

Sy(f)
(W/Hz)

 

F(Hz)

 

~7 -5-4 0 45 7
Figure P1.30

86 CHAPTER 1 & RANDOM PROCESSES

1.31 Consider a Gaussian noise (t) with zero mean and the power spectral density Sy(f)
shown in Figure P1.31.
(a) Find the probability density function of the envelope of x(t).
(b) What are the mean and variance of this envelope?

Swf)

 

Sf 9 L |
2B

Figure P1.31

Computer Experiments

1.32. In this computer experiment we study the statistical characterization of a random process

X(t) defined by
X(t) = A cos(2mrf,t + ©) + Wit)

where the phase @ of the sinusoidal component is a uniformly distributed random variable
over the interval [—a, 7], and Wit) is a white Gaussian noise component of zero mean
and power spectral density No/2. The two components of X(t) are statistically indepen-
dent; hence the autocorrelation function of X(t) is

A Ni
Rx(7) = ~~ cos(2af,1) + — ar)
2 2

This equation shows that for |7| > 0 the autocorrelation function Rx(z) has the same

sinusoidal waveform as the signal component of X(t).

The purpose of this computer experiment is to perform the computation of Rx(7)
using two different methods:

(a) Ensemble averaging. Generate M = 50 randomly picked realizations of the process
X(t). Hence compute the product x(t + 7)x(t) for some fixed time t, where x(t) is a
realization of X(t). Repeat the computation of x(t + 7)x(t) for the M realizations of
X(t), and thereby compute the average of these computations over M. Repeat this
sequence of computations for- different values of 7.

(b) Time averaging. Compute the time-averaged autocorrelation function

T

R,(1, T) = == zt + q)x(t) dt

where x(t) is a particular realization of X(t), and 2T is the total observation interval.
For this computation, use the Fourier-transform pair:

Raley T) = 5 |Xalf)|?

Problems 87

where | Xr(f)|?/2T is the periodogram of the process X(t). Specifically, compute the
Fourier transform X7(f) of the time-windowed function
(t) x(t), -T<xt=T
x(t) =
r 0, otherwise

Hence compute the inverse Fourier transform of | Xr(f){7/2T.

Compare the results of your computation of Rx(7) using these two approaches.

1.33 In this computer experiment we continue the study of the multipath channel described in
Section 1.14. Specifically, consider the situation where the received signal includes a line-
of-sight component, as shown by

N
X(t) = 3 A, cos(2aft + @,) + 4 cos(2nfz)
a=
where a cos(27f,t) is the directly received component. Following the material presented

in Section 1.14, compute the envelope of X(£) for N = 10,000, and a = 0, 1, 2, 3, 5.
Compare your results with the Rician distribution studied in Section 1.13.

 

CONTINUOUS-WAVE
MODULATION

In this chapter we study continuous-wave modulation, which is basic to the operation of
analog communication systems. The chapter is divided into two related parts. In the first
part we study the time-domain and frequency-domain descriptions of two basic families of
continuous-wave modulation: 7

» Amplitude modulation, in which the amplitude of a sinusoidal carrier is varied in
accordance with an incoming message signal.

® Angle modulation, in which the instantaneous frequency or phase of the sinusoidal carrier
is varied in accordance with the message signal.

The second part of the chapter focuses on the effects of channel noise on the performance
of the receivers pertaining to these modulation schemes.

Advantages and disadvantages of the different methods of continuous-wave
modulation are highlighted in light of the material presented herein.

i 2.1 Introduction

The purpose of a communication system is to transmit information-bearing signals through
a communication channel separating the transmitter from the receiver. Information-
bearing signals are also referred to as baseband signals. The term baseband is used to
designate the band of frequencies representing the original signal as delivered by a source
of information. The proper use of the communication channel requires a shift of the range
of baseband frequencies into other frequency ranges suitable for transmission, and a cor-
responding shift back to the original frequency range after reception. For example, a radio
system must operate with frequencies of 30 kHz and upward, whereas the baseband signal
usually contains frequencies in the audio frequency range, and so some form of frequency-
band shifting must be used for the system to operate satisfactorily. A shift of the range of
frequencies in a signal is accomplished by using modulation, which is defined as the process
by which some characteristic of a carrier is varied in accordance with a modulating wave
(signal). A common form of the carrier is a sinusoidal wave, in which case we speak ofa
continuous-wave modulation’ process. The baseband signal is referred to as the modulat-
ing wave, and the result of the modulation process is referred to as the modulated wave.
Modulation is performed at the transmitting end of the communication system. At the
receiving end of the system, we usually require the original baseband signal to be restored.
This is accomplished by using a process known as demodulation, which is the reverse of.
the modulation process.

In basic signal-processing terms, we thus find that the transmitter of an analog com-
munication system consists of a modulator and the receiver consists of a demodulator, as

88

2.1 Introduction 89

Message _ Modulated Channel Estimate of
signal Medulater wave output = Demodulater -—s message signal

(b)

 

 

 

 

Sinusoidal
carrier wave

(a)

Figure 2.1 Components of a continuous-wave modulation system: (a) transmitter, and (b)
receiver.

depicted in Figure 2.1. In addition to the signal received from the transmitter, the receiver
input includes channel noise. The degradation in receiver performance due to channel noise
is determined by the type of modulation used.

In this chapter we study two families of continuous-wave (CW) modulation systems,
namely, amplitude modulation and angle modulation. In amplitude modulation, the am-
plitude of the sinusoidal carrier wave is varied in accordance with the baseband signal. In
angle modulation, the angle of the sinusoidal carrier wave is varied in accordance with the
baseband signal. Figure 2.2 displays the waveforms of amplitude-modulated and angle-
modulated signals for the case of sinusoidal modulation. Parts (a) and (6) of the figure
show the sinusoidal carrier and modulating waves, respectively. Parts (c) and (d) show the

(a)

 

(»)

 

(a) Time —>

Figure 2.2 Illustrating AM and FM signals produced by a single tone. (2) Carrier wave. (b)
Sinusoidal modulating signal. (c) Amplitude-modulated signal. (2) Frequency-modulated signal.

90 CHAPTER 2 @ CONTINUOUS-WAVE MODULATION

corresponding amplitude-modulated and frequency-modulated waves, respectively; fre-
quency modulation is a form of angle modulation. This figure clearly illustrates the basic
differences between amplitude modulation and angle modulation, which are discussed in
what follows.

| 2.2 Amplitude Modulation

Consider a sinusoidal carrier wave c(t) defined by
c(t) = A, cos(2af,t) (2.1)

where A, is the carrier amplitude and f, is the carrier frequency. To simplify the exposition
without affecting the results obtained and conclusions reached, we have assumed that the
phase of the carrier wave is zero in Equation (2.1). Let s(t) denote the baseband signal
that carries the specification of the message. The source of carrier wave c(z) is physically
independent of the source responsible for generating m(t). Amplitude modulation (AM) is
defined as a process in which the amplitude of the carrier wave c(t) is varied about a mean
value, linearly with the baseband signal m(t). An amplitude-modulated (AM) wave may
thus be described, in its most general form, as a function of time as follows:

s(t) = A,[1 + &,m(t)] cos(2af.t) (2.2)

where k, is a constant called the amplitude sensitivity of the modulator responsible for the
generation of the modulated signal s(¢). Typically, the carrier amplitude A, and the message
signal m(t) are measured in volts, in which case &, is measured in volt™*.

Figure 2.34 shows a baseband signal m(z), and Figures 2.3 and 2.3¢ show the cor-
responding AM wave s(t) for two values of amplitude sensitivity &, and a carrier amplitude
A, = 1 volt. We observe that the envelope of s(t) has essentially the same shape as the

baseband signal m(t) provided that two requirements are satisfied:
1, The amplitude of £,#m(t) is always less than unity, that is,
|km(t)|<1 for allt (2.3)

This condition is illustrated in Figure 2.36; it ensures that the function 1 + kit)
is always positive, and since an envelope is a positive function, we may express the
envelope of the AM wave s(t) of Equation (2.2) as A,[1 + k,m(t)]. When the am-
plitude sensitivity &, of the modulator is large enough to make | k,m(t)| > 1 for any
t, the carrier wave becomes overmodulated, resulting in carrier phase reversals when-
ever the factor 1 + &,1m(t) crosses zero. The modulated wave then exhibits envelope
distortion, as in Figure 2.3c. It is therefore apparent. that by avoiding overmodula-
tion, a one-to-one relationship is maintained between the envelope of the AM wave
and the modulating wave for all values of time-~a useful feature, as we shall see later
on. The absolute maximum value of k,7(f) multiplied by 100 is referred to as the
percentage modulation.

2. The carrier frequency f, is much greater than the highest frequency component W of
the message signal m(t), that is

>> Ww (2.4)

We call W the message bandwidth. If the condition of Equation (2.4) is not satisfied,
an envelope cannot be visualized (and therefore detected) satisfactorily.

2.2 Amplitude Modulation 91

mit)

 

(a)

s(t) sft)

 

 

 

 

 

 

 

 

 

(b) {c}

Figure 2.3 [lustrating the amplitude modulation process. (a) Baseband signal m(t). (b) AM
wave for | k,nt(t)| < 1 for all t. (c) AM wave for |k,m(t)| > 1 for some t.

From Equation (2.2), we find that the Fourier transform of the AM wave s(£) is given
by

sin = Sear — y+ or + p+ MEM + Mr + 2S)

Suppose that the baseband signal m(£) is band-limited to the interval -W = f = W, as in
Figure 2.4a. The shape of the spectrum shown in this figure is intended for the purpose of
illustration only. We find from Equation (2.5) that the spectrum S(f) of the AM wave is
as shown in Figure 2.46 for the case when f, > W. This spectrum consists of two delta
functions weighted by the factor A,/2 and occurring at +f., and two versions of the
baseband spectrum translated in frequency by +f, and scaled in amplitude by &,A./2.
From the spectrum of Figure 2.46, we note the following:

1. As a result of the modulation process, the spectrum of the message signal s(t) for
negative frequencies extending from — W to 0 becomes completely visible for positive
(i.e., measurable) frequencies, provided that the carrier frequency satisfies the con-
dition f, > W; herein lies the importance of the idea of “negative” frequencies.

2. For positive frequencies, the portion of the spectrum of an AM wave lying above the
carrier frequency f, is referred to as the upper sideband, whereas the symmetric
portion below f, is referred to as the lower sideband. For negative frequencies, the
upper sideband is represented by the portion of the spectrum below ~f, and the
lower sideband by the portion above —f,. The condition f, > W ensures that
the sidebands do not overlap.

92

CHAPTER 2 CoNTINUOUS-WAVE MODULATION

M(f) s(f)

WwW

3.

  
     
    
   

Al -
Far -6

 

M(O)
$k, AMO)
Upper _— Upper
sideband sideband
0 Ww f fe fw f
{a} ()

FIGURE 2.4 (a) Spectrum of baseband signal. (b) Spectrum of AM wave.

For positive frequencies, the highest frequency component of the AM wave equals
f+ W, and the lowest frequency component equals f. — W. The difference between
these two frequencies defines the transmission bandwidth B for an AM wave, which
is exactly twice the message bandwidth W, that is,

Br = 2W ; (2.6)

@ VIRTUES AND LIMITATIONS OF AMPLITUDE MODULATION

Amplitude modulation is the oldest method of performing modulation. Its greatest virtue
is the simplicity of implementation:

» In the transmitter, amplitude modulation is accomplished using a nonlinear device.

For example, in the switching modulator discussed in Problem 2.3, the combined
sum of the message signal and carrier wave is applied to a diode, with the carrier
amplitude being large enough to swing across the characteristic curve of the diode.
Fourier analysis of the voltage developed across a resistive load reveals the generation
of an AM component, which may be extracted by means of a band-pass filter.

In the receiver, amplitude demodulation is also accomplished using a nonlinear de-
vice. For example, we may use a simple and yet highly effective circuit known as the
envelope detector, which is discussed in Problem 2.5. The circuit consists of a diode
connected in series with the parallel combination of a capacitor and load resistor.
Some version of this circuit is found in most commercial AM radio receivers. Pro-
vided that the carrier frequency is high enough and the percentage modulation is less
than 100 percent, the demodulator output developed across the load resistor is nearly
the same as the envelope of the incoming AM wave, hence the name “envelope
detector.”

Recall, however, that transmitted power and channel bandwidth are our two primary

communication resources, and they should be used efficiently. In this context, we find that
the standard form of amplitude modulation defined in Equation (2.2) suffers from two
major limitations:

1.

Amplitude modulation is wasteful of power. The carrier wave c(t) is completely
independent of the information-bearing signal m(t). The transmission of the carrier
wave therefore represents a waste of power, which means that in amplitude modu-
lation only a fraction of the total transmitted power is actually affected by m(t).

2.3 Linear Modulation Schemes 93

2. Amplitude modulation is wasteful of bandwidth. The upper and lower sidebands of
an AM wave are uniquely related to each other by virtue of their symmetry about
the carrier frequency; hence, given the magnitude and phase spectra of either side-
band, we can uniquely determine the other. This means that insofar as the transmis-
sion of information is concerned, only one sideband is necessary, and the commu-
nication channel therefore needs to provide only the same bandwidth as the baseband
signal. In light of this observation, amplitude modulation is wasteful of bandwidth
as it requires a transmission bandwidth equal to twice the message bandwidth.

To overcome these limitations, we must make certain modifications: suppress the
carrier and modify the sidebands of the AM wave. These modifications naturally result in
increased system complexity. In effect, we trade system complexity for improved use of
communication resources. The basis of this trade-off is linear modulation, which is dis-
cussed in the next section. In a strict sense, full amplitude modulation does not qualify as
linear modulation because of the presence of the carrier wave.

i 2.3 Linear Modulation Schemes

In its most general form, linear modulation is defined by
s(t) = s,(t) cos(2mf.2) 7 Sq(t) sin(27f.t) (2.7)

where s;(t} is the in-phase component of the modulated wave s(t), and so{t) is its quad-
rature component, Equation (2.7) is recognized as the canonical representation of a nar-
rowband signal, which is discussed in detail in Appendix 2. In linear modulation, both
s(t) and so{t) are low-pass signals that are linearly related to the message signal 7(t).

Indeed, depending on how these two components of s(z) are defined, we may identify
three types of linear modulation involving a single message signal:

1. Double sideband-suppressed carrier (DSB-SC) modulation, where only the upper and
lower sidebands are transmitted.

2. Single sideband (SSB) modulation, where only one sideband (the lower sideband or
the upper sideband) is transmitted.

3. Vestigial sideband (VSB) modulation, where only a vestige (i.e., trace) of one of the
sidebands and a correspondingly modified version of the other sideband are
transmitted.

Table 2.1 presents a summary of the definitions of these three special forms of linear
modulation: There are two important points to note from Table 2.1:

1. The in-phase component s;{Z) is solely dependent on the message signal ##(f}.

2. The quadrature component so(t) is a filtered version of m(t). The spectral modifi-
cation of the modulated wave s(t) is solely due to so(t).

To be more specific, the role of the quadrature component (if present) is merely to interfere
with the in-phase component, so as to reduce or eliminate power in one of the sidebands
of the modulated signal s(t), depending on how the quadrature component is defined.

94 CuapTer 2 & CONTINUOUS-WAVE MODULATION

i Taste 2.1 Different forms of linear modulation

 

 

 

 

In-Phase Quadrature
Component Component
Type of Modulation 5,(t) Sq(t) Comments
DSB-SC m(t) 0 m(t) = message signal
SSB:*
(a) Upper sideband im(t) 4ya(t) *(t) = Hilbert transform of m(t)
transmitted .
(b) Lower sideband dm(t) —jeh(t)
transmitted
VSB:
(a) Vestige of lower sideband im(t) din’ (t) m’(t) = output of the filter of
transmitted frequency response Ho(f)
(b) Vestige of upper mit) —}m'(t) due to m(t).
sideband transmitted For the definition of Ho(f),

see Eq. (2.16)

 

 

 

 

 

 

"For the mathematical description of single sideband modulation, see Problem 2.16.

# DOUBLE SIDEBAND-SUPPRESSED CARRIER (DSB-SC) MODULATION

 

This form of linear modulation is generated by using a product modulator that simply
multiplies the message signal m1(t) by the carrier wave A, cos(27f,2), as illustrated in Figure
2.5a. Specifically, we write

 

s(t) = Agn(t) cos(27f,t) (2.8)
mit)
DSB-SC
Baseband Product
signal m(e) modulator 72" modulated wave
s(t) = A,m{2) cos (2af,,4)

 

 

 

 

I ,

Carrier
A, 008 (2f,4)

(a) (b)

s(t)

Phase reversals

 

 

 

{c)

FIGURE 2.5 (a) Block diagram of product modulator. (b) Baseband signal. (c) DSB-SC modu-
lated wave.

2.3 Linear Modulation Schemes 95
MUP) stp)

M(O) $A-AM(O)

 

 

Ww 0 W | _| 0 _ he _
ow 2w
(a) (b)
FiGureE 2.6 (a) Spectrum of baseband signal. (b) Spectrum of DSB-SC madulated wave.

Figure 2.5¢ shows the modulated signal s(¢) for the arbitrary message waveform of Figure
2.5b. The modulated signal s(t) undergoes a phase reversal whenever the message signal
m(t) crosses zero. Consequently, the envelope of a DSB-SC modulated signal is different
from the message signal; this is unlike the case of an AM wave that has a percentage
modulation less than 100 percent.

From Equation (2.8), the Fourier transform of s(t) is obtained as

S(f) = 5 AdMUr — f) + M(f + fo] (2.9)

For the case when the baseband signal m(z) is limited to the interval -W = f = W, as in
Figure 2.6¢, we thus find that the spectrum S(f) of the DSB-SC wave s(t) is as illustrated
in Figure 2.6b. Except for a change in scale factor, the modulation process simply translates
the spectrum of the baseband signal by +f.. Of course, the transmission bandwidth re-
quired by DSB-SC modulation is the same as that for amplitude modulation, namely, 2W.

COHERENT DETECTION

The baseband signal m(t) can be uniquely recovered from a DSB-SC wave s(t) by first
multiplying s(t) with a locally generated sinusoidal wave and then low-pass filtering the
product, as in Figure 2.7. It is assumed that the local oscillator signal is exactly coherent
or synchronized, in both frequency and phase, with the carrier wave c(t) used in the prod-
uct modulator to generate s(¢). This method of demodulation is known as coherent detec-
tion or synchronous demodulation.

It is instructive to derive coherent detection as a special case of the more general
demodulation process using a local oscillator signal of the same frequency but arbitrary
phase difference @, measured with respect to the carrier wave c(t). Thus, denoting the local

 

utd

Product

modulator

Low-pass

so filter

> uv, (0)

 

 

 

hai cos (2rf,t + 6)

 

Local
oscillator

 

 

 

Figure 2.7 Coherent detector for demodulating DSB-SC modulated wave.

96

CHAPTER 2. & CONTINUOUS-WaAVE MODULATION

vif)
$4, Az M(O) cos

4A, AL MO)

 

 

an ee ee

 

Ficure 2.8 Illustrating the spectrum of a product modulator output with a DSB-SC modulated
wave as input.

oscillator signal by A‘ cos(2af.t + #), and using Equation (2.8) for the DSB-SC wave s(2),
we find that the product modulator output in Figure 2.7 is

u(t) = Al cos(2af,t + )s(t)
= A.A! cos(2 aft) cos(2mf,t + b)m(t) (2.10)

= 5 AAL cos(4af,t + b)m(t) + 5 AAL cos @ m(t)
The first term in Equation (2.10) represents a DSB-SC modulated signal with a carrier
frequency 2f., whereas the second term is proportional to the baseband signal #(t). This
is further illustrated by the spectrum V(f) shown in Figure, 2.8, where it is assumed that
the baseband signal 7n(2) is limited to the interval ~ W = f = W. It is therefore apparent
that the first term in Equation (2.10) is removed by the low-pass filter in Figure 2.7,
provided that the cut-off frequency of this filter is greater than W but less than 2f, — W.
This requirement is satisfied by choosing f, > W. At the filter ourput we then obtain a
signal given by

v(t) = 5 AA! cos & m({t) (2.11)

The demodulated signal v,(t) is therefore proportional to 7(z) when the phase error @
is a constant. The amplitude of this demodulated signal is maximum when ¢ = 0, and
it is minimum (zero) when @ = +2/2. The zero demodulated signal, which occurs for
= +72, represents the quadrature null effect of the coherent detector. Thus the phase
error ¢ in the local oscillator causes the detector output to be attenuated by a factor equal
to cos @. As long as the phase error ¢ is constant, the detector provides an undistorted
version of the original baseband signal s(t). In practice, however, we usually find that the
phase error ¢ varies randomly with time, due to random variations in the communication
channel, The result is that at the detector output, the multiplying factor cos ¢ also varies
randomly with time, which is obviously undesirable. Therefore, provision must be made
in the system to maintain the local oscillator in the receiver in perfect synchronism, in both
frequency and phase, with the carrier wave used to generate the DSB-SC modulated signal
in the transmitter. The resulting system complexity is the price that must be paid for
suppressing the carrier wave to save transmitter power.

a CosTas RECEIVER

One method of obtaining a practical synchronous receiver system, suitable for demodu-
lating DSB-SC waves, is to use the Costas receiver? shown in Figure 2.9. This receiver

2.3 Linear Modulation Schemes 97

 

Echannet
1
SA, cos ¢ m(t)
Product Low-pass “ Demodulated
modulator [> filter signal

 

 

 

cos (2arf.t+ 6)

Voltage-

Phase
controlled discriminator
oscillator

 

 

 

 

 

DSB-SC signal
A, cos (2rrf,t) m(t)

 

—90°
phase-shifter

 

| sin (2af,1 + 6)

 

Product Low-pass
modulator > filter

 

 

 

 

44, sin @ m(d)

 

@-channe!

FIGURE 2.9 Costas receiver.

consists of two coherent detectors supplied with the same input signal, namely, the incom-
ing DSB-SC wave A, cos(2af,z)m(t), but with individual local oscillator signals that are
in phase quadrature with respect to each other. The frequency of the local oscillator is
adjusted to be the same as the carrier frequency f., which is assumed known a priori. The
detector in the upper path is referred to as the in-phase coherent detector or I-channel,
and that in the lower path is referred to as the quadrature-phase coherent detector or
Q-channel. These two detectors are coupled together to form a negative feedback system
designed in such a way as to maintain the local oscillator synchronous with the carrier
wave. .

To understand the operation of this receiver, suppose that the local oscillator signal
is of the same phase as the carrier wave A,.cos(27f.t) used to generate the incoming
DSB-SC wave. Under these conditions, we find that the I-channel output contains the
desired demodulated signal m(t), whereas the Q-channel output is zero due to the quad-
rature null effect of the Q-channel. Suppose next that the local oscillator phase drifts from
its proper value by a small angle @ radians. The I-channel output will remain essentially
unchanged, but there will now be some signal appearing at the O-channel output, which
is proportional to sin @ = ¢ for small ¢. This Q-channel output will have the same polarity
as the I-channel output for one direction of local oscillator phase drift and opposite po-
larity for the opposite direction of local oscillator phase drift. Thus, by combining the
I- and Q-channel outputs in a phase discriminator (which consists of a multiplier followed
by a low-pass filter), as shown in Figure 2.9, a DC control signal is obtained that auto-
matically corrects for local phase errors in the voltage-controlled oscillator.

It is apparent that phase control in the Costas receiver ceases with modulation and
that phase-lock has to be reestablished with the reappearance of modulation. This is not
a serious problem when receiving voice transmission, because the lock-up process normally
occurs so rapidly that no distortion is perceptible.

QUADRATURE-CARRIER MULTIPLEXING

The quadrature null effect of the coherent detector may also be put to good use in the
construction of the so-called quadrature-carrier multiplexing or quadrature-amplitude

98

CHaPTeER 2 CONTINUOUS-WAVE MODULATION

Message
signal m,(z)

Message
signal nm (1)

 

   
  

Product + Multiplexed Product Low-
le) pass
modulator signal s() modulator filter Agmy®

 

= Ap cos (Zaft) t~ 2 cos (27f,1)

 

 

 

 

 

 

 

 

 

 

 

 

90° Multiplexed 90°
90° ional ~
phase-shifter signal s() phase-shifter
| A, Sin (2af,2) | 2sin (2af,0)
Product Product Low-pass A
modulator modulator [> filter Acari
fa) {b)

Figure 2.10 Quadrature-carrier multiplexing system. (¢) Transmitter. (b) Receiver.

modulation (QAM). This scheme enables two DSB-SC modulated waves (resulting from
the application of two physically independent message signals) to occupy the same channel
bandwidth, and yet it allows for the separation of the two message signals at the receiver
output. It is therefore a bandwidth-conservation scheme.

A block diagram of the quadrature-carrier multiplexing system is shown in Figure
2.10. The transmitter part of the system, shown in Figure 2.10a, involves the use of two
separate product modulators that are supplied with two carrier waves of the same fre-
quency but differing in phase by —90 degrees. The transmitted signal s(t) consists of the
sum of these two product modulator outputs, as shown by

s(t) = Agny(t) cos(2af.t) + Agma{t) sin(27f.2) (2,12)

where #,(t) and #,(t) denote the two different message signals applied to the product
modulators. Thus s(t) occupies a channel bandwidth of 2W centered at the carrier fre-
quency f., where W is the message bandwidth of m4(t) or m,(t). According to Equation
(2.12), we may view A,m,(t) as the in-phase component of the multiplexed band-pass
signal s(t) and —Agm,(t} as its quadrature component.

The receiver part of the system is shown in Figure 2.106. The multiplexed signal s(t)
is applied simultaneously to two separate coherent detectors that are supplied with two
local carriers of the same frequency but differing in phase by —90 degrees. The output of
the top detector is A,m,(t), whereas the output of the bottom detector is A,2(t). For the
system to operate satisfactorily, it is important to maintain the correct phase and frequency
relationships between the local oscillators used in the transmitter and receiver parts of the
system.

To maintain this synchronization, we may send a pilot signal outside the passband
of the modulated signal. In this method, the pilot signal typically consists of a low-power
sinusoidal tone whose frequency and phase are related to the carrier. wave c(£); at the
receiver, the pilot signal is extracted by means of a suitably tuned circuit and then trans-
lated to the correct frequency for use in the coherent detector.

@ SINGLE-SIDEBAND MODULATION

In single-sideband modulation, only the upper or lower sideband is transmitted. We may
generate such a modulated wave by using the frequency-discrimination method that con-
sists of two stages:

2.3 Linear Modulation Schemes 99

» The first stage is a product modulator, which generates a DSB-SC modulated wave.

® The second stage is a band-pass filter, which is designed to pass one of the sidebands
of this modulated wave and suppress the other.

From a practical viewpoint the most severe requirement of SSB generation using the fre-
quency discrimination method arises from the unwanted sideband. The nearest frequency
component of the unwanted sideband is séparated from the desired sideband by twice the
lowest frequency component of the message (modulating) signal. The implication here is
that for the generation of an SSB modulated signal to be possible, the message spectrum
must have an energy gap centered at the origin, as illustrated in Figure 2.114. This require-
ment is naturally satisfied by voice signals, whose energy gap is about 600 Hz wide (i.e.,
it extends from ~300 to +300 Hz). Thus, assuming that the upper sideband is retained,
the spectrum of the SSB modulated signal is as shown in Figure 2.11.

In designing the band-pass filter used in the frequency-discriminator for generating
a SSB-modulated wave, we must meet the three basic requirements:

» The desired sideband lies inside the passband of the filter.
® The unwanted sideband lies inside the stopband of the filter.

> The filter’s transition band, which separates the passband from the stopband, is twice
the lowest frequency component of the message signal.

This kind of frequency discrimination usually requires the use of highly selective filters,
which can only be realized in practice by means of crystal resonators.

To demodulate a SSB modulated signal s(t), we may use a coherent detector, which
multiplies s(t) by a locally generated carrier and then low-pass filters the product. This
method of demodulation assumes perfect synchronism between the oscillator in the co-
herent detector and the oscillator used to supply the carrier wave in the transmitter. This
requirement is usually met in one of two ways:

® A low-power pilot carrier is transmitted in addition to the selected sideband.

» A highly stable oscillator, tuned to the same frequency as the carrier frequency, is
used in the receiver.

In the latter method, it is inevitable that there would be some phase error ¢ in the local
oscillator output with respect to the carrier wave used to generate the incoming SSB mod-
ulated wave. The effect of this phase error is to introduce a phase distortion in the de-
modulated signal, where each frequency component of the original message signal under-
goes a constant phase shift . This phase distortion is tolerable in voice communications,

Wap) Isl
<< \ / ~ J f

“fe O f, font me o- fo\p yp fethh
—| kK Energy gap oe
{a) (6)

 

FiGuRE 2.11 (a) Spectrum of a message signal m(t) with an energy gap of width 2f,, centered
on the origin. (b) Spectrum of corresponding SSB signal containing the upper sideband.

100

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

because the human ear is relatively insensitive to phase distortion. In particular, the pres-
ence of phase distortion gives rise to a Donald Duck voice effect. In the transmission of
music and video signals, on the other hand, the presence of this form of waveform distor-
tion is utterly unacceptable.

VESTIGIAL SIDEBAND MODULATION

In vestigial sideband (VSB) modulation, one of the sidebands is partially suppressed and
a vestige of the other sideband is transmitted to compensate for that suppression. A popular
method for generating a VSB-modulated wave is to use the frequency discrimination
method. First, we generate a DSB-SC modulated wave and then pass it through a band-
pass filter, as shown in Figure 2.12; it is the special design of the band-pass filter that
distinguishes VSB modulation from SSB modulation. Assuming that a vestige of the lower
sideband is transmitted, the frequency response H(f) of the band-pass filter takes the form
shown in Figure 2.13. To simplify matters, only the response for positive frequencies is
shown here. This frequency response is normalized, so that at the carrier frequency f, we
have |H(j,)| = 1/2. The important feature to note from Figure 2.13 is that the cutoff
portion of the frequency response around the carrier frequency f, exhibits odd symmetry.
That is, inside the transition interval f. — f, =| f| =f. + f, the following two conditions
are satisfied:

1. The sum of the values of the magnitude response | H(f)| at any two frequencies
equally displaced above and below f, is unity.
2. The phase response arg(H(f)) is linear. That is, H(f) satisfies the condition

Hf-fJ+Hf+fp=1 for-Wsfsw (2.13)

Note also that outside the frequency band of interest (ie., | f| > f, + W), the frequency
response H(f) may have an arbitrary specification. Accordingly, the transmission band-
width of VSB modulation is

Br=W+ f, (2.14)

where W is the message bandwidth, and f, is the width of the vestigial sideband.
According to Table 2.1, the VSB modulated wave is described in the time domain as

1 1 v
s(t) = 3 Agn(t) cos(27f.t) + 3 Agm'(t) sin(27f,t) (2.15)
where the plus sign corresponds to the transmission of a vestige of the upper sideband,
and the minus sign corresponds to the transmission of a vestige of the lower sideband. The
signal m’(t) in the quadrature component of s(t) is obtained by passing the message signal

DSB-SC

modulated
“ee Band-pass modulated
reo
filter, H(f) wave

Ficure 2.12 Filtering scheme for the generation of VSB modulated wave.

    
  
 

Product
modulator

Message
signal m(2)

 

 

A, cos (2nf,1)
carrier wave

2.3 Linear Medulation Schemes 101

ACAI

1.0

0.5

 

 

i |
[ I
| !
i |
| |
!

 

o So-fo fe Sotho i+w ,

FIGURE 2.13 Magnitude response of VSB filter; only the positive-frequency portion is shown.

m(t) through a filter whose frequency response Ho(f) satisfies the following requirement
(see Problem 2.20):

Ho(f) = Hf - f) - Hf + f)] for -Wsfsw (2.16)

Figure 2.14 displays a plot of the frequency response Ho(f), scaled by 1/. The role of the
quadrature component determined by Ho(f) is to interfere with the in-phase component
in Equation (2.15) so as to partially reduce power in one of the sidebands of the modulated
wave s(t) and retain simply a vestige of the other sideband, as desired.

It is of interest to note that SSB modulation may be viewed as a special case of
VSB modulation. Specifically, when the vestigial sideband is reduced to zero (i.e., we set
f. = 0), the modulated wave s(t) of Equation (2.15) takes the limiting form of a single-
sideband modulated wave.

TELEVISION SIGNALS

A discussion of vestigial sideband modulation would be incomplete without a mention of
its role in commercial television (TV) broadcasting. The exact details of the modulation
format used to transmit the video signal characterizing a TV system are influenced by two
factors:

1. The video signal exhibits a large bandwidth and significant low-frequency content,
which suggest the use of vestigial sideband modulation.

2. The circuitry used for demodulation in the receiver should be simple and therefore
inexpensive; this suggests the use of envelope detection, which requires the addition
of a carrier to the VSB-modulated wave.

 

 

 

FiGuRE 2.14 Frequency response of a filter for producing the quadrature component of the
VSB modulated wave.

102

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

With regard to point 1, however, it should be stressed that although there is indeed
a basic desire to conserve bandwidth, in commercial TV broadcasting the transmitted
signal is not quite VSB modulated. The reason is that at the transmitter the power levels
are high, with the result that it would be expensive to rigidly control the filtering of side-
bands. Instead, a VSB filter is inserted in each receiver, where the power levels are low,
The overall performance is the same as conventional vestigial-sideband modulation, except
for some wasted power and bandwidth. These remarks are illustrated in Figure 2.15. In
particular, Figure 2.15a shows the idealized spectrum of a transmitted TV signal. The
upper sideband, 25 percent of the lower sideband, and the picture carrier are transmitted,
The frequency response of the VSB filter used to do the required spectrum shaping in the
receiver is shown in Figure 2.150.

The channel bandwidth used for TV broadcasting in North America is 6 MHz, as
indicated in Figure 2.15. This channel bandwidth not only accommodates the bandwidth
requirement of the VSB modulated video signal but also provides for the accompanying
sound signal that modulates a carrier of its own. The values presented on the frequency
axis in Figures 2.15a and 2,15 pertain to a specific TV channel. According to this figure,
the picture carrier frequency is at 55.25 MHz, and the sound carrier frequency is at 59.75
MHz. Note, however, that the information content of the TV signal lies in a baseband
spectrum extending from 1.25 MHz below the picture carrier to 4.5 MHz above it.

With regard to point 2, the use of envelope detection (applied to a VSB modulated

 

 

 

 

 

 

 

go —>\ Nine 4.5 MHz > 0.25 MHz
Sa \
$s K— 0.75 MHz |
BE OS 7 \
28 ;
af |
gs
awa
So L Picture ! Sound
Ee carrier | carrier
5.2 |
ES 1
se !
= li
ol MHz
“64 56 58 60 Mle)
{a)
Picture Sound
carrier carrier
2 ol
a
@
a=!
&
g O55
So
Zz
0 AY F(MHz)

 

(b)

FIGURE 2.15 (a) Idealized magnitude spectrum of a transmitted TV signal. (b) Magnitude re-
sponse of VSB shaping filter in the receiver.

2.4 Frequency Translation 103

wave plus carrier) produces waveform distortion in the video signal recovered at the de-
tector output. The distortion is produced by the quadrature component of the VSB mod-
ulated wave; this issue is discussed next.

The use of the time-domain description given in Equation (2.15) enables the deter-
mination of the waveform distortion caused by the envelope detector. Specifically, adding
the carrier component A, cos(27f,t) to the VSB-modulated wave of Equation (2.15), the
latter being scaled by a factor k,, modifies the modulated signal applied to the envelope
detector input as °

s(f) = ad + + komt cos(2af,t) £ ; k,Agn'(t) sin(27f.2) (2.17)

where the constant k, determines the percentage modulation. The envelope detector out-
put, denoted by a(t), is therefore

1 2 1 25 1/2
a(t) = ad|1 + 3 kam) + 3 kam] }

2> V2 2.18
1 ; kywm'(t) (2-18)
= A,| 1+ =k m(t)|4¢1 +
1+ i kmi(t)

2 @

Equation (2.18) indicates that the distortion is contributed by m’(t), which is responsible
for the quadrature component of the incoming VSB-modulated signal. This distortion can
be reduced by using two methods:

» Reducing the percentage modulation to reduce the amplitude sensitivity k,.
® Increasing the width of the vestigial sideband to reduce m'(t),

Both methods are in fact used in practice. In commercial TV broadcasting, the width of
the vestigial sideband (which is about 0.75 MHz, or one-sixth of a full sideband) is deter-
mined to keep the distortion due to m'(¢) within tolerable limits when the percentage
modulation is nearly 100.

 

| 2.4 Frequency Translation

The basic operation involved in single-sideband modulation is in facta form of frequency
translation, which is why single-sideband modulation is sometimes referred to as frequency
changing, mixing, or heterodyning. This operation is clearly illustrated in the spectrum of
the signal shown in Figure 2.11b compared to that of the original message signal in Figure
2.112. Specifically, we see that a message spectrum occupying the band from f, to f;, for
positive frequencies in Figure 2.114 is shifted upward by an amount equal to the carrier
frequency f, in Figure 2.11), and the message spectrum for negative frequencies is trans-
lated downward in a symmetric fashion.

The idea of frequency translation described herein may be generalized as follows.
Suppose that we have a modulated wave s,(t) whose spectrum is centered on a carrier
frequency f,, and the requirement is to translate it upward in frequency such that its carrier
frequency is changed from f, to a new value f,. This requirement may be accomplished
using the mixer shown in Figure 2.16. Specifically, the mixer is a device that consists of a
product modulator followed by a band-pass filter.

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

 

 
      

Product

Modulated wave s, (+)
modulator

with carrier frequency f,

Band-pass > Modulated wave s(t)
filter with carrier frequency f,

    

 

Ag, cos (2nf et)

FIGURE 2.16 Block diagram of mixer.

To explain the action of the mixer, consider the situation depicted in Figure 2.17,
where, for the purpose of illustration, it is assumed that the mixer input s;(¢) is an AM
signal with carrier frequency f, and bandwidth 2.W. Part (a) of Figure 2.17 displays the
AM spectrum S,(f} assuming that f; > W. Part (B) of the figure displays the spectrum
S'(f) of the resulting signal s’(t) at the product modulator output.

The signal s’(t) may be viewed as the sum of two modulated components: one com-
ponent represented by the shaded spectrum in Figure 2.17b, and the other component
represented by the unshaded spectrum in this figure. Depending on whether the incoming
carrier frequency f, is translated upward or downward, we may identify two different
situations, as described here:

Up conversion. In this case the translated carrier frequency f, is greater than the
incoming carrier frequency f,, and the required local oscillator frequency fr is there-

fore defined by
h=fAth
or :
f=h-h
oy

Pet

{@)

if

     
 
  

       

ZIN Zin 0 ich
eS) LD La

FiGuRE 2.17 (a) Spectrum of modulated signal s,(¢) at the mixer input. (b) Spectrum of the
corresponding signal s’(t) at the output of the product modulator in the mixer.

2.5 Frequency-Division Multiplexing 105

The unshaded part of the spectrum in Figure 2.17b defines the wanted modulated
signal s2(t), and the shaded part of this spectrum defines the iznage signal associated
with s,(t), For obvious reasons, the mixer in this case is referred to as a frequency-
up converter,

Down conversion. In this second case the translated carrier frequency f, is smaller
than the incoming carrier frequency f,, and the required oscillator frequency f; is
therefore defined by

h=fi-f

or

fh=fA-kh

The picture we have this time is the reverse of that pertaining to up conversion. In
particular, the shaded part of the spectrum in Figure 2.176 defines the wanted mod-
ulated signal s(t), and the unshaded part of this spectrum defines the associated
image signal. The mixer is now referred to as a frequency-down converter. Note that
in this case the translated carrier frequency f; has to be larger than W (i.e., one half
of the bandwidth of the modulated signal) to avoid sideband overlap.

The purpose of the band-pass filter in the mixer of Figure 2.16 is to pass the wanted
modulated signal s,(t) and eliminate the associated image signal. This objective is achieved
by aligning the midband frequency of the filter with the translated carrier frequency f, and
assigning it a bandwidth equal to that of the incoming modulated signal s,(t).

It is important to note that mixing is a linear operation. Accordingly, the relation of
the sidebands of the incoming modulated wave to the carrier is completely preserved at
the mixer output.

| 2.5 Frequency-Division Multiplexing

 

 

 

Another important signal processing operation is multiplexing, whereby a number of in-
dependent signals can be combined into a composite signal suitable for transmission over
a common channel. Voice frequencies transmitted over telephone systems, for example,
range from 300 to 3100 Hz. To transmit a number of these signals over the same channel,
the signals must be kept apart so that they do not interfere with each other, and thus they
can be separated at the receiving end. This is accomplished by separating the signals either
in frequency or in time. The technique of separating the signals in frequency is referred to
as frequency-division multiplexing (FDM), whereas the technique of separating the signals
in time is called time-division multiplexing (TDM). In this section, we discuss FDM sys-
tems, and TDM systems are discussed in Chapter 3.

A block diagram of an FDM system is shown in Figure 2.18. The incoming message
signals are assumed to be of the low-pass type, but their spectra do not necessarily have
nonzero values all the way down to zero frequency. Following each signal input, we have
shown a low-pass filter, which is designed to remove high-frequency components that do
not contribute significantly to signal representation but are capable of disturbing other
message signals that share the common channel. These low-pass filters may be omitted
only if the input signals are sufficiently band limited initially. The filtered signals are applied

 

 

 

 

 

 

 

 

 

 

 

 

 

106 CHAPTER 2 & CONTINUOUS-WAVE MODULATION
Message —_ Low-pass Band-pass Band-pass Low-pass Message
inputs filters Modulators filters filters Demoduiators filters outputs
l— > LP oe MOD -— > BP BP = DEM -———= (LP |. 1
2—> (LP BP —=> LP 2
Common
channel | ~
N-——=> LP BP > LP o> N

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Carrier Carer
supply supply

  

[.

 

 

 

 

 

Transmitter Receiver

FiGureE 2.18 Block diagram of FDM system.

to modulators that shift the frequency ranges of the signals so as to occupy mutually
exclusive frequency intervals. The necessary carrier frequencies needed to perform these
frequency translations are obtained from a carrier supply. For the modulation, we may
use any one of the methods described in previous sections of this chapter. However, the
most widely used method of modulation in frequency-division multiplexing is single side-
band modulation, which, in the case of voice signals, requires a bandwidth that is ap-
proximately equal to that of the original voice signal. In practice, each voice input is usually
assigned a bandwidth of 4 kHz. The band-pass filters following the modulators are used
to restrict the band of each modulated wave to its prescribed range. The resulting band-
pass filter outputs are next combined in parallel to form the input to the common channel.
At the receiving terminal, a bank of band-pass filters, with their inputs connected in par-
allel, is used to separate the message signals on a frequency-occupancy basis. Finally, the
original message signals are recovered by individual demodulators. Note that the FDM
system shown in Figure 2.18 operates in only one direction. To provide for two-way
transmission, as in telephony, for example, we have to completely duplicate the multi-
plexing facilities, with the components connected in reverse order and with the signal
waves proceeding from right to left.

® EXaMPLeE 2.1

The practical implementation of an FDM system usually involves many steps of modulation
and demodulation, as illustrated in Figure 2.19. The first multiplexing step combines 12 voice
inputs into a basic group, which is formed by having the ath input modulate a carrier at
frequency f, = 60 + 4n kHz, where # = 1, 2,..., 12. The lower sidebands are then selected
by band-pass filtering and combined to form a group of 12 lower sidebands (one for each
voice input). Thus the basic group occupies the frequency band 60 to 108 kHz. The next step
in the FDM hierarchy involves the combination of five basic groups into a supergroup. This
is accomplished by using the xth group to modulate a carrier of frequency f, = 372 + 48"
kHz, where n = 1,2,..., 5. Here again the lower sidebands are selected by filtering and then

2.6 Angle Modulation 107

Carrier frequencies (in kHz) Carrier frequencies

 

 

 

 

 

 

 

 

 

 

 

 

 

 

of voice inputs (in KHz) of groups
\ 108 kHz
108 {12 | 6i2—[ 5 ]o02 KH
p= 504
104— il 564 4
100 + 10 516—, 3 “se
96 7 9 468 2 408
92 8 420 1 ao
7 20 312
88 7
84 6 Supergroup
80 5 of § groups
76— 4
72— 3
68 —-| 2
4 kHz a eS
} 60
Basic group of 12
9 voice inputs

Voice band

FIGURE 2.19 Illustrating the modulation Steps in an FDM system.

and mastergroups are combined into very large groups. <q
-| 2.6 Angle Modulation
teen nnnnniiinnnan we

In the previous sections of this chapter, we investigated the effect of slowly varying the
amplitude of a sinusoidal carrier wave in accordance with the basehand (information-
carrying) signal. There is another way of modulating a sinusoidal carrier wave, namely,
angle modulation in which the angle of the carrier wave is varied according to the baseband
signal. In this method of modulation, the amplitude of the catrier wave is maintained
constant. An important feature of angle modulation is that it can provide better discrim-
ination against noise and interference than amplitude modulation. As will be shown later
in Section 2.7, however, this improvement in performance is achieved at the expense of
increased transmission bandwidth; that is, angle modulation provides us with a practical

means of exchanging channel bandwidth for improved noise performance. Such a trade-
off is not possible with amplitude modulation, regardless of its form.

# Basic DEFINITIONS

Let 6,(¢) denote the angle of a modulated sinusoidal carrier, assumed to be a function of
the message signal. We express the resulting angle-modulated wave as

s(t) = A. cos[@,(t)] (2.19)

108

CHaPprer 2 & CONTINUOUS-WAVE MODULATION

where A, is the carrier amplitude. A complete oscillation occurs whenever 6,(t) changes
by 27 radians. If 6,{f) increases monotonically with time, the average frequency in Hertz,
over an interval from f to t + At, is given by

6,(t + At) — 6,(t)

Qa At (2.20)

fadt) =

We may thus define the instantaneous frequency of the angle-modulated signal s(t) as
follows:

 

At
. 6,(¢ + At) — 6,(t)
~ fm, | 2m At (2.21)
= Ai d6,(t)
2n dt

Thus, according to Equation (2.19), we may interpret the angle-modulated signal
s(t) as a rotating phasor of length A. and angle 6,(t). The angular velocity of such a phasor
is d6,(t\/dt measured in radians per second, in accordance with Equation (2.21). In the
simple case of an unmodulated carrier, the angle 6,(t) is

6,(t) = 2nf.t + 6,

and the corresponding phasor rotates with a constant angular velocity equal to 2af,. The
constant @, is the value of @,(f) at t = 0.

There are an infinite number of ways in which the angle 0,(t) may be varied in some
manner with the message (baseband) signal. However, we shall consider only two com-
monly used methods, phase modulation and frequency modulation, defined as follows:

1. Phase modulation (PM) is that form of angle modulation in which the angle 0,(t) is
varied linearly with the message signal m(t), as shown by

6,(t) = 2nf.t + kpmlt) (2.22)

The term 27f,t represents the angle of the unmodulated carrier; and the constant k,
represents the phase sensitivity of the modulator, expressed in radians per volt on
the assumption that m(¢) is a voltage waveform. For convenience, we have assumed
in Equation (2.22) that the angle of the unmodulated carrier is zero at t = 0. The
phase-modulated signal s(t) is thus described in the time domain by

s(t) = A, cos[2af.t + Rpmi(t)] (2.23)

b

Frequency modulation (FM) is that form of angle modulation in which the instan-
taneous frequency f,(t) is varied linearly with the message signal m(t), as shown by

filt) = f, + Rem?) (2.24)

The term f, represents the frequency of the unmodulated carrier, and the constant
ky represents the frequency sensitivity of the modulator, expressed in Hertz per volt

2.7 Frequency Modulation 109

A Phase Modulating . " Frequency
vet Integrator |) modulator [> FM wave wave Differentiator modulator

A, cos (2rf.) A, COS (2r7f,1)
{a) (b)

 

   
  

ae PM wave

 

 

 

 

FIGURE 2.20 [lustrating the relationship between frequency modulation and phase modulation.
(a) Scheme for generating an FM wave by using a phase modulator. (b) Scheme for generating a
PM wave by using a frequency modulator.

on the assumption that m/(t) is a voltage waveform. Integrating Equation (2.24) with
respect to time and multiplying the result by 27, we get
La

0,(t) = Zaft + 2k, I, m(r) dr (2.25)

where, for convenience, we have assumed that the angle of the unmodulated carrier
wave is zero at tf = 0. The frequency-modulated signal is therefore described in the
time domain by

s(t) = A, cos] 2 + amy | mt) ir| (2.26)

A consequence of allowing the angle 0,(t) to become dependent on the message signal
m(t) as in Equation (2.22) or on its integral as in Equation (2.25) is that the zero crossings
of a PM signal or FM signal no longer have a perfect regularity in their spacing; zero
crossings refer to the instants of time at which a waveform changes from a negative to a
positive value or vice versa. This is one important feature that distinguishes both PM and
FM signals from an AM signal. Another important difference is that the envelope of a PM
or FM signal is constant (equal to the carrier amplitude), whereas the envelope of an AM
signal is dependent on the message signal.

Comparing Equation (2.23) with (2.26) reveals that an FM signal may be regarded
as a PM signal in which the modulating wave is J7m(7) dz in place of m(t). This means
that an FM signal can be generated by first integrating »(t) and then using the result as
the input to a phase modulator, as in Figure 2.204. Conversely, a PM signal can be gen-
erated by first differentiating m(t) and then using the result as the input to a frequency
modulator, as in Figure 2.20b. We may thus deduce all the properties of PM signals from
those of FM signals and vice versa. Henceforth, we concentrate our attention on FM

signals.

j 2.7 Frequency Modulation

The FM signal s(t) defined by Equation (2.26) is a nonlinear function of the modulating
signal m(t), which makes frequency modulation a nonlinear modulation process. Conse-
quently, unlike amplitude modulation, the spectrum of an FM signal is not related in a
simple manner to that of the modulating signal; rather, its analysis is much more difficult
than that of an AM signal.

110) =CuHaprer2 8 CoNTINUOUS-WAVE MODULATION

How then can we tackle the spectral analysis of an FM signal? We propose to provide
an empirical answer to this important question by proceeding in the following manner:

> We consider the simplest case possible, namely, that of a single-tone modulation that
produces a narrowband FM signal.

> We next consider the more general case also involving a single-tone modulation, but
this time the FM signal is wideband.

We could, of course, go on and consider the more elaborate case of a multitone FM signal,
However, we propose not to do so, because our immediate objective is to establish an
empirical relationship between the transmission bandwidth of an FM signal and the mes-
sage bandwidth. As we shall subsequently see, the two-stage spectral analysis described
here provides us with enough insight to propose a solution to the problem.

Consider then a sinusoidal modulating signal defined by

m(t) = A,, cos(27f,,t) (2.27)
The instantaneous frequency of the resulting FM signal equals

fit) = fo + RyAm cos(27f,,t)

= f, + Af cos(27f,,t) (2.28)

where
Af = kpAm (2.29)

The quantity Af is called the frequency deviation, representing the maximum departure

of the instantaneous frequency of the FM signal from the carrier frequency f.. A funda-

mental characteristic of an FM signal is that the frequency deviation Af is proportional

to the amplitude of the modulating signal and is independent of the modulation frequency.
Using Equation (2.28), the angle 6,(¢) of the FM signal is obtained as

0(¢) = 27 [, f;(t) dr

A
= laf.t + a sin(27f,,,t)
The ratio of the frequency deviation Af to the modulation frequency f,,, is commonly called
the modulation index of the FM signal. We denote it by B, and so write

(2.30)

Af
= 2.31
B 1, (2.31)
and

6(t) = 2arf.t + B sin(2afat) (2.32)

From Equation (2.32) we see that, in a physical sense, the parameter 8 represents the phase
deviation of the FM signal, that is, the maximum departure of the angle 6;(¢) from the
angle 27f,t of the unmodulated carrier; hence, 6 is measured in radians.

The FM signal itself is given by

s(t) = A, cos[2af,t + B sin(27f,,t)] (2.33)

2.7 Frequency Modulation 111

Depending on the value of the modulation index 8, we may distinguish two cases of
frequency modulation: :

> Narrowband FM, for which £ is small compared to one radian.
» Wideband FM, for which 8 is large compared to one radian.

These two cases are considered next, in that order.

sg NARROWBAND FREQUENCY MODULATION

Consider Equation (2.33), which defines an FM signal resulting from the use of a sinusoidal
modulating signal. Expanding this relation, we get

s(t) = A, cos(27f,t) cos[B sin(27f,,t)] + A, sin(27f.t) sin[B sin(27f,,t)] (2.34)

Assuming that the modulation index B is small compared to one radian, we may use the
following approximations:

cos[B sin(2nf,t)] ~ 1
and
sin[B sin(2af,2)] ~ B sin(2af,¢)
Hence, Equation (2.34) simplifies to
s(t) = A, cos(2af.t) — BA, sin(2af,t) sin(2af,2) (2.35)

Equation (2.35) defines the approximate form of a narrowband FM signal produced by a
sinusoidal modulating signal A,, cos(27f,,t). From this representation we deduce the mod-
ulator shown in block diagram form in Figure 2.21. This modulator involves splicting the
carrier wave A, cos(27f,t) into two paths. One path is direct; the other path contains a
—90 degree phase-shifting network and a product modulator, the combination of which
generates a DSB-SC modulated signal. The difference between these two signals produces
a narrowband FM signal, but with some distortion.

Ideally, an FM signal has a constant envelope and, for the case of a sinusoidal mod-
ulating signal of frequency f,,, the angle 6,(¢) is also sinusoidal with the same frequency.

Product —
modulator

Narrowband
FM wave

Modulating

wave Integrator

 
   

 

A, sin (21f,2)

   

-$0°
phase-shifter

Carrier wave
A, COS (27,2)

  

 

 

 

Narrowband phase modulator

FicuRE 2.21 Block diagram of a method for generating a narrowband FM signal.

112 CHAPTER 2 CONTINUOUS-WAVE MODULATION

But the modulated signal produced by the narrowband modulator of Figure 2.21 differs
from this ideal condition in two fundamental respects:

1. The envelope contains a residual amplitude modulation and, therefore, varies with
time.

2. For a sinusoidal modulating wave, the angle 6,(¢) contains harmonic distortion in
the form of third- and higher-order harmonics of the modulation frequency f,,.

However, by restricting the modulation index to 8 = 0.3 radians, the effects of residual
AM and harmonic PM are limited to negligible levels.
Returning to Equation (2.35), we may expand it as follows:

s(t) = A, cos(2af,t) + ; BAJ{cos[2a(f, + fxlé] — cos[2a(f. — fadél} (2.36)

This expression is somewhat similar to the corresponding one defining an AM signal,
which is as follows:

Sam(t) = A, cos(2mft) + + mA.tcosi2n(f + fixlt] + cos2a(f. — fndél} (2.37)

where p. is the modulation factor of the AM signal. Comparing Equations (2.36) and
(2.37), we see that in the case of sinusoidal modulation, the basic difference between an
AM signal and a narrowband FM signal is that the algebraic sign of the lower side fre-
quency in the narrowband FM is reversed. Thus, a narrowband FM signal requires essen-
tially the same transmission bandwidth (i.e., 2f,,) as the AM signal,

We may represent the narrowband FM signal with a phasor diagram as shown in
Figure 2.224, where we have used the carrier phasor as reference. We see that the resultant

Sum of side-
frequency phasors

    
 
 
      

Resultant

Upper side-
frequency

  

. frequency
Carrier

 

(a}

   
 
  

 

Upper side- ‘
frequency ‘
Carrier Sum of
hi side-frequency
phasors

Lower side-
frequency

{b)

FIGURE 2.22 A phasor comparison of narrowband FM and AM waves for sinusoidal modula-
tion. (2) Narrowband FM wave. (b) AM wave.

2.7 Frequency Modulation 113

of the two side-frequency phasors is always at right angles to the carrier phasor. The effect
of this is to produce a resultant phasor representing the narrowband FM signal that is
approximately of the same amplitude as the carrier phasor, but out of phase with respect
to it. This phasor diagram should be contrasted with that of Figure 2.225, representing
an AM signal. In this latter case we see that the resultant phasor representing the AM
signal has an amplitude that is different from that of the carrier phasor but always in phase
with it.

@ WIDEBAND FREQUENCY MODULATION

We next wish to determine the spectrum of the single-tone FM signal of Equation (2.33)
for an arbitrary value of the modulation index . In general, an FM signal produced by a
sinusoidal modulating signal, as in Equation (2.33), is in itself nonperiodic unless the
carrier frequency f, is an integral multiple of the modulation frequency f,,. However, we
may simplify matters by using the complex representation of band-pass signals described
in Appendix 2. Specifically, we assume that the carrier frequency f. is large enough (com-
pared to the bandwidth of the FM signal) to justify rewriting this equation in the form

s(t) = Re[A, exp(j2af.t + j6 sin(27f,,t))]
= Re[s(z) exp( j27f.2)]
where 5(t) is the complex envelope of the FM signal s(t), defined by
5(t) = A, exp[j6 sin(277,,2)] (2.39)

Thus, unlike the original FM signal s(z), the complex envelope (t) is a periodic function
of time with a fundamental frequency equal to the modulation frequency f,,. We may
therefore expand &(t) in the form of a complex Fourier series as follows:

(2.38)

a(t) = s c, exp( j270f,,t) (2.40)

where the complex Fourier coefficient c, is defined by
V2 fy
CG. = fn | §(t) exp(—j2anf,,t) at

-1/2f,,
U2F py (2.41)

= fade | | expliB sin 2nft) — j2mnfl de

Define a new variable:
x = 2nfat (2.42)

Hence, we may rewrite Equation (2.41) in the new form
A. [” ups
Cn = A: | exp[j(@ sin x — nx)] dx (2.43)

The integral on the right-hand side of Equation (2.43), except for a scaling factor, is
recognized as the nth order Bessel function of the first kind* and argument . This function
is commonly denoted by the symbol J,,(8), as shown by

JB) = 5 | expli(B sin x ~ x) de (2.44)

114

CHAPTER 2 CONTINUOUS-WAVE MODULATION

Accordingly, we may reduce Equation (2.43) to

co, = A.],AB) (2.45)

Substituting Equation (2.45) in (2.40), we get, in terms of the Bessel function J,,(8), the
following expansion for the complex envelope of the FM signal:

3(t)= A, dX JalB) exp(j2rnf,,t) (2.46)

nao

Next, substituting Equation (2.46) in (2.38), we get

s(t) = A, * Rel > JAB) exp[j2a(f. + nfl (2.47)

PES me 00

Interchanging the order of summation and evaluation of the real part in the right-hand
side of Equation (2.47), we finally get

cy

s(t) =A, >) JAB) cos[2a(f, + nfn)t] (2.48)

n= —00

This is the desired form for the Fourier series representation of the single-tone FM signal
s(t) for an arbitrary value of 8. The discrete spectrum of s(t) is obtained by taking the
Fourier transforms of both sides of Equation (2.48); we thus have

sip) = 52 DBI ~ fe - fn) + BUF + fe + rfl (2.49)

In Figure 2.23 we have plotted the Bessel function J,,(8) versus the modulation index
B for different positive integer values of 7. We can develop further insight into the behavior

1.06
0.8
0.6

04

JAB)

0.2

 

-0.2

 

 

-0.4
FIGURE 2.23 Plots of Bessel functions of the first kind for varying order.

2.7 Frequency Modulation 115
of the Bessel function J,(8) by making use of the following properties (see Appendix 3 for
more details):

1. JAB) = (-1)7J_,(B) for all , both positive and negative (2.50)

2. For small values of the modulation index 8, we have

Jo(B) = 1
af
Ji(B) = 7) (2.51)

3. > J2s) = 1 (2.52)

Thus, using Equations (2.49)-(2.52) and the curves of Figure 2.23, we may make
he following observations:

1. The spectrum of an FM signal contains a carrier component and an infinite set of
side frequencies located symmetrically on either side of the carrier at frequency sep-
arations of fins 2fns Sf °** ~ In this respect, the result is unlike that which prevails
in an AM system, since in an AM system a sinusoidal modulating signal gives rise
to only one pair of side frequencies.

For the special case of B small compared with unity, only the Bessel coefficients J,(B)

and J,() have significant values, so that the FM signal is effectively composed of a

carrier and a single pair of side frequencies at f. + f,,. This situation corresponds to

the special case of narrowband FM that was considered earlier.

3. The amplitude of the carrier component varies with 8 according to Jo(8). That is,
unlike an AM signal, the amplitude of the carrier component of an FM signal is
dependent on the modulation index 8. The physical explanation for this property is
that the envelope of an FM signal is constant, so that the average power of such a
signal developed across a 1-ohm resistor is also constant, as shown by

1

P= ze (2,53)

N

When the carrier is modulated to generate the FM signal, the power in the side
frequencies may appear only at the expense of the power originally in the carrier,
thereby making the amplitude of the carrier component dependent on 8. Note that
the average power of an FM signal may also be determined from Equation (2.48),
obtaining

Pasa? > 716) (2.54)

n= — 00

Substituting Equation (2.52) into (2.54), the expression for the average power P
reduces to Equation (2.53), and so it should.

> EXAMPLE 2.2

In this example, we wish to investigate the ways in which variations in the amplitude and
frequency of a sinusoidal modulating signal affect the spectrum of the FM signal. Consider

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

 

1.0
f
B=10 —>| 2af ke
(a)
1.0
A | | in f
B=2.0 >| 2af kk
®)
1.0
atl | inva f

 

B=5.0 te >| fn
— 2af aon
(c)

FIGURE 2.24 Discrete amplitude spectra of an FM signal, normalized with respect to the carrier
amplitude, for the case of sinusoidal modulation of fixed frequency and varying amplitude. Only
the spectra for positive frequencies are shown.

first the case when the frequency of the modulating signal is fixed, but its amplitude is varied,
producing a corresponding variation in the frequency deviation Af. Thus, keeping the mod-
ulation frequency f,, fixed, we find that the amplitude spectrum of the resulting FM signal is
as shown plotted in Figure 2.24 for 8 = 1, 2, and 5. In this diagram we have normalized the
spectrum with respect to the unmodulated carrier amplitude.

Consider next the case when the amplitude of the modulating signal is fixed; that is, the
frequency deviation Af is maintained constant, and the modulation frequency f,, is varied.
In this case we find that the amplitude spectrum of the resulting FM signal is as shown
plotted in Figure 2.25 for B = 1, 2, and 5. We see that when Af is fixed and is increased,
we have an increasing number of spectral lines crowding into the fixed frequency interval
fp - Af <|f |< f+ Af. That is, when 6 approaches infinity, the bandwidth of the FM wave

2.7 Frequency Modulation 117

 

 

p=10 DAF

{a)

1.0

Z ba,

2.0 hk 24g ———>}

 

u

(b)

1.0

: li! L, ive ;
™ es _—

FiGurE 2.25 Discrete amplitude spectra of an FM signal, normalized with respect to the carrier
amplitude, for the case of sinusoidal modulation of varying frequency and fixed amplitude. Only
the spectra for positive frequencies are shown.

 

approaches the limiting value of 2Af, which is an important point to keep in mind for later
discussion.

@ TRANSMISSION BANDWIDTH OF FM SIGNALS

In theory, an FM signal contains an infinite number of side frequencies so that the band-
width required to transmit such a signal is similarly infinite in extent. In practice, however,
we find that the FM signal is effectively limited to a finite number of significant side
frequencies compatible with a specified amount of distortion. We may therefore specify
an effective bandwidth required for the transmission of an FM signal. Consider first the

118

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

case of an FM signal generated by a single-tone modulating wave of frequency f,,- In such
an FM signal, the side frequencies that are separated from the carrier frequency f. by an
amount greater than the frequency deviation Af decrease rapidly toward zero, so that the
bandwidth always exceeds the total frequency excursion, but nevertheless is limited. Spe-
cifically, for large values of the modulation index f, the bandwidth approaches, and is
only slightly greater than, the total frequency excursion 2Af in accordance with the situ-
ation shown in Figure 2.25. On the other hand, for small values of the modnlation index
B, the spectrum of the FM signal is effectively limited to the carrier frequency f, and one
pair of side frequencies at f, + fi. 80 that the bandwidth approaches 2f,,. We may thus
define an approximate rule for the transmission bandwidth of an FM signal generated by
a single-tone modulating signal of frequency f,, as follows:

Br = 2Af + 2f,.= 2as( + t) (2.55)

This empirical relation is known as Carson's rule.*

For an alternative assessment of the bandwidth requirement of an FM signal, we
may use a definition based on retaining the maximum number of significant side frequen-
cies whose amplitudes are all greater than some selected value. A convenient choice for
this value is 1 percent of the unmodulated carrier amplitude. We may thus define the
transmission bandwidth of an FM wave as the separation between the two frequencies
beyond which none of the side frequencies is greater than 1 percent of the carrier amplitude
obtained when the modulation is removed. That is, we define the transmission bandwidth
as 2tmaxfns Where f,, is the modulation frequency and #,,» is the largest value of the
integer # that satisfies the requirement |J,(B)| > 0.01. The value of #,,.x varies with the
modulation index 8 and can be determined readily from tabulated values of the Bessel
function J,,(B). Table 2.2. shows the total number of significant side frequencies (including
both the upper and lower side frequencies) for different values of , calculated on the 1
percent basis explained herein. The transmission bandwidth By calculated using this pro-
cedure can be presented in the form of a universal curve by normalizing it with respect to
the frequency deviation Af and then plotting it versus B. This curve is shown in Figure
2.26, which is drawn as a best fit through the set of points obtained by using Table 2.2.
In Figure 2.26 we note that as the modulation index f is increased, the bandwidth occupied

TABLE 2.2 Number of significant side
frequencies of a wideband FM signal for varying

 

 

 

 

 

 

modulation index
Modulation Index Number of Significant Side Frequencies

B 2tynax
0.1 2
0.3 4
0.5 4
1.0 : 6
2.0 8
5.0. 16

10.0 28

20.0 50

30.0 70

 

2.7 Frequency Modulation 119

40 -

 

 

 

 

 

{il
0.1 0.2 04 6 810 2 4 6 810 20 40

FiGure 2.26 Universal curve for evaluating the 1 percent bandwidth of an FM wave.

by the significant side frequencies drops toward that over which the carrier frequency
actually deviates. This means that small values of the modulation index # are relatively
more extravagant in transmission bandwidth than are the larger values of p.

Consider next the more general case of an arbitrary modulating signal m(z) with its
highest frequency component denoted by W. The bandwidth required to transmit an
FM signal generated by this modulating signal is estimated by using a worst-case tone-
modulation analysis. Specifically, we first determine the so-called deviation ratio D, defined
as the ratio of the frequency deviation Af, which corresponds to the maximum possible
amplitude of the modulation signal s(t), to the highest modulation frequency W; these
conditions represent the extreme cases possible. The deviation ratio D plays the same role
for nonsinusoidal modulation that the modulation index B plays for the case of sinusoidal
modulation. Then, replacing 8 by D and replacing f,, with W, we may use Carson’s rule
given by Equation (2.55) or the universal curve of Figure 2.26 to obtain a value for the
transmission bandwidth of the FM signal. From a practical viewpoint, Carson’s rule some-
what underestimates the bandwidth requirement of an FM system, whereas using the uni-
versal curve of Figure 2.26 yields a somewhat conservative result. Thus, the choice of a
transmission bandwidth that lies between the bounds provided by these two rules of thumb
is acceptable for most practical purposes.

B EXAMPLE 2.3

In North America, the maximum value of frequency deviation Af is fixed at 75 kHz for
commercial FM broadcasting by radio. If we take the modulation frequency W = 15 kHz,
which is typically the “maximum” audio frequency of interest in FM transmission, we find
that the corresponding value of the deviation ratio is

D=-7=5

15

Using Carson’s rule of Equation (2.55), replacing B by D, and replacing f,, by W, the ap-
proximate value of the transmission bandwidth of the FM signal is obtained as

By = 2(75 + 15) = 180 kHz

126

CHAPTER 2 ® CONTINUOUS-WAVE MODULATION

On the other hand, use of the curve of Figure 2.26 gives the transmission bandwidth of the
FM signal to be

By = 3.2Af = 3.2 % 75 = 240 kHz

In practice, a bandwidth of 200 kHz is allocated to each FM transmitter. On this basis,
Carson’s rule underestimates the transmission bandwidth by 10 percent, whereas the universal
curve of Figure 2.26 overestimates it by 20 percent. q

= GENERATION OF FM SIGNALS

There are essentially two basic methods of generating frequency-modulated signals,
namely, direct FM and indirect FM. In the direct method the carrier frequency is directly
varied in accordance with the input baseband signal, which is readily accomplished using
a voltage-controlled oscillator. In the indirect method, the modulating signal is first used
to produce a narrowband FM signal, and frequency multiplication is next used to increase
the frequency deviation to the desired level, The indirect method is the preferred choice
for frequency modulation when the stability of carrier frequency is of major concern as in
commercial radio broadcasting, as described next.

Indirect FM’

A simplified block diagram of an indirect FM system is shown in Figure 2.27. The
message (baseband) signal m(¢) is first integrated and then used to phase-modulate a
crystal-controlled oscillator; the use of crystal control provides frequency stability. To
minimize the distortion inherent in the phase modulator, the maximum phase deviation
or modulation index B is kept small, thereby resulting in a narrowband FM signal; for the
implementation of the narrow-band phase modulator, we may use the arrangement de-
scribed in Figure 2.21, The narrowband FM signal is next multiplied in frequency by means
of a frequency multiplier so as to produce the desired wideband FM signal.

A frequency multiplier consists of a nonlinear device followed by a band-pass filter,
as shown in Figure 2.28. The implication of the nonlinear device being memoryless is that
it has no energy-storage elements. The input-output relation of such a device may be
expressed in the general form

v(t) = ays(t) + ays2(t) + +++ + a,s*(£) (2.56)

where 44, do,..., 4, are coefficients determined by the operating point of the device, and
n is the bighest order of nonlinearity. In other words, the memoryless nonlinear device is
an ath power-law device. The input s(t) is an FM signal defined by

s(t) = A, cos] 2 + amk, [ m(r) ar|

 

Baseband Narrowband :
signal Integrator -—s| phase -—s| Paiene —> FM signal
mt) modulator

ft

Crystal-
controlled
oscillator

 

 

 

 

 

 

FIGURE 2.27 Block diagram of the indirect method of generating a wideband FM signal.

2.7 Frequency Modulation 121

 

FM signal s‘(z) with
carrier frequency wf,
and modulation
index ng

FM signal s()
with carrier frequency
f, and modulation
index 8

ut) Band-pass filter
with midband
frequency af,

Memoryless

———— a " :
nonlinear device

 

 

 

 

Figure 2.28 Block diagram of frequency multiplier.

whose instantaneous frequency is
filt) = fo + Remit) (2.57)

The mid-band frequency of the band-pass filter in Figure 2.28 is set equal to #f,, where f,
is the carrier frequency of the incoming FM signal s(t). Moreover, the band-pass filter is
designed to have a bandwidth equal to » times the transmission bandwidth of s(t). In
Section 2.8 dealing with nonlinear effects in FM systems, we describe the spectral contri-
butions of such nonlinear terms as the second- and third-order terms in the input-output
relation of Equation (2.56). For now it suffices to say that after band-pass filtering of the
nonlinear device’s output v(t), we have a new FM signal defined by

s(t) = Al cos| 2a + 2ank; [ m(z7) ar| (2.58)

whose instantaneous frequency is
FAt) = nf. + nkymi(t) (2.59)

Thus, comparing Equation (2.59) with (2.57), we see that the nonlinear processing circuit
of Figure 2.28 acts as a frequency multiplier. The frequency multiplication ratio is deter-
mined by the highest power x in the input-output relation of Equation (2.56), character-
izing the memoryless nonlinear device.

6 DEMODULATION OF FM SIGNALS

 

Frequency demodulation is the process that enables us to recover the original modulating
signal from a frequency-modulated signal. The objective is to produce a transfer charac-
teristic that is the inverse of that of the frequency modulator, which can be realized directly
or indirectly. Here we describe a direct method of frequency demodulation involving the
use of a popular device known as a frequency discriminator, whose instantaneous output
amplitude is directly proportional to the instantaneous frequency of the input FM signal.
In Section 2.14, we describe an indirect method of frequency demodulation that uses
another popular device known as a phase-locked loop.

Basically, the frequency discriminator consists of a slope circuit followed by an en-
velope detector. An ideal slope circuit is characterized by a frequency response that is
purely imaginary, varying linearly with frequency inside a prescribed frequency interval.
Consider the frequency response depicted in Figure 2.294, which is defined by

 

; Br Br By
+ <fsf4+—
jamal f+ ), fe~ FSH SLAG

 

H,(f) = jana(f +f 22), 4-2 Br (2.60)

<fs-f.+
7) 3 BFS ET G

0, elsewhere

122

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

Af)

  
    

Slope = 27a

 

@ (b)

 

(c)

FIGURE 2.29 (a) Frequency response of ideal slope circuit. (b) Frequency response of the slope
circuit's complex low-pass equivalent. (c) Frequency response of the ideal slope circuit comple-
mentary to that of part (a).

where a is a constant. We wish to evaluate the response of this slope circuit, denoted by
s(t), which is produced by an FM signal s(t) of carrier frequency f, and transmission
bandwidth Br. It is assumed that the spectrum of s(t) is essentially zero outside the fre-
quency interval f. — Bp/2 =|f| =f. + Br/2.

We may simplify the analysis of the frequency discriminator by invoking the iso-
morphism between a real-valued band-pass filter and a correspondirig complex-valued
low-pass filter. This isomorphism is discussed in Appendix 2. According to the material
presented in that appendix, we may replace the band-pass filter with frequency response
H,(f) with an equivalent low-pass filter with frequency response H,(f) by doing two
things:

1. We shift H,(f) to the right by f., where f, is the midband frequency of the band-
pass filter; this operation aligns the translated frequency response of the equivalent
low-pass filter with that of the band-pass filter.

2. We set A,(f — f.) equal to 2H,(f) for f > 0.

Thus for the problem at hand we get

 

Af - f.) =2H(f), f>0 (2.61)
Hence, using Equations (2.60) and (2.61), we get
7 : Br Br Br
Fy(f) = jena f a) ), 2 =f59 (2.62)
elsewhere

>

which is plotted in Figure 2.29b.

2.7 Frequency Modulation 123

The incoming FM signal s(t) is defined by Equation (2.26), which is reproduced here
for convenience:

s(t) = A, cos] 2a + 2ak, [, m(7) ar|

Given that the carrier frequency f, is high compared to the transmission bandwidth of the
FM signal s(t), the complex envelope of s(¢) is

a(t) = A, exp, [ m(r) ar| (2.63)

Let §,(¢) denote the complex envelope of the response of the slope circuit defined by
Figure 2.294 due to &(t). Then, following the material presented in Appendix 2, we may
express the Fourier transform of 5,(t) as follows:

= 1. =
Sif) = 5 AUAS(F)
(2.64)
ry = Br B
janal f+ + Bap, 7 shez
0, elsewhere

where 5(f) is the Fourier transform of 3(t). From Fourier analysis we know that multipli-
cation of the Fourier transform of a signal by j27f is equivalent to differentiating the
signal in the time domain; see item 8 of Table A6.2. Hence, from Equation (2.64) we
deduce

 

$,(t) = | ) + jaBrili | (2.65)
Substituting Equation (2.63) into (2.65), we get
§,(f) = jmBoaA.| 1 + 2 mt exp| jam, [, m(z) ar| (2.66)
T

The desired response of the slope circuit is therefore
silt) = Re[8,(¢) exp( j27f1)|
2k 1 a) (2.67)
= 7ByAJ1+ B, m(t) | cos} 2af.t + 2k, [, m(t) dr + 3

The signal s,(t) is a hybrid-modulated signal in which both amplitude and frequency of
the carrier wave vary with the message signal m(t). However, provided that we choose
2h
Br
then we may use an envelope detector to recover the amplitude variations and thus, except
for a bias term, obtain the original message signal. The resulting envelope-detector output
is therefore

m(t)| <1 for all ¢

 

 

|5,(t)| = mByaA, E + a mi (2.68)

The bias term 7B7aA, in the right-hand side of Equation (2.68) is proportional to
the slope a of the transfer function of the slope circuit. This suggests that the bias may be

124 CHAPTER 2 CONTINUOUS-WAVE MODULATION

 

Slope circuit Envelope
Hf) > detector

 

Baseband

FM wave
signal

 

Slope circuit Envelope
Hf) > detector

 

 

 

 

 

Figure 2.30 Block diagram of frequency discriminator.

removed by subtracting from the envelope-detector output | §,(t)| the output of a second
envelope detector preceded by the complementary slope circuit with a frequency response
H,(f) as described in Figure 2.29¢. That is, the two slope circuits are related by

A,(f) = Hy(—f) (2.69)

Let s(t) denote the response of the complementary slope circuit produced by the incoming
FM signal s(t). Then, following a procedure similar to that just described, we may write

|3,(¢)| = mya) _ ake mt (2.70)
Br

where §,(t) is the complex envelope of the signal s,(t). The difference between the two

envelopes in Equations (2.68) and (2.70) is

solt) = |81()| — |$2(2)|
4ak aA m(t)

(2.71)

which is a scaled version of the original message signal 72(¢) and free from bias.

We may thus model the ideal frequency discriminator as a pair of slope circuits with
their complex transfer functions related by Equation (2.69), followed by envelope detectors
and finally a summer, as in Figure 2.30. This scheme is called a balanced frequency
discriminator.

FM STEREO MULTIPLEXINGS

Stereo multiplexing is a form of frequency-division multiplexing (FDM) designed to trans-
mit two separate signals via the same carrier. It is widely used in FM radio broadcasting
to send two different elements of a program (e.g., two different sections of an orchestra,
a vocalist and an accompanist) so as to give a spatial dimension to its perception by a
listener at the receiving end.

The specification of standards for FM stereo transmission is influenced by two
factors:

1. The transmission has to operate within the allocated FM broadcast channels.
2. It has to be compatible with monophonic radio receivers.

The first requirement sets the permissible frequency parameters, including frequency de-
viation. The second requirement constrains the way in which the transmitted signal is
configured.

Figure 2.314 shows the block diagram of the multiplexing system used in an FM
stereo transmitter. Let ,(t) and m,(t) denote the signals picked up by left-hand and

2.7 Frequency Modulation 125

Mairixer

    
 

Frequency
doubler

 

cos (22f.1)
{a)

Matrixer

 

mm) + mm)

    

 

 

 

 

 
  
   

 

 

 

 

 

 

 

 

 

 

 

Baseband
LPF 2m, (0)
BPF
m(t) centered at Baseband 2m,@)
2f,= 38 kHz m,(t) ~ m,(t)
Frequency
doubler
Narrowband
filter tuned to
F-= 19 kHz

 

 

 

(b)

Ficure 2.31 (a) Multiplexer in transmitter of FM stereo. (6) Demultiplexer in receiver of FM
stereo.

right-hand microphones at the transmitting end of the system. They are applied to a
simple matrixer that generates the sum signal, m,(t) + m,(t), and the difference signal,
m,(t) — m,(t), The sum signal is left unprocessed in its baseband form; it is available for
monophonic reception. The difference signal and a 38-kHz subcarrier (derived from a 19-
kHz crystal oscillator by frequency doubling) are applied to a product modulator, thereby
producing a DSB-SC modulated wave. In addition to the sum signal and this DSB-SC
modulated wave, the multiplexed signal m(t) also includes a 19-kHz pilot to provide a
reference for the coherent detection of the difference signal at the stereo receiver. Thus the
multiplexed signal is described by

m(t) = [11,(t) + m,(t)] + [ot — m,(t)] cos(4af.£) + K cos(27f.t) (2.72)

where f, = 19 kHz, and K is the amplitude of the pilot tone. The multiplexed signal m(t)
then frequency-modulates the main carrier to produce the transmitted signal. The pilot is
allotted between 8 and 10 percent of the peak frequency deviation; the amplitude K in
Equation (2.72) is chosen to satisfy this requirement.

At a stereo receiver, the multiplexed signal 7(t) is recovered by frequency demodu-
lating the incoming FM wave. Then m(t) is applied to the demultiplexing system shown

126 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

in Figure 2.31. The individual components of the multiplexed signal m/(t) are separated
by the use of three appropriate filters. The recovered pilot (using a narrowband filter tuned
to 19 kHz) is frequency doubled to produce the desired 38-kHz subcarrier. The availability
of this subcarrier enables the coherent detection of the DSB-SC modulated wave, thereby
recovering the difference signal, ,(t) — ,(t). The baseband low-pass filter in the top
path of Figure 2.310 is designed to pass the sum signal, #,(t) + ,(t). Finally, the simple
matrixer reconstructs the left-hand signal #;(t) and right-hand signal »,(t), except for
scaling factors, and applies them to their respective speakers.

| 2.8 Nonlinear Effects in FM Systems

In the preceding two sections, we studied frequency modulation theory and methods for
its generation and demodulation. We complete the discussion of frequency modulation by
considering nonlinear effects in FM systems.

Nonlinearities, in one form or another, are present in all electrical networks. There
are two basic forms of nonlinearity to consider:

 

1. The nonlinearity is said to be strong when it is introduced intentionally and in a
controlled manner for some specific application. Examples of strong nonlinearity
include square-law modulators, hard-limiters, and frequency multipliers.

2. The nonlinearity is said to be weak when a linear performance is desired, but non-
linearities of a parasitic nature arise due to imperfections. The effect of such weak
nonlinearities is to limit the useful signal levels in a system and thereby become an
important design consideration.

In this section we examine the effects of weak nonlinearities on frequency modulation,
Consider a communications channel, the transfer characteristic of which is defined
by the nonlinear input—output relation

v(t) = ayy(t) + aur (t) + asvi(t) (2.73)

where u,(t) and v,(t) are the input and output signals, respectively, and a, a2, and a3 are
constants; Equation (2.73) is a truncated version of Equation (2.56) used in the context
of frequency multiplication. The channel described in Equation (2.73) is said to be me-
moryless in that the output signal v,(t) is an instantaneous function of the input signal
v,(t) (Le., there is no energy storage involved in the description). We wish to determine the
effect of transmitting-a frequency-modulated wave through such a channel. The FM signal
is defined by

u(t) = A, cosl2aft + o(4)]

where

b(t).= ak, I, m(r) dr

For this input signal, the use of Equation (2.73) yields

v,(t) = a, A, cos[2af.t + b(t)] + 4.A2 cos*[2af.t + b(2)]

4
+ a3A3 cos(2af.t + 6(2)] a

2.8 Nonlinear Effects in FM Systems 127

Expanding the squared and cubed cosine terms in Equation (2.74) and then collecting
common terms, we get

1
v(t) = 5 a,A2 + (aa. + 2 «At cos[2af.t + o(t)]

+ 5 aA? cos[4af.t + 2¢ (t)] (2.75)

+ 5 a,A2 cos[6af.t + 3(t)]

Thus the channel output consists of a DC component and three frequency-modulated
signals with carrier frequencies f., 2f., and 3f.; the latter components are contributed by
the linear, second-order, and third-order terms of Equation (2.73), respectively.

To extract the desired FM signal from the channel output v,(¢), that is, the particular
component with carrier frequency, f,, it is necessary to separate the FM signal with this
carrier frequency from the one with the closest carrier frequency, 2f,. Let Af denote the
frequency deviation of the incoming FM signal v,(t), and W denote the highest frequency
component of the message signal s(t). Then, applying Carson’s rule and noting that the
frequency deviation about the second harmonic of the carrier frequency is doubled, we
find that the necessary condition for separating the desired FM signal with the carrier
frequency f, from that with the carrier frequency 2f, is

2f,- QAf+W)>f+Af+w

f. > 3Af + 2W (2.76)

Thus, by using a band-pass filter of midband frequency f, and bandwidth 2Af + 2W, the
channel output is reduced to

u(t) = (aa. + 3 aA?) cos[2af.t + o(t)] (2.77)

We see therefore that the only effect of passing an FM signal through a channel with
amplitude nonlinearities, followed by appropriate filtering, is simply to modify its ampli-
tude. That is, unlike amplitude modulation, frequency modulation is not affected by dis-
tortion produced by transmission through a channel with amplitude nonlinearities, It is
for this reason that we find frequency modulation used in microwave radio systems: It
permits the use of highly nonlinear amplifiers and power transmitters, which are particu-
larly important to producing a maximum power output at radio frequencies.

An FM system is extremely sensitive to phase nonlinearities, however, as we would
intuitively expect. A common type of phase nonlinearity that is encountered in microwave
radio systems is known as AM-to-PM conversion. This is the result of the phase charac-
teristic of repeaters or amplifiers used in the system being dependent on the instantaneous
amplitude of the input signal. In practice, AM-to-PM conversion is characterized by a
constant K, which is measured in degrees per dB and may be interpreted as the peak phase
change at the output for a 1-dB change in envelope at the input. When an FM wave is
transmitted through a microwave radio link, it picks up spurious amplitude variations due
to noise and interference during the course of transmission, and when such an FM wave
is passed through a repeater with AM-to-PM conversion, the output will contain unwanted

128 CHAPTER 2 CoNTINUOUS-WAVE MODULATION

phase modulation and resultant distortion. It is therefore important to keep the AM-to.
PM conversion at a low level. For example, for a good microwave repeater, the AM-to.
PM conversion constant K is less than 2 degrees per dB.

| 2.9 Superheterodyne Receiver’

In a broadcasting system, irrespective of whether it is based on amplitude modulation or
frequency modulation, the receiver not only has the task of demodulating the incoming
modulated signal, but it is also required to perform some other system functions:

» Carrier-frequency tuning, the purpose of which is to select the desired signal (ije,,
desired radio or TV station).

» Filtering, which is required to separate the desired signal from other modulated sig-
nals that may be picked up along the way.

» Amplification, which is intended to compensate for the loss of signal power incurred
in the course of transmission.

The superheterodyne receiver, or superbet as it is often referred to, is a special type of
receiver that fulfills all three functions, particularly the first two, in an elegant and practical
fashion. Specifically, it overcomes the difficulty of having to build a tunable highly selective
and variable filter. Indeed, practically all radio and TV receivers now being made are of
the superheterodyne type.

Basically, the receiver consists of a radio-frequency (RF) section, a mixer and local
oscillator, an intermediate-frequency (IF) section, demodulator, and power amplifier. Typ-
ical frequency parameters of commercial AM and FM radio receivers are listed in Table
2.3. Figure 2.32 shows the block diagram of a superheterodyne receiver for amplitude
modulation using an envelope detector for demodulation.

The incoming amplitude-modulated wave is picked up by the receiving antenna and
amplified in the RF section that is tuned to the carrier frequency of the incoming wave.
The combination of mixer and local oscillator (of adjustable frequency) provides a het-
erodyning function, whereby the incoming signal is converted to a predetermined fixed
intermediate frequency, usually lower than the incoming carrier frequency. This frequency
translation is achieved without disturbing the relation of the sidebands to the carrier; see
Section 2.4. The result of the heterodyning is to produce an intermediate-frequency carrier

defined by
fu = fro — fre (2.78)

where fio is the frequency of the local oscillator and fpr is the carrier frequency of the
incoming RF signal. We refer to ftp as the intermediate frequency (IF), because the signal

TABLE 2.3 Typical frequency parameters of AM and FM
radio receivers

 

AM Radio FM Radio
RF carrier range 0.535-1.605 MHz 88-108 MHz
Midband frequency of IF section 0.455 MHz 10.7 MHz

TF bandwidth 10 kHz 200 kHz

 

 

 

 

 

2.9 Superheterodyne Receiver 129

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Antenna
a
/ Loudspeaker
7
RF . IF Envelope Audio
section am) Mixer > section | detector >| amplifier >
7
/
La
/
/ Common (~) Local
7 tuning ~~~ oscillator

Ficure 2.32 Basic elements of an AM radio receiver of the superheterodyne type.

is neither at the original input frequency nor at the final baseband frequency. The mixer—
local oscillator combination is sometimes referred to as the first detector, in which case
the demodulator is called the second detector.

The IF section consists of one or more stages of tuned amplification, with a band-
width corresponding to that required for the particular type of modulation that the receiver
is intended to handle. The IF section provides most of the amplification and selectivity in
the receiver. The output of the IF section is applied to a demodulator, the purpose of which
is to recover the baseband signal. If coherent detection is used, then a coherent signal
source must be provided in the receiver. The final operation in the receiver is the power
amplification of the recovered message signal.

In a superheterodyne receiver the mixer will develop an intermediate frequency out-
put when the input signal frequency is greater or less than the local oscillator frequency
by an amount equal to the intermediate frequency. That is, there are two input frequencies,
namely, | fro + f|, which will result in fy at the mixer output. This introduces the
possibility of simultaneous reception of two signals differing in frequency by twice the
intermediate frequency. For example, a receiver tuned to 0.65 MHz and having an IF of
0.455 MHz is subject to an image interference at 1.56 MHz; indeed, any receiver with this
value of IF, when tuned to any station, is subject to image interference at a frequency of
0.910 MHz higher than the desired station. Since the function of the mixer is to produce
the difference between two applied frequencies, it is incapable of distinguishing between
the desired signal and its image in that it produces an IF output from either one of them.
The only practical cure for image interference is to employ highly selective stages in the
RF section (i.e., between the antenna and the mixer) in order to favor the desired signal
and discriminate against the undesired or image signal. The effectiveness of suppressing
unwanted image signals increases as the number of selective stages in the RF section in-
creases, and as the ratio of intermediate to signal frequency increases.

The basic difference between AM and FM superheterodyne receivers lies in the use
of an FM demodulator such as limiter-frequency discriminator. In an FM system, the
message information is transmitted by variations of the instantaneous frequency of a
sinusoidal carrier wave, and its amplitude is maintained constant. Therefore, any varia-
tions of the carrier amplitude at the receiver input must result from noise or interference.
An amplitude limiter, following the IF section, is used to remove amplitude variations
by clipping the modulated wave at the IF section output almost to the zero axis. The
resulting rectangular wave is rounded off by a band-pass filter that suppresses har-
monics of the carrier frequency. Thus the filter output is again sinusoidal, with an ampli-
tude that is practically independent of the carrier amplitude at the receiver input (see
Problem 2.42),

130 CHAPTER 2 CONTINUOUS-WAVE MODULATION

| 2.10 Noise in CW Modulation Systems

 

Up to this point in our discussion we have focused attention on the characterization of
continuous-wave (CW) modulation techniques, entirely from a deterministic perspective,
In the remainder of the chapter, we study the effects of channel noise on the reception of
CW modulated signals and thereby develop a deeper understanding of the behavior of
analog communications.

To undertake such a study we follow the customary practice by formulating two
models:

1. Channel model, which assumes a communication channel that is distortionless but
perturbed by additive white Gaussian noise (AWGN).

2. Receiver model, which assumes a receiver consisting of an ideal band-pass filter fol-
lowed by an ideal demodulator appropriate for the application at hand; the band-
pass filter is used to minimize the effect of channel noise.

These simplifying assumptions are made in order to obtain a basic understanding of the
way in which noise affects the performance of the receiver. Moreover, they provide a
framework for the comparison of different CW modulation-demodulation schemes.

Figure 2.33 shows the noisy receiver model that combines the above two assump-
tions. In this figure, s(t) denotes the incoming modulated signal and w(t) denotes the
channel noise. The received signal is therefore made up of the sum of s(¢) and w(t); this is
the signal that the receiver has to work on. The band-pass filter in the model of Figure
2.33 represents the combined filtering action of the tuned amplifiers used in the actual
receiver for the purpose of signal amplification prior to demodulation. The bandwidth of
this band-pass filter is just wide enough to pass the modulated signal s(t) without distor-
tion. As for the demodulator in the model of Figure 2.33, its details naturally depend on
the type of modulation used.

# SIGNAL-TO-NOISE RATIOS: BASIC DEFINITIONS

Let the power spectral density of the noise #(t) be denoted by No/2, defined for both
positive and negative frequencies; that is, No is the average noise power per unit bandwidth
measured at the front end of the receiver. We also assume that the band-pass filter in the
receiver model of Figure 2.33 is ideal, having a bandwidth equal to the transmission band-
width By of the modulated signal s(t) and a midband frequency equal to the carrier fre-
quency f.. The latter assumption is justified for double sideband—suppressed carrier (DSB-
SC) modulation, full amplitude modulation (AM), and frequency modulation (FM); the
cases of single sideband (SSB) modulation and vestigial sideband (VSB) modulation require
special considerations. Taking the midband frequency of the band-pass filter to be the
same as the carrier frequency f., we may model the power spectral density Sx(f) of the
noise x(t), resulting from the passage of the white noise w(t) through the filter, as shown

Modulated + x()
signal or Bane bass >>| Demodulator oe
s(t)
+

Noise
wit)

 

 

 

 

 

 

 

 

FIGURE 2.33 Receiver model.

2:10 Noise in CW Modulation Systems 131

in Figure 2.34. Typically, the carrier frequency f. is large compared to the transmission
bandwidth By. We may therefore treat the filtered noise n(t) as a narrowband noise rep-
resented in the canonical form

n(t) = n,(t) cos(2af.t) — molt) sin(27f.t) (2.79)

where #;(t) is the in-phase noise component and Q(t) is the quadrature noise component,
both measured with respect to the carrier wave A, cos(27rf,t). The filtered signal x(t) avail-
able for demodulation is defined by :

x(t) = s(t) + n(t) (2.80)

The details of s(t) depend on the type of modulation used. In any event, the average noise
power at the demodulator input is equal to the total area under the curve of the power
spectral density 5,(f). From Figure 2.34 we readily see that this average noise power is
equal to NoBy. Given the format of s(¢), we may also determine the average signal power
at the demodulator input. With the demodulated signal s(t) and the filtered noise (n(t)
appearing additively at the demodulator input in accordance with Equation (2.80), we
may go on to define an input signal-to-noise ratio, (SNR);, as the ratio of the average
power of the modulated signal s(t) to the average power of the filtered noise n(t).

A more useful measure of noise performance, however, is the output signal-to-noise
ratio, (SNR) 0, defined as the ratio of the average power of the demodulated message signal
to the average power of the noise, both measured at the receiver output. The output signal-
to-noise ratio provides an intuitive measure for describing the fidelity with which the de-
modulation process in the receiver recovers the message signal from the modulated signal
in the presence of additive noise. For such a criterion to be well defined, the recovered
message signal and the corruptive noise component must appear additively at the demod-
ulator output. This condition is perfectly valid in the case of a receiver using coherent
detection. On the other hand, when the receiver uses envelope detection as in full AM or
frequency discrimination as in FM, we have to assume that the average power of the filtered
noise u(t) is relatively low to justify the use of output signal-to-noise ratio as a measure of
receiver performance.

The output signal-to-noise ratio depends, among other factors, on the type of mod-
ulation used in the transmitter and the type of demodulation used in the receiver. Thus it
is informative to compare the output signal-to-noise ratios for different modulation-
demodulation systems. However, for this comparison to be of meaningful value, it must
be made on an equal basis as described here:

»® The modulated signal s(z) transmitted by each system has the same average power.

» The channel noise w(t) has the same average power measured in the message band-
width W.

 

fe ie) te

Ficure 2.34 Idealized characteristic of band-pass filtered noise.

132 CHAPTER 2 CONTINUOUS-WAVE MODULATION

 

 

 

 

Message signal with Low-pass
the same power as the filter of |} Output
modulated wave 4 bandwidth W
Noise
w(t)

FiGure 2.35 The baseband transmission model, assuming a message signal of bandwidth W,
used for calculating the channel signal-to-noise ratio.

Accordingly, as a frame of reference we define the channel signal-to-noise ratio, (SNR)c,
as the ratio of the average power of the modulated signal to the average power of channel
noise in the message bandwidth, both measured at the receiver input. This definition is
illustrated in Figure 2.35.

For the purpose of comparing different continuous-wave (CW) modulation systems,
we normalize the receiver performance by dividing the output signal-to-noise ratio by the
channel signal-to-noise ratio. We thus define a figure of merit for the receiver as follows:

: .. _ (SNR)o
Figure of merit (SNR)c (2.81)
Clearly, the higher the value of the figure of merit, the better will the noise performance
of the receiver be. The figure-of merit may.equal one, be less than one, or be greater than
one, depending on the type of modulation used, which will become apparent from the
discussion that follows.

2.11 Noise in Linear Receivers
Using Coherent Detection

 

 

From Sections 2.2 and 2.3 we recall that the demodulation of an amplitude-modulated
wave depends on whether the carrier is suppressed or not. When the carrier is suppressed
we usually require the use of coherent detection, in which case the receiver is linear. On
the other hand, when the amplitude modulation includes transmission of the carrier, de-
modulation is accomplished simply by using an envelope detector, in which case the re-
ceiver is nonlinear. In this section we study the effect of noise on the performance of a
linear receiver. The more difficult case of a nonlinear receiver is deferred to Section 2.12.

Consider the case of DSB-SC modulation Figure 2.36 shows the model of a DSB-SC
receiver using a coherent detector. The use of coherent detection requires multiplication
of the filtered signal x(z) by.a locally generated sinusoidal wave cos(27f,t) and then low-
pass filtering the product. To simplify the analysis, we assume that the amplitude of the
locally generated sinusoidal wave is unity. For this demodulation scheme to operate sat-
isfactorily, however, it is necessary that the local oscillator be synchronized both in phase
and in frequency with the oscillator generating the carrier wave in the transmitter. We
assume that this synchronization has been achieved.

The DSB-SC component of the filtered signal x(¢) is expressed as

s(t) = CA, cos(27f.t)m(#) (2.82)
where A, cos(27f,t) is the sinusoidal carrier wave and m(t) is the message signal. In the
expression for s(t) in Equation (2.82) we have included a system-dependent scaling factor

C, the purpose of which is to ensure that the signal component s(¢) is measured in the same
units as the additive noise component x(t). We assume that 772(£) is the sample function of

Coherent datector

 

 

 

2.11 Noise in Linear Receivers Using Coherent Detection 133
v(t) '

L .| Low-pass — y

I

DSB-sc __.* Band-pass [7
signal s(t) filter filter
* ]
- |
Noise te (20.0

w(t}

Product
modulator

 

 

 

 

 

 

 

q

 

 

Local -
oscillator

 

 

 

FIGURE 2.36 Model of DSB-SC receiver using coherent detection.

a stationary process of zero mean, whose power spectral density Su(f} is limited to a
maximum frequency W; that is, W is the message bandwidth. The average power P of the
message signal is the total area under the curve of power spectral density, as shown by

Ww
p= | suf) df (2.83)

The carrier wave is statistically independent of the message signal. To emphasize this
independence, the carrier should include a random phase that is uniformly distributed over
27 radians. In the defining equation for s(t) this random phase angle has been omitted for
convenience of presentation. Using the result of Example 1.7 of Chapter 1 on a modulated
random process, we may express the average power of the DSB-SC modulated signal
component s(¢) as C?A2P/2. With a noise spectral density of No/2, the average noise power
in the message bandwidth W is equal to WNo. The channel signal-to-noise ratio of the
DSB-SC modulation system is therefore

C°A2P
2WNo
where the constant C? in the numerator ensures that this ratio is dimensionless.
Next, we wish to determine the output signal-to-noise ratio of the system. Using the
narrowband representation of the filtered noise 7(t), the total signal at the coherent detec-
tor input may be expressed as
x(t) = s(t) + n(t)
CA, cos(2mf,t)m(t) + a(t) cos(2af.t) — no(t) sin(2af.t)
where m;(t) and #o(t) are the in-phase and quadrature components of m(¢) with respect to

the carrier. The output of the product-modulator component of the coherent detector is
therefore

(SNR)cpsp = (2.84)

(2.85)

u(t) = x(t) cos(27f,2)
3CA g(t) + 5n;(t)
+ 3[CA ant) + 2,(t)] cos(4af.t) — }no(t) sin(4f.t)
The low-pass filter in the coherent detector in Figure 2.36 removes the high-frequency
components of v(z), yielding the receiver output ;

y(t) = ZCAan(t) + 3m(t) (2.86)
Equation (2.86) indicates the following:

1. The message signal #(t) and in-phase noise component #;(£) of the filtered noise n(t)
appear additively at the receiver output.

134

CHAPTER 2 CONTINUOUS-WAVE MODULATION

2. The quadrature component z(t) of the noise n(t) is completely rejected by the co-
herent detector.

These two results are independent of the input signal-to-noise ratio. Thus, coherent detec.
tion distinguishes itself from other demodulation techniques in an important property: The
output message component is unmutilated and the noise component always appears ad-
ditively with the message, irrespective of the input signal-to-noise ratio.

The message signal component at the receiver output is CA m(t)/2. Therefore, the
average power of this component may be expressed as C*A2P/4, where P is the average
power of the original message signal m(t) and C is the system-dependent scaling factor
referred to earlier.

In the case of DSB-SC modulation, the band-pass filter in Figure 2.36 has a band-
width B; equal to 2W in order to accommodate the upper and lower sidebands of the
modulated signal s(#). It follows therefore that the average power of the filtered noise n(z)
is 2WNp. From the discussion of narrowband noise presented in Section 1.11, we know
that the average power of the (low-pass) in-phase noise component #;(¢) is the same as
that of the (band-pass) filtered noise n(t). Since from Equation (2.86) the noise component
at the receiver output is #,(#)/2, it follows that the average power of the noise at the receiver
output is

(5)°2WNo = 3WNo

The output signal-to-noise for a DSB-SC receiver using coherent detection is therefore

 

C7A2P/4
(SNR)o, pspsc = WN, /2- os
_ C*A2P $7
2WNo
Using Equations (2.84) and (2.87), we obtain the figure of merit
(SNR)o (2.88)

 

(SNR)c DSB-SC -

Note that the factor C? is common to both the output and channel signal-to-noise ratios,
and therefore cancels out in evaluating the figure of merit.

Following through the noise analysis of a coherent detector for SSB, we find that,
despite the fandamental differences between it and the coherent detector for DSB-SC mod-
ulation, the figure of merit is exactly the same for both of them; see Problem 2.49.

The important conclusions to-be drawn from the discussions presented in this section
and Problem 2.49 are two-fold:

1. For the same average transmitted or modulated signal power and the same average
noise power in the message bandwidth, a coherent SSB receiver wili have exactly the
same output signal-to-noise ratio as a coherent DSB-SC receiver.

2. In both cases, the noise performance of the receiver is exactly the same as that ob-
tained by simply transmitting the message signal in the presence of the same channel
noise. The only effect of the modulation process is to translate the message signal to
a different frequency band to facilitate its transmission over a band-pass channel.

Simply put, neither DSB-SC modulation nor SSB modulation offers the means for a trade-
off between improved noise performance and increased channel bandwidth. This is a se
rious problem when high quality of reception is a requirement.

2.12 Noise in AM Receivers Using Envelope Detection 135

2,12 Noise in AM Receivers
Using Envelope Detection

The next noise analysis we perform is for an amplitude modulation (AM) system using an
envelope detector in the receiver, as shown in the model of Figure 2.37. In a full AM signal,
both sidebands and the carrier wave are transmitted, as shown by

s(t) = A,[1 + k,en(t)] cos(27f.t) (2.89)

where A, cos(27f,t) is the carrier wave, 7(t) is the message signal, and &, is a constant
that determines the percentage modulation. In the expression for the amplitude-modulated
signal component s(t) given in Equation (2.89), we see no need for the use of a scaling
factor, because it is reasonable to assume that the carrier amplitude A, has the same units
as the additive noise component.

The average power of the carrier component in the AM signal s(t) is A2/2. The
average power of the information-bearing component A,k,mm(t) cos(27f,t) is A2k2P/2,
where P is the average power of the message signal m(z). The average power of the full
AM signal s(t) is therefore equal to A2(1 + &&P)/2. As for the DSB-SC system, the average
power of noise in the message bandwidth is WNp. The channel signal-to-noise ratio for
AM is therefore

A2(1 + k2P)
(SNR)cam = 2WN, (2.90)

To evaluate the output signal-to-noise ratio, we first represent the filtered noise x(t)
in terms of its in-phase and quadrature components. We may therefore define the filtered
signal x(t) applied to the envelope detector in the receiver model of Figure 2.37 as follows:

x(t) = s(t) + n(t)

[A. + A,k,mlt) + nj(t)] cos(2af.t) — ng(t) sin(27f,t) (2.91)

It is informative to represent the components that comprise the signal x(t) by means of
phasors, as in Figure 2.384. From this phasor diagram, the receiver output is readily ob-
tained as

envelope of x(z)
{[Act+ Ak,malt) + mye)? + 2o(e)y’?

(0) (2.92)

i

The signal y(z) defines the output of an ideal envelope detector. The phase of x(t) is of no
interest to us, because an ideal envelope detector is totally insensitive to variations in the
phase of x(z).

The expression defining y(t) is somewhat complex and needs to be simplified in some
manner to permit the derivation of insightful results. Specifically, we would like to ap-
proximate the output y(t) as the sum of a message term plus a term due to noise. In general,
this is quite difficult to achieve. However, when the average carrier power is large com-

 

 

Output
-—————»- signal
>t)

AM signal Band-pass x() Envelope
s(t) filter detector

 

 

 

 

 

 

Noise
w(t)

FiGure 2.37 Model of AM receiver.

136

CuHarrer 2 & CONTINUOUS-WAVE MODULATION

   

Resultant y(}

 
 

A. (1 + &,m()] nt)

{a)

r{(z)

(b)
Ficure 2.38 (a) Phasor diagram for AM wave plus narrowband noise for the case of high car-

Tier-to-noise ratio. (b) Phasor diagram for AM wave plus narrowband noise for the case of low
carrier-to-noise ratio.

pared with the average noise power, so that the receiver is operating satisfactorily, then
the signal term A,[1 + k,on(t)] will be large compared with the noise terms m;(¢) and no(t},
at least most of the time. Then we may approximate the output y(t) as (see Problem 2.51):

y(t) = A, + A,R,m(t) + 1,(t) (2.93)

The presence of the DC or constant term A, in the envelope detector output y(t) of
Equation (2.93) is due to demodulation of the transmitted carrier wave. We may ignore
this term, however, because it bears no relation whatsoever to the message signal 7(t), In
any case, it may be removed simply by means of a blocking capacitor. Thus if we neglect
the DC term A, in Equation (2.93), we find that the remainder has, except for scaling
factors, a form similar to the output of a DSB-SC receiver using coherent detection. Ac-
cordingly, the output signal-to-noise ratio of an AM receiver using an envelope detector
is approximately

A2k2P

(SNR)o,am ™ 2WN, (2.94)

Equation (2.94) is, however, valid only if the following two conditions are satisfied:

1. The average noise power is small compared to the average carrier power at the
envelope detector input.

2. The amplitude sensitivity &, is adjusted for a percentage modulation less than or
equal to 100 percent.

Using Equations (2.90) and (2.94), we obtain the following figure of merit for amplitude
modulation:
(SNR)o _ kop
(SNR)cl ay 1 + R2P

 

(2.95)

Thus, whereas the figure of merit of a DSB-SC receiver or that of an SSB receiver using
coherent detection is always unity, the corresponding figure of merit of an AM receiver
using envelope detection is always less than unity. In other words, the noise performance
of a full AM receiver is always inferior to that of a DSB-SC receiver. This is due to the

2.12 Noise in AM Receivers Using Envelope Detection 137

wastage of transmitter power, which results from transmitting the carrier as a component
of the AM wave.

> EXAMPLE 2.4 Single-Tone Modulation

Consider the special case of a sinusoidal wave of frequency f,, and amplitude A,, as the
modulating wave, as shown by .

m(t) = A,, cos(2arf,,t)
The corresponding AM wave is
s(t) = A[1 + p cos(2xf,,t)] cos(2Zarf.t)

where 4. = &,A,,, is the modulation factor. The average power of the modulating wave m(t) is
(assuming a load resistor of 1 ohm)

 

 

P= $A2,
Therefore, using Equation (2.95), we get
1 aye
(SNR)}o _ 2 kaAm
(SNR) AM 7 1 242
t+ 2 keAry (2.96)
2
2+ we

When » = 1, which corresponds to 100 percent modulation, we get a figure of merit equal
to. 1/3. This means that, other factors being equal, an AM system (using envelope detection)
must transmit three times as much average power as a suppressed-carrier system (using co-
herent detection) to achieve the same quality of noise performance.

@ THRESHOLD EFFECT

When the carrier-to-noise ratio is small compared with unity, the noise term dominates
and the performance of the envelope detector changes completely from that just described.
In this case it is more convenient to represent the narrowband noise n(t) in terms of its
envelope 7(t) and phase y(t), as shown by

n(t) = r(t) cos[2mft + w(t)] (2.97)

The corresponding phasor diagram for the detector input x(t) = s{t) + n(t) is shown in
Figure 2.386, where we have used the noise envelope as reference, because it is now the
dominant term. To the noise phasor r(t) we have added a phasor representing the signal
term A,[1 + k,y(¢)], with the angle between them being equal to the phase #(t) of the
noise v(t). In Figure 2.386 it is assumed that the carrier-to-noise ratio is so low that the
carrier amplitude A, is small compared with the noise envelope r(t), at least most of the
time. Then we may neglect the quadrature component of the signal with respect to the
noise, and thus find from Figure 2.386 that the envelope detector output is

y(t) = r(t) + A, cos[W(z)] + ARagn(t) cos[i(£)] (2.98)

This relation reveals that when the carrier-to-noise ratio is low, the detector output has
no component strictly proportional to the message signal m/(t). The last term of the ex-
pression defining y(t) contains the message signal 7(t) multiplied by noise in the form of

138

CHAPTER 2 CONTINUOUS-WAVE MODULATION

cos[i(t)}. From Section 1.11 we recall that the phase f(t) of the narrowband noise n(7) is
uniformly distributed over 27 radians. It follows therefore that we have a complete loss
of information in that the detector output does not contain the message signal m(t) at all.
The loss of a message in an envelope detector that operates at a low carrier-to-noise ratia
is referred to as the threshold effect. By threshold we mean a value of the carrier-to-noise
ratio below which the noise performance of a detector deteriorates much more rapidly
than proportionately to the carrier-to-noise ratio. It is important to recognize that every
nonlinear detector (e.g., envelope detector) exhibits a threshold effect. On the other hand,
such an effect does not arise in a coherent detector.

A rigorous mathematical analysis of the threshold effect for the general case of an
AM wave is beyond the scope of this book. In the next subsection we simplify matters by
considering the case of an unmodulated carrier. Despite this simplification, we can still
develop a great deal of insight into the threshold effect experienced in an envelope detector,

General Formula for (SNR) in Envelope Detection*
Consider an envelope detector whose input signal is defined by
x(t) = A, cos(2mf,t) + n(t) (2.99)

where A, cos(2zf.t) is the unmodulated carrier and x(t) is the sample function of band-
limited, zero-mean, white Gaussian noise N(t). The power spectral density of N(t) is

No <
sty=4q rif ~ f= W (2.100)
0 otherwise

Representing the narrowband noise n(t) in terms of its in-phase component 1;(t) and
quadrature component 7o({t), we may express the noisy signal at the detector input as

x(t) = (A, + 2,(t)) cos(2af,t) — no(t) sin(2af,t) (2.101)

The noise components 7;(t) and 9(t) are zero-mean, jointly Gaussian, mutually indepen-
dent low-pass random processes with identical power spectral densities (see Equation
1.101):

Sxif - f) + Sf +f) for| fl = W

2.102
0 otherwise (

Sulf) = Sno(f) = {

For the problem at hand, the input signal consists of an unmodulated carrier with
average power equal to A2/2. The average noise power at the detector input is

 

 

a2, = 2WNo (2.103)
The carrier-to-noiser ratio is therefore defined by
A2/2
p= a3
OX
ye (2.104)
~ 4AWNo

We may think of p as an input signal-to-noise ratio for the problem described herein.

*A reader who is not interested in the mathematical details of how noise affects the envelope detection of an
AM signal may skip the material up to Eq. (2.124) and read the two limiting cases of the formula in that equation.

2.12 Noise in AM Receivers Using Exvelope Detection 139

However, determination of the output signal-to-noise ratio is a more difficult un-
dertaking because the envelope detector output

yt) = ViA, ¥ alt)? + nd(2) (2.105)

is a nonlinear combination of signal and noise terms. With no clear-cut separation between
signal and noise at the detector output y(t), how then do we isolate the contribution of
the signal s(t) to y(t) from the contribution due to the noise (t)? To resolve this issue, we
adopt a heuristic approach based on signal averaging. Specifically, we introduce the fol-
lowing two definitions: 4

1. The mean output signal, s,, is the difference between the expectation of y(¢) in the
combined presence of signal and noise and the expectation of y(z) in the presence of
noise alone, as shown by

So = Efy(t)] — Ely.(t) (2.106)
where y(t) is itself defined by Equation (2.105) and - is defined by
Yolt) = Vakit) + nit) (2.107)

2. The mean output noise power is the difference between the mean-square value of the
detector output y(t) and the square of the mean value of y(z), as shown by

var[y(z)] = Ely7(e)] — (Ely(t)])? (2.108)
On this basis, we define the output signal-to-noise ratio as
83
var[y(t)]

From Section 1.12, we recall that the envelope detector output due to noise alone is
Rayleigh distributed; that is

y y?
fry) = Jeon(-25). y=0 (2.110)

0 otherwise

 

(SNR)g = (2.109)

The expectation of y,(t) is therefore
Elyo(t)] = E yfy,ly) dy
2 2
y BA
-| oF, e(-3 | dy

From the definition of the gamma function for real positive values of the argument x, we
have

(2,111)

 

T(x) = [ z*~! exp(—z) dz (2.112)
We may therefore rewrite Equation (2.111) as

Biylt] = V2ost(3)

7
- fen

(2.113)

140

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

where we have used the value I'(3/2) = V7/2. To calculate the mean signal s, at the
detector output, we also need the expectation of y(t). Due to the combined presence of
signal and noise, we recall from Section 1.13 that y(t) is Rician distributed, as shown by

2 2
y yi t Ac\, (Ay
ely) = 5 o% exo( 20%, Jo(42 for y = 0 (2.114)
0 otherwise

 

where Io(+) is the modified Bessel function of the first kind of zero order (see Appendix 3),

Hence,
2 2 2
: -! 2 Uy + Ac\, (Ady
E[y(z)] [, oh eo 2a}, )io( 42 dy (2.115)

Putting A,y/o% = 4 and recognizing that p = A2/2on, we may recast this expectation in
the form

 

 

Ely(t)] = aa exp(—p) [, “ exo) a du (2.116)

The integral in Equation (2.116) can be written in a concise form by using confluent
hypergeometric functions; see Appendix 4. In particular, using the integral representation

- Timl2)( (m1
[ uw" exp(—B?u*\Iy(u) du = yes (a(2 ‘sga)) (2.117}

 

with m = 3, T(/2) = Val2 and b? = 1/4p, we may express the expectation of y(¢) in
terms of the confluent hypergeometric function ,F,(3/2;13p) as

Ely] = ie On exp(-al( (3st) ) (2.118)

We may further simplify matters by using the following identity:
exp(—4)(1F,(a38;#)) = 1Fi(B — 0583—n) (2.119)

and so finally express the expectation of y(t) in the concise form

Efy(z)] = ia ox(sh(-s1-») (2.120)

Thus using Equations (2.113) and (2.120) in Equation (2.106) yields the mean output

signal as
$= it Fy} —=31;- - i) (2.121)
o 2 Ont 11 2? 3—P " .

whose dependence on the standard deviation oy of the noise x(t) is testimony to the
intermingling of signal and noise at the detector output.
Following a similar procedure, we may express the mean-square value of the detector

output y(z) as
3 2 2
3 y y* + Az Ay
*b (1 = J, oF | Qa, Joo ot dy (2.122)

= 2on(Fi(-151;-p))

 

 

2.12 Noise in AM Receivers Using Envelope Detection 141

Hence using. Equations (2.120) and (2.122) in Equation (2.108} yields the mean output
noise power as .

varly(e)] = 208( sits) - a8 (—da- )) ) (2.123)

Finally, using Equations (2.121) and (2.123) in Equation (2.109) yields the output
signal-to-noise ratio for the envelope detection problem at hand as

(1A(-$::-0) 7 1)
1 2
(ms (—18s-0)) - (vr,(-}- ))

Equation (2.124) is the general formula for the output signal-to-noise of an envelope
detector whose input consists of an unmodulated carrier and band-limited, white Gaussian
noise. Two limiting cases of this general formula are of particular interest:

(SNR)o =

 

(2.124)

1. Large carrier-to-noise ratio, For large p, we may use the following asymptotic for-
mula (see Appendix 4)

1
in(—das- ) = Je for p> & (2.125)

Moreover, the following identity
1F,{~1;1;-p) = 1+ p (2.126)

holds exactly for all p. Accordingly, the use of Equations (2.125) and (2.126) in
Equation (2.124) yields the following approximate formula for the output signal-to-
noise ratio:

(SNR)o =p for p> ~ (2.127)

where we have ignored contributions due to p’” and p° in the numerator of Equation
(2.124) as being subdominant compared to p for large p. Equation (2.127) shows
that for large carrier-to-noise p the envelope detector behaves like a coherent detector,
in that the output signal-to-noise ratio is proportional to the input signal-to-noise
ratio.

2. Small carrier-to-noise ratio. For small p, we have (see Appendix 4)

a

F,(a;c3—p) = 1 — =P for p—> 0 (2.128)

Hence, using this asymptotic formula, we may approximate the output signal-to-
noise ratio for small p as

mp?
16 ~ 4 (2.129)
= 0,91p? for p> 0

(SNR)o ~

where, in the denominator, we have ignored contributions due to p and p? as being
subdominant compared to p° for small p. Equation (2.129) shows that for a small
carrier-to-noise ratio, the output signal-to-noise ratio of the envelope detector is pro-
portional to the squared input signal-to-noise ratio.

142 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

qT

     
     
 

10-
5 7 (SNR)o = p
zs -
= LL
DB
gs [
s
» le
@ o£
2 c
Zz -
i= =
bo
a (SNR)g = 0.91p?
a
5
© ol

TTT

 

Pot a tititl fof ttt tii Lt tii his)
1.0 10.0 100.0

Carrier-to-noise ratio, p

0.01

 

Ficure 2.39 Output signal-to-noise ratio of an envelope detector for varying carrier-to-noise
ratio.

The conclusions drawn from the two limiting cases considered herein are that an
envelope detector favors strong signals and penalizes weak signals. The phenomenon of
weak signals being penalized by the detector is referred to as weak signal suppression,
which is a manifestation of the threshold effect.

Using the formula of Equation (2.124), in Figure 2,39 we have plotted the output
signal-to-noise ratio (SNR)o of the envelope detector versus the carrier-to-noise ratio p
using tabulated values of confluent hypergeometric functions. This figure also includes the
two asymptotes for large p and small p. From Figure 2.39 we see that the output signal-
to-noise ratio deviates from a linear behavior around a carrier-to-noise ratio of 10 dB (i.e,
p= 10).

| 2.13 Noise in FM Receivers

 

Finally, we turn our attention to the noise analysis of a frequency modulation (FM) system,
for which we use the receiver model shown in Figure 2.40. As before, the noise 1(t) is
modeled as white Gaussian noise of zero mean and power spectral density No/2. The

 

 

 

 

 

 

 

 

 

 

ati) v(t) | Baseband
FM Band-pass tees tenet Output
A - -——| low-pass -}—> |
signal s() > / filter Limiter Discriminator Nike signal
Noise

wilt)

FIGurRE 2.40 Model of an FM receiver.

2.13 Noise in FM Receivers 143

received FM signal s(t) has a carrier frequency f, and transmission bandwidth B., such
that only a negligible amount of power lies outside the frequency band f, + Bz/2 for
positive frequencies, and similarly for negative frequencies.

As in the AM case, the band-pass filter.has a midband frequency f, and bandwidth
By and therefore passes the FM signal essentially without distortion. Ordinarily, Br is
small compared with the midband frequency f., so that we may use the narrowband
representation for #(f), the filtered version of channel noise w(t), in terms of its in-phase
and quadrature components.

In an FM system, the message signal is transmitted by variations of the instantaneous
frequency of a sinusoidal carrier wave, and its amplitude is maintained constant. Therefore,
any variations of the carrier amplitude at the receiver input must result from noise or
interference. The amplitude limiter, following the band-pass filter in the receiver model of
Figure 2.40, is used to remove amplitude variations by clipping the modulated wave at
the filter output almost to the zero axis. The resulting rectangular wave is rounded off by
another band-pass filter that is an integral part of the limiter, thereby suppressing har-
monics of the carrier frequency. Thus, the filter output is again sinusoidal, with an am-
plitude that is practically independent of the carrier amplitude at the receiver input.

The discriminator in the model of Figure 2.40 consists of two components:

1. A slope network ox differentiator with a purely imaginary frequency response that
varies linearly with frequency. It produces a hybrid-modulated wave in which both
amplitude and frequency vary in accordance with the message signal.

2, An envelope detector that recovers the amplitude variation and thus reproduces the
message signal.

The slope network and envelope detector are usually implemented as integral parts of a
single physical unit.

The postdetection filter, labeled “baseband low-pass filter” in Figure 2.40, has a
bandwidth that is just large enough to accommodate the highest frequency component of
the message signal. This filter removes the out-of-band components of the noise at the
discriminator output and thereby keeps the effect of the output noise to a minimum.

The filtered noise #(t) at the band-pass filter output in Figure 2.40 is defined in terms
of its in-phase and quadrature components by

n{t) = y(t) cos(2af,t) — no(t) sin(2af.t)

Equivalently, we may express n(f) in terms of its envelope and phase as

n(t) = r(t) cos[(2mft) + y(t)] (2.130)
where the envelope is
r{t) = [n7(t) + na(z)]'? (2.131)
and the phase is
y(t) = can 2] (2.132)
ny(t)

The envelope r(f) is Rayleigh distributed, and the phase ¥(z) is uniformly distributed over
27 radians (see Section 1.12).
The incoming FM signal s(t) is defined by

s(t) = A, cos) 2a + 2ak, if m(t) ar| (2.133)

144

CHAPTER 2 CONTINUOUS-WAVE MODULATION

where A, is the carrier amplitude, f, is the carrier frequency, k; is the frequency sensitivity,
and m(t) is the message signal. Note that, as with the standard AM, in FM there is no
need to introduce a scaling factor in the definition of the modulated signal s(t), since it jg
reasonable to assume that its amplitude A, has the same units as the additive noise com.
ponent x(t). To proceed, we define

b(t) = 27k, [vet ar (2.134)

We may thus express s(f) in the simple form
s(t) = A, cos[2af,t + #(¢)] (2.135)
The noisy signal at the band-pass filter output is therefore

x(t) = s(t) + n(t)

= A. cos|2af.t + (2)] + r(t) cos[2af.t + wle)] (2.136)

It is informative to represent x(t) by means of a phasor diagram, as in Figure 2.41. In this
diagram we have used the signal term as reference. The phase 6(t) of the resultant phasor
representing x(t) is obtained directly from Figure 2.41 as

r(z) sin[ p(t) — $(0)] }
A. + rz) cos[w(t) — (2)

The envelope of x(z) is of no interest to us, because any envelope variations at the band-
pass filret output are removed by the limiter.

Our motivation is to determine the error in the instantaneous frequency of the carrier
wave caused by the presence of the filtered noise n(¢). With the discriminator assumed
ideal, its output is proportional to 6"(t}/27 where 6'(t) is the derivative of 6(t) with respect
to time. In view of the complexity of the expression defining 6(t), however, we need to
make certain simplifying approximations, so that our analysis may yield useful results.

We assume that the carrier-to-noise ratio measured at the discriminator input is large
compared with unity. Let -R denote the random variable obtained by observing (at some
fixed time) the envelope process with sample function r(£) [due to the noise n(Z)]. Then, at
least most of the time, the random variable R is small compared with the carrier amplitude
A,, and so the expression for the phase 6(¢) simplifies considerably as follows:

 

At) = H(t) + ean (2.137)

“Only — 0) (2.138)

 

alt) = P(t) +

or, using the expression for #(£) given in Equation (2.134),

 

 

Alt) = Qk, [ m(r) dr + r(e) sin[w(t) — (t)] (2.139)
0 A;
Resultant .
r()
ie a
A yn an A

 

FIGURE 2.41 Phasor diagram for FM wave plus narrowband noise for the case of high carrier”
to-noise ratio.

2.13 Noise in FM Receivers 145

The discriminator output is therefore

u(t) = i d6(t)
2a. dt
(2.140)

= kym(t) + n4(t)
where the noise term #,(t) is defined by

d
nalt) = >< © tre) sinlwte) — (0) (2.141)

We thus see that provided the carrier-to-noise ratio is high, the discriminator output v(t)
consists of the original message signal s(t) multiplied by the constant factor ky, plus an
additive noise component #,(é). Accordingly, we may use the output signal-to-noise ratio
as previously defined to assess the quality of performance of the FM receiver. Before doing
this, however, it is instructive to see if we can simplify the expression defining the noise
ng(t).

From the phasor diagram of Figure 2.41, we note that the effect of variations in the
phase 1f(t) of the narrowband noise appear referred to the signal term @(t). We know that
the phase y(t) is uniformly distributed over 27 radians. It would therefore be tempting to
assume that the phase difference s(t) — (tf) is also uniformly distributed over 27 radians.
If such an assumption were true, then the noise #,(t) at the discriminator output would
be independent of the modulating signal and would depend only on the characteristics of
the carrier and narrowband noise. Theoretical considerations show that this assumption
is justified provided that the carrier-to-noise ratio is high.? Then we may simplify Equation
(2.141) as:

~ 14
” 2nA, dt

However, from the defining equations for r(¢) and y(t), we note that the quadrature com-
ponent #(t) of the filtered noise (t) is

 

ng(t) {r({z) sin[y(e)]) (2,142)

 

no{t) = r(t) sin[ed(t)] (2.143)
Therefore, we may rewrite Equation (2.142) as
_ 1 dagit)
n,(t) nA. dt (2.144)

This means that the additive noise 2,(t) appearing at the discriminator output is deter-
mined effectively by the carrier amplitude A, and the quadrature component no(t) of the
narrowband noise n{t).

The output signal-to-noise ratio is defined as the ratio of the average output signal
power to the average output noise power. From Equation (2.140), we see that the message
component in the discriminator output, and therefore the low-pass filter output, is k m(t).
Hence, the average output signal power is equal to k2P, where P is the average power of
the message signal m(t).

To determine the average output noise power, we note that the noise n,(t) at the
discriminator output is proportional to the time derivative of the quadrature noise com-
ponent (2). Since the differentiation of a function with respect to time corresponds to
multiplication of its Fourier transform by j27f, it follows that we may obtain the noise
Process 7,(t) by passing mo(t) through a linear filter with a frequency response equal to

jauf _ if
2awA, A,

146 CH4pTER 2 # ContTINUOUS-WAVE MODULATION

This means that the power spectral density Sy(f) of the noise q(t) is related to the power
spectral density Sx(f) of the quadrature noise component 79(t) as follows:

f*
SuJAf) = Az Suolf) (2.145)

With the band-pass filter in the receiver model of Figure 2.40 having an ideal fre.
quency response characterized by bandwidth Br and midband frequency f., it follows that
the narrowband noise n({t) will have a power spectral density characteristic that is similarly
shaped. This means that the quadrature component no(t) of the narrowband noise n{(t}
will have the ideal low-pass characteristic shown in Figure 2.424. The corresponding power
spectral density of the noise #,(t) is shown in Figure 2.420; that is,

Nof? By
syifiey ar? IS (2.146)
0, otherwise

In the receiver model of Figure 2.40, the discriminator output is followed by a low-pass
filter with a bandwidth equal to the message bandwidth W. For wideband FM, we usually
find that W is smaller than Bz/2, where By is the transmission bandwidth of the FM
signal. This means that the out-of-band components of noise 74(t) will be rejected. There.
fore, the power spectral density Sx, (f) of the noise 7,(£) appearing at the receiver output

is defined by
Nof?
sy fr=d az? lS (2.147)
0, otherwise

as shown in Figure 2.42c. The average output noise power is determined by integrating
the power spectral density Sx(f) from —W to W. We thus get the following result:

w
Average power of output noise = re f’ df
2 J-w

 

 

 

 

2.148
_ 2NoW? (2.148)
3A2
Sygif) Sy MP) Sy (Ff)
No
f
_ Br ie} Br f _ Br 0 By f WW i) W
ra 2 2 2
(a) (b) (c}

FIGURE 2.42 Noise analysis of FM receiver. (a) Power spectral density of quadrature compo-
nent s(t) of narrowband noise nit). (b) Power spectral density of noise a(t) at the discriminator
output. (c) Power spectral density of noise n,(t) at the receiver output.

2.13 Noise in FM Receivers 147

Note that the average output noise power is inversely proportional to the average carrier
power A2/2. Accordingly, in an FM system, increasing the carrier power has a noise-
quieting effect.

Earlier we determined the average output signal power as k3P. Therefore, provided
the carrier-to-noise ratio is high, we may divide this average output signal power by the
average Output noise power of Equation (2.148) to obtain the output signal-to-noise ratio

3A2K7F

2NoW?

The average power in the modulated signal s(t) is A2/2, and the average noise power in

the message bandwidth is WNp. Thus the channel signal-to-noise ratio is

2

2WNo

Dividing the output signal-to-noise ratio by the channel signal-to-noise ratio, we get the

following figure of merit for frequency modulation:
(SNR)o| _ 343P

(SNR)clmy  W?

From Section 2.7 we recall that the frequency deviation Af is proportional to the
frequency sensitivity ky of the modulator. Also, by definition, the deviation ratio D is equal
to the frequency deviation Af divided by the message bandwidth W. In other words, the
deviation ratio D is proportional to the ratio k,P“7/W. It follows therefore from Equation
(2.151) that the figure of merit of a wideband FM system is a quadratic function of the
deviation ratio. Now, in wideband FM, the transmission bandwidth B- is approximately
proportional to the deviation ratio D. Accordingly, we may state that when the carrier-
to-noise ratio is high, an increase in the transmission bandwidth By provides a correspond-
ing quadratic increase in the output signal-to-noise ratio or figure of merit of the FM
system.The important point to note from this statement is that, unlike amplitude modu-
lation, the use of frequency modulation does provide a practical mechanism for the ex-
change of increased transmission bandwidth for improved noise performance.

(SNR)o,pm = (2.149)

(SNR) cEM = (2.150)

 

(2.151)

® EXAMPLE 2.5 Single-Tone Modulation
Consider the case of a sinusoidal wave of frequency f,, as the modulating signal, and assume

a peak frequency deviation Af. The modulated FM signal is thus defined by

s(t) = A, cos] 2a + Z sn ft)
Therefore, we may write
t
Qak, ii m(r) dr = ra sin(27rf,,t)

Differentiating both sides with respect to time and solving for m(t), we get

mt) = Af cos(2 aft)
Ry
Hence, the average power of the message signal m(z), developed across a 1-ohm load, is
2
> (af)

 

2

148

CHAPTER 2 # CONTINUOUS-WAVE MODULATION
Substituting this result into the formula for the output signal-to-noise ratio given in Equation
(2.149), we get

3A2(Af)*
(SNR)oem = “4NSW>
_ 3A2p"
4N,W

 

where 8 = Af/W is the modulation index. Using Equation (2.151) to evaluate the correspond.

ing figure of merit, we get
3 (2)
mw 2\W

aig
=7A

{(SNR)o
(SNR)c

 

 

(2.152)

I is important to note that the modulation index 8 = Af/W is determined by the bandwidth
W of the postdetection low-pass filter and is not related to the sinusoidal message frequency
fs except insofar as this filter is usually chosen so as to pass the spectrum of the desired
message; this is merely a matter of consistent design. For a specified system bandwidth W, the
sinusoidal message frequency f,, may lie anywhere between 0 and W and would yield the same
output signal-to-noise ratio.

It is of particular interest to compare the noise performance of AM and FM systems,
An insightful way of making this comparison is to consider the figures of merit of the two
systems based on a sinusoidal modulating signal. For an AM system operating with a sinu-
soidal modulating signal and 100 percent modulation, we have (from Example 2.4):

(SNR)o 1

(SNR)clam 3

 

Comparing this figure of merit with the corresponding result described in Equation (2.152)
for an FM system, we see that the use of frequency modulation offers the possibilicy of im-
proved noise performance over amplitude modulation when

3B > 5
that is,
V2
p> Zz = 0.471

We may therefore consider 8 = 0.5 as defining roughly the transition between narrowband
FM and wideband FM. This statement, based on noise considerations, further confirms
a similar observation that was made in Section 2.7 when considering the bandwidth of
FM waves.

@ CAPTURE EFFECT

The inherent ability of an FM system to minimize the effects of unwanted signals (e.g
noise, as just discussed) also applies to interference produced by another frequency:
modulated signal whose frequency content is close to the carrier frequency of the desired
FM wave. However, interference suppression in an FM receiver works well only when the
interference is weaker than the desired FM input. When the interference is the stronge!
one of the two, the receiver locks onto the stronger signal and thereby suppresses the

2.13 Noise in FM Receivers 149

desired FM input. When they are of nearly equal strength, the receiver fluctuates back and
forth between them. This phenomenon is known as the capture effect, which describes
another distinctive characteristic of frequency modulation.

a FM THRESHOLD EFFECT

The formula of Equation (2.149), defining the output signal-to-noise ratio of an FM re-
ceiver, is valid only if the carrier-to-noise ratio, measured at the discriminator input, is
high compared with unity. It is found experimentally that as the input noise power is
increased so that the carrier-to-noise ratio is decreased, the FM receiver breaks. At first,
individual clicks are heard in the receiver output, and as the carrier-to-noise ratio decreases
still further, the clicks rapidly merge into a crackling or sputtering sound. Near the break-
ing point, Equation (2,149) begins to fail by predicting values of output signal-to-noise
ratio larger than the actual ones. This phenomenon is known as the threshold effect.’ The
threshold is defined as the minimum carrier-to-noise ratio yielding an FM improvement
that is not significantly deteriorated from the value predicted by the usual signal-to-noise
formula assuming a small noise power.

For a qualitative discussion of the FM threshold effect, consider first the case when
there is a no signal present, so that the carrier wave is unmodulated. Then the composite
signal at the frequency discriminator input is

x(t) = [A, + n;(t)] cos(2af,t) — ao(t) sin(27f,t) (2.153)

where #;(t) and w(t) are the in-phase and quadrature components of the narrowband
noise #(z) with respect to the carrier wave. The phasor diagram of Figure 2.43 displays
the phase relations between the various components of x(t) in Equation (2.153). As the
amplitudes and phases of #;(t) and #9(t) change with time in a random manner, the point
P, [the tip of the phasor representing x(t)] wanders around the point P, (the tip of the
phasor representing the carrier), When the carrier-to-noise ratio is large, ;(t) and not)
are usually much smaller than the carrier amplitude A,, and so the wandering point P, in
Figure 2.43 spends most of its time near point P,. Thus the angle (t) is approximately
#Q(tVA, to within a multiple of 27. When the carrier-to-noise ratio is low, on the other
hand, the wandering point P, occasionally sweeps around the origin and 6(¢) increases or
decreases by 27 radians. Figure 2.44 illustrates how in a rough way the excursions in 6(2),
depicted in Figure 2.444, produce impulselike components in 6"(t) = d6/dt. The discrim-
inator output v(t) is equal to 9’(t)/27. These impulselike components have different heights
depending on how close the wandering point P, comes to the origin O, but all have areas
nearly equal to +27 radians, as illustrated in Figure 2.44b, When the signa! shown in
Figure 2.44b is passed through the postdetection low-pass filter, corresponding but wider
impulselike components are excited in the receiver output and are heard as clicks. The
clicks are produced only when 6(t) changes by +27 radians.

 

FIGURE 2.43 Phasor diagram interpretation of Equation (2.153).

150

CHapren 2 = CONTINUOUS-WAVE MODULATION

 

 

 

 

 

 

 

(b)

Ficure 2.44 Illustrating impulselike components in @’() = d6(t)/dt produced by changes of 2a
in 6(¢); (a) and (b) are graphs of @(#) and @’(t), respectively.

From the phasor diagram of Figure 2.43, we may deduce the conditions required for
clicks to occur. A positive-going click occurs when the envelope 7(¢) and phase y(¢) of the
narrowband noise n(t) satisfy the following conditions:

rt) > A,
it) <= we) + dy(e)
dy{t)

— >
dt 0

These conditions ensure that the phase 6(t) of the resultant phasor x(t) changes by 2a
radians in the time increment dt, during which the phase of the narrowband noise increases
by incremental amount dit). Similarly, the conditions for a negative-going click to occur
are as follows:
r(t) > A,
w(t) > —a7 > yt) + able)
dep(t)

ac
dt 0

These conditions ensure that 6(t) changes by —27 radians during the time increment di.
The carrier-to-noise ratio is defined by
Az

= 2.154)
P  2ByNo (

 

As pis decreased, the average number of clicks per unit time increases. When this numbet
becomes appreciably large, threshold is said to occur.

2.13 Noise in FM Receivers 151

The output signal-to-noise ratio is calculated as follows:

1. The output signal is taken as the receiver output measured in the absence of noise.
The average output signal power is calculated assuming a sinusoidal modulation that
produces a frequency deviation Af’equal to B/2, so that the carrier swings back
and forth across the entire input frequency band.

2. The average output noise power is calculated when there is no signal present; that
is, the carrier is unmodulated, with no restriction imposed on the value of the carrier-
to-noise ratio p.

On this heuristic basis, theory’ yields Curve I of Figure 2.45 presenting a plot of the
output signal-to-noise ratio versus the carrier-to-noise ratio when the ratio B/2 W is equal
to 5. This curve shows that the output signal-to-noise ratio deviates appreciably from a
linear function of the carrier-to-noise ratio p when p is less than about 10 dB. Curve II of
Figure 2.45 shows the effect of modulation on the output signal-to-noise ratio when the
modulating signal (assumed sinusoidal) and the noise are present at the same time. The
average output signal power pertaining to curve II may be taken to be effectively the same
as for curve I. The average output noise power, however, is strongly dependent on the
presence of the modulating signal, which accounts for the noticeable deviation of curve I
from curve I. In particular, we find that as p decreases from infinity, the output signal-to-

 

42,—— 1 T TT TTT
40

38/-

 

   
  
 
 

6b
(SNR)y = 3p (35);

VO
7

34/- 4
7

a

RD
2h
i
a
1

32
30
2
264
24
22
20

18);-

Output signal-to-noise ratio 10 logip (SNR)g, dB

16;-

Me

12 /-

 

 

10 } | | |
0 2 4 6 8 10 12 14 16 18 20

Carrier-to-noise ratio 10 log, p, dB .

 

FIGURE 2.45 Dependence of output signal-to-noise ratio on input carrier-to-noise ratio for FM
reciever. In curve I, the average output noise power is calculated-assuming an unmodulated car-

rier. In curve II, the average output noise power is calculated assuming a sinusoidally modulated
carrier. Both curves I and II are calculated from theory.

152

CHAPTER 2 = CONTINUOUS-WAVE MODULATION

noise deviates appreciably from a linear function of p when p is about 11 dB. Also when
the signal is present, the resulting modulation of the carrier tends to increase the average
number of clicks per second. Experimentally, it is found that occasional clicks are hearg
in the receiver output at a carrier-to-noise ratio of about 13 dB, which appears to be only
slightly higher than what theory indicates. Also it is of interest to note that the increase jp.
the average number of clicks per second tends to cause the output signal-to-noise ratio to
fall off somewhat more sharply just below the threshold level in the presence of
modulation.

From the foregoing discussion we may conclude that threshold effects in FM receivers
may be avoided in most practical cases of interest if the carrier-to-noise ratio p is equal tg
or greater than 20 or, equivalently, 13 dB. Thus using Equation (2.154) we find that the
loss of message at the discriminator output is negligible if

At
ca > 2
2BrNo °

or, equivalently, if the average transmitted power A2/2 satisfies the condition

 

Az
F = 20BrNo (2.155)

To use this formula, we may proceed as follows:

1. For a specified modulation index 8 and message bandwidth W, we determine the
transmission bandwidth of the FM wave, Br, using the universal curve of Figure 2.26
or Carson’s rule.

2. For a specified average noise power per unit bandwidth, No, we use Equation (2.155)
to determine the minimum value of the average transmitted power A;/2 that is nec-
essary to operate above threshold.

@ FM THRESHOLD REDUCTION

In communication systems using frequency modulation, there is particular interest in re-
ducing the noise threshold in an FM receiver so as to satisfactorily operate the receiver
with the minimum signal power possible. Threshold reduction in FM receivers may be
achieved by using an FM demodulator with negative feedback!? (commonly referred tos
an FMEB demodulator), or by using a phase-locked loop demodulator. Such devices art
referred to as extended-threshold demodulators, the idea of which is illustrated in Figure
2.46, The threshold extension shown in this figure is measured with respect to the standard
frequency discriminator {i.e., one without feedback).

The block diagram of an FMEB demodulator’? is shown in Figure 2.47. We see that
the local oscillator of the conventional FM receiver has been replaced by a voltage
controlled oscillator (VCO) whose instantaneous output frequency is controlled by the
demodulated signal. In order to understand the operation of this receiver, suppose for the
moment that the VCO is removed from the circuit and the feedback loop is left open.
Assume that a wideband EM signal is applied to the receiver input, and a second FM
signal, from the same source but whose modulation index is a fraction smaller, is applied
to the VCO terminal of the mixer. The output of the mixer would consist of the difference
frequency component, because the sum frequency component is removed by the band-past
filter, The frequency deviation of the mixer output would be small, although the frequency
deviation of both input FM waves is large, since the difference between their instantaneous
deviations is small. Hence, the modulation indices would subtract and the resulting FM
wave at the mixer output would have a smaller modulation index. The FM wave with

2.13 Noise in FM Receivers 153

 
 

Output signal-te noise ratio, dB

 

 

I
I
|
{
{
I
|
|
1
i

 

a
Extended Threshold —Carrier-to-noise ratio, dB
threshold

FIGURE 2.46 FM threshold extension.

reduced modulation index may be passed through a band-pass filter, whose bandwidth
need only be a fraction of that required for either wideband FM, and then frequency
demodulated. It is now apparent that the second wideband FM signal applied to the mixer
may be obtained by feeding the output of the frequency discriminator back to the VCO.

It will now be argued that the signal-to-noise ratio of an FMFB receiver is the same
as that of a conventional FM receiver with the same input signal and noise power if the
carrier-to-noise ratio is sufficiently large. Assume for the moment that there is no feed-
back around the demodulator. In the combined presence of an unmodulated carrier
A, cos(27f,t) and narrowband noise

a(t) = n;(t) cos(2af,t) — nog(t) sin(27f,t)

the phase of the composite signal x(z) at the limiter-discriminator input is approximately
equal to #Q(t)/A,, assuming that the carrier-to-noise ratio is high. The envelope of x(z) is
of no interest to us, because the limiter removes all variations in the envelope. Thus the
composite signal at the frequency discriminator input consists of a small index phase-
modulated wave with the modulation derived from the component no(t) of noise that is
in phase quadrature with the carrier. When feedback is applied, the VCO generates a
frequency-modulated signal that reduces the phase-modulation index of the wave in the
band-pass filter output, that is, the quadrature component no(¢) of noise. Thus we see that
as long as the carrier-to-noise ratio is sufficiently large, the FMFB receiver does not respond
to the in-phase noise component x(t), but that it would demodulate the quadrature noise
component 7 (t) in exactly the same fashion as it would demodulate signal modulation.

Received we Baseband
FM — | Mixer Bane pass di Limi et _ low-pass Qutput
wave ilter iscriminator filter signa

Voltage-
controlled

oscillator

 

   

 

 

 

 

  

 

 

 

 

 

Ficure 2.47 FM demodulator with negative feedback.

154

CHAPTER 2. 8 CONTINUOUS-WAVE MODULATION

Signal and quadrature noise are reduced in the same proportion by the applied feedback,
with the result that the baseband signal-to-noise ratio is independent of feedback. For large
carrier-to-noise ratios, the baseband signal-to-noise ratio of an FMFB receiver is then the
same as that of a conventional FM receiver.

The reason that an FMFB receiver is able to extend the threshold is that, unlike g
conventional FM receiver, it uses a very important piece of a priori information, namely,
that even though the carrier frequency of the incoming FM wave will usually have large
frequency deviations, its rate of change will be at the baseband rate. An FMFB demodp.
lator is essentially a tracking filter that can track only the slowly varying frequency of g
wideband FM signal, and consequently it responds only to a narrowband of noise centered
about the instantaneous carrier frequency. The bandwidth of noise to which the FMFB
receiver responds is precisely the band of noise that the VCO tracks. The end result is that
an FMEB receiver is capable of realizing a threshold extension on the order of 5-7 dB,
which represents a significant improvement in the design of minimum power FM systems,

Like the FMFB demodulator, the phase-locked loop (discussed later in Section 2,14)
is also a tracking filter and, as such, the noise bandwidth to which it responds is precisely
the band of noise tracked by the VCO. Indeed, the phase-locked loop demodulator offers
a threshold extension capability with a relatively simple circuit. Unfortunately, the amount
of threshold extension is not predictable by any existing theory, and it depends on signal
parameters. Roughly speaking, improvement by a few (on the order of 2, to 3) decibels ig
achieved in typical applications, which is not as good as an FMFB demodulator.

s PrReE-EMPHASIS AND DE-EMPHASIS IN FM

Equation (2.147) shows that the power spectral density of the noise at the output of an
FM receiver has a square-law dependence on the operating frequency; this is illustrated in
Figure 2.48a. In Figure 2.486, we have included the power spectral density of a typical
message source; audio and video signals typically have spectra of this form. In particular,
we see that the power spectral density of the message usually falls off appreciably at higher
frequencies. On the other hand, the power spectral density of the output noise mcreases
rapidly with frequency. Thus around f = + W, the relative spectral density of the message
is quite low, whereas that of the output noise is quite high in comparison. Clearly, the
message is not using the frequency band allotted to it in an efficient manner. It may appear
that one way of improving the noise performance of the system is to slightly reduce the’
bandwidth of the postdetection low-pass filter so as to reject a large amount of noise power
while losing only a small amount of message power. Such an approach, however, is usually
not satisfactory because the distortion of the message caused by the reduced filter band-
width, even though slight, may not be tolerable. For example, in the case of music, we
find that although the high-frequency notes contribute only a very small fraction of the
total power, nonetheless, they contribute a great deal from an esthetic viewpoint. .

A more satisfactory approach to the efficient use of the allowed frequency band is
based on the use of pre-emphasis in the transmitter and de-emphasis in the receiver,

8y,(f) Syl f)

Ww 0 Ww f Ww ie] Ww f

Ficure 2.48 (a) Power spectral density of noise at FM receiver output. (b) Power spectral den
sity of a typical message signal.

2.13 Noise in FM Receivers 155

Pre-emphasis FM ? FM De-emphasis Message
ay filter, Hoel f) - transmitter receiver filter, Hy) plus noise
+

Noise
wh}

 

 

 

 

 

 

 

 

FIGURE 2.49 Use of pre-emphasis and de-emphasis in an FM system,

illustrated in Figure 2.49. In this method, we artificially emphasize the high-frequency
components of the message signal prior to modulation in the transmitter, and therefore
before the noise is introduced in the receiver. In effect, the low-frequency and high-
frequency portions of the power spectral density of the message are equalized in such a
way that the message fully occupies the frequency band allotted to it. Then, at the discrim-
inator output in the receiver, we perform the inverse operation by de-emphasizing the
high-frequency components, so as to restore the original signal-power distribution of the
message. In such a process, the high-frequency components of the noise at the discriminator
output are also reduced, thereby effectively increasing the output signal-to-noise ratio of
the system. Such a pre-emphasis and de-emphasis process is widely used in commercial
FM radio transmission and reception.

In order to produce an undistorted version of the original message at the receiver
output, the pre-emphasis filter in the transmitter and the de-emphasis filter in the receiver
must ideally have frequency responses that are the inverse of each other. That is, if H,.(f)
designates the frequency response of the pre-emphasis filter, then the frequency response
Hig.(f) of the de-emphasis filter must ideally be (ignoring transmission delay)

_1_
Hel)”

This choice of frequency responses makes the average message power at the receiver output
independent of the pre-emphasis and de-emphasis procedure.

From our previous noise analysis in FM systems, assuming a high carrier-to-noise
ratio, the power spectral density of the noise m4(z) at the discriminator output is given by
Equation (2.146). The modified power spectral density of the noise at the de-emphasis
filter output is therefore

Ag f) = ~-W=/f=W (2.156)

Nof? , _Br
Half) /°Sx(f) = 4 az HelAl?s LAL (2.157)
0, otherwise

Recognizing, as before, that the postdetection low-pass filter has a bandwidth W that is,

in general, less than By/2, we find that the average power of the modified noise at the
receiver output is as follows:

Average output noise No i an

=— Hz f)|? d 2.158

(a with spur nos A2 J-w PH)" af )

Because the average message power at the receiver output is ideally unaffected by the

combined pre-emphasis and de-emphasis procedure, it follows that the improvement in

output signal-to-noise ratio produced by the use of pre-emphasis in the transmitter and

de-emphasis in the receiver is defined by

average Output noise power without pre-emphasis and de-emphasis
average output noise power with pre-emphasis and de-emphasis

 

 

(2.159)

156

CHAPTER 2 CONTINUOUS-WAVE MODULATION

Earlier we showed that the average output noise power without pre-emphasis and dp,
emphasis is equal to (2Nj)W7/3A2); see Equation (2.148). Therefore, after cancellation of
common terms, we may express the improvement factor I as

23

af” fl Half)|? af

 

 

(2.160)

It must be emphasized that this improvement factor assumes the use of a high carrier-to.
noise ratio at the discriminator input in the receiver.

& EXAMPLE 2.6

Asimple pre-emphasis filter that emphasizes high frequencies and is commonly used in practice
is defined by the frequency-response

if
fo

which is closely realized by the RC-amplifier network shown in Figure 2.50a, provided that
R << rand 2afCr <1 inside the frequency band of interest. The amplifier in Figure 2.50,
is intended to make up for the attenuation introduced by the RC network at low frequencies,
The frequency parameter fy is 1/(2aCr).

The corresponding de-emphasis filter in the receiver is defined by the frequency response

1
1+ iflfo
which can be realized using the simple RC network of Figure 2.50b.

The improvement in output signal-to-noise ratio of the FM receiver, resulting from thé
combined use of the pre-emphasis and de-emphasis filters of Figure 2.50, is therefore

er. ae
af” ft df af
3 of wi + (fff) (2.161)
(Wifa)?
3[(Wifo) ~ tan“ (WHF)

In commercial FM broadcasting, we typically have fy = 2.1 kHz, and we may reason-
ably assume W = 15 kHz. This set of values yields I = 22, which corresponds to an improve-
ment of 13 dB in the output signal-to-noise ratio of the receiver. The output signal-to-noise

H,(f) = 1 + =

Fadf)

 

 

r

Amplifier -—-o Ty
Output input Cc Output
signal signal signal

{a} @)

 
  
  
 

 

 

 

Input
signal

FiGgurk 2.50 (a) Pre-emphasis filter. (b) De-emphasis filter.

2.14 Computer Experiments: Phase-Locked Loop 157

ratio of an FM receiver without pre-emphasis and de-emphasis is typically 40-50 dB. We see,
therefore, that by using the simple pre-emphasis and de-emphasis filters shown in Figure 2.50,
we can realize a significant improvement in the noise performance of the receiver. |

The use of the simple linear pre-emphasis and de-emphasis filters just described is an
example of how the performance of an FM system may be improved by using the differ-
ences between characteristics of signals and noise in the system. These simple filters also
find application in audio tape-recording. Specifically, nonlinear pre-emphasis and de-
emphasis techniques have been applied successfully to tape recording. These techniques!*
(known as Dolby-A, Dolby-B, and DBX systems) use a combination of filtering and dy-
namic range compression to reduce the effects of noise, particularly when the signal level
is low.

2.14 Computer Experiments:
| Phase-Locked Loop

The experimental study presented in this section focuses on the use of a phase-locked loop
for the demodulation of a frequency modulated signal. Before proceeding with the exper-
iments, however, we first present a brief exposition of phase-locked loop theory.

Basically, the phase-locked loop consists of three major components: a multiplier, a
loop filter, and a voltage-controlled oscillator (VCO) connected together in the form of a
feedback system, as shown in Figure 2.51. The VCO is a sinusoidal generator whose
frequency is determined by a voltage applied to it from an external source. In effect, any
frequency modulator may serve as a VCO. We assume that initially we have adjusted the
VCO so that when the control voltage is zero, two conditions are satisfied:

1. The frequency of the VCO is precisely set at the unmodulated carrier frequency f..

2. The VCO output has a 90 degree phase-shift with respect to the unmodulated carrier
wave,

Suppose then that the input signal applied to the phase-locked loop is an FM signal defined
by

s(t) = A, sin[2rf.t + $4(t)]
where A, is the carrier amplitude. With a modulating signal m(t), the angle @;(t) is related

to m(t) by the integral

oi (t) = 2ak, if m(7) dr

Error

 

 

  
 

 

 

 

FM wave el Loop Output
s{t) filter v{)
Feedback
signal Voltage-
ri) controlled
oscillator

 

FIGURE 2.51 Phase-locked loop.

158

CHAPTER 2 @ CONTINUOUS-WAVE MODULATION

where k, is the frequency sensitivity of the frequency modulator. Let the VCO output ip
the phase-locked loop be defined by

r(t) = A, cos|2rf.t + b2(t)]

where A, is the amplitude. With a control voltage v(¢) applied to the VCO input, the angle
2(t) is related to v(t) by the integral

,(t) = 2ak, [, u(t) dr (2.162)

where &, is the frequency sensitivity of the VCO, measured in Hertz per volt. The object
of the phase-locked loop is to generate a VCO output 7(¢) that has the same phase angle
(except for the fixed difference of 90 degrees) as the input FM signal s(¢), The time-varying
phase angle #,(t) characterizing s(t) may be due to modulation by a message signal m/z),
in which case we wish to recover #,(£) and thereby produce an estimate of m(t). In other
applications of the phase-locked loop, the time-varying phase angle ¢ ,(¢) of the incoming
signal s(t) may be an unwanted phase shift caused by fluctuations in the communication
channel; in this latter case, we wish to track @ ,(t) so as to produce a signal with the same
phase angle for the purpose of coherent detection (synchronous demodulation).

a MopeEL OF THE PHASE-LOCKED Loop”

To develop an understanding of the phase-locked loop, it is desirable to have a model of
the loop. We start by developing a nonlinear model, which is subsequently linearized to
simplify the analysis. According to Figure 2.51, the incoming FM signal s(t) and the VCO
output r(¢) are applied to the multiplier, producing two components:

1. A high-frequency component, represented by the double-frequency term
kh, AA, sinl[4rft +.61(£) + b2{t)]
2. A low-frequency component represented by the difference-frequency term
kyAA, sinfoilt) — ba(t)]
where &,,, is the rultiplier gain, measured in volt™?.

The loop filter in the phase-locked loop is a low-pass filter, and its response to the high-

_frequency component will be negligible. The VCO also contributes to the attenuation of

this component. Therefore, discarding the high-frequency component (i.e., the double-
frequency term), the input to the loop filter is reduced to

elt) = kwA-A, sin[d(é)] (2.163)
where ¢,(t) is the phase error defined by
b(t) = bilt) — ba(é) (2.164)

é

= 6,(t) — 27k, 3 u(z) dr

The loop filter operates on the error e(¢) to produce an output u(t) defined by the convo-
lution integral:

v(t) = [ e(t)h(t — 7) dr (2.165)

2.14° Computer Experiments: Phase-Locked Loop 159

where 4(t) is the impulse response of the loop filter. Using Equations (2.164) and (2.165)
to relate #,(t) and &,(z), we obtain the following nonlinear integro-differential equation
as the descriptor of the dynamic behavior of the phase-locked loop:

apdt) _ deat) °

ho dT 27Ko EC sin[d,(7)]h(t — 7) dr (2.166)

where K, is as loop-gain parameter defined by
Ko = kak AA, (2.167)

The amplitudes A, and A, are both measured in volts, the multiplier gain &,, in volt” * and
the frequency sensitivity &, in Hertz per volt. Hence, it follows from Equation (2.167) that
Ky has the dimensions of frequency. Equation (2.166) suggests the model shown in Figure
2.52 for a phase-locked loop. In this model we have also included the relationship between
u(t) and e(t) as represented by Equations (2.163) and (2.165). We see that the model of
Figure 2.52 resembles the actual block diagram of Figure 2.51. The multiplier at the input
of the phase-locked loop is replaced by a subtracter and a sinusoidal nonlinearity, and the
VCO by an integrator.

The sinusoidal nonlinearity in the model of Figure 2.52 complicates the task of an-
alyzing the behavior of the phase-locked loop. It would be helpful to linearize this model
to simplify the analysis and yet give a good approximate description of the loop’s behavior
in certain modes of operation. When the ‘phase error ¢,(£) is zero, the phase-locked loop
is said to be in phase-lock. When ¢,(t) is at all times small compared with one radian, we
miay use the approximation

sin[p.(é)] = $,(t)

which is accurate to within 4 percent for ,(t) less than 0.5 radians. In this case, the loop
is said to be near phase-lock, and the sinusoidal nonlinearity of Figure 2.52 may be dis-
regarded. Under this condition, v(z) is approximately equal to m(t), except for the scaling
factor k,/k,.

The complexity of the phase-locked loop is determined by the frequency response
H(f) of the loop filter. The simplest form of a phase-locked loop is obtained when
H(f) = 1; that is, there is no loop filter, and the resulting phase-locked loop is referred to
as a first-order phase-locked loop. A major limitation of a first-order phase-locked loop is
that the loop gain parameter Ky controls both the loop bandwidth as well as the hold-in
frequency range of the loop; the hold-in frequency range refers to the range of frequencies

2nKg 2ak,

da Att) vu)

t
fe
0

FicureE 2.52 Nonlinear model of the phase-locked loop.

 

 

 

 

 

 

 

 

 

 

 

 

 

160

CHAPTER 2 CONTINUOUS-WAVE MODULATION

for which the loop remains phase-locked to the input signal. We may overcome this lim:
itation by using a loop filter with the frequency response

a
if
where a is a constant. Then with this loop filter in place and the phase-locked loop oper.

ating in its linear mode, we find from Equation (2.166) that the phase-locked loop behaves
as a second-order feedback system, as shown by the standard frequency response

(f) Giff)
@(f) 1+ 2 eciflf,) + (iflfaY”
where ®,(f) and ®,(f) are the Fourier transforms of @,(t) and ¢,(t), respectively. The

system is parameterized by the natural frequency, fs and damping factor, £, which are
respectively defined by

H(f)=1+ (2.168)

 

 

(2.169)

f. = VaK, (2.170)
and
= |Ko
c= [2 (2.171)

The second-order phase-locked loop so described is the subject of the computer experi-
ments presented next.

Experiment 1: Acquisition Mode

When a phase-locked loop is used for coherent detection (synchronous demodulation), the
loop must first lock onto the input signal and then follow the variations of its phase angle
with time. The process of bringing a loop into phase-lock is called acquisition, and the
ensuing process of following angular variations in the input signal is called tracking. In
the acquisition mode and quite possibly the tracking mode, the phase error ¢,(¢) between
the input signal s(t) and the VCO output r(¢) will certainly be large, thereby mandating
the use of the nonlinear model of Figure 2.52. However, a nonlinear analysis of the ac-
quisition process based on this latter model is beyond the scope of this book. In this
experiment, we use computer simulations to study the acquisition process and thereby
develop insight into some of its features.

Consider a second-order phase-locked loop using the loop filter of Equation (2.168)
and having the following parameters:

1
Natural frequency f,, = on Hz

Damping factor £ = 0.3, 0.707, 1.0

To accommodate variation in Z, the filter parameter a is varied in accordance with the
formula

fr
2¢

which follows from Equations (2.170) and (2.171). Figure 2.53 presents the variation it
the phase error @,(t) with time for each of the three specified values of damping factor bs

a=

2.14 Computer Experiments: Phase-Locked Loop 161

9S
B

9
w

9
a

Phase error ¢,(4), radians
Oo
Ny

o

 

 

 

-0.2 | | |
0 5 10 15 20 25

Time ¢, seconds

 

Figure 2.53 Variation of the phase error for three different values of damping factor.

assuming a frequency step of 0.125 Hz. These results show that the damping factor
£ = 0.707 gives the best compromise between a fast response time and an underdamped
oscillatory behavior.

Experiment 2: Phase-Plane Portrait

A phase-plane portrait is a family of trajectories, with each trajectory representing a single
solution of Equation (2.166). For the second experiment we plot the phase-plane portrait
of a second-order phase-locked loop for the case of sinusoidal modulation. The system
parameters of the loop are as follows:
. 50
Loop-gain parameter Ky = =~ Hz
2a
Loop-natural frequency f, = So Hz
Pp q YIn n/t

50
Si idal modulation f, (= H:
inusoidal modulation frequency f, IaV/In Zz

 

Figure 2.54 presents the phase-plane portrait of the phase-locked loop adjusted for
critical damping, where the trajectories (frequency error versus phase error) are plotted
for different starting points. From this portrait we make the following observations:

1. For a sinusoidal nonlinearity, the phase-plane portrait is itself periodic with period
2 in the phase error #,, but it is aperiodic in d,/dt.
2. For an initial frequency error
1 dd
K dt
with an absolute value less than or equal to 1, the phase-locked loop is assured of
attaining a stable (equilibrium) point at (0, 0) or (0, 277); the multiplicity of equilib-
rium points is a manifestation of periodicity of the phase-plane portrait.

162

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

Frequency error, (Hz)

 

 

 

Phase error, radians

FicurRE 2.54 Phase-plane portrait for critical damping and sinusoidal modulation.

3. For an initial frequency error
1 doe
K at
with an absolute value equal to 2, we have a saddle point at (0, ar) where the slightest

perturbation applied to the phase-locked loop causes it to shift to the equilibrium
point (0, 0) or (0, 277).

4.2.15 Summary and Discussion

 

In this chapter we studied the principles of continuous-wave (CW) modulation. This an-
alog form of modulation uses a sinusoidal carrier whose amplitude or angle is varied in
accordance with a message signal. We may thus distinguish two families of CW modula-
tion: amplitude modulation and angle modulation.

= AMPLITUDE MODULATION

Amplitude modulation may itself be classified into four types, depending on the spectral
content of the modulated signal. The four types of amplitude modulation and their prac-
tical merits are as follows:

1. Full amplitude modulation (AM), in which the upper and lower sidebands are trans-
mitted in full, accompanied by the carrier wave.

Accordingly, demodulation of an AM signal is accomplished rather simply in the receiver
by using an envelope detector, for example. It is for this reason we find that full AM is
commonly used in commercial AM radio broadcasting, which involves a single powerful
transmitter and numerous receivers that are relatively inexpensive to build.

2.15 Summary and Discussion 163

2. Double sideband-suppressed carrier (DSB-SC) modulation, in which only the upper
and lower sidebands are transmitted.

The suppression of the carrier wave means that DSB-SC modulation requires much less
power than full AM to transmit the same message signal; this advantage of DSB-SC mod-
ulation over full AM is, however, attained at the expense of increased receiver complexity.
DSB-SC modulation is therefore well suited for point-to-point communication involving
one transmitter and one receiver; in this form of communication, transmitted power is at
a premium and the use of a complex receiver is therefore justifiable.

3. Single sideband (SSB) modulation, in which only the upper sideband or lower sideband
is transmitted.

SSB modulation is the optimum form of CW modulation in the sense that it requires the
minimum transmitted power and the minimum channel bandwidth for conveying a mes-
sage signal from one point to another. However, its use is limited to message signals with
an energy gap centered on zero frequency.

4, Vestigial sideband modulation, in which almost all of one sideband and a vestige of
the other sideband are transmitted in a prescribed complementary fashion.

VSB modulation requires a channel bandwidth that is between that required for SSB and
DSB-SC systems, and the saving in bandwidth can be significant if modulating signals with
large bandwidths are being handled, as in the case of television signals and high-speed
data.

DSB-SC, SSB, and VSB are examples of linear modulation, whereas, strictly speaking, full
AM is nonlinear. However, the deviation of full AM from linearity is of a mild sort.
Accordingly, all four forms of amplitude modulation lend themselves readily to spectral
analysis using the Fourier transform.

ANGLE MODULATION

Angle modulation ‘may be classified into frequency modulation (FM) and phase modula-
tion (PM). In FM, the instantaneous frequency of a sinusoidal carrier is varied in propor-
tion to the message signal. In PM, on the other hand, it is the phase of the carrier that is
varied in proportion to the message signal. The instantaneous frequency is defined as the
derivative of the phase with respect to time, except for the scaling factor 1/(27). Accord-
ingly, FM and PM are closely related to each other; if we know the properties of the one,
we can determine those of the other. For this reason, and because FM is commonly used
in broadcasting, much of the material on angle modulation in the chapter was devoted
to FM.

Unlike amplitude modulation, FM is a nonlinear modulation process. Accordingly,
spectral analysis of FM is more difficult than for AM. Nevertheless, by studying single-
tone FM, we were able to develop a great deal of insight into the spectral properties of
FM. In particular, we derived an empirical rule known as Carson’s rule for an approximate
evaluation of the transmission bandwidth Bz of FM. According to this rule, Bis controlled
by a single parameter: the modulation index @ for sinusoidal FM, or the deviation ratio
D for nonsinusoidal FM.

NOISE ANALYSIS

We conclude the chapter on CW modulation systems by presenting a comparison of their
noise performances. For this comparison, we assume that the modulation is produced by

164

CHAPTER 2 ® CONTINUOUS-WAVE MODULATION

7O;-
Iv
60 ~~ ml
8dB
50 [-

Output signal-to-noise ratio, dB

 

 

 

| I |
0 10 20 30 40 50

Channel signal-to-noise ratio, dB

 

FIGURE 2.55 Comparison of the noise performance of various CW modulation systems. Curve [:
Full AM, 4 = 1. Curve Il: DSB-SC, SSB. Curve III: FM, 8 = 2. Curve IV: FM, 8 = 5. (Curves
IIL and IV include 13-dB pre-emphasis, de-emphasis improvement.)

a sinusoidal wave. For the comparison to be meaningful, we also assume that the modu-
lation systems operate with exactly the same channel signal-to-noise ratio. We may thus
plot the output signal-to-noise ratio versus the channel signal-to-noise ratio as in Figure
2.55 for the following modulation schemes:

» Full AM with 100 percent modulation
» Coherent DSB-SC, SSB
» FM with B = 2 and B= 5
Figure 2.55 also includes the AM and FM threshold effects. In making the comparison, it

is informative to keep in mind the transmission bandwidth requirement of the modulation
system in question. In this regard, we use a normalized transmission bandwidth defined by

_ Br
W
where B- is the transmission bandwidth of the modulated signal, and W is the message

bandwidth. Table 2.4 presents the values of B, for the different CW modulation schemes.
From Figure 2.55 and Table 2.4 we make the following observations:

B,

» Among the family of AM systems, SSB modulation is optimum with regard to noise
performance as well as bandwidth conservation.

» The use of FM improves noise performance but at the expense of an excessive trans
mission bandwidth. This assumes that the FM system operates above threshold fot
the noise improvement to be realized.

Notes ard References 165

TABLE 2.4 Values of B,, for

various CW modulation schemes

 

FM
AM, DSB-SC SSB B=2 BPS

 

2 1

oo
=
Nn

 

 

On an important point to conclude the discussion on CW modulation, only frequency

modulation offers the capability to trade off transmission bandwidth for improved noise

pe

tformance. The trade-off follows a square law, which is the best that we can do with

CW modulation (ie., analog communications). In Chapter 3 we describe pulse-code mod-
ulation, which is basic to the transmission of analog information-bearing signals by a
digital communication system, and which can indeed do much better.

I NoTES AND REFERENCES

1.

10.

11.

12.

It appears that the terms continuous wave and heterodyning were first used by Reginald
Fessenden in the early 1900s.

. The Costas receiver is named in honor of its inventor; see the paper by Costas (1956).

. Bessel functions play an important role in the study of both analog and digital communi-
cation systems. They can be of the so-called first kind or second kind. Appendix 3 discusses
mathematical details and properties of both kinds of Bessel functions. A table of Bessel
functions of the first kind is presented in Table A6.5.

. Carson’s rule for the bandwidth of FM signals is named in honor of its originator; Carson
and Fry (1937) wrote one of the early classic papers on frequency modulation theory.

. The indirect method of generating a wideband FM wave was first proposed by Armstrong
(1936). Armstrong was also the first to recognize the noise-robustness properties of fre-
quency modulation.

. Stereo multiplexing usually involves the use of frequency modulation for radio transmis-
sion. However, it may also be transmitted using amplitude modulation as discussed in
Problem 2.14; for more details, see the paper by Mennie (1978).

. For detailed description of the superheterodyne receiver, see the Radio Engineering Hand-
book edited by Henney (1958, pp. 19-34-19-41).

. The qualitative study of threshold in envelope detection presented here follows Downing
(1964, p. 71).

. For a justification of the critical assumption on which the simplification presented in Equa-
tion (2.142) rests, see Rice (1963).

For a detailed discussion of the threshold effect in FM receivers, see the paper by Rice
(1963) and the book by Schwartz, Bennett, and Stein (1966, pp. 129-163).

Figure 2.45 is adapted from Rice (1963). The validity of the theoretical curve II in this
figure has been confirmed experimentally; see Schwartz, Bennett, and Stein (1966, p. 153).
For some earlier experimental work on the threshold phenomenon in FM, see the paper
by Crosby (1937).

The idea of using feedback around an FM demodulator was originally proposed by Chaffee
(1939).

166 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

13. The treatment of the FMFB demodulator presented in Section 2.13 is based on the pape,
by Enloe (1962); see also Roberts (1977, pp. 166-181).

14. For a detailed discussion of Dolby systems mentioned in the latter part of Section 2.13, seg
Stremler (1990, pp. 732-734).

15. For a full treatment of the nonlinear analysis of a phase-locked loop, see Gardner ( 1979)
Lindsey (1972), and Viterbi (1966).

.

| PROBLEMS

Amplitude Modulation

2.1 Suppose that nonlinear devices are available for which the output current é, and input
voltage v, are related by

i, = ayy, + asu;
where a; and a3 are constants. Explain how these devices may be used to provide: (a) a
product modulator and (b) an amplitude modulator.

2.2 Figure P2.2 shows the circuit diagram of a square-law modulator. The signal applied to
the nonlinear device is relatively weak, such that it can be represented by a square law:

v,(t) = ayu(t) + anvi(t)

where a, and a, are constants, v;,(t) is the input voltage, and v(t) is the output voltage.
The input voltage is defined by

u(t) = A, cos(2af.t) + mt)

where mm(t) is a message signal and A, cos(27f,t) is the carrier wave.
(a) Evaluate the output voltage v,(z).

(b) Specify the frequency response that the tuned circuit in Figure P2.2 must satisfy in
order to generate an AM signal with f, as the carrier frequency.

(c) What is the amplitude sensitivity of this AM signal?

 

Nonlinear
device

   

 

 

 

mn

A, cos (2arf,1)

Tuned to f,

Figure P2.2

2.3 Figure P2.3a shows the circuit diagram of a switching modulator. Assume that the carrit!
wave c(t) applied to the diode is large in amplitude, so that the diode acts like an ideal
switch: it presents zero impedance when forward biased (i.e., c(t) > 0). We may thus

Problems 167

approximate the transfer characteristic of the diode-load resistor combination by a piece-
wise-linear characteristic defined as (see Figure P2.3)

_ J vilt), c(t) > 0
valt) = {° > et <0

That is, the load voltage v,(t) varies periodically between the values v,(¢) and zero at a
rate equal to the carrier frequency f.. Hence, we may write

u(t) ~ [A, cos(2mf.t) + m(t)]g7,(t)
where g7,(t) is a periodic pulse train defined by

—1

 

&r(t} = > ste 2a cos[2af,t(2n — 1)]

(a) Find the AM wave component red in the output voltage v,[t).

(b) Specify the unwanted components in v(t) that need to be removed by a band-pass
filter of suitable design.

ug
c() =A, cos (27.0)
©) —F

mit) C) oR S09 Slope = 1

 

{a) ()
FIGuRE P2.3

2.4 Consider the AM signal

s(t) = Al + yp cos(2f,,t)] cos(27f,t)

produced by a sinusoidal modulating signal of frequency f,,. Assume that the modulation
factor is 4 = 2, and the carrier frequency f, is much greater than f,,. The AM signal s(t)
is applied to an ideal envelope detector, producing the output v(t).

(a) Determine the Fourier series representation of u(t).

(b) What is the ratio of second-harmonic amplitude to fundamental amplitude in v(t)?

Figure P2.5 shows the circuit diagram of an envelope detector. It consists simply of a
diode and resistor-capacitor (RC) filter. On a positive half-cycle of the input signal, the
diode is forward-biased and the capacitor C charges up rapidly to. the peak value of the
input signal, When the input signal falls below this value, the diode becomes reverse-
biased and the capacitor C discharges slowly through the load resistor R;. The discharging
process continues until the next positive half-cycle. Thereafter, the charging-discharging
routine is continued.

(a) Specify the condition that must be satisfied by the capacitor C for it to charge rapidly
and thereby follow the input voltage up to the positive peak when the diode is
conducting.

(b) Specify the condition which the load resistor R; must satisfy so that the capacitor C
discharges slowly between positive peaks of the carrier wave, but not so long that the

168 CHAPTER 2 CONTINUOUS-WAVE MODULATION

capacitor voltage will not discharge at the maximum rate of change of the modulating
wave.

c Ry Output

AM wave
re)
FiGure P2.5

2.6 Consider a square-law detector, using a nonlinear device whose transfer characteristic ig

defined by
va(t) = ayu,(t) + anu;(t}

where a, and a, are constants, v;(£) is the input, and v,(t) is the output. The input consists
of the AM wave

vu,(t) = A[1 + k,m(t)] cos(27f.t)

(a) Evaluate the output v2(2).
(b) Find the conditions for which the message signal #z(t) may be recovered from v(t).

2.7 The AM signal
s(t) = AJ[1 + kan(t)] cos(2af.t)
is applied to the system shown in Figure P2.7. Assuming that | k,zn(t)| <1 for all ¢ and

the message signal m(t) is limited to the interval —-W = f = W and that the cartier
frequency f. > 2W show that #(t) can be obtained from the square-rooter output v;(¢}.

vi Low-pass ale) Square
s(Q) ae} Squarer filter T——| Tooter b> 030

ars
ad = 50 val = Vos
FiGcure P2.7

 

 

 

 

2.8 Consider a message signal (2) with the spectrum shown in Figure P2.8. The message
bandwidth W = 1 kHz. This signal is applied to a product modulator, together with a
carrier wave A, cos(2.f,t), producing the DSB-SC modulated signal s(t). The modulated
signal is next applied to a coherent detector. Assuming perfect synchronism between thé
carrier waves in the modulator and detector, determine the spectrum of the detectot
output when: (a) the carrier frequency f, = 1.25 kHz and (b) the carrier frequency
f, = 0.75 kHz. What is the lowest carrier frequency for which each component of the
modulated signal s(z) is uniquely determined by m(t)?

Problems 169

Mf)
w Oo W f
Ficure P2.8

2.9 Figure P2.9 shows the circuit diagram of a balanced’modulator. The input applied to the
top AM modulator is #(t], whereas that applied to the lower AM modulator is —m(¢);
these two modulators have the same amplitude sensitivity. Show that the output s(t) of
the balanced modulator consists of a DSB-SC modulated signal.

 

AM ald

mt) — modulator

 

 

 

A, Cos (20,0)

 

  

s(t)

  

Oscillator

 

 

 

A,cos (2nf,2)

 

AM

~ mt) > edulator

 

 

 

FIGURE P2.9

2.10 A DSB-SC modulated signal is demodulated by applying it to a coherent detector.

(a) Evaluate the effect of a frequency error Af in the local carrier frequency of the de-
tector, measured with respect to the carrier frequency of the incoming DSB-SC signal.

(b) For the case of a sinusoidal modulating wave, show that because of this frequency
error, the demodulated signal exhibits beats at the error frequency. Illustrate your
answer with a sketch of this demodulated signal.

2.11 Consider the DSB-SC signal
s(t) = A, cos(2afitim(t)

where A, cos(27f,t) is the carrier wave and m(t) is the message signal. This modulated
signal is applied to a square-law device characterized by

y(t} = s7(t)

The output y(£) is next applied to a narrowband filter with a passband magnitude response
of one, midband frequency 2f., and bandwidth Af. Assume that Af is small enough to
treat the spectrum of y(f) as essentially constant inside the passband of the filter.

(a) Determine the spectrum of the square-law device output y(f).
(b) Show that the filter output v(t) is approximately sinusoidal, given by

AZ
v(t) = > E Af cos(47f.t)

where E is the energy of the message signal s(t).


 
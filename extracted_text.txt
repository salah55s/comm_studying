Binomial Distribution
‚Ä¢The binomial distribution occurs inmany cases of
interest tous.
‚Ä¢Wewill useittofind theprobability ofaspecified
number ofbiterrors inadigital transmission .
‚Ä¢This distribution isillustrated viatheexperiment of
tossing acoinùëõ
Binomial Distribution
‚Ä¢Consider theexperiment oftossing acoinùëõtimes .Let
theprobability ofheads beùëù,andoftails beùëû.
‚Ä¢Now suppose wewish tofind theprobability ofùëò
heads outofùëõtosses .
‚Ä¢One possible sequence is:thefirstùëòtosses areheads,
andtheremaining ùëõ-ùëòtosses aretails.
Binomial Distribution
‚Ä¢Theprobability ofthatparticular sequence is
ùëùùëòùëûùëõ‚àíùëò
‚Ä¢Another possible sequence istostart withùëõ-ùëòtails,
followed byùëòheads .
‚Ä¢The probability ofthis second sequence any other
ordering ordering ofùëòheads andùëõ-ùëòtails is
ùëùùëòùëûùëõ‚àíùëò
Binomial Distribution
‚Ä¢Theoverall probability ofùëòheads (inanyposition) is
found byadding theindividual probabilities together .
‚Ä¢Thus ,weneed tomultiply theprobability ofthe
sequence bythenumber ofways wecandistribute ùëò
heads amongùëõpositions .
‚Ä¢This number isfound by
Binomial Distribution
where ùëõ
ùëòis the binomial coefficient. 
‚Ä¢Thus, the probability of ùëòheads is given by 
ùëÉùëã=ùëò=ùëõ
ùëòùëùùëòùëûùëõ‚àíùëò=ùëõ!
ùëò!ùëõ‚àíùëò!ùëùùëòùëûùëõ‚àíùëò
Uniform Distribution
‚Ä¢Itisthesimplest continuous density function .
‚Ä¢The value of the density function is a constant over a 
range of the x -axis.

Uniform Distribution
‚Ä¢That constant must beselected such thatthetotal area
under thedensity function isunity .
‚Ä¢Asanapplication example, suppose theoperation of
turning onasinusoidal generator .The output ofthe
generator willbe:
ùë†ùë°=ùê¥ùëêùëúùë†(2ùúãùëìùëúùë°+ùúë)
Uniform Density
‚Ä¢The absolute time ofturning onthegenerator is
random, andthus theangleùúëisrandom .
‚Ä¢Itisexpected thatùúëisuniformly distributed between
0and2ùúã.
‚Ä¢Thus a=0,b=2ùúã
Block Diagram of a Digital System


Source Encoder
‚Ä¢Operates upon one ormore analog signals toproduce a
periodic train ofsymbols .
‚Ä¢May contain amultiplexer, with channels which areused to
communicate from more than onesource atthesame time.

Encryptor
‚Ä¢Provides security byensuring that only theintended
receiver canunderstand themessage .
‚Ä¢Encryption isameans ofsecuring data byencoding it
mathematically such that itcan only beread, or
decrypted, bythose with thecorrect keyorcipher .
‚Ä¢Encryption iscrucial inadigitally -connected world to
keep private information, messages, and financial
transactions private andsecure .
Channel Encoder
‚Ä¢Itincreases efficiency and decreases the effects of
transmission errors duetonoise .
‚Ä¢000,001,010,011,100,101,110,111
If101istransmitted andanerror occur inthethird bit,100
isreceived .There isnoway forthereceiver toknow that
100wasnotthetransmitted word .
‚Ä¢0000,0011,0101,0110,1001,1010,1100,1111
If1010 istransmitted, and1011 isreceived, thisisnotone
oftheeight acceptable words .
Line Coding
‚Ä¢Is the process of converting digital data to baseband 
signals. 

Carrier Modulator
‚Ä¢Baseband signals cannotbetransmitted over aradio linkor
satellites, because this would require large antennas to
efficiently radiate thelowfrequency spectrum ofthesignal .
‚Ä¢Aspectrum shift tohigher frequencies isalso required to
transmit several messages simultaneously, bysharing the
large bandwidth ofthetransmission medium (FDM) .
Carrier Modulator

Carrier Modulator

Spread Spectrum Modulator
‚Ä¢Provides additional security .
‚Ä¢Multiple access techniques combine signals that might
originate from different sources, sothat they canshare a
portion ofthecommunication resource (e.g.,spectrum,
time) .
‚Ä¢Frequency spreading canproduce asignal thatisrelatively
invulnerable tointerference andcanbeused toenhance the
system privacy .Itisavaluable technique used formultiple
access .
Receiver and Symbol Synchronizer
‚Ä¢Thereceiver issimply amirror image ofthetransmitter .
‚Ä¢The only difference isthat thecarrier modulator ofthe
transmitter hasbeen replaced bythecarrier demodulator
andthesymbol synchronizer .
‚Ä¢The symbol synchronizer partitions theoverall signal into
segments corresponding toeach symbol and toeach
message .
Advantages of Digital Systems
‚Ä¢Errors often canbecorrected
Theonly decision atthereceiver istheselection between two
possible pulses, notthedetails ofthepulse shape .

Advantages of Digital Systems
‚Ä¢The regenerative repeater not only performs the function 
of amplification, but also cleans up the signal. 

Advantages of Digital Systems
2.Signal manipulation (e.g.encryption) issimple to
perform
Digital systems deals with numbers, rather than
waveforms .These numbers can bemanipulated by
simple logic circuits .Analog operations require complex
hardware .
Disadvantages of Digital Systems
‚Ä¢Generally requires more bandwidth than analog. 
‚Ä¢Synchronization is required. 
DIGITAL
COMMUNICATIONS
Fundamentals and Applications
Second Edition
BERNARD SKLAR
Communications Engineering Services, Tarzana, California
and
University of California, Los Angeles
Prentice Hall P T R
Upper Saddle River, New Jersey 07458
www.phptr.com
Contents
PREFACE xvii
1 SIGNALS AND SPECTRA 1
1.1 Digital Communication Signal Processing, 3
1.1.1 Why Digital?, 3
1.1.2 Typical Block Diagram and Transformations, 4
1.1.3 Basic Digital Communication Nomenclature, 11
1.1.4 Digital versus Analog Performance Criteria, 13
1.2 Classification of Signals, 14
1.2.1 Deterministic and Random Signals, 14
1.2.2 Periodic and Nonperiodic Signals, 14
1.2.3 Analog and Discrete Signals, 14
1.2.4 Energy and Power Signals, 14
1.2.5 The Unit Impulse Function, 16
1.3 Spectral Density, 16
1.3.1 Energy Spectral Density, 17
1.3.2 Power Spectral Density, 17
1.4 Autocorrelation, 19
1.4.1 Autocorrelation of an Energy Signal, 19
1.4.2 Autocorrelation of a Periodic (Power) Signal, 20
1.5 Random Signals, 20
1.5.1 Random Variables, 20
1.5.2 Random Processes, 22
1.5.3 Time Averaging and Ergodicity, 25
1.5.4 Power Spectral Density of a Random Process, 26
1.5.5 Noise in Communication Systems, 30
v
1.6 Signal Transmission through Linear Systems, 33
1.6.1 Impulse Response, 34
1.6.2 Frequency Transfer Function, 35
1.6.3 Distortionless Transmission, 36
1.6.4 Signals, Circuits, and Spectra, 42
1.7 Bandwidth of Digital Data, 45
1.7.1 Baseband versus Bandpass, 45
1.7.2 The Bandwidth Dilemma, 47
1.8 Conclusion, 51
2 FORMATTING AND BASEBAND MODULATION 55
2.1 Baseband Systems, 56
2.2 Formatting Textual Data (Character Coding), 58
2.3 Messages, Characters, and Symbols, 61
2.3.7 Example of Messages, Characters, and Symbols, 61
2.4 Formatting Analog Information, 62
2.4.1 The Sampling Theorem, 63
2.4.2 Aliasing, 69
2.4.3 Why Oversample? 72
2.4.4 Signal Interface for a Digital System, 75
2.5 Sources of Corruption, 76
2.5.7 Sampling and Quantizing Effects, 76
2.5.2 Channel Effects, 77
2.5.3 Signal-to-Noise Ratio for Quantized Pulses, 78
2.6 Pulse Code Modulation, 79
2.7 Uniform and Nonuniform Quantization, 81
2.7.7 Statistics of Speech Amplitudes, 81
2.7.2 Nonuniform Quantization, 83
2.7.3 Companding Characteristics, 84
2.8 Baseband Modulation, 85
2.8.1 Waveform Representation of Binary Digits, 85
2.8.2 PCM Waveform Types, 85
2.8.3 Spectral Attributes of PCM Waveforms, 89
2.8.4 Bits per PCM Word and Bits per Symbol, 90
2.8.5 M-ary Pulse Modulation Waveforms, 91
2.9 Correlative Coding, 94
2.9.7 Duobinary Signaling, 94
2.9.2 Duobinary Decoding, 95
2.9.3 Preceding, 96
2.9.4 Duobinary Equivalent Transfer Function, 97
2.9.5 Comparison of Binary with Duobinary Signaling, 98
2.9.6 Poly binary Signaling, 99
2.10 Conclusion, 100
vi Contents
3 BASEBAND DEMODULATION/DETECTION 104
3.1 Signals and Noise, 106
3.1.1 Error-Performance Degradation in Communication Systems, 106
3.1.2 Demodulation and Detection, 107
3.1.3 A Vectorial View of Signals and Noise, 110
3.1.4 The Basic SNR Parameter for Digital Communication Systems, 117
3.1.5 Why Eb/N0 Is a Natural Figure of Merit, 118
3.2 Detection of Binary Signals in Gaussian Noise, 119
3.2.1 Maximum Likelihood Receiver Structure, 119
3.2.2 The Matched Filter, 122
3.2.3 Correlation Realization of the Matched Filter, 124
3.2.4 Optimizing Error Performance, 127
3.2.5 Error Probability Performance of Binary Signaling, 131
3.3 Intersymbol Interference, 136
3.3.1 Pulse Shaping to Reduce ISI, 138
3.3.2 Two Types of Error-Performance Degradation, 142
3.3.3 Demodulation/Detection of Shaped Pulses, 145
3.4 Equalization, 149
3.4.1 Channel Characterization, 149
3.4.2 Eye Pattern, 151
3.4.3 Equalizer Filter Types, 152
3.4.4 Preset and Adaptive Equalization, 158
3.4.5 Filter Update Rate, 160
3.5 Conclusion, 161
4 BANDPASS MODULATION AND DEMODULATION/
DETECTION 167
4.1 Why Modulate? 168
4.2 Digital Bandpass Modulation Techniques, 169
4.2.1 Phasor Representation of a Sinusoid, 171
4.2.2 Phase Shift Keying, 173
4.2.3 Frequency Shift Keying, 175
4.2.4 Amplitude Shift Keying, 175
4.2.5 Amplitude Phase Keying, 176
4.2.6 Waveform Amplitude Coefficient, 176
4.3 Detection of Signals in Gaussian Noise, 177
4.3.1 Decision Regions, 177
4.3.2 Correlation Receiver, 178
4.4 Coherent Detection, 183
4.4.1 Coherent Detection of PSK, 183
4.4.2 Sampled Matched Filter, 184
4.4.3 Coherent Detection of Multiple Phase Shift Keying, 188
4.4.4 Coherent Detection of FSK, 191
Contents vii
4.5 Noncoherent Detection, 194
4.5.1 Detection of Differential PSK, 194
4.5.2 Binary Differential PSK Example, 196
4.5.3 Noncoherent Detection of FSK, 198
4.5.4 Required Tone Spacing for Noncoherent Orthogonal FSK, 200
4.6 Complex Envelope, 204
4.6.1 Quadrature Implementation of a Modulator, 205
4.6.2 D8PSK Modulator Example, 206
4.6.3 D8PSK Demodulator Example, 208
4.7 Error Performance for Binary Systems, 209
4.7.1 Probability of Bit Error for Coherently Detected BPSK, 209
4.7.2 Probability of Bit Error for Coherently Detected
Differentially Encoded Binary PSK, 211
4.7.3 Probability of Bit Error for Coherently Detected
Binary Orthogonal FSK, 213
4.7.4 Probability of Bit Error for Noncoherently Detected
Binary Orthogonal FSK, 213
4.7.5 Probability of Bit Error for Binary DPSK, 216
4.7.6 Comparison of Bit Error Performance for Various
Modulation Types, 218
4.8 M-ary Signaling and Performance, 219
4.8.1 Ideal Probability of Bit Error Performance, 219
4.8.2 M-ary Signaling, 220
4.8.3 Vectorial View of MPSK Signaling, 222
4.8.4 BPSK and QPSK Have the Same Bit Error Probability, 223
4.8.5 Vectorial View of MFSK Signaling, 225
4.9 Symbol Error Performance for M-ary Systems (M > 2), 229
4.9.1 Probability of Symbol Error for MPSK, 229
4.9.2 Probability of Symbol Error for MFSK, 230
4.9.3 Bit Error Probability versus Symbol Error Probability
for Orthogonal Signals, 232
4.9.4 Bit Error Probability versus Symbol Error Probability
for Multiple Phase Signaling, 234
4.9.5 Effects of Intersymbol Interference, 235
4.10 Conclusion, 236
5 COMMUNICATIONS LINK ANALYSIS 242
5.1 What the System Link Budget Tells the System Engineer, 243
5.2 The Channel, 244
5.2.7 The Concept of Free Space, 244
5.2.2 Error-Performance Degradation, 245
5.2.3 Sources of Signal Loss and Noise, 245
viii Contents
5.3 Received Signal Power and Noise Power, 250
5.3J The Range Equation, 250
5.3.2 Received Signal Power as a Function of Frequency, 254
5.3.3 Path Loss is Frequency Dependent, 256
5.3.4 Thermal Noise Power, 258
5.4 Link Budget Analysis, 259
5.4.1 Two E//NQ Values of Interest, 262
5.4.2 Link Budgets are Typically Calculated in Decibels, 263
5.4.3 How Much Link Margin is Enough? 264
5.4.4 Link Availability, 266
5.5 Noise Figure, Noise Temperature, and System Temperature, 270
5.5J Noise Figure, 270
5.5.2 Noise Temperature, 273
5.5.3 Line Loss, 274
5.5.4 Composite Noise Figure and Composite Noise Temperature, 276
5.5.5 System Effective Temperature, 277
5.5.6 Sky Noise Temperature, 282
5.6 Sample Link Analysis, 286
5.6.1 Link Budget Details, 287
5.6.2 Receiver Figure of Merit, 289
5.6.3 Received Isotropic Power, 289
5.7 Satellite Repeaters, 290
5.7.7 Nonregenerative Repeaters, 291
5.7.2 Nonlinear Repeater Amplifiers, 295
5.8 System Trade-Offs, 296
5.9 Conclusion, 297
6 CHANNEL CODING: PART 1 304
6.1 Waveform Coding and Structured Sequences, 305
6.1.1 Antipodal and Orthogonal Signals, 307
6.1.2 M-ary Signaling, 308
6.1.3 Waveform Coding, 309
6.1.4 Waveform-Coding System Example, 313
6.2 Types of Error Control, 315
6.2.1 Terminal Connectivity, 315
6.2.2 Automatic Repeat Request, 316
6.3 Structured Sequences, 317
6.3.1 Channel Models, 318
6.3.2 Code Rate and Redundancy, 320
6.3.3 Parity Check Codes, 321
6.3.4 Why Use Error-Correction Coding? 323
Contents ix
6.4 Linear Block Codes, 328
6.4.1 Vector Spaces, 329
6.4.2 Vector Subspaces, 329
6.4.3 A (6, 3) Linear Block Code Example, 330
6.4.4 Generator Matrix, 331
6.4.5 Systematic Linear Block Codes, 333
6.4.6 Parity-Check Matrix, 334
6.4.7 Syndrome Testing, 335
6.4.8 Error Correction, 336
6.4.9 Decoder Implementation, 340
6.5 Error-Detecting and Correcting Capability, 342
6.5.1 Weight and Distance of Binary Vectors, 342
6.5.2 Minimum Distance of a Linear Code, 343
6.5.3 Error Detection and Correction, 343
6.5.4 Visualization of a 6-Tuple Space, 347
6.5.5 Erasure Correction, 348
6.6 Usefulness of the Standard Array, 349
6.6.1 Estimating Code Capability, 349
6.6.2 An (n, k) Example, 351
6.6.3 Designing the (8, 2) Code, 352
6.6.4 Error Detection versus Error Correction Trade-Offs, 352
6.6.5 The Standard Array Provides Insight, 356
6.7 Cyclic Codes, 356
6.7.7 Algebraic Structure of Cyclic Codes, 357
6.7.2 Binary Cyclic Code Properties, 358
6.7.3 Encoding in Systematic Form, 359
6.7.4 Circuit for Dividing Polynomials, 360
6.7.5 Systematic Encoding with an (n - k)-Stage Shift Register, 363
6.7.6 Error Detection with an (n - k)-Stage Shift Register, 365
6.8 Weil-Known Block Codes, 366
6.8.1 Hamming Codes, 366
6.8.2 Extended Golay Code, 369
6.8.3 BCH Codes, 370
6.9 Conclusion, 374
7 CHANNEL CODING: PART 2 381
7.1 Convolutional Encoding, 382
7.2 Convolutional Encoder Representation, 384
7.2.1 Connection Representation, 385
7.2.2 State Representation and the State Diagram, 389
7.2.3 The Tree Diagram, 391
7.2.4 The Trellis Diagram, 393
7.3 Formulation of the Convolutional Decoding Problem, 395
7.3.1 Maximum Likelihood Decoding, 395
x Contents
7.3.2 Channel Models: Hard versus Soft Decisions, 396
7.3.3 The Viterbi Convolutional Decoding Algorithm, 401
7.3.4 An Example of Viterbi Convolutional Decoding, 401
7.3.5 Decoder Implementation, 405
7.3.6 Path Memory and Synchronization, 408
7.4 Properties of Convolutional Codes, 408
7.4.1 Distance Properties of Convolutional Codes, 408
7.4.2 Systematic and Nonsystematic Convolutional Codes, 413
7.4.3 Catastrophic Error Propagation in Convolutional Codes, 414
7.4.4 Performance Bounds for Convolutional Codes, 415
7.4.5 Coding Gain, 416
7.4.6 Best Known Convolutional Codes, 418
7.4.7 Convolutional Code Rate Trade-Off, 420
7.4.8 Soft-Decision Viterbi Decoding, 420
7.5 Other Convolutional Decoding Algorithms, 422
7.5.1 Sequential Decoding, 422
7.5.2 Comparisons and Limitations of Viterbi and Sequential Decoding, 425
7.5.3 Feedback Decoding, 427
7.6 Conclusion, 429
8 CHANNEL CODING: PART 3 436
8.1 Reed-Solomon Codes, 437
8.1.1 Reed-Solomon Error Probability, 438
8.1.2 Why R-S Codes Perform Well Against Burst Noise, 441
8.1.3 R-S Performance as a Function of Size,
Redundancy, and Code Rate, 441
8.1.4 Finite Fields, 445
8.1.5 Reed-Solomon Encoding, 450
8.1.6 Reed-Solomon Decoding, 454
8.2 Interleaving and Concatenated Codes, 461
8.2.1 Block Interleaving, 463
8.2.2 Convolutional Interleaving, 466
8.2.3 Concatenated Codes, 468
8.3 Coding and Interleaving Applied to the Compact Disc
Digital Audio System, 469
8.3.1 CIRC Encoding, 470
8.3.2 CIRC Decoding, 472
8.3.3 Interpolation and Muting, 474
8.4 Turbo Codes, 475
8.4.1 Turbo Code Concepts, 477
8.4.2 Log-Likelihood Algebra, 481
8.4.3 Product Code Example, 482
8.4.4 Encoding with Recursive Systematic Codes, 488
8.4.5 A Feedback Decoder, 493
Contents xi
8.4.6 The MAP Decoding Algorithm, 498
8.4.7 MAP Decoding Example, 504
8.5 Conclusion, 509
Appendix 8A The Sum of Log-Likelihood Ratios, 510
9 MODULATION AND CODING TRADE-OFFS 520
9.1 Goals of the Communications System Designer, 521
9.2 Error Probability Plane, 522
9.3 Nyquist Minimum Bandwidth, 524
9.4 Shannon-Hartley Capacity Theorem, 525
9.4.1 Shannon Limit, 528
9.4.2 Entropy, 529
9.4.3 Equivocation and Effective Transmission Rate, 532
9.5 Bandwidth Efficiency Plane, 534
9.5.7 Bandwidth Efficiency ofMPSK and MFSK Modulation, 535
9.5.2 Analogies Between Bandwidth-Efficiency
and Error Probability Planes, 536
9.6 Modulation and Coding Trade-Offs, 537
9.7 Defining, Designing, and Evaluating Digital
Communication Systems, 538
9.7.7 M-ary Signaling, 539
9.7.2 Bandwidth-Limited Systems, 540
9.7.3 Power-Limited Systems, 541
9.7.4 Requirements for MPSK and MFSK Signaling, 542
9.7.5 Bandwidth-Limited Uncoded System Example, 543
9.7.6 Power-Limited Uncoded System Example, 545
9.7.7 Bandwidth-Limited and Power-Limited
Coded System Example, 547
9.8 Bandwidth-Efficient Modulation, 555
9.5.7 QPSK and Offset QPSK Signaling, 555
9.8.2 Minimum Shift Keying, 559
9.8.3 Quadrature Amplitude Modulation, 563
9.9 Modulation and Coding for Bandlimited Channels, 566
9.9.7 Commercial Telephone Modems, 567
9.9.2 Signal Constellation Boundaries, 568
9.9.3 Higher Dimensional Signal Constellations, 569
9.9.4 Higher-Density Lattice Structures, 572
9.9.5 Combined Gain: N-Sphere Mapping and Dense Lattice, 573
9.10 Trellis-Coded Modulation, 573
9.70.7 The Idea Behind Trellis-Coded Modulation (TCM), 574
9.10.2 TCM Encoding, 576
9.10.3 TCM Decoding, 580
9.10.4 Other Trellis Codes, 583
xii Contents
9.10.5 Trellis-Coded Modulation Example, 585
9.10.6 Multi-Dimensional Trellis-Coded Modulation, 589
9.11 Conclusion, 590
10 SYNCHRONIZATION 598
10.1 Introduction, 599
10.1.1 Synchronization Defined, 599
10.1.2 Costs versus Benefits, 601
10.1.3 Approach and Assumptions, 602
10.2 Receiver Synchronization, 603
10.2.1 Frequency and Phase Synchronization, 603
10.2.2 Symbol Synchronization‚ÄîDiscrete Symbol Modulations, 625
10.2.3 Synchronization with Continuous-Phase Modulations (CPM), 631
10.2.4 Frame Synchronization, 639
10.3 Network Synchronization, 643
10.3.1 Open-Loop Transmitter Synchronization, 644
10.3.2 Closed-Loop Transmitter Synchronization, 647
10.4 Conclusion, 649
11 MULTIPLEXING AND MULTIPLE ACCESS 656
11.1 Allocation of the Communications Resource, 657
11.1.1 Frequency-Division Multiplexing/Multiple Access, 660
11.1.2 Time-Division Multiplexing/Multiple Access, 665
11.1.3 Communications Resource Channelization, 668
11.1.4 Performance Comparison ofFDMA and TDMA, 668
11.1.5 Code-Division Multiple Access, 672
11.1.6 Space-Division and Polarization-Division Multiple Access, 674
11.2 Multiple Access Communications System and Architecture, 676
11.2.1 Multiple Access Information Flow, 677
11.2.2 Demand Assignment Multiple Access, 678
11.3 Access Algorithms, 678
11.3.1 ALOHA, 678
11.3.2 Slotted ALOHA, 682
11.3.3 Reservation-ALOHA, 683
11.3.4 Performance Comparison ofS-ALOHA and R-ALOHA, 684
11.3.5 Polling Techniques, 686
11.4 Multiple Access Techniques Employed with INTELSAT, 689
11.4.1 Preassigned FDM/FM/FDMA or MCPC Operation, 690
11.4.2 MCPC Modes of Accessing an INTELSA T Satellite, 690
11.4.3 SPADE Operation, 693
11.4.4 TDMA in INTELSAT, 698
11.4.5 Satellite-Switched TDMA in INTELSAT, 704
Contents xiii
11.5 Multiple Access Techniques for Local Area Networks, 708
11.5.1 Carrier-Sense Multiple Access Networks, 708
11.5.2 Token-Ring Networks, 710
11.5.3 Performance Comparison of CSMA/CD and Token-Ring Networks, 711
11.6 Conclusion, 713
12 SPREAD-SPECTRUM TECHNIQUES 718
12.1 Spread-Spectrum Overview, 719
12.1.1 The Beneficial Attributes of Spread-Spectrum Systems, 720
12.1.2 A Catalog of Spreading Techniques, 724
12.1.3 Model for Direct-Sequence Spread-Spectrum
Interference Rejection, 726
12.1.4 Historical Background, 727
12.2 Pseudonoise Sequences, 728
72.2.1 Randomness Properties, 729
12.2.2 Shift Register Sequences, 729
12.2.3 PN Autocorrelation Function, 730
12.3 Direct-Sequence Spread-Spectrum Systems, 732
12.3.1 Example of Direct Sequencing, 734
12.3.2 Processing Gain and Performance, 735
12.4 Frequency Hopping Systems, 738
12.4.1 Frequency Hopping Example, 740
12.4.2 Robustness, 741
12.4.3 Frequency Hopping with Diversity, 741
12.4.4 Fast Hopping versus Slow Hopping, 742
12.4.5 FFH/MFSK Demodulator, 744
12.4.6 Processing Gain, 745
12.5 Synchronization, 745
12.5.1 Acquisition, 746
12.5.2 Tracking, 751
12.6 Jamming Considerations, 754
12.6.1 The Jamming Game, 754
12.6.2 Broadband Noise Jamming, 759
12.6.3 ^Partial-Band Noise Jamming, 760
12.6.4 . Multiple-Tone Jamming, 763
12.6.5 Pulse Jamming, 763
12.6.6 Repeat-Back Jamming, 765
12.6.7 BLADES System, 768
12.7 Commercial Applications, 769
12.7.1 Code-Division Multiple Access, 769
12.7.2 Multipath Channels, 771
12.7.3 The FCC Part 15 Rules for Spread-Spectrum Systems, 772
12.7.4 Direct Sequence versus Frequency Hopping, 773
12.8 Cellular Systems, 775
12.8.1 Direct Sequence CDMA, 776
xiv Contents
12.8.2 Analog FM versus TDMA versus CDMA, 779
12.8.3 Interference-Limited versus Dimension-Limited Systems, 781
12.8.4 IS-95 CDMA Digital Cellular System, 782
12.9 Conclusion, 795
13 SOURCE CODING 803
13.1 Sources, 804
13.1.1 Discrete Sources, 804
13.1.2 Waveform Sources, 809
13.2 Amplitude Quantizing, 811
13.2.1 Quantizing Noise, 813
13.2.2 Uniform Quantizing, 816
13.2.3 Saturation, 820
13.2.4 Dithering, 823
13.2.5 Nonuniform Quantizing, 826
13.3 Differential Pulse-Code Modulation, 835
13.3.1 One-Tap Prediction, 838
13.3.2 N-Tap Prediction, 839
13.3.3 Delta Modulation, 841
13.3.4 Sigma-Delta Modulation, 842
13.3.5 Sigma-Delta A-to-D Converter (ADC), 847
13.3.6 Sigma-Delta D-to-A Converter (DAC), 848
13.4 Adaptive Prediction, 850
13.4.1 Forward Prediction, 851
13.4.2 Synthesis/Analysis Coding, 852
13.5 Block Coding, 853
13.5.1 Vector Quantizing, 854
13.6 Transform Coding, 856
13.6.1 Quantization for Transform Coding, 857
13.6.2 Subband Coding, 857
13.7 Source Coding for Digital Data, 859
13.7.1 Properties of Codes, 860
13.7.2 Huffman Codes, 862
13.7.3 Run-Length Codes, 866
13.8 Examples of Source Coding, 870
13.8.1 Audio Compression, 870
13.8.2 Image Compression, 875
13.9 Conclusion, 884
14 ENCRYPTION AND DECRYPTION 890
14.1 Models, Goals, and Early Cipher Systems, 891
14.1.1 A Model of the Encryption and Decryption Process, 893
14.1.2 System Goals, 893
14.1.3 Classic Threats, 893
Contents xv
14.1.4 Classic Ciphers, 894
14.2 The Secrecy of a Cipher System, 897
14.2.1 Perfect Secrecy, 897
14.2.2 Entropy and Equivocation, 900
14.2.3 Rate of a Language and Redundancy, 902
14.2.4 Unicity Distance and Ideal Secrecy, 902
14.3 Practical Security, 905
14.3.1 Confusion and Diffusion, 905
14.3.2 Substitution, 905
14.3.3 Permutation, 907
14.3.4 Product Cipher Systems, 908
14.3.5 The Data Encryption Standard, 909
14.4 Stream Encryption, 915
14.4.1 Example of Key Generation Using a Linear
Feedback Shift Register, 916
14.4.2 Vulnerabilities of Linear Feedback Shift Registers, 917
14.4.3 Synchronous and Self-Synchronous Stream
Encryption Systems, 919
14.5 Public Key Cryptosystems, 920
14.5.1 Signature Authentication using a Public Key Cryptosystem, 921
14.5.2 A Trapdoor One-Way Function, 922
14.5.3 The Rivest-Shamir-Adelman Scheme, 923
14.5.4 The Knapsack Problem, 925
14.5.5 A Public Key Cryptosystem based on a Trapdoor Knapsack, 927
14.6 Pretty Good Privacy, 929
14.6.1 Triple-DBS, CAST, and IDEA, 931
14.6.2 Diffie-Hellman (Elgamal Variation) and RSA, 935
14.6.3 PGP Message Encryption, 936
14.6.4 PGP Authentication and Signature, 937
14.7 Conclusion, 940
15 FADING CHANNELS 944
15.1 The Challenge of Communicating over Fading Channels, 945
15.2 Characterizing Mobile-Radio Propagation, 947
75.2.7 Large-Scale Fading, 951
15.2.2 Small-Scale Fading, 953
15.3 Signal Time-Spreading, 958
75.3.7 Signal Time-Spreading Viewed in the Time-Delay Domain, 958
15.3.2 Signal Time-Spreading Viewed in the Frequency Domain, 960
15.3.3 Examples of Flat Fading and Frequency-Selective Fading, 965
15.4 Time Variance of the Channel Caused by Motion, 966
75.4.7 Time Variance Viewed in the Time Domain, 966
15.4.2 Time Variance Viewed in the Doppler-Shift Domain, 969
15.4.3 Performance over a Slow-and Flat-Fading Rayleigh Channel, 975
xvi Contents
15.5 Mitigating the Degradation Effects of Fading, 978
75.5.7 Mitigation to Combat Frequency-Selective Distortion, 980
75.5.2 Mitigation to Combat Fast-Fading Distortion, 982
15.5.3 Mitigation to Combat Loss in SNR, 983
15.5.4 Diversity Techniques, 984
15.5.5 Modulation Types for Fading Channels, 987
15.5.6 The Role of an Interleaver, 988
15.6 Summary of the Key Parameters Characterizing Fading Channels, 992
75.6.7 Fast Fading Distortion: Case 1, 992
15.6.2 Frequency-Selective Fading Distortion: Case 2, 993
15.6.3 Fast-Fading and Frequency-Selective Fading Distortion: Case 3, 993
15.7 Applications: Mitigating the Effects of Frequency-Selective Fading, 996
75.7.7 The Viterbi Equalizer as Applied to GSM, 996
15.7.2 The Rake Receiver as Applied to Direct-Sequence
Spread-Spectrum (DS/SS) Systems, 999
15.8 Conclusion, 1001
A A REVIEW OF FOURIER TECHNIQUES 1012
A.I Signals, Spectra, and Linear Systems, 1012
A.2 Fourier Techniques for Linear System Analysis, 1012
A2.7 Fourier Series Transform, 1014
A.2.2 Spectrum of a Pulse Train, 1018
A.2.3 Fourier Integral Transform, 1020
A.3 Fourier Transform Properties, 1021
A.3.1 Time Shifting Property, 1022
A.3.2 Frequency Shifting Property, 1022
A.4 Useful Functions, 1023
A.4.1 Unit Impulse Function, 1023
A.4.2 Spectrum of a Sinusoid, 1023
A.5 Convolution, 1025
A5.7 Graphical Example of Convolution, 1027
A.5.2 Time Convolution Property, 1028
A.5.3 Frequency Convolution Property, 1030
A.5.4 Convolution of a Function with a Unit Impulse, 1030
A.5.5 Demodulation Application of Convolution, 1031
A.6 Tables of Fourier Transforms and Operations, 1033
B FUNDAMENTALS OF STATISTICAL DECISION THEORY 1035
B.I Bayes' Theorem, 1035
5.7.7 Discrete Form of Bayes'Theorem, 1036
B.1.2 Mixed Form of Bayes'Theorem, 1038
B.2 Decision Theory, 1040
5.2.7 Components of the Decision Theory Problem, 1040
Contents xvii
B.2.2 The Likelihood Ratio Test and the Maximum
A Posteriori Criterion, 1041
B.2.3 The Maximum Likelihood Criterion, 1042
B.3 Signal Detection Example, 1042
B.3.1 The Maximum Likelihood Binary Decision, 1042
B.3.2 Probability of Bit Error, 1044
C RESPONSE OF A CORRELATOR TO WHITE NOISE 1047
D OFTEN-USED IDENTITIES 1049
E s-DOMAIN, z-DOMAIN AND DIGITAL FILTERING 1051
E.I The Laplace Transform, 1051
¬£.7.7 Standard Laplace Transforms, 1052
E.1.2 Laplace Transform Properties, 1053
E.1.3 Using the Laplace Transform, 1054
E.1.4 Transfer Function, 1055
E.1.5 RC Circuit Low Pass Filtering, 1056
E.1.6 Poles and Zeroes, 1056
E.1.7 Linear System Stability, 1057
E.2 The z-Transform, 1058
E.2.1 Calculating the z-Transform, 1058
E.2.2 The Inverse z-Transform, 1059
E.3 Digital Filtering, 1060
E.3.1 Digital Filter Transfer Function, 1061
E.3.2 Single Pole Filter Stability, 1062
E.3.3 General Digital Filter Stability, 1063
E.3.4 z-Plane Pole-Zero Diagram and the Unit Circle, 1063
¬£.3.5 Discrete Fourier Transform of Digital Filter Impulse Response, 1064
E.4 Finite Impulse Response Filter Design, 1065
E.4.1 FIR Filter Design, 1065
E.4.2 The FIR Differentiator, 1067
E.5 Infinite Impulse Response Filter Design, 1069
E.5.1 Backward Difference Operator, 1069
¬£.5.2 HR Filter Design using the Bilinear Transform, 1070
E.5.3 The IIR Integrator, 1071
F LIST OF SYMBOLS 1072
INDEX 1074
xviii Contents
Preface
This second edition of Digital Communications: Fundamentals and Applications
represents an update of the original publication. The key features that have been
updated are:
‚Ä¢ The error-correction coding chapters have been expanded, particularly in
the areas of Reed-Solomon codes, turbo codes, and trellis-coded modula-
tion.
‚Ä¢ A new chapter on fading channels and how to mitigate the degrading ef-
fects of fading has been introduced.
‚Ä¢ Explanations and descriptions of essential digital communication concepts
have been amplified.
‚Ä¢ End-of-chapter problem sets have been expanded. Also, end-of-chapter
question sets (and where to find the answers), as well as end-of-chapter
CD exercises have been added.
‚Ä¢ A compact disc (CD) containing an educational version of the design soft-
ware System View by ELANIX¬Æ accompanies the textbook. The CD con-
tains a workbook with over 200 exercises, as well as a concise tutorial on
digital signal processing (DSP). CD exercises in the workbook reinforce
material in the textbook; concepts can be explored by viewing waveforms
with a windows-based PC and by changing parameters to see the effects on
the overall system. Some of the exercises provide basic training in using
System View; others provide additional training in DSP techniques.
xix
The teaching of a one-semester university course proceeds in a very different
manner compared with that of a short-course in the same subject. At the university,
one has the luxury of time‚Äîtime to develop the needed skills and mathematical tools,
time to practice the ideas with homework exercises. In a short-course, the treatment is
almost backwards compared with the university. Because of the time factor, a short-
course teacher must "jump in" early with essential concepts and applications. One of
the vehicles that I found useful in structuring a short course was to start by handing out
a check list. This was not merely an outline of the curriculum. It represented a collec-
tion of concepts and nomenclature that are not clearly documented, and are often mis-
understood. The short-course students were thus initiated into the course by being
challenged. I promised them that once they felt comfortable describing each issue, or
answering each question on the list, they would be well on their way toward becoming
knowledgeable in the field of digital communications. I have learned that this list of es-
sential concepts is just as valuable for teaching full-semester courses as it is for short
courses. Here then is my "check list" for digital communications.
1. What mathematical dilemma is the cause for there being several definitions
of bandwidth? (See Section 1.7.2.)
2. Why is the ratio of bit energy-to-noise power spectral density, Eb/N0, a nat-
ural figure-to-merit for digital communication systems? (See Section 3.1.5.)
3. When representing timed events, what dilemma can easily result in confusing
the most-significant bit (MSB) and the least-significant bit (LSB)? (See Sec-
tion 3.2.3.1.)
4. The error performance of digital signaling suffers primarily from two degra-
dation types, a) loss in signal-to-noise ratio, b) distortion resulting in an irre-
ducible bit-error probability. How do they differ? (See Section 3.3.2.)
5. Often times, providing more Eb/N0 will not mitigate the degradation due to
intersymbol interference (ISI). Explain why. (See Section 3.3.2.)
6. At what location in the system is Eb/N0 defined? (See Section 4.3.2.)
7. Digital modulation schemes fall into one of two classes with opposite behav-
ior characteristics, a) orthogonal signaling, b) phase/amplitude signaling. De-
scribe the behavior of each class. (See Sections 4.8.2 and 9.7.)
8. Why do binary phase shift keying (BPSK) and quaternary phase shift keying
(QPSK) manifest the same bit-error-probability relationship? Does the same
hold true for M-ary pulse amplitude modulation (M-PAM) and M2-ary quad-
rature amplitude modulation (M2-QAM) bit-error probability? (See Sections
4.8.4 and 9.8.3.1.)
9. In orthogonal signaling, why does error-performance improve with higher di-
mensional signaling? (See Section 4.8.5.)
10. Why is free-space loss a function of wavelength? (See Section 5.3.3.)
11. What is the relationship between received signal to noise (S/N) ratio and car-
rier to noise (C/N) ratio? (See Section 5.4.)
12. Describe four types of trade-offs that can be accomplished by using an error-
correcting code. (See Section 6.3.4.)
xx Preface
13. Why do traditional error-correcting codes yield error-performance degrada-
tion at low values of Eb/N0? (See Section 6.3.4.)
14. Of what use is the standard array in understanding a block code, and in eval-
uating its capability? (See Section 6.6.5.)
15. Why is the Shannon limit of -1.6 dB not a useful goal in the design of real sys-
tems? (See Section 8.4.5.2.)
16. What are the consequences of the fact that the Viterbi decoding algorithm
does not yield a posteriori probabilities? What is a more descriptive name for
the Viterbi algorithm? (See Section 8.4.6.)
17. Why do binary and 4-ary orthogonal frequency shift keying (FSK) manifest
the same bandwidth-efficiency relationship? (See Section 9.5.1.)
18. Describe the subtle energy and rate transformations of received signals: from
data-bits to channel-bits to symbols to chips. (See Section 9.7.7.)
19. Define the following terms: Baud, State, Communications Resource, Chip,
Robust Signal. (See Sections 1.1.3 and 7.2.2, Chapter 11, and Sections 12.3.2
and 12.4.2.)
20. In a fading channel, why is signal dispersion independent of fading rapidity?
(See Section 15.1.1.1.)
I hope you find it useful to be challenged in this way. Now, let us describe the
purpose of the book in a more methodical way. This second edition is intended
to provide a comprehensive coverage of digital communication systems for se-
nior level undergraduates, first year graduate students, and practicing engineers.
Though the emphasis is on digital communications, necessary analog fundamentals
are included since analog waveforms are used for the radio transmission of digital
signals. The key feature of a digital communication system is that it deals with a fi-
nite set of discrete messages, in contrast to an analog communication system in
which messages are defined on a continuum. The objective at the receiver of the
digital system is not to reproduce a waveform with precision; it is instead to deter-
mine from a noise-perturbed signal, which of the finite set of waveforms had been
sent by the transmitter. In fulfillment of this objective, there has arisen an impres-
sive assortment of signal processing techniques.
The book develops these techniques in the context of a unified structure. The
structure, in block diagram form, appears at the beginning of each chapter; blocks in
the diagram are emphasized, when appropriate, to correspond to the subject of that
chapter. Major purposes of the book are to add organization and structure to a field
that has grown and continues to grow rapidly, and to insure awareness of the "big
picture" even while delving into the details. Signals and key processing steps are
traced from the information source through the transmitter, channel, receiver, and
ultimately to the information sink. Signal transformations are organized according to
nine functional classes: Formatting and source coding, Baseband signaling, Band-
pass signaling, Equalization, Channel coding, Muliplexing and multiple access,
Spreading, Encryption, and Synchronization. Throughout the book, emphasis is
placed on system goals and the need to trade off basic system parameters such as
signal-to-noise ratio, probability of error, and bandwidth expenditure.
Preface xxi
ORGANIZATION OF THE BOOK
Chapter 1 introduces the overall digital communication system and the basic signal
transformations that are highlighted in subsequent chapters. Some basic ideas of
random variables and the additive white Gaussian noise (AWGN) model are re-
viewed. Also, the relationship between power spectral density and autocorrelation,
and the basics of signal transmission through linear systems are established. Chap-
ter 2 covers the signal processing step, known as formatting, in order to render an
information signal compatible with a digital system. Chapter 3 emphasizes base-
band signaling, the detection of signals in Gaussian noise, and receiver optimiza-
tion. Chapter 4 deals with bandpass signaling and its associated modulation and
demodulation/detection techniques. Chapter 5 deals with link analysis, an im-
portant subject for providing overall system insight; it considers some subtleties
that are often missed. Chapters 6, 7, and 8 deal with channel coding‚Äîa cost-
effective way of providing a variety of system performance trade-offs. Chapter 6
emphasizes linear block codes, Chapter 7 deals with convolutional codes, and Chap-
ter 8 deals with Reed-Solomon codes and concatenated codes such as turbo codes.
Chapter 9 considers various modulation/coding system trade-offs dealing with
probability of bit-error performance, bandwidth efficiency, and signal-to-noise
ratio. It also treats the important area of coded modulation, particularly trellis-coded
modulation. Chapter 10 deals with synchronization for digital systems. It covers
phase-locked loop implementation for achieving carrier synchronization. It covers
bit synchronization, frame synchronization, and network synchronization, and it
introduces some ways of performing synchronization using digital methods.
Chapter 11 treats multiplexing and multiple access. It explores techniques that
are available for utilizing the communication resource efficiently. Chapter 12 intro-
duces spread spectrum techniques and their application in such areas as multiple
access, ranging, and interference rejection. This technology is important for both
military and commercial applications. Chapter 13 deals with source coding which is
a special class of data formatting. Both formatting and source coding involve digiti-
zation of data; the main difference between them is that source coding additionally
involves data redundancy reduction. Rather than considering source coding imme-
diately after formatting, it is purposely treated in a later chapter so as not to inter-
rupt the presentation flow of the basic processing steps. Chapter 14 covers basic
encryption/decryption ideas. It includes some classical concepts, as well as a class of
systems called public key cryptosystems, and the widely used E-mail encryption
software known as Pretty Good Privacy (PGP). Chapter 15 deals with fading chan-
nels. Here, we deal with applications, such as mobile radios, where characteriza-
tion of the channel is much more involved than that of a nonfading one. The design
of a communication system that will withstand the degradation effects of fading can
be much more challenging than the design of its nonfading counterpart. In this
chapter, we describe a variety of techniques that can mitigate the effects of fading,
and we show some successful designs that have been implemented.
It is assumed that the reader is familiar with Fourier methods and convolu-
tion. Appendix A reviews these techniques, emphasizing those properties that are
xxii Preface
particularly useful in the study of communication theory. It also assumed that the
reader has a knowledge of basic probability and has some familiarity with random
variables. Appendix B builds on these disciplines for a short treatment on statistical
decision theory with emphasis on hypothesis testing‚Äîso important in the under-
standing of detection theory. A new section, Appendix E, has been added to serve
as a short tutorial on s-domain, z-domain, and digital filtering. A concise DSP tu-
torial also appears on the CD that accompanies the book.
If the book is used for a two-term course, a simple partitioning is suggested;
the first seven chapters can be taught in the first term, and the last eight chapters
in the second term. If the book is used for a one-term introductory course, it is sug-
gested that the course material be selected from the following chapters: 1, 2, 3, 4,
5, 6, 7, 9, 10, 12.
ACKNOWLEDGMENTS
It is difficult to write a technical book without contributions from others. I have re-
ceived an abundance of such assistance, for which I am deeply grateful. For their
generous help, I want to thank Dr. Andrew Viterbi, Dr. Chuck Wheatley, Dr. Ed
Tiedeman, Dr. Joe Odenwalder, and Serge Willinegger of Qualcomm. I also want
to thank Dr. Dariush Divsalar of Jet Propulsion Laboratory (JPL), Dr. Bob
Bogusch of Mission Research, Dr. Tom Stanley of the Federal Communications
Commission, Professor Larry Milstein of the University of California, San Diego,
Professor Ray Pickholtz of George Washington University, Professor Daniel
Costello of Notre Dame University, Professor Ted Rappaport of Virginia Poly-
technic Institute, Phil Kossin of Lincom, Les Brown of Motorola, as well as
Dr. Bob Price and Frank Amoroso.
I also want to acknowledge those people who played a big part in helping me
with the first edition of the book. They are: Dr. Maurice King, Don Martin and
Ned Feldman of The Aerospace Corporation, Dr. Marv Simon of JPL, Dr. Bill
Lindsey of Lincom, Professor Wayne Stark of the University of Michigan, as well
as Dr. Jim Omura, Dr. Adam Lender, and Dr. Todd Citron.
I want to thank Dr. Maurice King for contributing Chapter 10 on Synchro-
nization, and Professor Fred Harris of San Diego State University for contributing
Chapter 13 on Source Coding. Also, thanks to Michelle Landry for writing the sec-
tions on Pretty Good Privacy in Chapter 14, and to Andrew Guidi for contributing
end-of-chapter problems in Chapter 15.
I am particularly indebted to my friends and colleagues Fred Harris, Profes-
sor Dan Bukofzer of California State University at Fresno, and Dr. Maury Schiff
of Elanix, who put up with my incessant argumentative discussions anytime that I
called on them. I also want to thank my very best teachers‚Äîthey are my students at
the University of California, Los Angeles, as well as those students all over the
world who attended my short courses. Their questions motivated me and provoked
me to write this second edition. I hope that I have answered all their questions with
clarity.
Preface xxiii
I offer special thanks for technical clarifications that my son, Dean Sklar,
suggested; he took on the difficult role of being his father's chief critic and "devil's
advocate." I am particularly indebted to Professor Bob Stewart of the University of
Strathclyde, Glasgow, who contributed countless hours of work in writing and
preparing the CD and in authoring Appendix E. I thank Rose Kernan, my editor,
for watching over me and this project, and I thank Bernard Goodwin, Publisher at
Prentice Hall, for indulging me and believing in me. His recommendations were
invaluable. Finally, I am extremely grateful to my wife, Gwen, for her encourage-
ment, devotion, and valuable advice. She protected me from the "slings and ar-
rows" of everyday life, making it possible for me to complete this second edition.
BERNARD SKLAR
Tarzana, California
xxiv Preface




































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Discrete Memoryless Channels
‚Ä¢The channel ismemoryless whenùëÜùëúùë¢ùë°(ùëõ)depends only
uponùëÜùëñùëõ(ùëõ)and notuponùëÜùëñùëõ(ùëõ‚àí1)orany other input
sample values .

Transition Matrix
‚Ä¢The memoryless channel can becharacterized bya
Transition Matrix composed composed ofconditional
probabilities .
ùëá=ùëÉ00ùëÉ01
ùëÉ10ùëÉ11
‚Ä¢With no noise nor distortion
ùëá=10
01
‚Ä¢summation of probabilities in any column is unity. 
Transition Diagram
‚Ä¢Analternative way ofdisplaying transition probabilities is
byuseoftheTransition Diagram .
‚Ä¢Forthebinary channel
‚Ä¢Thesummation ofprobabilities leaving anynode isunity .

Binary Symmetric Channel (BSC)
‚Ä¢Achannel inwhich the two conditional error
probabilities areequal .

Tandem Connections of BSCs
‚Ä¢Suppose intransmitting adigital signal over along
distance ,thesignal path includes anumber ofrepeaters .
‚Ä¢Further, suppose thatthepath between each repeater and
thefollowing repeater canbeviewed asBSC .
‚Ä¢The overall channel can beviewed asatandem
connections ofBSCs .

Source Coding
Delta Modulation Techniques
Delta Modulation
‚Ä¢Delta modulation isasimple technique forreducing thedynamic range ofthe
numbers tobecoded .
‚Ä¢Instead ofsending each sample value, wesend thedifference between a
sample andtheprevious sample .
‚Ä¢Delta modulation quantizes thisdifference using only onebitofquantization .
Thus, a‚Äú1‚Äùissent ifthedifference ispositive, anda‚Äú0‚Äùissent ifthedifference
isnegative .
‚Ä¢Weshall refer tothese two possibilities aseither +ùûìor-ùûì.Atevery sample
point, thequantized waveform canincrease ordecrease byùûì.
Implementation of the quantizer using a comparator and a staircase generator
The resulting bit train               1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 

‚Ä¢Since thequantized waveform can only either increase ordecrease byùûìateach
sample point, weshall attempt tofitastaircase approximation totheanalog
waveform .
‚Ä¢Ifthestaircase isbelow theanalog sample value, thedecision istoincrement
positively (an upstep) .Ifthestaircase isabove, weincrement negatively
(down step) .
‚Ä¢Thekeytoeffective useofdelta modulation istheintelligent choice ofthetwo
parameters, step sizeandsampling rate.
‚Ä¢These parameters must bechosen such that thestaircase signal isaclose
approximation totheanalog waveform .
‚Ä¢Increasing thesampling frequency means that thedelta -modulated waveform
requires alarger bandwidth .Increasing the step size increases the
quantization error .
Too small step size                         Too large step size

‚Ä¢Ifthesteps aretoosmall, wecan experience aslope overload condition, where
thestaircase cannot track rapid changes intheanalog signal .
‚Ä¢If,ontheother hand, thesteps aretoolarge, considerable overshoot will occur
during periods when the signal isnot changing rapidly .We have significant
quantization noise, known asgranular noise .
‚Ä¢InPCM, asingle biterror will cause anerror inreconstructing the associated
sample value .The error willaffect only that single reconstructed sample .
‚Ä¢Ifabiterror occurs indelta modulation, theD/A converter inthereceiver willgo
upinstead ofdown (orvise versa) and alllater values willcontain anerror .
Differential PCM
‚Ä¢Is another technique for sending information about changes in the 
samples rather than about the sample values themselves. 
‚Ä¢The modulator sends the difference between a sample and its 
predicted value. 
CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

Principles of Probability
ùëÉùëüùê¥=lim
ùëÅ‚Üí‚àûùëõùê¥
ùëÅ
Where ùëõùê¥is the number of times that the event ùê¥occurs in ùëÅ
performances of the experiment. 
‚Ä¢In the coin tossing experiment, we may expect that out of a 
million tosses of a fair coin, about one half of them will 
show up heads. 
ùëÉùëüùê¥ùëúùëüùêµ=ùëÉùëüùê¥+ùëÉùëüùêµ
Distribution Function and Probability Density 
Function 
‚Ä¢Iftherandom variable isdenoted byùëã,then thedistribution
functionùêπùë•is:
ùêπùë•0=ùëÉùëü{ùëã‚â§ùë•0}
‚Ä¢Weusecapital letters forrandom variables, andlower case
letters forthevalues they take on.
‚Ä¢Thus,ùëã=ùë•0means thattherandom variableùëãisequal to
thenumberùë•0
Distribution Function and Probability Density 
Function 
‚Ä¢Note :
ùêπ‚àí‚àû=0 ùêπ+‚àû=1
‚Ä¢Wenow define thederivative ofthedistribution function
as:
ùëÉùë•=ùëëùêπùë•
ùëëùë•
WhereùëÉùë•isknown asthedensity function ofthe
random variable ùëã.
Distribution Function and Probability Density 
Function 
ùêπùë•0= 
‚àí‚àûùë•0
ùëÉùë•ùëëùë•
ùëÉùëü{ùë•1‚â§ùëã‚â§ùë•2}= ‚àí‚àûùë•2ùëÉùë•ùëëùë• ‚àí‚àûùë•1ùëÉùë•ùëëùë•
ùëÉùëü{ùë•1‚â§ùëã‚â§ùë•2}= ùë•1ùë•2ùëÉùë•ùëëùë•
Distribution Function and Probability Density 
Function 
‚Ä¢The probability thatùëãisbetween anytwolimits isfound
byintegrating thedensity function between these two
limits .
‚Ä¢This explains thereason fortheterminology ‚Äúdensity
function‚Äù .
‚Ä¢Thetotal area under thedensity function isunity
 
‚àí‚àû+‚àû
ùëÉùë•ùëëùë•=1
Gaussian Density
‚Ä¢Isthemost common density encountered intherealworld .
ùëÉùë•=1
ùúé2ùúãexp(‚àí(ùë•‚àíùëö)2
2ùúé2)
ùëöandùúéareconstants .
‚Ä¢Theparameter ùëöindicates thecenter position orsymmetry
point ofthedensity .
‚Ä¢Theparameter ùúéindicates thespread ofthedensity .
Gaussian Density

Gaussian Density
‚Ä¢The Gaussian density issufficiently important, such that
this integral hasbeen computed and tabulated under the
name error function .
erfùë•=2
ùúã 
0ùë•
ùëí‚àíùë¢2ùëëùë¢
erfcùë•=1‚àíerfùë•=2
ùúã 
ùë•‚àû
ùëí‚àíùë¢2ùëëùë¢
‚Ä¢
‚Ä¢
Information Theory
Entropy Coding 
Information Theory
‚Ä¢The concept ofinformation content isrelated to
predictability .
‚Ä¢The more likely aparticular message, the less
information isgiven bytransmitting thatmessage .
ùêºùë•=ùëôùëúùëî1
ùëÉùë•
ùêºùë•:information content ofmessageùë•.
ùëÉùë•:probability ofoccurrence ofthemessageùë•.
Information Theory
‚Ä¢Entropy :isdefined astheaverage information per
message .
ùêª= 
ùëñ=1ùëõ
ùëÉùë•ùëñùêºùë•ùëñ
‚Ä¢Information content can beinterpreted asthe
minimum number ofbinary digits required toencode a
message .
Information Theory
‚Ä¢Entropy can be interpreted as the minimum number of 
digits per message required, on average, for encoding. 
‚Ä¢Consider a communication scheme made up of two 
possible messages 
ùëÉùë•2=1‚àíùëÉùë•1
ùêª=ùëÉùë•1ùëôùëúùëî1
ùëÉùë•1+1‚àíùëÉùë•1log(1
1‚àíùëÉùë•1)
Information Theory
‚Ä¢The result is sketched as shown:
‚Ä¢Note thataseither ofthetwomessages becomes more
likely, theentropy decreases .
‚Ä¢When either message hasprobability 1,theentropy
goes tozero.

Information Theory
‚Ä¢This isreasonable, since atthese points, theoutcome
iscertain .
‚Ä¢IfùëÉùë•1=1,weknow that thefirst message will be
sent allthetime.Noinformation istransmitted by
sending themessage .
‚Ä¢Maximum Entropy occurs incase ofequally probable
messages .
Coding
‚Ä¢Given Mpossible messages, wewish toconvert them
intoMpossible code words .
‚Ä¢The code words canbeselected toachieve objectives
such asefficiency, error correction orsecurity .
‚Ä¢We discuss codes toachieve efficiency under the
heading entropy coding .Such codes attempt tosend the
information using theminimum number ofbits.
Coding 
‚Ä¢We illustrate error detection and correction using
block Hamming codes .
‚Ä¢The prefix property :nocode word forms thestarting
sequences (known astheprefix) ofany other code
word .
‚Ä¢The prefix restriction issufficient but necessary
conditions forthecodes tobeuniquely decodable .
Entropy Coding: Huffman and Shannon
‚Ä¢Itisofinterest tofind uniquely decodable codes of
minimum length .
‚Ä¢This isachieved byassigning theshorter codewords to
themost probable messages .
‚Ä¢Different messages are encoded into words of
different lengths .
Entropy Coding: Huffman and Shannon
‚Ä¢When talking about thelength ofacode, wetherefore
must refer totheaverage length ofthecode words .
‚Ä¢This average iscomputed bytaking theprobabilities of
each message intoaccount .
‚Ä¢For binary -coding alphabets, the average code word
length isgrater than orequal totheentropy .
‚Ä¢Fortheaverage length tobeequal totheentropy, the
probability ofevery message must beinverse power of2.
 
 
 
&
 
 
Fourier Series
 
¬© Basem Hesham Lecture 1
 
 
Introduction 
 
 
 
 
 ÿ≥ÿ±ŸäÿπÿßŸÉÿØÿß ŸáŸÜÿπÿ±ŸÅ  ÿ£Ÿä ÿßŸÑÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÑŸÑŸä ŸáŸÜÿØÿ±ÿ≥Ÿáÿß ŸÅŸä ÿßŸÑŸÉŸàÿ±ÿ≥ ÿØÿß ŸàŸáŸÜÿπÿ±ŸÅ ŸÑŸäŸá ÿ®ŸÜÿØÿ±ÿ≥Ÿáÿß ŸàŸÖÿØŸâ  
ÿ£ŸáŸÖŸäÿ™Ÿá ÿπÿ¥ÿßŸÜ ÿØÿß ŸáŸäÿÆŸÑŸäŸÉ ÿ™ŸÇÿØÿ± ÿ™ŸÖÿ¥Ÿä ŸÅŸä ÿßŸÑŸÉŸàÿ±ÿ≥ ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ÿ™ÿ≠ÿ≥ ÿßŸÜŸÉ ÿ™ÿßŸäŸá.  
 
Ÿáÿ™ÿØŸäŸÜÿß ÿØ/ŸáŸÜÿßÿ° ÿ¥ÿßŸÉÿ± ŸàÿßŸÑŸÉŸàÿ±ÿ≥ ŸáŸÜÿØÿ±ÿ≥Ÿá ŸÅŸä   22  ŸÖÿ≠ÿßÿ∂ÿ±ÿ© ŸàŸÉŸÑÿ£ÿ≥ÿ®Ÿàÿπ  ŸáŸÜÿßÿÆÿØ ŸÖÿ≠ÿßÿ∂ÿ±ÿ™ŸäŸÜ ÿ®ÿ≥  
Ÿàÿ±ÿß ÿ®ÿπÿ∂ ŸàÿØŸä ÿ™Ÿàÿ≤Ÿäÿπ ÿßŸÑÿØÿ±ÿ¨ÿßÿ™ ŸÅŸä ÿßŸÑÿßŸÑŸäÿ≠ÿ©  
 
Grading 
50 Coursework 
100 Final Exam 
150 Total 
 
 ÿßÿπŸÖÿßŸÑ ÿßŸÑÿ≥ŸÜÿ© ŸÖŸÜ50  ÿØÿ±ÿ¨ÿ© ŸÖÿ™ÿ∂ŸÖŸÜŸá ÿØÿ±ÿ¨ÿ© ÿßŸÑŸÖŸäÿØÿ™ÿ±ŸÖŸàÿßŸÑÿ≥ŸÉÿßÿ¥ŸÜ ŸàŸáŸäŸÉŸàŸÜ ŸÅŸäŸá ŸÉŸàŸäÿ≤ ŸÅŸä  
ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿßÿ™ ŸàŸÅŸäŸá ÿ®ŸàŸÜÿµ ŸÑŸÑŸÜÿßÿ≥ ÿßŸÑŸÑŸä Ÿáÿ™ÿ¨ÿßŸàÿ® ÿ®ÿ≥ÿ±ÿπÿ© Ÿàÿ™ÿ¥ÿßÿ±ŸÉ, Ÿàÿ™ŸÅÿßÿµŸäŸÑ ÿ™Ÿàÿ≤Ÿäÿπ ÿßÿπŸÖÿßŸÑ ÿßŸÑÿ≥ŸÜÿ©   
ÿßŸÑÿØŸÉÿ™Ÿàÿ± Ÿáÿ™ÿ®ŸÑÿ∫ŸÜÿß ÿ®ŸäŸáÿß ÿßŸÜ ÿ¥ÿßÿ° ŸáŸÑŸÑÿß.  
 
 
ÿßŸÑŸÖÿßÿØÿ© ÿ®ÿ¥ŸÉŸÑ ÿπÿßŸÖ ÿßŸÖÿ™ÿ≠ÿßŸÜŸáÿß ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿßÿ™ ŸàÿßŸÑÿ≥ŸÉÿßÿ¥ŸÜ ŸàŸÑŸà ÿπÿßŸäÿ≤ ÿ™ÿ™ÿπŸÖŸÇ ÿßŸÉÿ™ÿ± ŸÖŸÖŸÉŸÜ ÿ™ÿ¥ŸàŸÅ ÿßŸÑŸÄ  
references  :ÿØŸä 
 
" Digital Communications Fundamentals and Applications , by Bernard Sklar  " 
" Modern digital and analog communication systems , by B.p. lathi  " 
" Digital and Data Communication Systems , by Martin S. Roden " 
" Digital Communication Systems -Wiley , by Simon S. Haykin " 
 
: ŸàŸáÿ™ÿßŸÑŸÇŸäŸáŸÖ ŸÖŸàÿ¨ŸàÿØŸäŸÜ ŸÅŸä ÿßŸÑŸÑŸäŸÜŸÉ ÿØÿß https://bit.ly/3tgsoFw  
 
 
 
 
 
 
 Introductio n
n 
 
Course Outlines  
 
‚ñ™ Block Diagram of digital communication system  
 ŸáŸÜÿ±ÿ≥ŸÖ block diagram ŸÉÿßŸÖŸÑ ŸÑŸÑŸÄ digital system  ŸÖŸÜ ÿ®ÿØÿßŸäÿ© ŸÖÿßÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ™ÿØÿÆŸÑ Ÿàÿ™ÿ™ÿ®ÿπÿ™ ŸÖŸÜ ÿßŸÑŸÄ transmitter  
 ŸàŸäŸÉŸàŸÜ ÿßÿµŸÑŸáÿß analog ÿ£Ÿä ÿßŸÑŸÑŸä ÿ®Ÿäÿ™ŸÖ ÿπŸÑŸäŸáÿß Ÿàÿ®ÿ™ŸÖÿ± ÿ® ŸÄ stages ŸàŸÖÿ±ÿßÿ≠ŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑÿ≠ÿØ ŸÖÿß ÿßŸÑŸÄ receiver Ÿäÿ≥ÿ™ŸÇÿ®ŸÑŸáÿß, 
ŸàŸÉŸÑ ÿπŸÖŸÑŸäŸá ÿ™ŸÖÿ™ ŸÅŸä ÿßŸÑŸÄ transmitter   ÿ™ŸÖ ÿπŸÉÿ≥Ÿáÿß ŸÅŸäÿßŸÑŸÄ receiver . 
 ŸÉŸÑ block ÿ™ŸÇÿ±Ÿäÿ®ÿß ŸáŸà ÿ¥ÿßÿ®ÿ™ÿ± ŸÅŸä ÿßŸÑŸÖŸÜŸáÿ¨ ŸáŸäÿ™ÿ¥ÿ±ÿ≠ ŸÅŸäŸá ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ.    
 
‚ñ™ Theory of Probability  
 ŸÜÿ∏ÿ±ŸäŸÄŸÄÿ© ÿßÿ™ÿ≠ÿ™ŸÖŸÄŸÄÿßÿ™ÿ™ ŸÖŸáŸÖŸÄŸÄÿ© ÿ¨ŸÄŸÄÿØÿß Ÿàÿ®ÿ™ŸÄŸÄŸàŸÅÿ±ÿ£ÿ≥ŸÄŸÄÿ≥ ÿ±Ÿäÿßÿ∂ŸÄŸÄŸäÿ© ŸÇŸàŸäŸÄŸÄÿ© ŸÑŸÅŸáŸÄŸÄŸÖ Ÿàÿ™ÿ≠ŸÑŸäŸÄŸÄŸÑ ÿßŸÑÿ∏ŸÄŸÄŸàÿßŸáÿ± ÿßŸÑÿπÿ¥ŸÄŸÄŸàÿß Ÿäÿ© ŸÅŸÄŸÄŸä ŸÖÿ¨ŸÄŸÄÿßŸÑ 
ÿßÿ™ÿ™ÿµÿßÿ™ÿ™. 
ÿßŸÑŸÄ digital  signal ÿ®ÿπÿØ ŸÖÿß Ÿäÿ™ÿπŸÖŸÄŸÄŸÑ ŸÑŸáŸÄŸÄÿß modulation Ÿàÿ™ÿ™ÿ®ÿπŸÄŸÄÿ™ ŸÅŸÄŸÄŸä ÿßŸÑŸáŸÄŸÄŸàÿß ŸàÿßŸÑŸÄŸÄ ŸÄ receiver  Ÿäÿ≥ŸÄŸÄÿ™ŸÇÿ®ŸÑŸáÿß Ÿàÿßÿ±ÿØ  ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© 
ÿÆŸÄŸÄÿßŸÑŸÑ Ÿäÿ±ŸäŸÇŸáŸÄŸÄÿß ÿ™ŸÉŸÄŸÄŸàŸÜ ÿ™ÿπÿ±ÿ∂ŸÄŸÄÿ™ ŸÑŸÄŸÄ ŸÄ noise ŸàŸàÿßÿ±ÿØ Ÿäÿ≠ÿµŸÄŸÄŸÑ ŸÖÿ¥ŸÄŸÄÿßŸÉŸÑ ŸÅŸÄŸÄŸä ÿßÿ™ŸÜÿ™ÿ¥ŸÄŸÄÿßÿ± multi path propagation 
problems    ŸàÿØÿß ÿ®ÿ®ÿ≥ÿßŸäÿ© Ÿàÿßÿ≠ŸÜÿß ÿ®ŸÜŸÜŸÇŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™  ŸÅŸä ÿßŸÑŸáŸàÿß ŸÖŸÖŸÉŸÜ ÿ™ÿµŸäÿØŸÖ ÿ®ÿßŸä ŸÖÿ®ŸÜŸâ Ÿàÿ™ÿ±ÿ™ÿØ ŸÖŸÜŸÄŸÄŸá ŸÖŸÄŸÄÿ±ŸÜ ÿ™ÿßŸÜŸäŸÄŸÄÿ© ŸÖÿπŸÜŸÄŸÄŸâ 
ŸÉÿØÿß ÿßŸÜ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  Ÿáÿ™ŸàÿµŸÑ ŸÑŸÑŸÄ receiver  ÿ®ŸÄ phase  ÿ∫Ÿäÿ± ÿßŸÑÿ™ÿßŸÜŸä ŸàŸäÿ®ÿπÿß ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ŸáŸÜÿß ÿßŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ÿØŸä ÿßŸÑŸÄ receiver 
  ŸáŸäÿ¨ŸÖÿπŸáÿß ŸàŸÑŸà ÿπŸÜÿØŸä ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ  sinewave ŸÖÿ´ÿßŸÑ Ÿà ÿ≠ÿµŸÑ   delayÿ≥ÿ®ÿ® shift  phase 180   ŸÉÿØÿß ŸÖÿ¨ŸÖŸàÿπŸáŸÖ ÿ®ŸÇŸâ0. 
 
ŸÅŸÜÿ∏ÿ±ÿß ŸÑŸÑŸÄ noise ŸàÿßŸÑŸÄ propagation problems  ÿ™ÿ≤ŸÖ ŸÜÿ≠Ÿä ŸÑŸÑŸÄŸÄ ŸÄ digital system   ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßÿ™ÿ™ ÿßŸÑŸàÿµŸÄŸÄŸàŸÑ ÿßŸÑÿµŸÄŸÄÿ≠Ÿä
Ÿàÿßÿ≠ÿ™ŸÖŸÄŸÄÿßÿ™ÿ™ ÿßŸÑŸàÿµŸÄŸÄŸàŸÑ ÿßŸÑÿÆŸäŸÄŸÄÿ™ ŸÅÿßŸÑŸÖÿ™ÿÆÿµÿµŸÄŸÄŸäŸÜ ÿ®ŸäÿπŸÖŸÑŸÄŸÄŸàÿß planning  ŸàŸäÿØÿ±ÿ≥ŸÄŸÄŸàÿß ÿ∏ŸÄŸÄÿ±ŸàŸÅ ÿßŸÑŸÜÿ∏ŸÄŸÄÿßŸÖ ŸàŸäÿ≠ÿ≥ŸÄŸÄÿ®Ÿàÿß ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ
ŸàÿµŸàŸÑ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿä  ÿ®ŸÉÿ∞ÿß Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ ŸàÿµŸàŸÑ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ®ÿ¥ŸÉŸÑ ÿÆŸäÿ™ ÿ®ŸÉÿ∞ÿß  Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ™ŸÉŸÄŸÄŸàŸÜ  ÿ∂ŸÄŸÄÿπŸäŸÅÿ© 
ŸÅŸä ÿßŸÑŸÖŸÉÿßŸÜ ÿØÿß ÿ®ŸÉÿ∞ÿß  ,ŸÅŸÖŸÅŸáŸàŸÖ ÿßŸÑŸÄ Probability ŸÑŸÑŸÄ digital system ŸÖŸáŸÖ ÿ¨ÿØÿß. 
  
‚ñ™ Source Coding  
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÑŸä ÿ®ŸÜÿ®ÿπÿ™Ÿáÿß ÿπŸÜÿØ ÿßŸÑŸÄ  transmitter    ÿ®ŸäŸÉŸàŸÜ ÿßÿµŸÑŸáÿß analog    ŸÅÿ™ŸàŸÑ ŸÖÿ±ÿ≠ŸÑÿ© Ÿáÿ™ŸÖŸÄŸÄÿ± ÿ®ŸäŸáŸÄŸÄÿß ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿÆŸÄŸÄÿßŸÑŸÑÿßŸÑŸÄŸÄ ŸÄ 
digital system  ŸáŸà ÿ™ÿ≠ŸàŸäŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿØŸä ŸÖŸÜ continuous  ŸÅŸäÿßŸÑŸÄŸÄ ŸÄ time ŸÑŸÑŸÖŸÉŸÄŸÄÿßŸÅŸÑ ŸÑŸäŸáŸÄŸÄÿß ŸÅŸÄŸÄŸä ÿßŸÑŸÄŸÄ ŸÄ binary  ŸàÿßŸÑŸÑŸÄŸÄŸä
ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸá ÿØŸàÿß ÿ±  Analog to digital converter . 
  Ÿàÿßÿ≥ŸÖŸá Source ÿ£ŸÑŸÜŸá ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸâ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ£ŸÑÿµŸÑŸäÿ© source signal ŸàŸäÿ≠ŸàŸÑŸáÿß ÿ£ŸÑÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ. 
 
 
 
 
 
‚ñ™
 
Channel Coding
 
ŸáŸà ÿπŸÖŸÑŸäÿ© ÿ™ŸáÿØŸÅ ÿßŸÑŸâ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßÿ£ŸÑÿÆŸäÿßÿ°
  
ÿßŸÑŸÑŸä ŸÖŸÖŸÉŸÜ ÿ™ÿ≠ÿµŸÑ ÿßÿ´ŸÜÿßÿ° ÿπŸÖŸÑŸäÿ© ÿßÿ™ÿ±ÿ≥ÿßŸÑ
. 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿ®Ÿäÿ™ŸÖ ÿßÿ±ÿ≥ÿßŸÑŸáÿß ŸÅŸÄŸÄŸä Ÿàÿ≥ŸÄŸÄŸä 
ŸÖŸÑŸäÿßŸÜ  
noise
  
  ŸàŸÑŸà ÿ≠ÿµŸÑ
ÿ£Ÿä
  
error
  
 ŸáŸäÿ®ŸÇŸâ ŸÖÿ¥ŸÉŸÑÿ© ŸÉÿ®Ÿäÿ±ŸÜ ÿ™ŸÜ ÿØÿßÿ™ÿß ŸÉÿ™ŸäŸÄŸÄÿ± Ÿáÿ™ÿ∂ŸÄŸÄŸäÿπ ŸÅÿπÿ¥ŸÄŸÄÿßŸÜ ŸÉŸÄŸÄÿØÿß ÿ™ÿ≤ŸÖ ŸäŸÉŸÄŸÄŸàŸÜ ŸÖŸàÿ¨ŸÄŸÄŸàÿØ
ÿπŸÜÿØŸÜÿß ÿ≠ÿßÿ¨ÿ© ÿ®ÿ™ÿπŸÖŸÑ  
Error Detection and Correction
.
 
 
 ÿßŸÑŸÄŸÄ
ŸÄ
 
Channel Encoding
 
 ÿ®Ÿäÿ∂ŸÄŸÄŸäŸÅ
bits
 
 ÿ≤ŸäŸÄŸÄÿßÿØÿ©
ÿ®ŸÜÿ≥ŸÄŸÄŸÖŸäŸáÿß 
R
edundant
 
Bits
 
 ÿßŸà
Check bits
 
 ÿßŸà
parity bits
 
  ÿπÿ¥ÿßŸÜ ŸÜÿπŸÖŸÑ
Error Detection and Correction
 
ÿßŸÑ
ŸÄ
 
R
edundant
 
Bits
 
 ÿ®ÿ™ÿ≤ŸàÿØ ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÄŸÄŸÖŸáÿß
distance
 
 ŸÖŸÄŸÄÿß ÿ®ŸÄŸÄŸäŸÜ ÿßÿ™ŸÉŸÄŸÄŸàÿßÿØ Ÿà
ÿßŸÑŸÄŸÄ 
ŸÄ
 
distance
 
ŸáŸÄŸÄŸà ÿπŸÄŸÄÿØÿØ
 
ÿßŸÑŸÄŸÄ 
ŸÄ
 
bits
 
 ÿßŸÑŸÑŸÄŸÄŸä
ÿ®ŸäÿÆÿ™ŸÑŸÅ ŸÅŸäŸáÿß ŸÉŸàÿØ ÿπŸÜ ŸÉŸàÿØ ÿßÿÆÿ±
 
.
 
 
 
ŸÖÿ´ÿßŸÑ ŸÑŸà ÿπŸÜÿØŸä 
3 bits
 
 Ÿäÿ®ŸÇŸâ ŸÉÿØÿß ÿπŸÜÿØŸä
8
 
 ÿßÿ≠ÿ™ŸÖÿßÿ™ÿ™ ŸàŸáÿ∂ŸäŸÅ
bit
 
 ÿ±ÿßÿ®ÿπŸá ÿßŸÑŸÑŸä ŸáŸä ŸÅŸä
ÿßŸÑÿµŸÄŸÄŸàÿ±ÿ©
 
ÿØŸä 
Y
 
 Ÿäÿ®ŸÇŸÄŸÄŸâ ŸÉŸÄŸÄÿØÿß
ŸÖÿπÿßŸäÿß 
4 bits
 
 ŸäÿπŸÜŸä
16
 
 ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ®ÿ≥ ŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÖŸÜŸáŸÖ
8
 
 ÿ®ÿ≥ Ÿä
ÿ® 
ŸÇŸâ ŸÅŸäŸá ŸÉÿØÿß 
8
 
 ÿ™ÿßŸÜŸäŸäŸÜ ŸÖÿ¥ ŸÖÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖŸäŸÜ
ŸàŸáŸÄŸÄŸÖ ÿØŸàŸÑ ÿßŸÑŸÑŸÄŸÄŸä 
ŸáŸäÿØŸàÿß ŸÅÿ±ÿµÿ© ÿßŸÜ ŸÑŸà ÿ≠ÿµŸÑ ÿÆŸäÿ™ ŸÅŸä  
bit
  
ÿ®ÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÉŸàÿØ ÿ™ÿßŸÜŸä ŸÖÿ¥ Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿßŸÑ
ŸÄ
  
8
  
  ÿßŸÑŸÖÿ™ÿπÿßÿ±ŸÅ ÿπŸÑŸäŸáŸÖ ŸÖÿß ÿ®ŸäŸÜ
ÿß
ŸÑŸÄŸÄŸÄ
 
transmitter
  
 Ÿà
ÿß
ŸÑŸÄ
  
receiver
  
  ŸàŸáŸÜÿß
ÿß
ŸÑŸÄ
  
receiver
  
 ŸäŸÇŸÄŸÄÿØÿ± ŸäŸÉÿ™ÿ¥ŸÄŸÄŸÅ ÿßŸÑÿÆŸäŸÄŸÄÿ™ ÿØÿß, ŸàŸÉŸÄŸÄŸÑ ŸÖŸÄŸÄÿß ÿßŸÑŸÄŸÄ
ŸÄ
 
distance
 
 ÿ™ÿ≤ŸäŸÄŸÄÿØ ŸÉŸÄŸÄŸÑ ŸÖŸÄŸÄÿß
ÿ•ŸÖŸÉÿßŸÜŸäÿ©
  
ÿ™ÿµÿ≠Ÿä   
ÿßÿ£ŸÑÿÆŸäÿßÿ°
  
ÿ™ÿ≤ŸäÿØ.
 
 
 
ŸÑŸà ÿßŸÜÿß ÿ®ÿßÿπÿ™ ŸÉŸàÿØ 
1
01
 
 ŸàŸàÿµŸÑ
100
 
ŸÑŸÑ
ŸÄ
 
receiver
  
ŸÉ
ÿØ
ÿß
 
ŸÖÿ¥ ŸáŸäŸÉÿ™ÿ¥ŸÅ ÿßŸÑÿÆŸäÿ™ 
ÿ£ŸÑŸÜŸá
 
ŸÉŸàÿØ ŸÖŸÄŸÄŸÜ ÿßŸÑŸÑŸÄŸÄŸä ŸÖÿ™ÿπŸÄŸÄÿßÿ±ŸÅ ÿπŸÑŸäŸÄŸÄŸá 
ŸÑŸÉŸÜ ÿßŸÖÿß  
ÿßŸÑŸÖÿ≥ÿßŸÅÿ©
  
ÿ≤ÿßÿ™ ÿßÿ™ŸÉŸàÿßÿØ ÿ≤ÿßÿØÿ™ ŸÅÿ™ÿµÿ®  ŸÅŸä  
8
  
ÿßŸÉŸàÿßÿØ ŸÖÿ¥ ŸáŸäÿ≥ÿ™ÿÆÿØŸáŸÖ Ÿàÿ®ŸÉÿØŸÜ Ÿáÿπÿ±ŸÅ ÿßŸÉÿ™ÿ¥ŸÅ ÿßŸÑÿÆŸäÿ™
  
ÿ≤Ÿä ÿßŸÜŸÜŸÄŸÄÿß 
ŸÜÿ®ÿπÿ™  
1010
 
  ŸàŸäÿ≥ÿ™ŸÇÿ®ŸÑŸáÿß
1011
 
ŸáŸäŸÉÿ™ÿ¥ŸÅ ÿßŸÑÿÆŸäÿ™ ÿ™ŸÜ ÿØÿß ŸÖÿ¥ ŸÖŸÜ ÿ∂ŸÖŸÜ ÿßÿ™ŸÉŸàÿßÿØ ÿßŸÑŸÑŸä ŸÖÿ™ÿπÿßÿ±ŸÅ ÿπŸÑŸäŸáÿß
.
 
ŸÅŸäŸá ÿ¥ŸÄŸÄÿßÿ®ÿ™ÿ± ŸÉÿßŸÖŸÄŸÄŸÑ ŸáŸÜŸÄŸÄÿØÿ±ÿ≥ ŸÅŸäŸÄŸÄŸá Ÿäÿ±ŸäŸÇŸÄŸÄÿ© ÿßŸà ÿßÿ™ŸÜŸÄŸÄŸäŸÜ ŸÑŸÑŸÄŸÄ 
ŸÄ
 
Error Detection and Correction
 
 ŸàŸáŸÜÿπŸÄŸÄÿ±ŸÅ ÿ®ŸÄŸÄÿ™ŸÖ ÿßÿ≤ÿßŸä
ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ
.
  
 
 
Ÿàÿßÿ≥ŸÄŸÄŸÖŸá 
Channel 
Encoding
 
ÿ™ŸÜ
 
ÿØÿß ÿÆŸÄŸÄÿßÿµ ÿ®ÿßŸÉÿ™ÿ¥ŸÄŸÄÿßŸÅ 
ÿßÿ£ŸÑÿÆŸäŸÄŸÄÿßÿ°
 
ÿßŸÑŸÜÿßÿ™ÿ¨ŸÄŸÄÿ© ÿπŸÄŸÄŸÜ ÿßŸÑŸÄŸÄ 
ŸÄ
 
channel
 
 ŸàÿØÿß ÿßŸÑŸàÿ≥ŸÄŸÄŸä ÿßŸÑŸÑŸÄŸÄŸä
ÿ®Ÿäÿ™ÿ®ÿπÿ™ ŸÅŸäŸá 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ŸàŸáŸà ÿØÿß ÿßŸÑŸÑŸä ÿ≥ÿ®ÿ® ÿßŸÜ ŸÅŸäŸá 
ÿ£ÿÆŸäÿßÿ°
 
ÿ™ÿ≠ÿµŸÑ ŸÅŸä ÿßŸÑ
ŸÄ
 
signal
 
 ŸÅÿ®ÿ∂ŸäŸÅ
bits
 
ÿ≤ŸäÿßÿØÿ©
 
ŸÅŸäŸá
.
 
 
Ÿàÿßÿ≠ŸÜÿß ÿ®ŸÜÿØÿ±ÿ≥ ÿßŸÑÿ¥ÿßÿ®ÿ™ÿ± ÿØÿß ŸÅŸäŸá ÿ¥ÿßÿ®ÿ™ÿ± Ÿäÿπÿ™ÿ®ÿ± ÿßŸÇÿ±ÿ® ŸÑŸÑ
ŸÄ
 
Source 
C
oding
 
 ŸàŸÑŸÉŸÜ ŸáŸÜÿØÿ±ÿ≥Ÿá ŸáŸÜÿß ŸàŸáŸà ÿπŸÜ ÿ≠ÿßÿ¨ŸÄŸÄÿ© ÿßÿ≥ŸÄŸÄŸÖŸáÿß
Information Theory
 

 
‚ñ™
 
Information Theory
 
 ÿπÿ¥ÿßŸÜ ŸÜŸÇÿØÿ± ŸÜÿπŸÖŸÑ
Error Detection and Correction
 
 Ÿà
ŸÜÿπŸÖŸÑ 
Source coding
 
 ÿ®ÿßŸÇŸÄŸÄŸÑ ÿπŸÄŸÄÿØÿØ ŸÖŸÖŸÉŸÄŸÄŸÜ ŸÖŸÄŸÄŸÜ ÿßŸÑŸÄŸÄ
ŸÄ
 
bits
 
 ŸÅŸáŸÜÿπŸÖŸÑ ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÖŸáÿß
Lossless Compression
 
 ÿ∂ŸÄŸÄÿ®Ÿä
ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿßÿ™
 
Ÿàÿ™ŸÇŸÑŸäŸÄŸÄŸÑ ÿ≠ÿ¨ŸÖŸáŸÄŸÄÿß ŸÑŸäŸÄŸÄŸá ŸÜŸÄŸÄŸàÿπŸäŸÜ ,ÿßŸÑŸÜŸÄŸÄŸàÿπ 
ÿßÿ£ŸÑŸàŸÑ
 
 
Lossy Compression
 ŸàÿØÿß ÿ®Ÿäÿ®ŸÇŸÄŸÄŸâ ŸÅŸäŸÄŸÄŸá ŸÅŸÇŸÄŸÄÿØ ŸÑŸÑŸÄŸÄ
ŸÄ
 
information
  
  ŸàÿßŸÑŸÜŸàÿπ ÿßŸÑÿ™ÿßŸÜŸä
Compression
  
Lossless
  
ŸàÿØÿß ÿ®Ÿäÿ∂ÿ∫Ÿä ÿßŸÑ
ŸÄ
 
information
 
 ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß Ÿäÿ≠ÿµŸÄŸÄŸÑ ŸÅŸÇŸÄŸÄÿØ
ŸäÿπŸÜŸä Ÿáÿ≠ŸàŸÑ ÿπÿØÿØ ÿßÿ™ÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿ≠ÿßŸäÿØ ÿßŸÑŸÉÿ®Ÿäÿ± ŸÑÿπÿØÿØ ÿßŸÇŸÑ ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ÿßŸÅŸÇÿØ ÿ≠ÿßÿ¨ÿ© ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàÿØÿß ÿπÿ¥ÿßŸÜ ÿßÿπŸÖŸÑŸÄŸÄŸá ÿ™ÿ≤ŸÖ 
ÿ£ŸÉŸàŸÜ
  
ÿØÿßÿ±ÿ≥ 
Information Theory
.
 
 
 
‚ñ™
 
Line Coding
 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿßŸÑ
ŸÄ
 
Analog
 
 ÿ≠ŸàŸÑŸÜÿßŸáÿß ŸÑÿ¥ŸàŸäÿ© ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ ŸàŸÖŸÄŸÄÿ´ÿßŸÑ ŸáŸÜÿπÿ™ÿ®ŸÄŸÄÿ± ÿßŸÜ
binary 1
 
 ŸáŸÄŸÄŸà
5 v
 
 Ÿà
binary 0
 
 ŸáŸÄŸÄŸà
-
5
 
v
 
 Ÿà
ÿØÿß
 
ÿßÿ≠ÿØ ÿßÿ¥ŸÉÿßŸÑ ÿ™ŸÖÿ´ŸäŸÑ
 
ÿßŸÑ
ŸÄ
 
 
Baseband Signal
 
ŸàŸÑŸÉŸÜ ŸÖÿ¥ ÿØÿß ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑŸàÿ≠ŸäŸÄŸÄÿØ ŸàŸÅŸäŸÄŸÄŸá ÿπŸäŸÄŸÄŸàÿ®
 
ÿ≤Ÿä ÿßŸÜ ŸÑŸÄŸÄŸà ÿ®ÿπŸÄŸÄÿ™ ŸÖŸÄŸÄÿ´ÿßŸÑ 
ÿπÿØÿØ ÿßÿµŸÅÿßÿ± ŸÉÿ®Ÿäÿ± Ÿàÿ±ÿß ÿ®ÿπÿ∂ ÿßŸà ÿπÿØÿØ Ÿàÿ≠ÿßŸäÿØ ŸÉÿ®Ÿäÿ± ŸÖÿ¥ ŸáŸäÿ≠ÿµŸÑ  
transition
  
  
ŸÑŸÑ
ŸÄ
  
signal
  
 ŸÑŸÅÿ™ÿ±ÿ© ŸàÿØÿß ŸáŸäÿ≥ÿ®ÿ® ŸÖÿ¥ÿßŸÉŸÑ
ŸÅŸä ÿßŸÑ
ŸÄ
  
timing
  
 Ÿà
ÿßŸÑ 
ŸÄ 
  
synchronization
  
  ÿ™ŸÜ ÿ®ŸäŸÉŸÄŸÄŸàŸÜ ŸÅŸäŸÄŸÄŸá
clock
  
  ŸÅŸÄŸÄŸä
transmitter
  
  Ÿà
clock
  
  ŸÅŸÄŸÄŸä
receiver
  
 ÿßÿ™ÿ™ŸÜŸÄŸÄŸäŸÜ
ÿØŸàŸÑ ÿ™ÿ≤ŸÖ ŸäŸÉŸàŸÜŸÄŸÄŸÄŸàÿß 
synchronized
 
 ŸÖŸÄŸÄŸÄÿπ ÿ®ÿπŸÄŸÄŸÄÿ∂ ŸÅŸÑÿ≠ÿ∏ŸÄŸÄŸÄÿßÿ™ ÿßÿ™ŸÜÿ™ŸÇŸÄŸÄŸÄÿßŸÑ ÿØŸä ŸáŸÄŸÄŸÄŸä ÿßŸÑŸÑŸÄŸÄŸÄŸä ÿ®ÿ™ÿ≥ŸÄŸÄŸÄÿßÿπÿØŸÜÿß ŸÅŸÄŸÄŸÄŸä ÿ™ÿ≠ŸÇŸäŸÄŸÄŸÄŸÇ ÿßŸÑ
synchronization
 
  ÿßŸÑÿπŸäÿ® ÿßŸÑÿ™ÿßŸÜŸä ÿßŸÜ ŸÑŸà ÿ™Ÿä ÿ≥ÿ®ÿ® ŸÖŸÜ ÿßÿ£ŸÑÿ≥ÿ®ÿßÿ® ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™
180
  
ÿØÿ±ÿ¨ÿ© ŸÉŸÑ ÿßÿ™ÿµŸÅÿßÿ±
 
Ÿáÿ™ÿ®ŸÇŸâ Ÿàÿ≠ÿßŸäŸÄŸÄÿØ ŸàŸÉŸÄŸÄŸÑ ÿßŸÑŸàÿ≠ÿßŸäŸÄŸÄÿØ 
Ÿáÿ™ÿ®ŸÇŸâ Ÿàÿ≠ÿßŸäÿØ ŸàÿßŸÑÿØÿßÿ™ÿß ŸÉŸÑŸáÿß ÿ®ŸÇÿ™ ÿ∫ŸÑŸä
 
ÿßÿ≥ŸÖ
  
ÿßŸÑÿ¥ŸÉŸÑ
  
ÿßŸÑŸÑŸä ŸÅÿßÿ™  
non
 
return to zero 
(
NRZ
)
  
,
Ÿàÿ£ÿ≠ÿØ
 
ÿßŸÑ
ŸÄ
  
forms
  
  ÿßŸÑŸÑŸä ŸáŸÜÿØÿ±ÿ≥Ÿáÿß
ÿßŸÑ
ŸÄ
  
binary 1
 
 Ÿáÿπÿ®ÿ±ÿπŸÜŸá ÿ®
ŸÄ
5 
v
 
 ŸÅŸä ÿßŸÑŸÜÿµ ÿßÿ£ŸÑŸàŸÑ Ÿà
 
-
5
 
v
 
ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÜŸä ŸàÿßŸÑ
ŸÄ
 
binary 0
 
 Ÿáÿπÿ®ÿ±ÿπŸÜŸá ÿ®
ŸÄ
 
-
5
 
v
 
 ŸÅŸä ÿßŸÑŸÄŸÄŸÜÿµ ÿßÿ£ŸÑŸàŸÑ Ÿà
 
5
 
v
 
 ŸÅŸÄŸÄŸä ÿßŸÑŸÄŸÄŸÜÿµ
ÿßŸÑÿ™ÿßŸÜŸä ŸàÿßŸÑÿµŸàÿ±ÿ© ÿØŸä ÿ®ÿ™Ÿàÿ∂  ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÑŸÜŸàÿπŸäŸÜ  
 
 
 
ÿßŸÑ 
ÿ¥ŸÉŸÑ ÿßŸÑŸÑŸä ŸÅÿßÿ™ ÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑ 
ŸÄ 
  
synchronization
  
ÿ™ŸÜ ÿßŸÑ
ŸÄ
  
transitions
  
ÿ≤ÿßÿØ
ÿå
  
ŸàŸÑŸÉŸÄŸÄŸÜ ÿßŸÑŸÖÿ¥ŸÄŸÄŸÉŸÑÿ© ÿßŸÑÿ™ÿßŸÜŸäŸÄŸÄŸá ŸÑÿ≥ŸÄŸÄŸá ŸÖŸàÿ¨ŸÄŸÄŸàÿØ
ŸÜ
 
ŸàŸáŸä ÿßŸÜ ŸÑŸà ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™ 
180
 
.ÿØÿ±ÿ¨ÿ©
 
ÿ¥ÿßÿ®ÿ™ÿ± ÿßŸÑ
ŸÄ
Line Coding 
 
 ÿÆÿßÿµ ŸÑŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßÿ™ÿ¥ŸÄŸÄŸÉÿßŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅŸÄŸÄÿ© ŸÑŸÑŸÄŸÄ
ŸÄ
Baseband Signals 
 
 ŸàŸÖÿ≤ÿßŸäŸÄŸÄÿß ŸàÿπŸäŸÄŸÄŸàÿ® ŸÉŸÄŸÄŸÑ
ÿ¥ŸÉŸÑ ŸÅŸäŸáÿß ŸàŸáŸÜÿØÿ±ÿ≥ ÿ®ÿπÿ∂ ÿßŸÑÿØŸàÿß ÿ± ÿßŸÑÿ®ÿ≥ŸäŸäÿ© ÿßŸÑŸÑŸä ŸÖŸÜ ÿÆÿßŸÑŸÑŸáÿß ŸÜÿπŸÖŸÑ 
generation
 
ŸÑÿ£ŸÑÿ¥ŸÉÿßŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©
 
ŸÑŸÑ
ŸÄ
Baseband 
Signals
 
 

 
‚ñ™ Carrier  Modulation  
ÿßŸÑŸÄ Baseband Signal ÿ•ÿ¥ÿßÿ±ÿ© ÿ™ÿ±ÿØÿØŸáÿß ŸÇŸÑŸäŸÑ ÿ™ ÿ™ÿµŸÑ  ÿßŸÜŸáÿß ÿ™ÿ™ÿ®ÿπÿ™ ŸÅŸä ÿßŸÑŸáŸàÿß ŸÅÿßŸÑÿ≤ŸÖ ÿßÿπŸÖŸÄŸÄŸÑ modulation  Ÿàÿßÿ≠ŸÖŸÑŸáŸÄŸÄÿß
ÿπŸÑŸâ  carrier    ŸàŸáŸäÿ•ÿ¥ÿßÿ±ÿ©  sinewave  ÿ™ÿ±ÿØÿØŸáÿß ÿπÿßŸÑŸä  (ÿßŸÑŸÄ  carrier    ÿØÿßŸäŸÖÿß analog)   Ÿàÿ®ÿπÿØ ÿπŸÖŸÑŸäŸÄŸÄÿ© ÿßŸÑŸÄŸÄŸÄ  modulation 
  ÿ®ÿ™ŸäŸÑÿπÿ•ÿ¥ÿßÿ±ÿ© ŸÖÿπÿØŸÑÿ© modulated signal 
ÿ£ŸÜŸàÿßÿπ  ÿßŸÑ modulation  ŸÅŸä ÿßŸÑ digital communications: 
‚ñ™ ASK (Amplitude Shift Keying)  
‚ñ™ FSK (Frequency Shift Keying)  
‚ñ™ PSK (Phase Shift Keying)  
‚ñ™ Spread spectrum modulation  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ŸáŸÜÿ±ÿßÿ¨ÿπ ÿ≥ÿ±Ÿäÿπÿß ŸÅŸä ÿßŸÑÿ¨ŸÄŸÄÿ≤ÿ° ÿØÿß ÿπŸÑŸÄŸÄŸâ  ÿßŸÑŸÖÿπŸÜŸÄŸÄŸâ  ŸàÿßŸÑŸÖŸÅŸáŸÄŸÄŸàŸÖ  ÿ™ŸÜ ÿØÿß ÿ£ÿ≥ŸÄŸÄÿßÿ≥ ÿ™ ÿ∫ŸÜŸÄŸÄŸâ ÿπŸÜŸÄŸÄŸá  ŸàŸÖŸÄŸÄÿ¥ ŸáŸÜÿ™ŸäŸÄŸÄÿ±ŸÇ ÿ•ŸÑÿ´ÿ®ÿßÿ™ŸÄŸÄÿßÿ™ 
ŸàŸÖÿ≥ÿß ŸÑ ŸÅŸä ÿ¨ÿ≤ÿ°  Fourier Series  Ÿà    Fourier Transform 
 
   Fourier Series  Ÿà Fourier Transform  ŸáŸÖÿß ÿ™ÿ≠ŸÑŸäÿßŸÑÿ™ ÿ±Ÿäÿßÿ∂Ÿäÿ© ÿßŸÑŸáÿØŸÅ ŸÖŸÜŸáÿß ÿ•Ÿäÿ¨ÿßÿØ ŸÖÿ≠ÿ™ŸÄŸÄŸàŸâ ÿ£Ÿä ÿ•ÿ¥ŸÄŸÄÿßÿ±ÿ© ŸÖŸÄŸÄŸÜ
ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸàÿØÿß ÿ®ŸÜÿ≠ÿ™ÿßÿ¨ ŸÜÿπÿ±ŸÅŸá ŸÑŸÉÿ∞ÿß ÿ≥ÿ®ÿ® ÿ≤Ÿä ÿßŸÜ  ŸÅŸäŸá  ÿ™ÿ±ÿØÿØÿßÿ™ ŸÑŸÄŸÄÿ®ÿπÿ∂ ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿßÿ™  unwanted  ŸàŸÅŸäŸÄŸÄŸá ÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ ÿßŸÑŸÄŸÄŸÄ 
receiver   ÿ®ŸäÿπÿØŸäŸáÿß ŸàŸÖÿ® ŸäÿπÿØŸäÿ¥ ÿ∫Ÿäÿ±Ÿáÿß ,ÿßŸÜÿß ÿπÿßŸäÿ≤ ŸÖÿ´ÿßŸÑ ÿßŸÑŸÄ receiver  Ÿäÿ≥ŸÄŸÄÿ™ŸÇÿ®ŸÑ ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸÖŸÄŸÄŸÜ ÿßŸÑŸÖŸàÿ®ÿßŸäŸÄŸÄŸÑ ÿ®ÿ™ŸÄŸÄÿßÿπŸä ÿπŸÑŸÄŸÄŸâ
ÿ™ÿ±ÿØÿØ ŸÖÿπŸäŸÜ ŸàŸäÿ±ŸÅÿ∂ ÿ®ÿßŸÇŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ŸàÿØÿß ÿ®Ÿäÿ™ŸÖ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿØŸàÿß ÿ± ÿßŸÑŸÅŸÑÿ™ÿ± ŸàŸäÿ®ÿπÿß ŸÖŸäŸÜŸÅÿπÿ¥ ŸÜÿµŸÖŸÖ ÿßŸÑŸÅŸÑÿ™ÿ± ŸÖŸÜ ÿ∫ŸäŸÄŸÄÿ± ŸÖŸÄŸÄÿß 
ŸÜÿπÿ±ŸÅ ŸÖÿ≠ÿ™ŸàŸâ ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸÖŸÄŸÄŸÜ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™  ŸàŸÜÿπŸÄŸÄÿ±ŸÅ ŸÜÿπŸÄŸÄÿØŸä ÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ ÿ£Ÿä ŸàŸÜŸÄŸÄÿ±ŸÅÿ∂ ÿ£Ÿä ŸàÿØÿß  ÿ®ŸÜŸÄŸÄÿßÿ°ÿß    ÿπŸÑŸÄŸÄŸâ ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿßÿ™ 
ÿßŸÑŸÖÿ±ÿ∫Ÿàÿ® ŸÅŸäŸáÿß ÿ£ŸÉŸàŸÜ ÿπÿßÿ±ŸÅ ÿ™ÿ±ÿØÿØÿßÿ™Ÿáÿß ŸÉÿßŸÖ. 
 
ŸÅŸä Fourier Transform  ÿ±ÿ≥ŸÖŸÜÿß ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÖŸáÿß amplitude spectrum  ŸàŸáŸà ÿπÿßŸÑŸÇÿ© ÿ®ŸäŸÜ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØ ÿπŸÑŸÄŸÄŸâ ÿßŸÑŸÖÿ≠ŸÄŸÄŸàÿ±
ÿßÿ™ŸÅŸÇŸä ŸàÿßŸÑŸÄ  magnitude    ÿπŸÑŸâ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿ±ÿ£ÿ≥ŸÄŸÄŸäŸà ÿßŸÑŸÄŸÄŸÄ  magnitude    ÿØÿß ŸáŸÄŸÄŸà amplitude  ŸÑÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ©  ÿπŸÜŸÄŸÄÿØ  ÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™  
ŸÖÿÆÿ™ŸÑŸÅÿ©. 
 
ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿØÿß ŸÑŸÄ amplitude spectrum  ÿ®ŸäŸàÿ∂  ÿßŸÜ ÿπŸÜÿØŸÜÿß ŸÇŸäŸÖŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÖŸÜ 0Hz  ŸÑÿ≠ŸÄŸÄÿØ 1000 Hz  ŸàŸÑŸÉŸÄŸÄŸÜ ÿπŸÜŸÄŸÄÿØ 
1000 Hz ÿ™ÿ≠ÿØŸäÿØÿß ŸÇŸäŸÖÿ© ÿßŸÑŸÄ amplitude ÿ®ÿµŸÅÿ± Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ±ÿ≥ŸÖÿ© ÿØŸä Ÿàÿ∂ŸÄŸÄÿ≠ÿ™ ŸÖÿ≠ÿ™ŸÄŸÄŸàŸâ ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸÖŸÄŸÄŸÜ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ 
ŸàŸÇŸäŸÖÿ©  ÿßŸÑŸÄ amplitude ÿπŸÜÿØ ŸÉŸÑ ÿ™ÿ±ÿØÿØ. 
 
 
 
 
 
 
 
 
 
ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿØŸä ŸÜŸÇÿØÿ± ŸÜŸÇŸàŸÑ ÿßŸÜ ÿßŸÑŸÄ Band Width  )(ŸáŸà ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßÿπŸÑŸâ ÿ™ŸÄŸÄÿ±ÿØÿØ ŸàÿßŸÇŸÄŸÄŸÑ ÿ™ŸÄŸÄÿ±ÿØÿØŸáŸÄŸÄŸà  1000 Hz 
 ŸàŸáŸà ŸÜŸäÿßŸÇ ÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ŸàŸäŸÄŸÄŸá ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸàŸÖŸÖŸÉŸÄŸÄŸÜ ŸÜŸÇŸÄŸÄŸàŸÑ ÿπŸÑŸäŸÄŸÄŸá Band Limited to 1000 Hz  ŸäÿπŸÜŸÄŸÄŸä ÿßÿπŸÑŸÄŸÄŸâ
ÿ™ÿ±ÿØÿØ ŸÅŸäŸáÿß ŸáŸà  1000 Hz. 
 
 Ÿàÿßÿ∂  ŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÖ ÿßŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØŸÜ ÿ®ŸÜŸÅÿ≥ ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸÅŸäŸá ÿ™ÿ±ÿØÿØÿßÿ™ ŸÖŸàÿ¨ŸàÿØŸÜ ÿ®ÿ¥ŸÉŸÑ ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ
ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ÿßÿ£ŸÑÿÆÿ±Ÿâ ŸàŸÖŸÜ ÿØÿß ŸÜŸÇÿØÿ± ŸÜŸÇŸàŸÑ ÿßŸÜ ÿßŸÑŸÄ amplitude spectrum  ŸáŸà ŸÇŸäŸÖÿ© ÿ™ÿπÿ®ÿ± ÿπŸÜ ŸÜÿ≥ÿ®ÿ© Ÿàÿ¨ŸÄŸÄŸàÿØ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØ ÿØÿß
ŸÖŸÇÿßÿ±ŸÜÿ© ÿ®ÿ®ÿßŸÇŸä ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
 
ŸàŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÖÿ© ŸáŸÜÿßŸÑŸÇŸÄŸÄŸä ŸÅŸäŸÄŸÄ Ÿá DC Voltage  ÿ™ŸÜ ŸÖŸàÿ¨ŸÄŸÄŸàÿØ ŸÇŸäŸÖŸÄŸÄÿ©ŸÑÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸÅŸÄŸÄŸä amplitude spectrum  ÿπŸÜŸÄŸÄÿØ ÿ™ŸÄŸÄÿ±ÿØÿØ
ÿµŸÅÿ± ŸàŸÅŸä ÿßŸÑŸÄ Time domain   Ÿáÿ™ÿ∏Ÿáÿ±ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸáŸä ŸÅŸäŸáÿß shift   ŸÅŸä ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿ±ÿ£ÿ≥Ÿäÿ®ŸÖŸÇÿØÿßÿ± ŸÇŸäŸÖÿ©  ÿßŸÑŸÄ DC  Fourier Analysis  
amp spectrum  
ùíá 
1000 Hz  0 Hz 
 
 
 
 ŸÖÿ™ÿ≥ŸÑÿ≥ŸÑÿ© ŸÅŸàÿ±Ÿäÿ± ŸáŸäÿ£ÿØÿßÿ© ÿ±Ÿäÿßÿ∂Ÿäÿ© ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä ÿ™ÿ≠ŸàŸäŸÑ ÿ£Ÿä ÿ•ÿ¥ÿßÿ±ÿ© ÿØŸàÿ±Ÿäÿ© Periodic Signal  ŸÖŸáŸÖŸÄŸÄÿß ŸÉÿßŸÜŸÄŸÄÿ™ ÿØÿ±ÿ¨ŸÄŸÄÿ©
ÿ™ÿπŸÇŸäÿØŸáÿß ÿ•ŸÑŸâ ŸÖÿ¨ŸÖŸàÿπÿßÿ™ ŸÖŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿ®ÿ≥Ÿäÿ© Sin and Cos waves   ÿ®ÿ™ÿ±ÿØÿØÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©ŸÉŸÖÿß  ŸäŸÑŸä: 
ùëì(ùë°)= ùëéùëú+ ‚àëùëéùëõcos(ùëõùúîùëúùë°)‚àû
ùëõ=1+ ‚àëùëèùëõsin(ùëõùúîùëúùë°)‚àû
ùëõ=1 
ùúîùëú=2ùúãùëìùëú 
ùëìùëú=1
ùëá 
 
ŸÅŸä ÿßŸÑÿ±ÿ≥ŸÖÿ© ÿßÿ™ÿ™ŸäŸá  ŸáŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ™ÿ™ŸÉÿ±ÿ± ŸÉŸÑ  ùëá  ŸàŸÑŸÉŸÜ ŸáŸÑ ÿØÿß ŸÖÿπŸÜÿßŸÜ ÿßŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ™ÿ±ÿØÿØŸáÿß ùëìùëú=1
ùëá ÿü Ÿäÿ®ÿπÿß ÿ™ 
ÿ™ÿ±ÿØÿØ  ùëìùëú    ÿØÿß ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ£ŸÑÿ≥ÿßÿ≥Ÿä  ÿßŸÑŸÑŸä ÿ∏ÿßŸáÿ± ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÖÿ© ŸÑŸÉŸÄŸÄŸÜ ŸÅÿπŸÑŸäŸÄŸÄÿß ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿØŸä ŸÖŸÉŸàŸÜŸÄŸÄŸá ŸÖŸÄŸÄŸÜ ÿπŸÄŸÄÿØÿØ ŸÉÿ®ŸäŸÄŸÄÿ± ÿ¨ŸÄŸÄÿØÿß ŸÖŸÄŸÄŸÜ 
ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ Ÿàÿπÿ¥ŸÄŸÄÿßŸÜ ŸÜÿπŸÄŸÄÿ±ŸÅ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ ÿØŸä ŸÜÿ±ÿ¨ŸÄŸÄÿπ ŸÑŸÖÿπÿßÿØŸÑŸÄŸÄÿ© Fourier series  ÿ≤Ÿä ŸÖŸÄŸÄÿß ŸÉŸÜŸÄŸÄÿß ÿ®ŸÜÿ≠ÿ≥ŸÄŸÄÿ®Ÿáÿß ŸÇÿ®ŸÄŸÄŸÑ ŸÉŸÄŸÄÿØÿß ŸÑŸÖŸÄŸÄÿß
ÿØÿ±ÿ≥ŸÜÿßŸáÿß. 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÄ  periodic   ÿØŸä ŸÖŸÉŸàŸÜŸá ŸÖŸÜ ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿß ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸáŸÖùëìùëú  ŸàŸÖÿ∂ÿßÿπŸÅÿßÿ™Ÿá ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ŸÅŸÇŸä ŸàŸÉŸÑ ÿ™ŸÄŸÄÿ±ÿØÿØ
ŸÖŸàÿ¨ŸÄŸÄŸàÿØ ÿ®ŸÜÿ≥ŸÄŸÄÿ®ÿ© ÿßŸà weight  ÿßŸà Amplitude ŸÖÿÆÿ™ŸÑŸÄŸÄŸÅ ÿπŸÄŸÄŸÜ ÿßŸÑÿ™ŸÄŸÄÿßŸÜŸä ŸàÿßŸÑŸÑŸÄŸÄŸä ÿ®Ÿäÿπÿ®ŸÄŸÄÿ± ÿπŸÄŸÄŸÜ ŸÜÿ≥ŸÄŸÄÿ®ÿ© ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØ ŸáŸÄŸÄŸä ÿßŸÑŸÄŸÄ ŸÄ 
coefficient s     ŸàŸÉŸÑ ŸÖÿßŸÉÿßŸÜ ÿßŸÑÿ±ŸÇŸÖ ÿØÿß ŸÉÿ®Ÿäÿ±   ŸÅÿØÿß ŸÖÿπŸÜŸÄŸÄÿßŸÜ ÿßŸÜ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØ ÿØÿß ŸÖŸàÿ¨ŸÄŸÄŸàÿØ  ÿ®ŸÄŸÄŸÄ  Amplitude  ŸÉÿ®ŸäŸÄŸÄÿ±  ÿØÿßÿÆŸÄŸÄŸÑ ÿßŸÑŸÖŸàÿ¨ŸÄŸÄÿ© 
ÿßÿ£ŸÑÿµŸÑŸäÿ© ŸàÿßŸÑÿπŸÉÿ≥ ÿ®ÿßŸÑÿπŸÉÿ≥, Ÿàÿßÿ≠ŸäÿßŸÜÿß Ÿàÿßÿ≠ŸÜÿß ÿ®ŸÜÿ≠ŸÑ ŸÖÿ≥ÿß ŸÑ ŸÅŸàÿ±Ÿäÿ± ŸÉŸÜŸÄŸÄÿß ÿ®ŸÜÿßŸÑŸÇŸÄŸÄŸä ŸÇŸÄŸÄŸäŸÖ   ÿßŸÑŸÄŸÄ ŸÄ  coefficients  ÿπŸÜŸÄŸÄÿØ  n   ŸÖÿπŸäŸÜŸÄŸÄŸá
ÿ®ÿµŸÅÿ± ŸàÿØÿß ŸÖÿπŸÜÿßŸÜ ÿßŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÖŸÜÿßÿ∏ÿ± ŸÑŸÑŸÄ n ÿØÿß ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØ  ŸÅŸä ÿßÿ™ÿ¥ÿßÿ±ÿ©. 
 
 
 
  
 
 
 
 
Fourier Series  
ùúîùëú‚Üí  ùëìùë¢ùëõùëëùëéùëöùëíùëõùë°ùëéùëô  ùëìùëüùëíùëûùë¢ùëíùëõùëêùë¶   
ùëá 
 
ŸÑŸà ÿ±ÿ≥ŸÖŸÜÿß ÿßŸÑŸÄ  sin  ŸàÿßŸÑŸÄ  cos   ŸÅŸä Ÿàÿ±ŸÇÿ© ŸÖÿ´ÿßŸÑ ÿ™ÿ≠ÿ™ ÿ®ÿπŸÄŸÄÿ∂ Ÿàÿ®ÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ùëìùëú  ŸàŸÖÿ∂ŸÄŸÄÿßÿπŸÅÿßÿ™Ÿáÿß Ÿàÿ¨ŸÖÿπŸÄŸÄÿ™ŸáŸÖ ŸÜŸÄŸÄÿßÿ™ÿ¨ ÿ™ÿ¨ŸÖŸÄŸÄŸäÿπŸáŸÖ
ŸáŸäŸÜÿ™ÿ¨ ŸÅŸä ÿßÿ™ÿÆÿ± ÿ¥ŸÉŸÑ  periodic ŸÖŸáŸÖÿß ŸÉÿßŸÜ ÿπÿØÿØ ÿßŸÑŸÄ sin ŸàÿßŸÑŸÄ cos 
ÿØÿß ŸÖÿ´ÿßŸÑ ÿ®ÿ≥ŸäŸä ŸÑŸÄ 3  ÿ™ÿ±ÿØÿØÿßÿ™ ŸÖÿ¨ŸÖŸàÿπŸáŸÖ ÿ®ŸäŸÜÿ™ÿ¨ ÿ•ÿ¥ÿßÿ±ÿ© periodic 
 
 
 ÿßÿ≤ÿßŸä ŸÉŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ÿßŸÑŸÄ periodic Ÿäÿ™ŸÉŸàŸÜŸàÿß  ŸÖŸÜ  sin  Ÿàcos ŸàŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ ŸÑŸäŸáŸÖ ÿßÿ¥ŸÉÿßŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ©ÿü 
ÿ®ÿ≥ÿ®ÿ® ÿßÿÆÿ™ÿßŸÑŸÅ ÿßŸÑŸÄ  fundamental  frequency ÿßŸÑŸÑŸä ÿ®ŸÜÿ®ÿØÿß ÿ®ŸäŸá ŸàŸÑŸà ŸÖÿ´ÿßŸÑ ùëìùëú=100  ùêªùëß  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© Ÿáÿ™ÿ™ŸÉŸÄŸÄŸàŸÜ ŸÖŸÄŸÄŸÜ 
ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ 100 , 200 , 300 , 400 , 500 ,‚Ä¶  ŸàŸÑŸÄŸÄŸà ŸÉŸÄŸÄÿßŸÜ ùëìùëú=1000  ùêªùëß   ÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿ™ÿßŸÜŸäŸÄŸÄŸá Ÿáÿ™ÿ™ŸÉŸÄŸÄŸàŸÜ ŸÖŸÄŸÄŸÜ
ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ 1000 , 2000 , 3000 , 4000 , 5000 ,‚Ä¶   ŸàÿØÿß ŸáŸäÿÆŸÑŸä ÿßŸÑÿ¥ŸÉŸÑ ŸäÿÆÿ™ŸÑŸÅ 
ÿ≥ÿ®ÿ® ÿ™ÿßŸÜŸä ŸÖŸÖŸÉŸÜ ŸäÿÆŸÑŸä ÿßŸÑÿ¥ŸÉŸÑ ŸäÿÆÿ™ŸÑŸÅ ŸàŸáŸà  ŸÇŸäŸÖ ÿßŸÑŸÄ coefficient s  .ŸÑŸà ÿßÿÆÿ™ŸÑŸÅÿ™ ÿßŸÑÿ¥ŸÉŸÑ ŸáŸäÿÆÿ™ŸÑŸÅ 
 
ÿ™ÿ±ÿØÿØ ÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÄ sin ŸàÿßŸÑŸÄ cos  ŸáŸÄŸÄŸà ÿ™ÿ±ÿØÿØŸáŸÄŸÄÿß ÿßÿ£ŸÑÿ≥ÿßÿ≥ŸÄŸÄŸä fundamental  frequency  ùëìùëú  ŸÅŸÇŸÄŸÄŸä ŸàÿØŸä ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿßŸÑŸÑŸÄŸÄŸä
ŸÖŸÖŸÉŸÜ ÿ£ŸÇŸàŸÑ ÿßŸÜŸáÿß ÿ™ÿ™ŸÉŸàŸÜ ŸÖŸÜ ÿ™ÿ±ÿØÿØ Ÿàÿßÿ≠ÿØ, Ÿàÿπÿ¥ÿßŸÜ ŸÉÿØÿß ÿ®ŸÜŸÇŸàŸÑ ÿßŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™  ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ŸÖŸàÿ¨ŸÄŸÄŸàÿØŸÜ ŸÅŸÄŸÄŸä 
ÿ¥ŸÉŸÑ sin  Ÿàcos , ŸàŸÑŸÖÿß ŸÜÿπŸÖŸÑ Fourier Transform  ÿ•ŸÑÿ¥ÿßÿ±ÿ©sin  Ÿàcos  ÿ®ÿ™ŸäŸÑÿπ ŸÅŸäÿßŸÑŸÄ spectrum  ÿπŸÜÿØ ÿßŸÑÿ™ÿ±ÿØÿØùëìùëú 
impulse  signal ÿ™ÿØŸÑ ÿßŸÜ ÿßŸÑŸÄ weight ÿ®ÿ™ÿßÿπ ÿßŸÑŸÄ signal ÿπÿßŸÑŸä ÿ¨ÿØÿß ÿπŸÜÿØ ùëìùëú  ÿ™ŸÜ ŸÖŸÅŸÄŸÄŸäÿ¥ ÿ™ŸÄŸÄÿ±ÿØÿØ ÿ∫ŸäŸÄŸÄÿ±ŸÜ ŸÅŸÄŸÄŸä ÿßŸÑÿ±ÿ≥ŸÄŸÄŸÖÿ©
ÿßÿµÿßŸÑ 

 
 
 
 
 
 
   
 
¬©
 
Basem Hesham
Binomial Distribution & Random Process
Lecture
 
4
 
 
 
 ŸáŸÜÿ≥ÿ™ŸÉŸÖŸÑ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ©ÿ•Ÿäÿ¨ÿßÿØ ÿßŸÑŸÄ probability    ÿßŸÑÿÆÿßÿµŸÄŸÄÿ®ÿßŸÑŸÄŸÄ ŸÄ random variable   ŸÄŸÄÿß ÿßŸÑŸÖŸÄŸÄÿ±  ÿØÿ™ ŸáŸÜÿ™ ÿßŸÖŸÄŸÄŸÑ
ŸÖÿπ discrete random variables  ŸàŸÖŸÜ ÿßÿ¥Ÿáÿ± ÿßŸÑÿ™Ÿàÿ≤Ÿä ÿßÿ™ ÿßŸÑŸÖŸèÿ≥ÿ™ÿÆÿØŸÖÿ® ŸÅŸä ÿßŸÑÿ™ ÿßŸÖŸÑ ŸÖÿπ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä
ÿßŸÑŸÖŸÜŸÅÿµŸÑ ŸáŸä Binomial Distribution 
 
 ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ¶ŸäÿßŸÑŸÖŸÜŸÅÿµŸÑ  ( Discrete Random Variable)  ŸáŸà ŸÜŸàÿπ ŸÖŸÜ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿäÿ® ÿßŸÑÿ™ŸÄŸÄŸä
ÿ™ÿ£ÿÆÿ∞ ŸÇŸäŸÖÿßŸã ŸÖÿ≠ÿØÿØÿ©. Ÿä ŸÜŸä ÿ∞ŸÑŸÉ ÿ£ŸÜ ÿßŸÑŸÜÿ™ÿß ÿ¨ ÿßŸÑŸÖŸÖŸÉŸÜÿ® ŸÑŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ÿßŸÑŸÖŸÜŸÅÿµŸÑ ÿ™ŸÉŸàŸÜ ŸÇÿß ŸÑÿ® ŸÑŸÑ ÿØ Ÿàÿ∫Ÿäÿ± ŸÖÿ™ÿµŸÄŸÄŸÑÿ® 
   ÿ∂Ÿáÿß ÿßŸÑ  ÿ∂. ÿπŸÑŸâ ÿ≥ ŸäŸÑ ÿßŸÑŸÖÿ´ÿßŸÑÿå ÿ•ÿ∞ÿß ŸÉŸÜÿß ŸÜÿ±ŸÖÿ≤ ŸÑÿ±ŸÖŸÄŸÄŸä ÿßŸÑŸÜŸÄŸÄÿ±ÿØ  ŸÄŸÄÿ±ŸÇ ÿå ŸÅŸÄŸÄŸäŸÜ ÿßŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä  ÿßŸÑŸÖŸÜŸÅÿµŸÄŸÄŸÑ 
ÿ≥ŸäŸÉŸàŸÜ ŸÇŸäŸÖŸá ŸáŸä ÿßÿ£ŸÑÿ±ŸÇÿß  ŸÖŸÜ 1   ÿ•ŸÑŸâ6. 
 
 ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ¶ŸäÿßŸÑŸÖÿ≥ÿ™ÿ™ŸÖÿ±  ( Continuous Random Variable)  ŸÅŸáŸÄŸÄŸà ŸÜŸÄŸÄŸàÿπ ŸÖÿÆŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ ÿßŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ±ÿßÿ™
ÿßŸÑ ÿ¥Ÿàÿß Ÿäÿ®  ŸäŸÖŸÉŸÜ ÿ£ŸÜ Ÿäÿ£ÿÆŸÄŸÄÿ∞ ŸÇŸäŸÖŸÄŸÄÿßŸã ŸÅŸÄŸÄŸä ŸÜÿ™ŸÄŸÄÿß  ŸÖÿ≥ŸÄŸÄÿ™ŸÖÿ±. Ÿä ŸÜŸÄŸÄŸä ÿ∞ŸÑŸÄŸÄŸÉ ÿ£ŸÜ ÿßŸÑŸÜÿ™ŸÄŸÄÿß ÿ¨ ÿßŸÑŸÖŸÖŸÉŸÜŸÄŸÄÿ® ŸÑŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä 
ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±  ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ¥ŸÖŸÑ ÿ£ÿ™ ŸÇŸäŸÖÿ® ÿ∂ŸÖŸÜ ŸÜÿ™ÿß  ŸÖÿ≠ÿØÿØ. ÿπŸÑŸâ ÿ≥ŸÄŸÄ ŸäŸÑ ÿßŸÑŸÖÿ´ŸÄŸÄÿßŸÑ ÿØÿ±ÿ¨ŸÄŸÄÿ® ÿßŸÑÿ≠ŸÄŸÄÿ±ÿßÿ±ÿ© ÿßŸÑ ÿ™ÿ£ÿÆŸÄŸÄÿ∞ ŸÇŸÄŸÄŸä  
ŸÖÿ≠ÿØÿØ  ŸàŸäŸÖŸÉŸÜ ÿßŸÜ ÿ™ÿ£ÿÆÿ∞  ÿπÿØÿØ ÿßŸÑŸÜŸáÿß Ÿä ŸÖŸÜ ÿßŸÑŸÇŸä  ÿ∂ŸÖŸÜ ŸÜÿ™ÿß  ŸÖÿ≠ÿØÿØ.   
gaussian density   ÿßŸÑŸÑŸä ÿ™  ÿØÿ±ÿßÿ≥ÿ™Ÿá ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑÿ≥ÿß ŸÇÿ® Ÿä  ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß ŸäÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±.   
 
‚ñ™ Consider the experiment of tossing a coin ùíè times. Let the probability of heads 
be ùëù, and of tails be ùëû. 
ùíí=ùüè‚àíùíë 
‚ñ™ Now suppose we want  to find the probability of ùëò heads out of ùëõ tosses.  
ŸÅŸÄŸÄŸä ÿ™ÿ¨ÿ± ŸÄŸÄÿ® ÿßŸÑŸÇŸÄŸÄÿßŸÑ ÿßŸÑ ŸÖŸÑŸÄŸÄÿ® ŸÜŸÅÿ™ŸÄŸÄÿ±ÿ∂ ÿßÿ¨ŸÄŸÄÿ±ÿßŸÑ ÿßŸÑÿ™ÿ¨ÿ± ŸÄŸÄÿ® ÿπŸÄŸÄÿØÿØ ùëõ  ŸÖŸÄŸÄŸÜ ÿßŸÑŸÖŸÄŸÄÿ±ÿßÿ™ ŸàŸÑŸÄŸÄÿ™ŸÉŸÜ10  ŸÖŸÄŸÄÿ±ÿßÿ™ ŸàŸÜŸÅÿ™ŸÄŸÄÿ±ÿ∂
ÿßŸÜ ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ŸÄŸÄÿØŸà  head  ŸáŸÄŸÄŸàùíë   Ÿàÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ŸÄŸÄÿØŸàtail  ŸáŸÄŸÄŸàùíí  ,ŸàŸÖÿ¨ŸÖŸÄŸÄŸàÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑŸäŸÜ Ÿäÿ≥ŸÄŸÄÿßŸàÿ™ Ÿàÿßÿ≠ŸÄŸÄÿØ
Ÿà ÿßŸÑÿ™ÿßŸÑŸä  ùíí=ùüè‚àíùíë   
 
   ŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜŸÜÿß ÿπÿßŸäÿ≤ŸäŸÜ ŸÜÿ¨Ÿäÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿßŸÑŸÄŸÄ ŸÄ  heads    ÿπŸÄŸÄÿØÿØùíå    ŸÖŸÄŸÄŸÜ ÿßŸÑŸÖŸÄŸÄÿ±ÿßÿ™ ŸàŸÑŸÜŸÅÿ™ŸÄŸÄÿ±ÿ∂ ÿßŸÜ ùíå=ùüï  Ÿä ŸÜŸÄŸÄŸä
Ÿáÿßÿ™ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà   7  ŸÖÿ±ÿßÿ™ heads  ŸÖŸÜ ÿ∂ŸÖŸÜÿßŸÑŸÄ 10 .ŸÖÿ±ÿßÿ™ ÿßŸÑŸÑŸä ÿπŸÖŸÑŸÜÿß ŸÅŸäŸá  ÿßŸÑÿ™ÿ¨ÿ± ÿ® 
 
‚ñ™ One possible sequence is  to start with ùíè‚àíùíå tails followed by ùíå heads . 
‚ñ™ The probability of any ordering of ùíå heads and ùíè‚àíùíå tails 
ùíëùíå ùííùíè‚àíùíå 
 
ÿßŸÅÿ™ÿ±ÿ∂ŸÜÿß ÿßŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà    ÿßŸÑŸÄ  head   ÿßŸÑŸàÿßÿ≠ÿØÿ©ùíë   ÿßÿ∞ÿß ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà head s 7  ŸáŸäùíëùüï   Ÿàÿ™ÿ∂ÿ±ÿ® ŸÇŸÄŸÄŸä  ÿßŸÑŸÄŸÄ ŸÄ 
ùíë  ÿ£ŸÑŸÜŸÜÿß  ÿπÿßŸäÿ≤ŸäŸÜ  ÿßŸÑŸÄ  7    ÿØŸàŸÑ Ÿäÿ∏Ÿáÿ±Ÿàÿß ŸÖÿπ   ŸÄŸÄÿ∂ ŸÅŸÄŸÄŸä ŸÜŸÅŸÄŸÄÿßÿßŸÑŸÄŸÄ ŸÄ  sequence     Ÿà ÿßŸÑŸÖÿ´ŸÄŸÄŸÑ ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ŸÄŸÄÿØŸàÿßŸÑŸÄŸÄ ŸÄ  tail 
 ÿßŸÑŸàÿßÿ≠ÿØÿ©ùíí   Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿßŸÑŸÄ tail 3  ŸáŸàùííùüë, ÿßÿ∞ÿß ÿßÿßŸÑÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿßŸÑŸÉŸÑŸÄŸÄŸä ŸÑÿ≠ŸÄŸÄÿØŸà  7  ŸÖŸÄŸÄÿ±ÿßÿ™ heads  Ÿà 3 
 ŸÖÿ±ÿßÿ™tails   ŸáŸà  ùíëùüï ùííùüë  Ÿàÿ™  ÿ∂ÿ±ÿ® ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ£ŸÑŸÜŸáÿß ÿ™ÿ≠ÿØ  ŸÖ ÿß ŸÉŸÄ sequence Ÿàÿßÿ≠ÿØ 
 Binomial Distribution  
 
  ÿßÿ∞ÿß ùíëùíå ùííùíè‚àíùíå ŸáŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà  ÿπÿØÿØ ŸÖ ŸäŸÜ ŸÖŸÜ ÿßŸÑŸÄ  heads   Ÿà ÿßŸÑŸÄ  tails   ŸÄŸÄÿß ÿßÿßŸÑÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿØÿßŸÑŸÑŸÄŸÄ ŸÄ sequence 
 ÿßŸÑŸàÿßÿ≠ÿØ  ÿß. Ÿä ŸÜŸä ÿπŸÜÿØÿ™ÿπÿØÿØ ŸÉ Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÄ sequence s   ŸÖŸÖŸÉŸÜÿ™ ÿ≠ÿµŸÑ Ÿàÿπÿ¥ÿßŸÜ ÿßŸÇÿØÿ± ÿ≠ÿµŸÑ ÿπŸÑŸÄŸÄŸâ ÿßŸÑŸÄŸÄ ŸÄ  total 
probability   Ÿáÿ≠ÿ™ÿßÿ¨ ÿßÿ≠ÿ≥ÿ®ÿπÿØÿØ ÿßŸÑŸÄ sequences 
  ÿπŸÜÿØÿ™ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ sequences : ŸÉÿ™Ÿäÿ± ŸÖŸÖŸÉŸÜ ÿ™ÿ≠ÿµŸÑ ÿ≤ÿ™ ŸÖÿ´ÿßŸÑ 
TTTHHHHHHH , HHHHHHHTTT , HTHTHTHHHH , HHHHHTTTHH , ‚Ä¶‚Ä¶.  
ÿπÿ¥ÿßŸÜ ÿßÿπÿ±ŸÅ ÿßÿ≠ÿ≥ÿ® ÿπÿØÿØ ÿßŸÑŸÄ sequence s    ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ® ÿßŸÑŸÑŸä ŸáŸäÿ∏Ÿáÿ± ŸÅŸäŸá7   ŸÖÿ±ÿßÿ™ heads  Ÿà 3   ŸÖÿ±ÿßÿ™tails  
 Ÿáÿ≠ÿ≥ Ÿá  ÿπŸÜ ÿ™ÿ±ŸäŸÇ ÿßŸÑÿ™ ÿßÿØŸäŸÑ ŸàÿßŸÑÿ™ŸàÿßŸÅŸäŸÇ ŸÖŸÜ ÿßŸÑŸÇÿßŸÜŸàŸÜ 
(ùíè
ùíå)=ùíè!
ùíå!(ùíè‚àíùíå)! 
 
‚ñ™ The overall probability of ùëò heads (in any position) is  found by adding the 
individual probabilities together  
 
‚ñ™ Thus, we need to multiply the probability of the  sequence by the number of 
ways we can distribute ùëò heads among ùëõ positions.   
This number is found by : 
(ùíè
ùíå)=ùíè!
ùíå!(ùíè‚àíùíå)! 
where (ùíè
ùíå) is the binomial coefficient  
Thus, the probability of ùëò heads is given by  
 
ùíë(ùëø=ùë≤)=(ùíè
ùíå) ùíëùíå ùííùíè‚àíùíå=ùíè!
ùíå!(ùíè‚àíùíå)!ùíëùíå ùííùíè‚àíùíå 
 
 ŸÖ ŸÜŸâ ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÜÿßŸÑŸÄ Random variable X ŸÑŸÇŸäŸÖÿ® ŸÖÿ≠ÿØÿØ  ŸàŸáŸä K   Ÿäÿ≥ÿßŸàÿ™ ÿπÿØÿØÿßŸÑŸÄ sequences   ÿßŸÑŸÖŸÖŸÉŸÜÿ®
(ùíè
ùíå)  ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑÿßŸÑŸÄ sequence ÿßŸÑŸàÿßÿ≠ÿØ  ùíëùíå ùííùíè‚àíùíå 
 ŸÅŸä ÿ™ÿ¨ÿ± ÿ® ÿßŸÑŸÇÿßŸÑ ÿßŸÑ ŸÖŸÑÿ®10  ŸÖÿ±ÿßÿ™ ŸÖÿ™ŸÑŸàÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ∏ŸáŸàÿ± 7 Heads  ŸÅŸäÿßŸÑŸÄ  10 ŸÖÿ±ÿßÿ™ 
 
 ùëø ŸáŸä ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ŸàŸáŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ∏ŸáŸàÿ± ÿßŸÑŸÄ  Head 
ùë≤  ŸáŸä ÿßŸÑÿ±ŸÇ  ÿßŸÑŸÖÿ≠ÿØÿØ ÿßŸÑŸÑŸä ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ŸáŸäÿ≥ÿßŸàŸäŸá ŸàŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿØÿßK  ÿ™ÿ≥ÿßŸàÿ™7   ŸàÿßŸÑŸÑŸä  ÿ™  ÿ± ÿπŸÜ ÿπÿØÿØ
ÿ∏ŸáŸàÿ± ÿßŸÑŸÄ Head 
 ùíè ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿßÿ¨ÿ±ÿßŸÑ ÿßŸÑÿ™ÿ¨ÿ± ÿ® ŸàŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿØÿß Ÿäÿ≥ÿßŸàÿ™ 10 
ùíë  : probability of Head              ,ùíí  : probability of tails    
 
 
 
 
A noisy transmission channel has a per -digit error probability ùë∑ùíÜ=ùüé.ùüéùüè 
calculate the probability of having more than one error in 10 received digits.  
Solution  
 ÿπŸÜÿØÿ™ channel     ÿ™ ŸÅŸäŸáÿß  sequence binary ŸÖŸÉŸàŸÜ ŸÖŸÜ 10 bits   ÿ™ÿ™ ÿ±ÿ∂ŸÑŸÄ noise  Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ
Ÿäÿ≠ÿµŸÑ ÿÆÿ™ÿ£ ŸÅŸä ŸÉŸÑ bit    ŸáŸà ùëÉùëí=0.01  ŸàŸÖÿ™ŸÑŸàÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ Ÿäÿ≠ÿµŸÑ error  ŸÅŸä ÿßŸÉÿ™ÿ± ŸÖŸÜbit 
 
Random variable X  ŸáŸÜÿß ŸáŸà ÿπÿØÿØ ÿßŸÑŸÄ bits  ÿßŸÑŸÑŸä ŸÅŸäŸáÿß ÿÆÿ™ÿ£ 
 ùíë ŸáŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ™ÿ£ ŸÅŸä ÿßŸÑŸÄ bit ( ÿßŸÑŸàÿßÿ≠ÿØÿ© probability of error ) 
 
ùíë=ùüé.ùüéùüè 
ùíí=ùüè‚àíùíë=ùüè‚àíùüé.ùüéùüè=ùüé.ùüóùüó 
ùíè=ùüèùüé 
ùíë(ùëø=ùüé)+ùíë(ùëø=ùüè) + ùíë(ùëø=ùüê)+‚ãØ+ ùíë(ùëø=ùüèùüé)=ùüè 
 
 ŸÖÿ™ŸÑŸàÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ Ÿäÿ≠ÿµŸÑ ÿßŸÉÿ™ÿ± ŸÖŸÜ error  Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿπŸÜÿØÿ™ ŸáŸä  ÿØÿßŸäÿ® ŸÖŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ Ÿäÿ≠ÿµŸÑ  
2 error  ŸÑÿ≠ÿØ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ Ÿäÿ≠ÿµŸÑ 10 error  Ÿä ŸÜŸä Ÿáÿ¨ŸÖÿπ  ŸÉŸÑ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿØÿ™ Ÿàÿ™  ÿß ÿØÿß ŸáŸäÿßÿÆÿØ ŸàŸÇÿ™ ŸÉ Ÿäÿ±  
ŸÅŸä ÿßŸÑÿ≠ŸÑ.  
ŸÑŸà ÿßŸÑÿ≠ÿ∏ŸÜÿß ÿßŸÜ ŸÖÿ¨ŸÖŸàÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™   ÿØÿßŸäÿ® ŸÖŸÜ ÿπÿØ  ÿ≠ÿØŸà  error  ùíë(ùëø=ùüé)  ŸÑÿ≠ÿØ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßŸÑŸÄ 10 bits  
  ŸÉŸÑŸá  Ÿäÿ≠ÿµŸÑ ŸÅŸäŸá error  ùíë(ùëø=ùüèùüé)   Ÿäÿ≥ÿßŸàÿ™ Ÿàÿßÿ≠ÿØ, Ÿà ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸà ÿ™ÿ±ÿ≠ŸÜÿß ŸÖŸÜ1   ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ŸÖŸäÿ≠ÿµŸÑÿ¥
error  Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ Ÿäÿ≠ÿµŸÑ error   Ÿàÿßÿ≠ÿØ Ÿáÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÜÿßÿ™ÿ¨ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà  ÿßŸÉÿ™ÿ± ŸÖŸÜ error . 
 
ùíë(ùëø>ùüè)=ùüè‚àíùíë(ùëø=ùüè)‚àíùíë(ùëø=ùüé) 
ùíë(ùëø>ùüè)=ùüè‚àí(ùüèùüé
ùüé)(ùüé.ùüéùüè)ùüé (ùüé.ùüóùüó)ùüèùüé‚àí(ùüèùüé
ùüè)(ùüé.ùüéùüè)ùüè (ùüé.ùüóùüó)ùüó 
=ùüè‚àíùüèùüé!
ùüé!(ùüèùüé)!(ùüé.ùüéùüè)ùüé (ùüé.ùüóùüó)ùüèùüé‚àíùüèùüé!
ùüè!(ùüó)!(ùüé.ùüéùüè)ùüè (ùüé.ùüóùüó)ùüó 
= ùüé.ùüéùüéùüíùüê  
 
 
 
 
 Example  1 
 
 
 
   
Expected Value   ŸáŸä ŸÜŸÅÿ≥ŸáÿßÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ ÿßŸÑÿ≠ÿ≥ÿß Ÿä  ( mean)  Ÿàÿ™Ÿè ÿ™ ÿ± ŸÖŸÇŸäÿßÿ≥Ÿãÿß ŸÑŸÑŸÇŸäŸÖÿ® ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ÿ® ÿßŸÑŸÖÿ™ŸàŸÇ ÿ® ŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ±  
ÿπÿ¥Ÿàÿß Ÿä. 
 
The mean value ùíéùíô, or expected value of a random variable X, is defined by  
 
ùë¨{ùëø}=ùíéùíô= ‚à´ùíô ùíë(ùíô) ùíÖùíô‚àû
‚àí‚àû 
 
 ùê∏{ùëã} ÿ™ŸèŸÇÿ±ÿ£  ÿßŸÑŸÄ   Expected ValueŸÑŸÑŸÄ Random Variable X  
 
 ŸÑŸà ÿπŸÜÿØŸÜÿß ŸÇŸä  discrete  ŸÖÿ´ÿßŸÑ ŸÉŸÜÿß  ŸÜÿ¨Ÿäÿ®ÿßŸÑŸÄ mean     ÿπŸÜ ÿ™ÿ±ŸäŸÇ ÿ∂ÿ±ÿ® ŸÉŸÑ ŸÇŸäŸÖÿ® ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ŸÄŸÄÿØŸàÿ´Ÿáÿß ÿ´ŸÄŸÄ  ŸäŸÄŸÄÿ™
ÿ¨ŸÖÿπ ÿßŸÑŸÜŸàÿßÿ™ÿ¨ ŸàŸÑŸà ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸÑŸÉŸÑ ŸÇŸäŸÖÿ® ŸÖÿ™ÿ≥ÿßŸàŸäÿ® ŸÖŸÖŸÉŸÜ ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ ÿπŸÜ ÿ™ÿ±ŸäŸÇ ÿ¨ŸÖÿπ ÿßŸÑŸÇŸä  ÿ´ŸÄŸÄ  ŸÜŸÇÿ≥ŸÄŸÄŸÖŸá  
ÿπŸÑŸâ ÿπÿØÿØŸá . 
 
ÿπÿ¥ÿßŸÜ ŸÜÿ¨Ÿäÿ® ÿßŸÑŸÄ mean   ÿßŸàÿßŸÑŸÄŸÄ ŸÄ average value  ŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿπÿ¥ŸÄŸÄŸàÿß Ÿä continuous   ŸÄŸÄÿØŸÑ ŸÖŸÄŸÄÿß ŸÜÿßÿÆŸÄŸÄÿØ ŸÉŸÄŸÄŸÑ ŸÇŸäŸÖŸÄŸÄÿ®
Ÿàÿ∂ÿ± Ÿáÿß ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ŸÄŸÄÿØŸàÿ´Ÿáÿß ŸáŸÜÿßÿÆŸÄŸÄÿØ ÿßŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä ŸÉÿ£ŸÜŸÄŸÄŸá ÿßŸÑŸÇŸÄŸÄŸä   ÿ™ÿßÿπÿ™ŸÜŸÄŸÄÿß  ŸÄŸÄÿß  continuous   Ÿà ŸÄŸÄÿØŸÑ ŸÖŸÄŸÄÿß
ÿßÿ∂ÿ±ÿ® ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿ´Ÿá ÿ£ŸÑŸÜŸá ŸÖÿ¥ ÿ±ŸÇ  ŸÖÿ≠ÿØÿØ ŸÅ ÿ∂ŸÄŸÄÿ±ÿ® ŸÅŸÄŸÄŸä probability density function ùíë(ùíô)  
  ÿ™ÿßÿπÿ® ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä Ÿà ÿØŸÑ ÿ¨ŸÖ Ÿá  ŸáŸÉÿßŸÖŸÑ ŸÖŸÜ‚àû-   ÿßŸÑŸâ‚àû  
 
 ŸÖŸÜ ÿßŸÑŸÖŸÇÿßŸäŸäÿßÿßÿ•ŸÑÿ≠ÿµÿß Ÿäÿ®  ÿßŸÑŸÖŸáŸÖÿ® ŸàÿßŸÑŸÑŸä ÿßÿÆÿØŸÜÿß ŸÅŸÉÿ±  ÿπŸÜŸá ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ŸàŸáŸä ÿßŸÑÿ™ ÿßŸäŸÜ variance 
 
ùùàùüê=ùë¨{(ùëø‚àíùíéùíô)ùüê}= ‚à´(ùíô‚àíùíéùíô)ùüê ùíë(ùíô) ùíÖùíô‚àû
‚àí‚àû 
 
ŸÑÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ ŸÄŸÄÿßŸäŸÜ variance ÿå ŸäŸÄŸÄÿ™  ÿ≠ÿ≥ŸÄŸÄÿßÿ® ÿßŸÑŸÅŸÄŸÄÿ±   ŸÄŸÄŸäŸÜ ŸÉŸÄŸÄŸÑ ŸÇŸäŸÖŸÄŸÄÿ® ŸÖŸÖŸÉŸÜŸÄŸÄÿ® ŸÑŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä ( Random 
variable  ùëø ) ŸàÿßŸÑŸÇŸäŸÖÿ® ÿßŸÑŸÖÿ™ŸàŸÇ ÿ® ÿßŸà ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ (ùíéùíô mean  )  ÿ≠ÿ™Ÿâ ŸÜÿ≠ÿ≥ÿ® ŸÖÿØŸâ  Ÿè ÿØ ÿßŸÑŸÇŸä  ÿπŸÄŸÄŸÜÿßŸÑŸÄŸÄ  ŸÄ mean   ÿå
ÿ´  Ÿäÿ™  ÿ±ŸÅÿπ Ÿáÿ∞ÿß ÿßŸÑŸÅÿ±  ÿ•ŸÑŸâ ÿßÿ£ŸÑÿß ÿßŸÑÿ´ÿßŸÜŸä ÿ≠ÿ™Ÿâ ŸÜÿ™ÿßŸÑÿ¥Ÿâ ÿßŸÑŸÇŸä  ÿßŸÑÿ≥ÿßŸÑ ÿ® ÿßŸÑŸÜ ŸÇŸäŸÖÿ® ÿßŸÑÿ™ ÿßŸäŸÜ ŸÖŸÖŸÉŸÜ ÿ™ŸÉŸàŸÜ ŸÖŸàÿ¨ŸÄŸÄÿ® ÿßŸà 
ÿ≥ÿßŸÑÿ®.  
 
 
 
 
 
 
 
 
 Expected Values  (mean)  
 
 
X is u niformly distributed as shown in the following figure. Find  ùë¨{ùëø}, ùë¨{ùëøùüê},  
ùë¨{ùíÑùíêùíî ùëø} and ùë¨{(ùëø‚àíùíéùíô)ùüê} 
 
 
 
 
 
 
 
Solution  
 
Uniform Distribution is the simplest continuous density function.  The value of the density 
function is a constant over a  range of the x -axis. 
 
 ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑÿ≥ÿß ŸÇ ŸáŸà ŸÖŸÜ ÿßÿ¥ŸÉÿßŸÑÿßŸÑŸÄ probability density function  ŸàŸáŸà uniform distribution   ŸàŸáŸà ÿ™Ÿàÿ≤Ÿäÿπ
ŸÑŸá ÿØÿßŸÑÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ùëù(ùë•)  ÿ´ÿß ÿ™ÿ® ŸàÿØÿß ÿ≥ ÿ® ÿ™ÿ≥ŸÖŸäÿ™Ÿáÿß uniform    ÿ£ŸÑŸÜŸáÿß ŸÑÿØŸäŸáÿß ŸÇŸäŸÖÿ® ÿ´ÿß ÿ™Ÿá ÿ™ÿ≥ÿßŸàÿ™1
2ùúã  ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÖÿ´ÿßŸÑ 
 
1) ùë¨{ùëø} 
 
ùê∏{ùëã}=‚à´ ùë• ùëù(ùë•)ùëëùë•2ùúã
0=‚à´ ùëã 1
2ùúãùëëùë•=1
2ùúã ùëã2
2|
02ùúã2ùúã
0=ùúã 
 
2) ùë¨{ùëøùüê} 
 
ùê∏{ùëã2}=‚à´ ùë•2 ùëù(ùë•)ùëëùë•2ùúã
0=‚à´ ùëã2 1
2ùúãùëëùë•=1
2ùúã ùëã3
3|
02ùúã2ùúã
0=4
3ùúã2 
 
 
3) ùë¨{ùíÑùíêùíî ùëø} 
 
ùê∏{ùëêùëúùë† ùëã}=‚à´ ùëêùëúùë† ùë• ùëù(ùë•)ùëëùë•2ùúã
0=‚à´ ùëêùëúùë† ùë• 1
2ùúãùëëùë•=1
2ùúã ùë†ùëñùëõ ùë•|02ùúã2ùúã
0=0 
 
 
 Example  2 
ùüêùùÖ ùüè
ùüêùùÖ 
ùëø ùíë(ùíô) 
Probability density function of X  
 
 ùë¨{ùíÑùíêùíî ùëø}   ŸÖ ŸÜÿßŸáÿß ÿßŸÜŸä  ÿ¨Ÿäÿ® ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ ŸÑŸÇŸä cos x   Ÿà ÿßŸÑÿ™ÿßŸÑŸäŸÇŸäŸÖÿ™Ÿáÿß Ÿáÿ™ÿ≥ÿßŸàÿ™ ÿµŸÅÿ± Ÿàÿ±Ÿäÿßÿ∂Ÿäÿß ŸÑŸà ÿ≠ŸÑŸÜÿßŸáÿß  ŸÅÿØÿß 
ÿ™ŸÉÿßŸÖŸÑ  cos x    ÿπŸÑŸâ cycle    ŸÉÿßŸÖŸÑŸá ŸÖŸÜ0    ÿßŸÑŸâ2ùúã  ŸàÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸáŸà ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ® ÿ™ÿ≠ÿ™ ÿßŸÑŸÖŸÜÿ≠ŸÜŸâ Ÿà ÿßŸÑÿ™ÿßŸÑŸä  ÿßŸÑŸÄ  positive half 
cycle  Ÿáÿ™ÿßŸÑÿ¥ŸäÿßŸÑŸÄ negative half cycle 
 
 
4) ùë¨{(ùëø‚àíùíéùíô)ùüê} 
 ŸÉÿßŸÜŸá  ŸäŸÇŸàŸÑ ÿßÿ≠ÿ≥ÿ®ÿßŸÑŸÄ variance 
 ùíéùíô ŸáŸàÿßŸÑŸÄ Expected Values  ÿßŸÑŸÑŸä ÿ¨ ŸÜÿß  ŸÅŸä ÿßŸàŸÑ ŸÖÿ™ŸÑŸàÿ® 
 
ùê∏{(ùëã‚àíùëöùë•)2}=‚à´ (ùë•‚àíùëöùë•)2 ùëù(ùë•)ùëëùë•2ùúã
0=‚à´ (ùë•‚àíùúã)2 1
2ùúãùëëùë•2ùúã
0 
1
2ùúã (ùë•‚àíùúã)3
3|
02ùúã
=ùúã2
3 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
X is a Gaussian -distributed random variable with density function  
ùë∑(ùíô)= ùüè
‚àöùüêùùÖ ùùà ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê 
1) Find ùë¨{ùëø}  
2) Find ùë¨{(ùëø‚àíùíéùíô)ùüê} 
Solution  
 ŸÅŸä ÿßŸÑŸÖÿ™ŸÑŸàÿ® ÿßÿ£ŸÑŸàŸÑ ŸÉÿßŸÜŸá ÿπŸÄŸÄÿßŸäÿ≤ Ÿäÿ´ ŸÄŸÄÿ™ ÿßŸÜ ÿßŸÑŸÄŸÄŸÄ mean    ÿ™ŸÄŸÄÿßÿπ Gaussian density function  ŸáŸÄŸÄŸàùíé 
ŸàÿßŸÑŸÖÿ™ŸÑŸàÿ® ÿßŸÑÿ™ÿßŸÜŸä ŸÉÿßŸÜŸá ÿπÿßŸäÿ≤ Ÿäÿ´ ÿ™ ÿßŸÜ ÿßŸÑŸÄ variance  Ÿäÿ≥ÿßŸàÿ™ùùàùüê  ŸàÿØÿß ÿßÿ≠ŸÜÿß ÿπÿßÿ±ŸÅŸäŸÜŸá ŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÄŸÄ  ŸàŸÑŸÉŸÄŸÄŸÜ ŸáŸÜŸÄŸÄÿß
ÿπÿßŸäÿ≤ŸäŸÜ ŸÜÿ´ ÿ™Ÿá ÿ±Ÿäÿßÿ∂ŸäÿßŸã  
1) Find ùë¨{ùëø}  
ùë¨{ùëø}=ùíéùíô= ‚à´ùíô ùíë(ùíô) ùíÖùíô‚àû
‚àí‚àû=ùüè
‚àöùüêùùÖ ùùà‚à´ ùíô ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû 
 
  ÿπÿ¥ÿßŸÜ ŸÜ ÿ±ŸÅ ŸÜÿ≠ŸÑ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿØÿß ÿßŸÑÿ≤  ŸÜŸàÿ¨ÿØ ÿ™ŸÅÿßÿ∂ŸÑ ÿßÿßŸÑÿß ŸÇÿØÿßÿßŸÑŸÄ exponential  
 
 :ÿ™ŸÅÿßÿ∂ŸÑ ÿßÿßŸÑÿß 
‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê‚Üí‚àíùüê(ùíô‚àíùíé)
ùüê ùùàùüê=‚àí(ùíô‚àíùíé)
 ùùàùüê 
 
ùë¨{ùëø}=ùüè
‚àöùüêùùÖ ùùà‚à´(ùíô‚àíùíé+ùíé) ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû 
 
=ùüè
‚àöùüêùùÖ ùùà‚à´(ùíô‚àíùíé) ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô+ùüè
‚àöùüêùùÖ ùùà‚àû
‚àí‚àû‚à´ ùíé ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû 
 
 
ÿßŸÑÿ¨ÿ≤ŸÑ ÿßŸÑÿ™ÿßŸÜŸä ŸÖŸÜ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿπ ŸÄŸÄÿßÿ±ÿ© ÿπŸÄŸÄŸÜ ÿ™ŸÉÿßŸÖŸÄŸÄŸÑ ÿ´ÿß ŸÄŸÄÿ™ ÿßŸÑŸÑŸÄŸÄŸä ŸáŸÄŸÄŸà  ùíé  ŸÖÿ∂ŸÄŸÄÿ±Ÿàÿ® ŸÅŸÄŸÄŸä ùëù(ùë•)  ÿßŸÑŸÑŸÄŸÄŸä ŸáŸÄŸÄŸä ÿßŸÑŸÄŸÄŸÄ 
gaussian  Ÿàÿ™ŸÉÿßŸÖŸÑ ùëù(ùë•)   ŸÖŸÜ‚àí‚àû  ÿßŸÑŸâ‚àû Ÿäÿ≥ÿßŸàÿ™ Ÿàÿßÿ≠ÿØ ÿßŸÑŸÜ ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ® ŸÉŸÑŸáÿß ÿ™ÿ≠ŸÄŸÄÿ™ ÿ£ÿ™ density 
function  ÿ™ÿ≥ÿßŸàÿ™ Ÿàÿßÿ≠ÿØ Ÿà ÿßŸÑÿ™ÿßŸÑŸäÿ™ŸÉÿßŸÖŸÑ  ÿßŸÑÿ¨ÿ≤ŸÑ ÿØÿß Ÿäÿ≥ÿßŸàÿ™  ùíé 
 
ùë¨{ùíÑùíêùíèùíîùíïùíÇùíèùíï }=ùíÑùíêùíèùíîùíïùíÇùíèùíï   
 
 
 Example  3 
 
=‚àíùùàùüê
‚àöùüêùùÖ ùùà‚à´‚àí(ùíô‚àíùíé)
ùùàùüê ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô+‚àû
‚àí‚àûùíé 
ÿ∂ÿ± ŸÜÿß ŸÅŸä  ‚àíùùàùüê
‚àíùùàùüê ÿπÿ¥ÿßŸÜ ŸÜŸàÿ¨ÿØ ÿ™ŸÅÿßÿ∂ŸÑ ÿßÿßŸÑÿß ŸÇÿØÿß  ÿßŸÑŸÄ exponential 
=‚àíùùà
‚àöùüêùùÖ [ ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê|
‚àí‚àû‚àû
] +ùíé=(ùüé‚àíùüé)+ùíé=ùíé 
‚à¥ùë¨{ùëø}=ùíé 
 
 
2) Find ùë¨{(ùëø‚àíùíéùíô)ùüê} 
 
ùë¨{(ùëø‚àíùíé)ùüê}= ‚à´(ùíô‚àíùíé)ùüê ùíë(ùíô) ùíÖùíô‚àû
‚àí‚àû 
=ùüè
‚àöùüêùùÖ ùùà‚à´(ùíô‚àíùíé)ùüê  ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû 
=‚àíùùàùüê
‚àöùüêùùÖ ùùà‚à´(ùíô‚àíùíé)‚àí(ùíô‚àíùíé)
ùùàùüê  ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû 
 
ŸÜÿ≠ŸÑŸáÿß ÿπŸÜ ÿ™ÿ±ŸäŸÇ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ   ÿßŸÑÿ™ÿ¨ÿ≤ÿ™ŸÑ  
 
ùíñ=(ùíô‚àíùíé)              ùíÖùíó=‚àí(ùíô‚àíùíé)
ùùàùüê  ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüêùíÖùíô 
ùíÖùíñ=ùíÖùíô            ùíó=ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê 
 
‚à´ùíñ ùíÖùíó=ùíñùíó‚àí ‚à´ùíó ùíÖùíñ 
 
‚à¥ùë¨{(ùëø‚àíùíé)ùüê}=‚àíùùàùüê
‚àöùüêùùÖ ùùà[(ùíô‚àíùíé) ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê|
‚àí‚àû‚àû
‚àí‚à´ ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû] 
=‚àíùùàùüê[ùüè
‚àöùüêùùÖ ùùà(ùíô‚àíùíé) ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê|
‚àí‚àû‚àû
‚àíùüè
‚àöùüêùùÖ ùùà‚à´ ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê ùíÖùíô‚àû
‚àí‚àû] 
=‚àíùùàùüê[(ùüé‚àíùüé)‚àíùüè]=ùùàùüê 
 
   ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ® ŸÉŸÑŸáÿß ÿ™ÿ≠ÿ™
ÿßŸÑŸÖŸÜÿ≠ŸÜŸâ ÿ™ÿ≥ÿßŸàÿ™ Ÿàÿßÿ≠ÿØ  
 
 
 
 
‚ñ™ A random or stochastic process is a collection of  infinite number of sample 
functions together with  associated statistical properties.  
ÿßŸÑ ŸÖŸÑŸäÿ® ÿßŸÑ ÿ¥Ÿàÿß Ÿäÿ® Random Process  ŸáŸä ŸÖÿ¨ŸÖŸàÿπÿ® ŸÖŸÜ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑ ÿ¥Ÿàÿß Ÿäÿ® Ÿäÿ±ÿ™ ÿ™ ŸÉŸÄŸÄŸÑ ŸÖŸÜŸáŸÄŸÄÿß  ŸÜŸÇÿ™ŸÄŸÄÿ® ÿ≤ŸÖŸÜŸäŸÄŸÄÿ®
ŸÖÿ≠ÿØÿØÿ©.  ŸÅŸä ÿßŸÑ ŸÖŸÑŸäÿ® ÿØÿ™  ÿ¨Ÿäÿ®  samples     ÿπÿØÿØŸáÿß ŸÉ ŸäŸÄŸÄÿ± ÿ¨ŸÄŸÄÿØÿßŸã ÿπÿ¥ŸÄŸÄÿßŸÜ ŸÜŸÇŸÄŸÄÿØÿ± ŸÜÿ≠ŸÄŸÄÿØÿØ ÿßŸÑÿÆŸÄŸÄŸàÿßÿßÿ•ŸÑÿ≠ÿµŸÄŸÄÿß Ÿäÿ®   ŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± 
ÿπÿ¥Ÿàÿß Ÿä. 
 
ŸÑŸÜŸÅÿ™ÿ±ÿ∂ ÿ£ŸÜŸÜÿß ŸÜŸÇŸà   ŸÇŸäÿßÿß ÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÅŸä ŸÖÿØŸäŸÜÿ® ŸÖÿß ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ® 12   ÿ∏Ÿáÿ±Ÿãÿß ŸäŸàŸÖŸäŸãÿß ŸàŸÜÿ±ÿµÿØ ŸáŸÄŸÄÿ∞ÿßŸÑ ŸäÿßŸÜŸÄŸÄÿßÿ™ ÿπŸÑŸÄŸÄŸâ  
ŸÖÿØŸâ ŸÅÿ™ÿ±ÿ© ÿ™ŸàŸäŸÑÿ®,  ŸÖÿ´ÿßŸÑ  ÿ≥ŸÜŸÇŸà   ÿ±ÿµÿØ ÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÅŸä  ÿßŸÑŸäŸà  ÿßÿ£ŸÑŸàŸÑ ÿßŸÑÿ≥ÿßÿπÿ® 12  ÿ∏Ÿáÿ±ÿßŸãŸàŸÇŸäŸÖÿ® ÿØÿ±ÿ¨ŸÄŸÄÿ® ÿßŸÑÿ≠ŸÄŸÄÿ±ÿßÿ±ÿ©  
ŸÅŸä ÿßŸÑŸäŸà  ÿßŸÑÿ™ÿßŸÜŸä ÿßŸÑÿ≥ÿßÿπÿ®  12    ÿ∏Ÿáÿ±ÿßŸãŸàŸáŸÉÿ∞ÿß ŸÅŸä  ÿßŸÇŸä   ÿßÿ£ŸÑŸäÿß   Ÿä ŸÜŸä ŸáÿßÿÆÿØ  samples    ŸÉÿ™Ÿäÿ± ÿ¨ÿØÿß ÿπŸÑŸâ ŸÖÿØÿßÿ±ÿ£Ÿäÿß   ŸÉÿ™ŸäŸÄŸÄÿ± 
ÿßŸÑÿ≥ÿßÿπÿ® 12  ŸàŸÖŸÜÿßŸÑŸÄ  samples  ÿØÿ™ ŸáŸÇÿØÿ± ÿßÿ¨Ÿäÿ® ÿßŸÑŸÖŸÇÿßŸäŸäÿßÿßÿ•ŸÑÿ≠ÿµÿß Ÿäÿ®.    
ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÖÿ´ÿßŸÑ ŸäŸÖŸÉŸÜŸÜÿß ÿ£ŸÜ ŸÜ ÿ™ ÿ± Ÿáÿ∞  ÿßŸÑÿ≥ŸÑÿ≥ŸÑÿ® ŸÖŸÄŸÄŸÜ ÿßŸÑŸÇŸÄŸÄÿ±ÿßŸÑÿßÿ™ ŸÉ ŸÖŸÑŸäŸÄŸÄÿ® ÿπÿ¥ŸÄŸÄŸàÿß Ÿäÿ® Random Process ÿå ŸàÿØÿ±ÿ¨ŸÄŸÄÿ®
ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸáŸä ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ÿßŸÑÿ∞ÿ™ Ÿäÿ±ÿ™ ÿ™  ŸÉŸÑ ŸÜŸÇÿ™ÿ® ÿ≤ŸÖŸÜŸäÿ®.    
 
Example: Random Process to represent the  temperature of a city  
 
 
 
ÿßÿ£ŸÑÿ± ÿπ ÿ±ÿ≥ŸàŸÖÿßÿ™ ŸÜŸÅÿß ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùë•(ùë°)  ÿßŸà ŸÜŸÅÿß ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±  ŸàŸÑŸÉŸÜ ŸÅŸä ÿ£Ÿäÿß   ŸÖÿÆÿ™ŸÑŸÅÿ® 
 
 
 
 
Random Process  
 
 
 
 ŸÖŸÜÿ∂ŸÖŸÜ ÿßŸÑŸÖŸÇÿßŸäŸäÿß ÿßÿ•ŸÑÿ≠ÿµÿß Ÿäÿ® ŸáŸä ÿßŸÑŸÄ Correlation    Ÿàÿ≤ÿ™ ŸÖÿß ÿπÿ±ŸÅŸÜÿß ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿßÿ™ ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÜŸÄŸÄŸá  Ÿäÿ≠ŸÄŸÄÿØÿØ
ŸÖÿØŸâ ÿ™ÿ±ÿß ÿ™ Ÿà ÿ™ÿ¥ÿß Ÿá  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™  ŸàŸÖŸÜ ÿ∂ŸÄŸÄŸÖŸÜ ÿßŸÜŸàÿßÿπŸÄŸÄŸá ŸáŸÄŸÄŸà  autocorrelation   ŸàÿØÿß  ŸäŸÇŸÄŸÄŸäÿß ŸÖŸÄŸÄÿØŸâ ÿßŸÑÿ™ÿ¥ŸÄŸÄÿß Ÿá  ŸÄŸÄŸäŸÜ
ŸÜŸÅÿß  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™. 
 
ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ  ÿßŸÑŸÑŸä ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ©  ÿπŸÜÿØŸÜÿß  ŸÖÿ™ÿ∫Ÿäÿ±ŸäŸÜ ÿπÿ¥Ÿàÿß ŸäŸäŸÜ ÿßÿßŸÑŸàŸÑ ÿßÿ≥ŸÖŸá  ùë•(ùë°)   ŸàÿßŸÑÿ™ÿßŸÜŸä  ùë¶(ùë°)   ŸàŸÉÿßŸÑŸáŸÖÿß ÿπ ÿßÿ±ÿ© ÿπŸÜ  
Random variable  Ÿà function  ŸÅŸÄŸÄŸä ÿßŸÑŸÄŸÄÿ≤ŸÖŸÜùë°   ,Ÿà ÿßŸÑŸÑÿ≠ÿ∏ÿ™ŸÄŸÄŸäŸÜ ÿßŸÑŸÑŸÄŸÄŸä ŸáŸÇŸÄŸÄÿßÿ±ŸÜ ÿπŸÜŸÄŸÄÿØŸáùë°1  Ÿàùë°2  ŸàŸÜŸÅÿ™ŸÄŸÄÿ±ÿ∂ ÿßŸÜ
ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ÿØÿß ŸáŸà ÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ©  ŸÇŸäÿ≥Ÿáÿß ÿπŸÑŸâ ŸÖÿØÿßÿ± ÿßŸÑŸäŸà  ŸàÿπÿßŸäÿ≤ ÿßÿπÿ±ŸÅ ÿßŸÑ ÿßŸÑŸÇŸÄŸÄÿ®  ŸÄŸÄŸäŸÜ ÿØÿ±ÿ¨ŸÄŸÄÿ® ÿßŸÑÿ≠ŸÄŸÄÿ±ÿßÿ±ÿ© 
ÿπŸÜÿØ  ùë°1    ŸàŸÑŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜŸáÿß12    ÿßŸÑÿ∂Ÿáÿ± Ÿàùë°2    ÿßŸÑÿ≥ÿßÿπÿ®3   ÿßŸÑ ÿµÿ± ŸàÿπÿßŸäÿ≤ ÿßÿπÿ±ŸÅ ÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÅŸä ÿßŸÑŸÑÿ≠ÿ∏ÿ™ŸäŸÜ ÿØŸàŸÑ
ŸÖÿ™ŸÇÿßÿ± ŸäŸÜ ŸÅŸä ÿßŸÑŸÇŸä  ŸàÿßŸÑ ŸÖÿ™ ÿßÿπÿØŸäŸÜ ,ŸàÿØÿß ÿßŸÑŸÑŸä ŸáŸäŸÇŸäÿ≥Ÿá ÿßŸÑŸÄ  autocorrelation    ŸáŸäÿ¨Ÿäÿ® ŸÖÿØŸâ ÿßŸÑÿ™ÿ¥ÿß Ÿá ŸÖÿß  ŸÄŸÄŸäŸÜ ŸÇŸÄŸÄŸäÿßŸÑŸÄŸÄ ŸÄ 
Random variables .ÿπŸÜÿØ ŸÑÿ≠ÿ∏ÿ™ŸäŸÜ ŸÖÿÆÿ™ŸÑŸÅÿ™ŸäŸÜ 
 
ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä  ùë•(ùë°)    ŸÅŸä ÿßŸàŸÑ sample    ŸáŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ÿπŸÜÿØùë°1    Ÿàùë°2   ŸÉÿßŸÑŸáŸÖÿß ŸÖŸàÿ¨ŸÄŸÄÿ® ŸàŸÇŸÄŸÄÿ±Ÿä ŸäŸÜ ŸÖŸÄŸÄŸÜ
  ÿ∂ ŸàŸÅŸä ÿ™ÿßŸÜŸä  sample    ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ÿ≥ÿßŸÑ ŸäŸÜ ŸàŸÇÿ±Ÿä ŸäŸÜ ŸÖŸÜ   ÿ∂ ŸàŸáŸÉÿ∞ÿß ŸÅŸä  ÿßŸÇŸäÿßŸÑŸÄ  samples   ,ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ŸÖŸÜ ŸÉÿØÿß
ÿßŸÜ ŸÇŸäŸÖÿ®  ÿßŸÑŸÄ  autocorrelation    ŸÑŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ùë•(ùë°)  ŸáŸäÿ™ŸÑÿπÿπÿßŸÑŸä  ÿßŸÑŸÜ ÿßŸÑÿ™ÿ¥ÿß Ÿá  ŸäŸÜ ÿßŸÑŸÇŸä  ŸÉ Ÿäÿ± 
ŸÅŸä ÿßŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä  ùë¶(ùë°)   ŸáŸÄŸÄŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ŸÅŸÄŸÄŸä ŸÉŸÄŸÄŸÑ sample  ÿßŸÑŸÇŸÄŸÄŸä  ÿπŸÜŸÄŸÄÿØ ÿßŸÑŸÑÿ≠ÿ∏ÿ™ŸÄŸÄŸäŸÜùë°1  Ÿàùë°2   ŸÇŸÄŸÄŸäŸÖŸá  ŸÖÿÆÿ™ŸÑŸÅŸÄŸÄÿ®
Ÿà  ŸäÿØ  ÿπŸÜ   ÿ∂ Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ  autocorrelation   ŸÑŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ùë¶(ùë°)   ŸáŸäÿ™ŸÑÿπŸÇŸÑŸäŸÑ  
  
 
 
ŸÑŸà ÿπÿßŸäÿ≤ ÿßÿ¨Ÿäÿ® ŸÖ ÿßÿØŸÑÿ® ÿ™  ÿ± ÿπŸÜ ÿßŸÑŸÄ  autocorrelation   ŸàŸÉŸÜÿß ÿßÿÆÿØŸÜÿß ŸÅŸÉÿ±  ŸÇ ŸÑ ŸÉÿØÿß ÿπŸÜ ÿßŸÑŸÇÿßŸÜŸàŸÜ ŸàŸáŸàÿ™ŸÉÿßŸÖŸÑ  
ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ® ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ ÿßŸà ŸÜŸÇÿ™ÿ™ŸäŸÜ  ŸàŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿßŸÑŸÑŸä ŸÅÿßÿ™ ŸáŸà ÿ™ŸÉÿßŸÖŸÑ  ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ® ÿßŸÑŸÜŸÇÿ™ÿ™ŸäŸÜ  ùë°1   Ÿàùë°2  
 
ùëπùíôùíô(ùíïùüè ,ùíïùüê)=ùë¨{ùíô(ùíïùüè) ,ùíô(ùíïùüê)}=ùíô(ùíïùüè) ùíô(ùíïùüê) ÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖÃÖ     
 
Correlation Function of a Random Process  
 
‚ÜêùëÖùë•ùë•  ŸáŸä ÿ±ŸÖÿ≤ ÿßŸÑŸÄŸÄŸÄ  correlation  Ÿàùë•ùë• ŸÖ ŸÜÿßŸáŸÄŸÄÿß ŸÑŸÄŸÄŸÜŸÅÿß ÿßŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä ùë•  ÿ£ŸÑŸÜŸáŸÄŸÄÿß autocorrelation 
 Ÿàÿßÿ≠ŸäÿßŸÜÿß ŸÖŸÖŸÉŸÜ Ÿäÿ¥ŸäŸÑŸáÿß ŸàŸäŸÉÿ™ŸÄŸÄÿ®ùëÖ  ŸÅŸÄŸÄŸÜŸÅŸá  ÿßŸÜŸáŸÄŸÄÿß autocorrelation  ÿßŸÖŸÄŸÄÿß ŸÑŸÄŸÄŸà ŸÉÿßŸÜŸÄŸÄÿ™ cross  correlation   ÿßŸÑÿ≤
ÿ™ÿ™ŸÉÿ™ÿ®  ÿ≤ÿ™ ŸÖÿ´ÿßŸÑ  ùëÖùë•ùë¶  
 
 ÿ™ ÿ±ŸäŸÅÿßŸÑŸÄ correlation  ŸáŸàÿßŸÑŸÄ Expected Value    ŸÑÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ® ŸÇŸäŸÖÿ®ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ÿπŸÜÿØ ÿßŸÑŸÑÿ≠ÿ∏ŸÄŸÄÿ®  
ùë°1  ŸÅŸä ŸÇŸäŸÖÿ® ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä ÿπŸÜÿØ ÿßŸÑŸÑÿ≠ÿ∏ŸÄŸÄÿ®ùë°2  ,Ÿàÿ≤ÿ™ ŸÖŸÄŸÄÿß ÿ™ ŸÑŸÖŸÜŸÄŸÄÿß ÿßŸÜÿßŸÑŸÄŸÄ ŸÄ Expected Value    ŸáŸÄŸÄŸäÿ£ÿµŸÄŸÄÿßŸÑ 
ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™ ÿßŸà  ÿßŸÑŸÄ average  Ÿà ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÖŸÉŸÜ ŸÜŸÇŸÄŸÄŸàŸÑ ÿßŸÑÿ™ ÿ±ŸäŸÄŸÄŸÅ  ÿ™ÿ±ŸäŸÇŸÄŸÄŸáÿ™ÿßŸÜŸäŸÄŸÄÿ® ŸàŸáŸÄŸÄŸä ÿßŸÜ ÿßŸÑŸÄŸÄ ŸÄ correlation  ŸáŸÄŸÄŸä
ŸÖÿ™Ÿàÿ≥ÿ™ ÿ≠ÿßÿµŸÑ ÿßŸÑÿ∂ŸÄŸÄÿ±ÿ® ŸÑŸÑŸÖÿ™ÿ∫ŸäŸÄŸÄÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä ÿπŸÜŸÄŸÄÿØ  ùë°1    Ÿàùë°2   Ÿà ÿ¥ŸÄŸÄŸÉŸÑ ÿπŸÄŸÄÿß    ÿßŸÑŸÄŸÄ ŸÄ  correlation   ŸáŸÄŸÄŸà ŸÖÿ™Ÿàÿ≥ŸÄŸÄÿ™
ÿ≠ÿßÿµŸÑ ÿßŸÑÿ∂ÿ±ÿ®  ŸäŸÜ ÿ≠ÿßÿ¨ÿ™ŸäŸÜ ÿπÿßŸäÿ≤ ÿßÿ¨Ÿäÿ®  ÿßŸÑÿ™ÿ¥ÿß Ÿá ŸÖÿß  ŸäŸÜŸá .  
 
ÿßŸÑŸÉÿßŸÑ   ÿØÿß ŸÖŸÜÿ™ŸÇŸä  ÿßŸÑŸÜ ŸÑŸà ÿ¥ŸàŸÅŸÜÿß ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥ŸÄŸÄŸàÿß Ÿä ùë•(ùë°)  ŸÖŸÄŸÄÿ´ÿßŸÑ Ÿàÿ≠ÿ≥ŸÄŸÄ ŸÜÿß ÿ≠ÿßÿµŸÄŸÄŸÑ ÿßŸÑÿ∂ŸÄŸÄÿ±ÿ® ÿπŸÜŸÄŸÄÿØùë°1  Ÿàùë°2 
  ŸÑŸÉŸÑ sample  Ÿàÿ¨ ŸÜÿß ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ™  ŸáŸäÿ™ŸÑÿπ ÿ±ŸÇ  ŸÉ Ÿäÿ±  ÿßŸÑŸÜ ÿßŸÑŸÇŸä  ŸÇÿ±Ÿä Ÿá ŸÖŸÜ   ÿ∂  ŸàÿßŸÑÿ±ŸÇ   ÿßŸÑŸÉ Ÿäÿ± ÿØÿß  ŸäÿØŸÑ ÿßŸÜ ŸÖÿØŸâ 
ÿßŸÑÿ™ÿ¥ÿß Ÿá  ŸäŸÜ ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ŸÉ Ÿäÿ±. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A process with overall statistical properties  that are independent of time  
 ŸáŸä ÿπŸÖŸÑŸäÿ® ÿÆÿµÿß ÿµŸáÿßÿßÿ•ŸÑÿ≠ÿµÿß Ÿäÿ® ÿßŸÑ ÿ™ ÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿ≤ŸÖŸÜ Ÿä ŸÜŸä ŸÖŸáŸÖÿß ÿ™ÿ∫Ÿäÿ± ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿÆŸàÿß  ÿßÿ•ŸÑÿ≠ÿµÿß Ÿäÿ® 
ÿ´ÿß ÿ™Ÿá.  
 
The random process representing the temperature of a city  is an example of 
non-stationary random process.  The temperature statistics (mean value), for 
example,  depend on the time of the day. 
 ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿßŸÑÿ≥ÿß ŸÇ ŸÑÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±Ÿâ ŸÑŸà ŸÇŸÖŸÜÿß  ŸÇŸäÿßÿß ŸÖÿ™Ÿàÿ≥ÿ™ ÿØÿ±ÿ¨ÿ® ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÅŸäÿ£ŸàŸÇÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ® ŸÅŸä ŸÜŸÅÿß  
ÿßŸÑŸäŸà  ŸáŸÜÿßŸÑŸÇŸä ÿßŸÑŸÇŸä   ÿ™ÿÆÿ™ŸÑŸÅ Ÿàÿ™ÿ≥ŸÖŸâ Ÿáÿ∞  ÿßŸÑ ŸÖŸÑŸäÿ® non-stationary random process . 
 
 ŸÅŸäŸá ÿ≠ÿßŸÑÿ® ÿÆÿßÿµÿ® ŸÖŸÜŸáÿßŸà ÿßŸÑŸÑŸä ŸáŸÜÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸäŸáÿß ŸÅŸä ÿßŸÑŸÖÿ≥ÿß ŸÑ ŸàŸáŸä Wide -Sense Stationary Process  
 
 
 
 
 
A process in which only the mean and autocorrelation function are independent 
of time.  
 ŸáŸä ÿ≠ÿßŸÑÿ® ÿÆÿßÿµÿ® ŸÖŸÜ Stationary Process  ŸÅŸäŸáŸÄŸÄÿßÿßŸÑŸÄŸÄ ŸÄ  mean  ŸàÿßŸÑŸÄŸÄ ŸÄ autocorrelation function  ŸÅŸÇŸÄŸÄÿ™ÿßŸÑ 
Ÿä ÿ™ŸÖÿØŸàÿß ÿπŸÑŸâ ÿßŸÑÿ≤ŸÖŸÜ Ÿà ÿßŸÑÿ™ÿßŸÑŸä ŸÖÿ™Ÿàÿ≥ÿ™ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± ÿßŸÑ ÿ¥Ÿàÿß Ÿä  ùë•(ùë°)   Ÿäÿ≥ÿßŸàÿ™ constant 
ùíô(ùíï)ÃÖÃÖÃÖÃÖÃÖÃÖ=ùêúùê®ùêßùê¨ùê≠ùêöùêßùê≠  
 
autocorrelation function  ÿßŸÑ ÿ™ ÿ™ŸÖÿØ ÿπŸÑŸâ ŸÖŸÉÿßŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÑÿ≠ÿ∏ÿ™ŸäŸÜùë°1  Ÿàùë°2 ŸÅŸäŸÜ ŸàŸÑŸÉŸÜ ÿ™ ÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑŸÅŸÄŸÄÿ±  
ÿßŸÑÿ≤ŸÖŸÜŸä   ŸäŸÜŸá  ŸÅŸÇÿ™  ŸàÿßŸÑ ÿ™ ÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿ≤ŸÖŸÜ ŸÜŸÅÿ≥Ÿá Ÿàÿ™ÿßŸÑŸÖŸÄŸÄÿß ÿßŸÑŸÅŸÄŸÄÿ±  ÿ´ÿß ŸÄŸÄÿ™ Ÿáÿ™ÿ™ŸÑŸÄŸÄÿπ ŸÇŸäŸÖŸÄŸÄÿ® ÿßŸÑŸÄŸÄ ŸÄ autocorrelation 
  ÿ´ÿß ÿ™Ÿá Ÿà ÿßŸÑÿ™ÿßŸÑŸä ŸáŸäŸÉŸàŸÜ function  ŸÅŸä ÿßŸÑŸÅÿ±  ŸÖÿß  ŸäŸÜŸá  ŸÅŸÇÿ™ùùâ 
  Ÿà ÿßŸÑÿ™ÿßŸÑŸä ŸáŸÜ ÿØŸÑ ÿßŸÑŸÇÿßŸÜŸàŸÜ ŸàŸÜÿ¥ŸäŸÑùíïùüè    ŸàŸÜŸÉÿ™ Ÿáÿßùíï   ŸàŸÜÿ¥ŸäŸÑùíïùüê   ŸàŸÜŸÉÿ™ Ÿáÿß ùíï+ùùâ Ÿàÿ™ŸÉŸàŸÜ  ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
 
ùëπùíôùíô(ùíïùüè ,ùíïùüê)=ùëπùíôùíô( ùíïùüê‚àíùíïùüè)=ùëπùíôùíô(ùùâ)    ,   ùùâ= ùíïùüê‚àíùíïùüè 
ùëπùíôùíô(ùùâ)=ùë¨{ùíô(ùíï) ‚àô  ùíô(ùíï+ùùâ)} 
 
 
 Stationary Process  
Wide -Sense Stationary Process  
 
ÿßŸä ÿßŸÑŸÑŸä ŸÖŸÖŸÉŸÜ ŸÜÿ≥ÿ™ŸÅÿßÿØŸá ŸÖŸÜ ÿßŸÑÿ™ autocorrelation  ÿü 
1. Frequency content  of the process  
The frequency content of a process depends on the  rapidity of the amplitude change 
with time.  This can be measured by correlating amplitudes at  
ùë° and ùë°+ ùúè. The process ùë•(ùë°) is a slowly varying process  compared to the process 
ùë¶(ùë°) . 
Thus, the autocorrelation function provides valuable  information about the frequency 
content of the  process.  
ÿ≠ÿ≥ÿß ŸÜÿß ŸÑŸÑŸÄ autocorrelation function  ÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÖ ŸäŸÜŸá ŸÖŸÖŸÉŸÜ ŸÜ ÿ±ŸÅ ŸÖŸÜŸá ŸÖÿ≠ÿ™ŸàŸâÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÖŸÜ ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ ,ŸÑŸÄŸÄŸà 
ŸÇŸäŸÖÿ®  ÿßŸÑŸÄ  correlation    ÿπÿßŸÑŸäÿ® ŸÖÿß  ŸäŸÜ ÿßŸÑŸÜŸÇÿ™ ÿØÿß ŸÖ ŸÜÿß  ÿßŸÜ ÿßŸÑÿ™ÿ∫Ÿäÿ± ŸÅŸä ÿßÿßŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿßŸÑŸÑŸÄŸÄŸä  Ÿäÿ≠ÿµŸÄŸÄŸÑ  ŸÄŸÄŸäŸÜ ÿßŸÑŸÜŸÇÿ™ÿ™ŸÄŸÄŸäŸÜ ÿØŸàŸÑ
 ÿ™ÿ¶ Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ™ŸÄŸÄÿ±ÿØÿØÿßÿ™ ŸÇŸÑŸäŸÑŸÄŸÄŸá, ŸàÿßŸÑ ŸÉŸÄŸÄÿß ŸÑŸÄŸÄŸà ŸÇŸäŸÖŸÄŸÄÿ® ÿßŸÑŸÄŸÄ ŸÄ correlation  ŸÇŸÑŸäŸÑŸÄŸÄŸá ÿØÿß ŸÖ ŸÜŸÄŸÄÿß  ÿßŸÜ ÿßŸÑÿ™ÿ∫ŸäŸäŸÄŸÄÿ±ÿßÿ™  ÿ™ÿ≠ÿµŸÄŸÄŸÑ
 ÿ≥ÿ±ÿπÿ® Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ™ÿ±ÿØÿØ ÿπÿßŸÑŸä. 
 
2. Average power of the signal  
We have a way of finding the average power of the  output of a system when the 
input is a stochastic  process.  
value at ùúè=0 is equal to the average power of the signal  
 
ŸÖŸÖŸÉŸÜ ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸÄ power  ÿ™ÿßÿπ ÿßŸÑŸÄ system  ŸÖŸÜ ÿÆÿßŸÑŸÑÿßŸÑÿ™ ŸàŸäÿ∂    ŸÄ ùúè=0   ŸÅŸä ŸÇÿßŸÜŸàŸÜÿßŸÑŸÄ autocorrelation 
ùëÖùë•ùë•(ùúè=0)=ùê∏{ùë•(ùë°) ,ùë•(ùë°)}=ùê∏{ùë•(ùë°)2}=‚à´ ùë•(ùë°)2 ùëù(ùë•) ùëëùë•‚àû
‚àí‚àû   
 
ÿßŸÑŸÄ  Expected ŸáŸä ÿßŸÑŸÄ average  Ÿà ùê∏{ùë•(ùë°)2}  ŸÖ ŸÜÿßŸáŸÄŸÄÿß average  ÿßŸÑÿØÿßŸÑŸÄŸÄÿ® ÿ™ÿ± ŸäŸÄŸÄÿπ ŸàŸáŸÄŸÄŸà ÿØÿß ÿßŸÑ ŸÄŸÄÿßŸàÿ±  ÿ™ŸÄŸÄÿßÿπ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© 
 
ŸÖŸÑÿ≠Ÿàÿ∏ÿ© :  ÿßŸÑŸÄ  Energy     ÿ™ÿßÿπÿ®ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸáŸä ŸÖŸÇŸäÿßÿß ŸÑŸÑÿ™  Ÿäÿ± ÿπŸÜ ÿ≠ÿ¨ŸÄŸÄ   ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ©  ,Ÿàÿ≠ÿ¨ŸÄŸÄ  ÿßÿßŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿπ ŸÄŸÄÿßÿ±ÿ© 
ÿπŸÜ  amplitude    Ÿà duration    ŸàŸÑŸà ŸÉÿßŸÜÿ™ ÿßÿßŸÑÿ¥ÿßÿ±ÿ© ŸÑŸäŸáŸÄŸÄÿß amplitude    Ÿà duration  ŸÉ ŸäŸÄŸÄÿ± ÿßÿ∞ÿß ÿßŸÑŸÄŸÄ ŸÄ  Energy 
 ÿ™ÿßÿπÿ™Ÿáÿß  ÿπÿßŸÑŸäÿ® Ÿà ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ power ÿπÿßŸÑŸä 
 
 
 
 
 
 
 
 
 
 
you are given
 
a statistic process 
ùíô
(
ùíï
)
 
with mean value 
ùíé
ùíô
 
and autocorrelation 
function 
ùëπ
(
ùùâ
)
. obviously ,the process is at least wide
-
sense stationary. If not, 
the mean and  autocorrelation could not have been given in 
this
 
form. Find the 
mean and autocorrelation of a process 
ùíö
(
ùíï
)
, where 
 
ùíö
(
ùíï
)
=
ùíô
(
ùíï
)
‚àí
ùíô
(
ùíï
‚àí
ùëª
)
 
Solution
 
Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿ™ŸàÿµŸäŸÅ ÿßŸÜ ÿßŸÑ
ŸÄ
 
process
 
 ŸáŸä
Wide
-
Sense Stationary
 
 ÿßŸÑŸÜ
ÿßŸÑ
ŸÄ
 
mean
 
  ŸÖ ÿ™Ÿâ ŸÇŸäŸÖÿ® ÿ´ÿß ÿ™Ÿá
Ÿà 
ÿßŸÑ
ŸÄ
 
autocorrelation
 
  ÿØÿßŸÑÿ® ŸÅŸä
ùùâ
 
 
1)
 
Mean
 
ùíé
ùíö
=
ùë¨
{
ùíö
(
ùíï
)
}
=
ùë¨
{
ùíô
(
ùíï
)
‚àí
ùíô
(
ùíï
‚àí
ùëª
)
}
 
=
ùë¨
{
ùíô
(
ùíï
)
}
‚àí
ùë¨
{
ùíô
(
ùíï
‚àí
ùëª
)
}
=
ùíé
ùíô
‚àí
ùíé
ùíô
=
ùüé
 
 
 
ùê∏
{
ùë•
(
ùë°
)
}
  Ÿà
ùê∏
{
ùë•
(
ùë°
‚àí
ùëá
)
}
 
  ŸÉÿßŸÑŸáŸÖÿß Ÿäÿ≥ÿßŸàÿ™
ùëö
ùë•
 
 ÿßŸÑŸÜ ÿßŸÑ ŸÖŸÑŸäŸá ŸáŸÜÿß
Wide
-
Sense Stationary
 
  ŸÅŸÖŸáŸÖÿß
ÿß
ÿ∫Ÿäÿ± ÿßŸÑŸÑÿ≠ÿ∏ÿ® ÿßŸÑŸÑŸä  ÿ≠ÿ≥ÿ® ÿπŸÜÿØŸáÿß 
ÿßŸÑ
ŸÄ
 
mean
 
 ŸÇŸäŸÖÿ™Ÿá ÿ´ÿß ÿ™Ÿá
 
 
2)
 
Autocorrelation
 
 
ùëπ
ùíöùíö
(
ùùâ
)
=
ùë¨
{
ùíö
(
ùíï
)
 
ùíö
(
ùíï
+
ùùâ
)
}
 
=
ùë¨
{
[
ùíô
(
ùíï
)
‚àí
ùíô
(
ùíï
‚àí
ùëª
)
]
 
[
ùíô
(
ùíï
+
ùùâ
)
‚àí
ùíô
(
ùíï
‚àí
ùëª
+
ùùâ
)
]
}
 
=
ùë¨
{
ùíô
(
ùíï
)
 
ùíô
(
ùíï
+
ùùâ
)
}
‚àí
ùë¨
{
ùíô
(
ùíï
)
 
ùíô
(
ùíï
‚àí
ùëª
+
ùùâ
)
}
 
‚àí
ùë¨
{
ùíô
(
ùíï
‚àí
ùëª
)
 
ùíô
(
ùíï
+
ùùâ
)
}
+
 
ùë¨
{
ùíô
(
ùíï
‚àí
ùëª
)
 
ùíô
(
ùíï
‚àí
ùëª
+
ùùâ
)
}
 
 
Remember that 
for 
Wide
-
Sense Stationary
: 
ùë¨
{
ùíô
(
ùíï
ùüè
)
 
ùíô
(
ùíï
ùüê
)
}
=
ùëπ
ùíôùíô
(
 
ùíï
ùüê
‚àí
ùíï
ùüè
)
=
ùëπ
ùíôùíô
(
ùùâ
)
 
 
 
 
=
ùëπ
ùíôùíô
(
 
(
ùíï
+
ùùâ
)
‚àí
(
ùíï
)
)
‚àí
ùëπ
ùíôùíô
(
(
ùíï
‚àí
ùëª
+
ùùâ
)
‚àí
(
ùíï
)
)
‚àí
 
ùëπ
ùíôùíô
(
(
ùíï
+
ùùâ
)
‚àí
(
ùíï
‚àí
ùëª
)
)
+
ùëπ
ùíôùíô
(
(
ùíï
‚àí
ùëª
+
ùùâ
)
‚àí
(
ùíï
‚àí
ùëª
)
)
 
 
 
ùëπ
ùíôùíô
(
ùùâ
)
‚àí
ùëπ
ùíôùíô
(
ùùâ
‚àí
ùëª
)
‚àí
ùëπ
ùíôùíô
(
ùùâ
+
ùëª
)
+
ùëπ
ùíôùíô
(
ùùâ
)
 
=
ùüê
ùëπ
ùíôùíô
(
ùùâ
)
‚àí
ùëπ
ùíôùíô
(
ùùâ
‚àí
ùëª
)
‚àí
ùëπ
ùíôùíô
(
ùùâ
+
ùëª
)
 
Example
 
4
 
Error Detection and Correction 
Linear Block Coding
Algebraic Codes
‚Ä¢We now investigate  an 
organized  technique  for 
formulating  code  words  and 
recovering  the original  words  
and identifying  errors  at the 
receiver . 
‚Ä¢Consider  the single  parity  
check  code : 

Algebraic Codes
‚Ä¢The coding  process  can be described  as an addition  of 
a fourth  bit such that the total number  of ones is even . 
‚Ä¢We now generalize  this type of coding . Suppose  that 
the message  words  consist  of ùëö bits. 
‚Ä¢We have  2ùëö distinct  message  words . 
‚Ä¢Consider  code  words  that add ùëõ parity  bits to the ùëö 
message  bits to end up with code  words  of length
    ùëö+ùëõ
Algebraic Codes
‚Ä¢The code word will be of the form:
‚Ä¢Each check bit is chosen to achieve even parity when 
combined with specific message bits. 
‚Ä¢Matrix skills and Boolean algebra permit us to express 
this relationship in general format.

Algebraic Codes
‚Ä¢The check bits are chosen to satisfy the following:

Algebraic Codes
‚Ä¢Note  that the ùëõùë•ùëõ matrix  formed  by partitioning  the 
right  part of ùêª is an identity  matrix . 
‚Ä¢If the number  of ones is even,  the sum is zero:

Example

Example

Decoding at the receiver
‚Ä¢The receiver  forms  the product  of the received  word  
with the matrix  ùêª.
‚Ä¢If the product  is not equal  to zero,  the receiver  knows  
that at least one error  was made  during  transmission,  
and the received  word  is not one of the acceptable  
code  words . 
‚Ä¢The value  of algebraic  coding  arises  in multiple  error  
detection  and in error  correction . 
Decoding at the receiver
‚Ä¢We define  an error  vector  which  contains  a ‚Äú1‚Äù in 
each bit position  in which  an error  occurs . The 
received  vector  is therefore  of the form :
‚Ä¢Error  detection  is possible  if the error  vector  can be 
isolated  at the  receiver .

Decoding at the receiver
‚Ä¢If we multiply  the received  vector  by ùêª, we have
‚Ä¢The syndrome  characterizes  the specific  bit error . 
‚Ä¢The result  is a vector  that is identical  to one column  of 
ùêª, that column  being  the one corresponding  to the bit 
position  in error . 

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner

CamScanner


Chapter 2 
Source Coding 
Quantization Noise: Uniform Quantization
‚Ä¢Thequantization noise isdefined as:
whereùëì(ùëõùëáùë†)istheoriginal sample value andùëìùëû(ùëõùëáùë†)is
thequantized sample value .
‚Ä¢The amplitude range ofthesignal isdivided into L
uniformly spaced intervals, each ofwidth‚àÜùë†

Quantization Noise: Uniform Quantization
‚Ä¢Themaximum quantization error is‚àí+‚àáùë†
2.
‚Ä¢Thequantization error liesintherange (-ùõªùë†
2,+ùõªùë†
2).
‚Ä¢Assuming that the error isequally likely tolie
anywhere inthisrange .
‚Ä¢Themean squared quantization error isgiven by
Quantization Noise: Uniform Quantization

Non-Uniform Quantization
‚Ä¢The most common form ofnon-uniform quantization
isknown ascompanding .
‚Ä¢Theuniform quantization provides thesame resolution
level athigh levels asatlow.

Non-Uniform Quantization
‚Ä¢Forsome signals, likevoice signals, itisdesirable touse
small quantization steps atthelower levels, and larger
steps atthehigher levels .

Non-Uniform Quantization
‚Ä¢The average quantization error may well decease using
thisapproach .
‚Ä¢Prior toquantization, thesignal iscompressed .
‚Ä¢The operation compresses the extreme values while
enhancing thesmall values ofthesignal .
Non-Uniform Quantization

Non-Uniform Quantization
‚Ä¢Theanalog signal forms theinput tothecompressor, and
theoutput isuniformly quantized .
‚Ä¢Theresult isequivalent toquantizing with steps thatstart
outsmall andgetlarger with higher signal values .
‚Ä¢Atthereceiver, expansion isapplied sothat theoverall
transmission isnotdistorted .
Non-Uniform Quantization
‚Ä¢The processing pair (compression and expansion) is 
called companding . 

Non-Uniform Quantization
‚Ä¢The most common application ofcompanding isin
voice transmission .

Non-Uniform Quantization
‚Ä¢North America and Japan have adopted astandard
compression curve known asùùª-lowcompanding .
‚Ä¢Europe hasadopted another standard known asA-law
companding .

Random Process
Definition of a Random Process
‚Ä¢Arandom orstochastic process isacollection of
infinite number ofsample functions together with
associated statistical properties .
‚Ä¢This infinite number ofsamples form theensemble .
‚Ä¢Forevery specific value oftimeùë°=ùë°ùëú,ùë•(ùë°0)isa
random variable .
Example: Random Process to represent the 
temperature of a city

Correlation Function of a Random Process

Correlation Function of a Random Process
‚Ä¢For xùë°,the amplitudes at ùë°1and  ùë°1+ùúèare similar, 
that is have stronger correlation. 
ùëÖùë•ùë•ùë°1,ùë°2=ùê∏{ùë•ùë°1ùë•ùë°2}=ùë•ùë°1ùë•ùë°2
‚Ä¢The correlation iscomputed by multiplying
amplitudes atùë°1andùë°2ofasample function andthen
averaging thisproduct over theensemble .
Stationary Process
‚Ä¢Aprocess with overall statistics that areindependent of
time.
‚Ä¢Therandom process representing thetemperature ofacity
isanexample ofnon-stationary random process .
‚Ä¢The temperature statistics (mean value), forexample,
depend onthetime oftheday.
Wide -Sense Stationary Process
‚Ä¢Aprocess inwhich only themean andautocorrelation
function areindependent oftime.
ùë•ùë°=constant
ùëÖùë•ùë•ùë°1,ùë°2=ùëÖùë•ùë•(ùúè),ùúè=ùë°2‚àíùë°1
ùëÖùë•ùë•ùë°1,ùë°2=ùëÖùë•ùë•(ùë°2-ùë°1)=ùê∏ùë•ùë°1ùë•ùë°2
ùëÖùë•ùë•(ùë°,ùë°+ùúè)=ùëÖùë•ùë•(ùúè)=ùê∏ùë•ùë°ùë•ùë°+ùúè
Wide -Sense Stationary Process
‚Ä¢ùëÖùë•ùë•(ùë°,ùë°+ùúè)does notdepend ontheactual values of
ùë°1andùë°2,butonly upon thedifference .
‚Ä¢One ofthemost important characteristics ofarandom
process isitsautocorrelation function, which leads to
thespectral information oftherandom process .
‚Ä¢The frequency content ofaprocess depends onthe
rapidity oftheamplitude change with time.
Wide -Sense Stationary Process
‚Ä¢This can bemeasured bycorrelating amplitudes at
ùë°andùë°+ùúè.
‚Ä¢The process xùë° isaslowly varying process
compared totheprocessyùë°.
‚Ä¢Thus, theautocorrelation function provides valuable
information about the frequency content ofthe
process .
Wide -Sense Stationary Process
‚Ä¢The Power Spectral Density (PSD) ofxùë°isthe
Fourier transform oftheautocorrelation function .
ùê∫ùëì=ùêπùëÖùë°= 
‚àí‚àû‚àû
ùëÖ(ùë°)ùëí‚àíùëó2ùúãùëìùë°ùëëùë°
Whereùê∫ùëìisthePSD.
ùëÖùë°= 
‚àí‚àû‚àû
ùê∫ùëìùëëùëì=ùëÖ(0)
Wide -Sense Stationary Process
ùëÖùë•ùë•(ùúè)=ùê∏ùë•ùë°ùë•ùë°+ùúè
ùëÖ0=ùê∏{ùë•2(ùë°)}
‚Ä¢Theobservation thatùëÖ0istheaverage power makes
agreat deal ofsense .
‚Ä¢Wehave away offinding theaverage power ofthe
output ofasystem when theinput isastochastic
process .
Difference Between Correlation and  Convolution
‚Ä¢Correlation isameasurement ofthesimilarity between two
signals/sequences .
‚Ä¢Convolution isameasurement oftheeffect ofonesignal onthe
other signal .
‚Ä¢Convolution isthe common operation alinear and time
invariant system canperform onagiven input signal .
‚Ä¢Inconvolution, there issome input -output relationship, sothis
actslikeafiltering operation .
Lecture 1
Angle Modulation
References
‚Ä¢Modern Digital and Analog Communication Systems
B.P .Lathi
‚Ä¢Communication Systems
Simon Haykin
Amplitude Modulation


Frequency Modulation (FM)

ùëìùëöùë°=ùëéùëêùëúùë†2 ùúãùëìùëöùë°
ùëìùëêùë°=ùê¥ùëêùëúùë†(2ùúãùëìùëêùë°+ùúô)
note:                                     ùëìùë†ùë°=ùê¥ùëêùëúùë† ùúÉùë°
ùëìùëñ=·à∂ùúÉ
2ùúã
·à∂ùúÉ=2ùúãùëìùëñ
ùúÉùë°=2ùúã◊¨0ùë°ùëìùëñùëëùë°
ùëìùëñ=ùëìùëê+ùëòùëìùëìùëöùë°
‚à¥ùúÉùë°=2ùúãùëìùëêùë°+ùëòùëì◊¨0ùë°ùëìùëöùë°ùëëùë°
‚à¥ùëìùë†ùë°=ùê¥ùëêùëúùë† 2ùúãùëìùëêùë°+2ùúãùëòùëì◊¨0ùë°ùëìùëöùë°ùëëùë°ùëêùëúùë†2 ùúãùëìùë°
          ùúÉùë°=2ùúãùëì
·à∂ùúÉ
2ùúã= ùëì
‚Ä¢The frequency of ùëìùë†ùë° varies from ùëìùëê+ùëòùëì(min ùëúùëìùëìùëöùë° )
                                                          to   ùëìùëê+ùëòùëì(mùëéùë•ùëúùëìùëìùëöùë° )
                                        If ùëìùëöùë°=ùëé ùëêùëúùë†2 ùúãùëìùëöùë°
 ‚à¥ùëéùëòùëì=‚àÜùëìùëê                           Maximum Frequency Deviation
The maximum change in carrier frequency
‚Ä¢The modulation index is defined as
ùõΩ=‚àÜùëìùëê
ùëìùëö
‚Ä¢For sinusoidal signals
                 ‚à¥ ùëìùë†ùë°=ùê¥ùëêùëúùë† 2ùúãùëìùëêùë°+2ùúãùëòùëì
2ùúãùëìùëöùëé ùë†ùëñùëõ2ùúãùëìùëöùë° 
 ‚à¥ ùëìùë†ùë°=ùê¥ùëêùëúùë† 2ùúãùëìùëêùë°+ùõΩùë†ùëñùëõ2ùúãùëìùëöùë° ùëìùëöùë°=ùëé ùëêùëúùë†2 ùúãùëìùëöùë°
Phase Modulation PM
ùëìùëöùë°=ùëé ùëêùëúùë†2 ùúãùëìùëöùë°
                                       ùëìùëêùë°=ùê¥ ùëêùëúùë†(2ùúãùëìùëêùë°+ùúô)
                                            ùëìùë†ùë°=ùê¥ùëêùëúùë† ùúÉùë°
 ùúÉùë°= 2ùúãùëìùëêùë°+ùëòùëùùëìùëöùë°
 ‚à¥ ùëìùë†ùë°=ùê¥ùëêùëúùë† 2ùúãùëìùëêùë°+ùëòùëùùëìùëöùë°
 ùëìùëñ=·à∂ùúÉ
2ùúã=ùëìùëê+ùëòùëù
2ùúãùëë
ùëëùë°ùëìùëöùë°
‚Ä¢For sinusoidal signals
ùëìùë†ùë°=ùê¥ùëêùëúùë† 2ùúãùëìùëêùë°+ùëéùëòùëùùëêùëúùë†2 ùúãùëìùëöùë° 
 ùëìùëñ=·à∂ùúÉ
2ùúã=ùëìùëê+ùëòùëù
2ùúãùëé2ùúã ùëìùëö(‚àíùë†ùëñùëõ 2ùúãùëìùëöùë° )
 ùëìùëñ=ùëìùëê‚àíùëéùëòùëùùëìùëö ùë†ùëñùëõ 2ùúãùëìùëöùë° 
Chapter 2 
Source Coding 
Sampling Theory
‚Ä¢States that, iftheFourier transform ofatime function is
zero forùëì>ùëìùëö,andthevalues ofthetime function are
known forùë°=ùëõùëáùë†,forallinteger values ofùëõ,then the
time function isknown forallvalues ofùë°provided that
thesamples areclose enough together .
‚Ä¢Therestriction isthat
ùëìùë†‚â•2ùëìùëö
whereùëìùë†isthesampling frequency .
Pulse Code Modulation (PCM)
‚Ä¢Isatechnique forrounding -offtheamplitudes ofa
waveform .
‚Ä¢This isthe second operation after the sampling
process .
‚Ä¢The rounding -offoperation isknown asquantization,
and theround -offerror isknown asquantization
noise .
Pulse Code Modulation (PCM)
‚Ä¢PCM codes thevarious levels into binary numbers, and
sends thebinary code corresponding tothe particular
round -offlevel .

PCM Modulators
‚Ä¢APCM modulator isnothing more than ananalog -to-
digital converter .
‚Ä¢The converter first samples thewaveform, and then
quantizes each sample value .
‚Ä¢There arethree generic forms forthequantizer :
ÔÉòCounting Quantizer
ÔÉòSerail Quantizer
ÔÉòParallel quantizer
Counting Quantizer

Counting Quantizer

Counting Quantizer
‚Ä¢Theramp generator starts ateach sampling point, and
abinary counter issimultaneously started .
‚Ä¢The time duration oftheramp, and therefore the
duration ofthecount isproportional tothesample
value (ramp slope isconstant) .
‚Ä¢The ramp slope must besufficient toreach the
maximum possible values within onesampling period .
Counting Quantizer
‚Ä¢The clock frequency issuch thatthecounter hasenough
time tocount toitshighest count foraramp duration
corresponding tothemaximum possible sample .
‚Ä¢Theending counts onthecounter willcorrespond binary
equivalent value oftheinput .
Serial Quantizer

‚Ä¢Theserial quantizer successively divides theordinates intotworegions .
‚Ä¢Itfirstdivides theaxisinhalf, andobserves whether thesample isinthe
upper orlower half.
‚Ä¢The result ofthisobservation generates themost significant bitinthe
code word .
Serial 3 -bit quantizer
‚Ä¢The half region inwhich thesample liesisthen subdivided into two
regions, andacomparison isagain performed .This generates thenext
bit.
‚Ä¢Theprocess continues anumber oftimes equal tothenumber ofbitsof
encoding .
‚Ä¢Thefigure isshown for3-bitcode words andforarange ofinput values
between 0and1volt.
‚Ä¢Iftheinput range ofsignal sample values were not0to1,thesignal
could benormalized toachieve values within thisrange .Serial 3 -bit quantizer
‚Ä¢Ifmore orfewer bits are required, the appropriate
comparison blocks canbeadded orremoved .
‚Ä¢Note :
Value greater than1
2-1
2>1
4
Value greater than1
2>=3
4=6
8
Value greater than6
8-1
4-1
2>1
8
Value greater than6
8>1
8+1
2+1
4>7
8Serial 3 -bit quantizer
Parallel Quantizer
3-bit Parallel Encoder

‚Ä¢The parallel quantizer isthefastest inoperation, since it
develops allbitsofthecode words simultaneously .
‚Ä¢Itisalso the most complex, requiring anumber of
comparators thatisonly onelessthan thenumber oflevels
ofquantization .
‚Ä¢Theblock labeled‚Äúcoder‚Äù observes theoutput oftheseven
comparators .Parallel Quantizer
‚Ä¢Ifallseven outputs are1(yes), thecoder output is111,
since thesample value hadtobegreater than 7/8.
‚Ä¢Ifcomparator outputs 1through 6are1,andoutput 7is0,
thecoder output is110,since thesample hadtobebetween
6/8and7/8.
‚Ä¢We continue through alllevels and finally, ifall
comparator outputs arelow, thesample hadtobelessthan
1/8,sothecoder output is000.Parallel Quantizer
 
 
 
 
 
 
¬© Basem Hesham Lecture 
2
 
 
Digital System & Fourier Transform
 
 
 
 ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿØÿß ŸáŸÜÿßÿÆÿØ overview  ÿπŸÜÿ£Ÿä ŸÜÿ∏ÿßŸÖ ÿ±ŸÇŸÖŸä Digital System  ÿ®ÿ¥ŸÉŸÑ ÿπÿßŸÖ ÿ®Ÿäÿ™ŸÉŸàŸÜ ŸÖŸÜÿ£Ÿä ŸàŸÅŸäŸá ŸÉÿßŸÑŸÖ ŸÉÿ™Ÿäÿ±ÿ±ÿ± 
ŸÖŸÜ ÿßŸÑŸÑŸä ÿßÿ™ÿ¥ÿ±ÿ≠ ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ŸáŸäÿ™ŸÉÿ±ÿ± ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿØŸä ÿ™ÿßŸÜŸä ÿ®ÿ≥ ÿ®ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÉÿ™ÿ±. 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÑŸä ÿ®ŸÜÿ®ÿØÿ£ ÿ®ŸäŸáÿß ÿßÿµŸÑŸáÿß analog ŸàŸÅŸä ŸÉŸàÿ±ÿ≥ ÿßŸÑÿ± analog communication  ÿßŸÑÿ≥ŸÜÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ŸÉŸÜÿ±ÿ±ÿß ÿ®ŸÜÿ™ŸÖÿßŸÖÿ±ÿ±ŸÑ
ŸÖÿ±ÿ±  ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿßÿ™ ÿ≤Ÿä ŸÖÿ±ÿ±ÿß Ÿáÿ±ÿ±Ÿä  analog  ŸàÿØÿ±ÿ≥ÿ±ÿ±ŸÜÿß ÿßŸÑÿ±ÿ±ÿ± modulation techniques  ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ±ÿ±ÿ© ÿ≤ŸäAM  ŸàFM  ŸàPM 
   ,ÿßŸÑÿ≥ŸÜÿ© ÿØŸä ŸáŸÜÿ™ŸÖÿßŸÖŸÑ ŸÖÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© digital   ŸàŸáŸÜÿ™ŸÖŸÑŸÖÿ£Ÿä ÿßŸÑŸÖŸÖŸäÿ≤ÿßÿ™ ÿßŸÑŸÑŸä ÿÆŸÑÿ™ŸÜÿß ŸÜÿ™ŸÖÿßŸÖŸÑ ŸÖ  ÿßŸÑÿ± Digital System. 
 
  ÿ≤Ÿä ŸÖÿß ŸÇŸàŸÑŸÜÿßÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸÅŸä ÿßÿµŸÑŸáÿß  analog    ÿ≤Ÿäÿ•ÿ¥ÿßÿ±ÿ©  ÿßŸÑÿµŸàÿ™  ŸÖÿ´ÿßŸÑÿå  ŸàŸÑŸÉŸÜ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÑŸä ÿ®ÿ®ŸÖÿ™Ÿáÿß ŸÖŸÜ ÿßŸÑŸÖŸàÿ®ÿßŸäÿ±ÿ±ŸÑ  ŸÖÿ±ÿ±ÿ´ÿßŸÑ 
ÿ®ÿ™ŸÉŸàŸÜ  digital   ŸàŸáŸÜÿ™ŸÖŸÑŸÖ ÿßŸÑŸÜŸáÿßÿ±ÿØŸáÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿØŸä ÿ™ÿ≠ŸàŸÑÿ™ ÿßÿ≤ÿßŸä ŸàÿßŸä ÿßŸÑŸÖÿ±ÿßÿ≠ŸÑ ÿßŸÑŸÑŸä ŸÖÿ±ÿ™ ÿπŸÑŸäŸáÿß ŸÑÿ≠ÿØ ŸÖÿß ÿßÿ≥ÿ™ŸÇÿ®ŸÑŸÜÿßŸáÿß. 
 
 
 
 
 
 
 
 
 
 
ŸÉŸÑ  block    ŸÖŸÜ ÿßŸÑŸÑŸä ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ© ÿπŸÑŸäŸáÿß chapter   ŸÖŸÅÿµŸÑ ŸÅŸä ÿßŸÑŸÖŸÜŸáÿ¨ , ŸáŸÜÿßÿÆÿ±ÿ±ÿØ ŸÅÿ±ÿ±Ÿä ŸÖÿ≠ÿßÿ∂ÿ±ÿ±ÿ±ÿ© ÿßŸÑŸÜŸáÿ±ÿ±ÿßÿ±ÿØÿ© overview 
ÿπŸÖŸàŸÖÿß ÿπŸÜ ÿßŸÑÿ± digital system  ÿ®Ÿäÿ™ŸÉŸàŸÜ ŸÖŸÜÿ£Ÿä Ÿàÿ®ÿ±ÿØŸà ŸáŸÜÿßÿÆÿØ ŸÅŸÉÿ±ÿ© ÿπŸÜ ÿßŸÑŸÖŸÜŸáÿ¨ ÿßŸÑŸÑŸä ŸáŸÜÿØÿ±ÿ≥ÿ±ÿ±Ÿá ŸàŸÅÿ±ÿ±Ÿä ŸÜŸÅÿ±ÿ±ÿ≥ ÿßŸÑŸàŸÇÿ±ÿ±ÿ™ 
ŸáŸä ŸÖÿ¥ ŸÖŸÇÿØŸÖÿ© ÿØÿß ÿ≥ÿ§ÿßŸÑ Ÿàÿßÿ±ÿØ Ÿäÿ¨Ÿä ŸÅŸä ÿßÿßŸÑŸÖÿ™ÿ≠ÿßŸÜ ŸÉŸÜÿ∏ÿ±Ÿä ÿ≤Ÿä ÿ±ÿ≥ŸÖ ÿßŸÑÿ± block diagram  ÿßŸà ÿ£ÿ¨ÿ≤ÿßÿ° ŸÖŸÜŸá ÿßŸà ÿ¥ÿ±ÿ±ÿ±ÿ≠
ÿ®ŸÖÿ∂ ÿßÿ£ŸÑÿ¨ÿ≤ÿßÿ°. 
 
 
 
ÿßŸÑÿ± Source Encoder   ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ŸáŸà ÿßŸÑÿØÿßŸäÿ±ÿ© ÿßŸÑŸÑŸä ÿ®ÿ™ÿ≠ŸàŸÑ ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßŸÑŸÖÿ™ÿµÿ±ÿ±ŸÑÿ© ŸÑŸÑŸÖŸÉÿ±ÿ±ÿßŸÅÿßŸÑÿ±ÿ± ÿ± binary  ÿ®ÿ™ÿßÿπŸáÿ±ÿ±ÿß ŸàÿßŸÑŸÑÿ±ÿ±Ÿä
ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸáÿß  analog to digital converter (ADC)   
ŸÅŸäŸá ÿ¥ÿßÿ®ÿ™ÿ± ŸÉÿßŸÖŸÑ ŸáŸÜÿßÿÆÿØŸá ÿπŸÑŸâ ÿßŸÑÿ± Source Encoder  ŸáŸÜÿ™ŸÖÿ±ŸÅ ÿπŸÑŸâ ÿ®ŸÖÿ∂ ÿØŸàÿßŸäÿ± ADC  ŸàŸÜŸÖÿ±ŸÅ ÿ®ÿ™ÿ¥ÿ™ÿ∫ŸÑ ÿßÿ≤ÿßŸä
ŸàÿßŸÑÿ¥ÿßÿ®ÿ™ÿ± ÿØÿß ŸÅŸäŸá ŸÜÿ∏ÿ±Ÿä ŸàŸÖÿ≥ÿßÿ¶ŸÑ, ŸàŸáŸÜÿ™ŸÖŸÑŸÖ ÿ®ŸÖÿ∂ ÿßŸÑÿ±  techniques  ÿßŸÑŸÑŸä ÿ®ÿ™ŸÖŸÉŸÜŸä ŸÖŸÜ ÿßŸÑÿ™ÿ≠ŸàŸäŸÑ ŸÑŸÑŸÖŸÉÿßŸÅ  ÿßŸÑÿ±  binary 
  ÿ®ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜŸÖŸÜ ÿßŸÑÿ±  bits   ŸàÿØÿß ÿπÿ¥ÿßŸÜ ÿßÿ≠ŸÜÿß ŸÖŸÇŸäÿØŸäŸÜ ÿ®ÿ±ÿ±ÿ±  bandwidth  ŸÖÿ≠ÿ±ÿ±ÿØÿØ  ŸàŸÉÿ±ÿ±ŸÑ ŸÜÿ∏ÿ±ÿ±ÿßŸÖ ŸÑŸäÿ±ÿ±Ÿá ŸÜÿ∑ÿ±ÿ±ÿßŸÜ ŸÖŸÖÿ±ÿ±ŸäŸÜ ŸÖÿ±ÿ±ŸÜ 
ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑ ŸÅŸäŸá Ÿàÿπÿ¥ÿßŸÜ ŸäŸÉŸàŸÜ ŸÅŸäŸá ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÖÿ´ÿ±ÿ±ŸÑ efficient utilization) ) ŸÑŸÑÿ±ÿ±ÿ± bandwidth  Ÿàÿßÿ®ŸÖÿ±ÿ±ÿ™ ÿπŸÑŸäÿ±ÿ±Ÿá
ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ±  users  ŸÅÿßŸÑÿ≤ŸÖ ÿßÿ®ŸÖÿ™ ŸÑŸÑÿ±  user  ÿßŸÑŸàÿßÿ≠ÿØ ÿπÿØÿØ ÿßŸÇŸÑ ŸÖŸÜ ÿßŸÑÿ±  bits,  Ÿàÿπÿ¥ÿßŸÜ ÿßŸÇÿØÿ± ÿßÿ®ŸÖÿ™ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ÿ± bit 
rate  ÿ≥ÿ±Ÿä ÿßŸÑÿ≤ŸÖ ÿßÿ∂ÿ∫ÿ∑ŸáŸÖ Ÿà ÿßŸÇŸÑŸÑ ÿπÿØÿØŸáŸÖ ÿßŸÑŸÜ ÿßŸÑÿ± bit rate  ÿßŸÑÿ≥ÿ±Ÿä ÿ®Ÿäÿ≠ÿ™ÿ±ÿ±ÿß  bandwidth ŸÉÿ®Ÿäÿ±ÿ±ÿ± (ÿ®ŸÜŸÇÿ±ÿ±Ÿäÿ≥ ŸÖŸÖÿ±ÿ±ÿØŸÑ 
ŸÜŸÇŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ® ÿ±  bits per second) Block Diagram of Digital System  
Block Diagram of Digital System  
Source Encoder  
 
  ŸÅŸä ÿßŸÑÿ¥ÿßÿ®ÿ™ÿ± ÿØÿß ŸáŸÜÿ™ŸÖŸÑŸÖ ÿ∑ÿ±ŸÜ ÿ™ÿ≠ŸàŸäŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ™ÿµŸÑÿ© ŸÑŸÑŸÖŸÉÿßŸÅ    ÿßŸÑÿ±  binary   ŸÑŸÖÿ±ÿ±ÿØÿØ ŸÇŸÑŸäÿ±ÿ±ŸÑ ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±ÿ±  bits   ŸàŸÑŸÖÿ±ÿ±ÿß ŸÜÿ±ÿ±ÿØÿÆŸÑ
ÿπŸÑÿ±ÿ±Ÿâ ÿ¥ÿ±ÿ±ÿßÿ®ÿ™ÿ± ÿßŸÑÿ±ÿ± ÿ± Information theory  ŸáŸÜÿ±ÿ±ÿ™ŸÖŸÑŸÖ ÿ∑ÿ±ÿ±ÿ±ŸÜ ŸÑŸÑÿ±ÿ±ÿ± compression  ÿßÿ≥ÿ±ÿ±ŸÖŸáÿß Lossless compression 
techniques  ÿ≤Ÿä Huffman coding  ŸáŸÜŸÖÿ±ŸÅ ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿßŸÑÿ∑ÿ±ŸÜ ÿØŸä ÿßŸÜŸÜÿß ŸÜÿ®ŸÖÿ™ ÿßŸÑÿØÿßÿ™ÿß ÿ®ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿ±ÿ±ÿ± 
bits   ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß Ÿäÿ≠ÿµŸÑÿ£Ÿä ŸÅŸÇÿØ ŸÑŸÑÿ± information. 
 ŸÅŸäÿ±ÿ±Ÿá ÿ∑ÿ±ÿ±ÿ±ŸÜ compression  ŸÖŸÖŸÉÿ±ÿ±ŸÜ ÿßŸÅŸÇÿ±ÿ±ÿØ ŸÅŸäŸáÿ±ÿ±ÿß ÿ¨ÿ±ÿ±ÿ≤ÿ° ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±ÿ± information  Ÿàÿ®ŸÜÿ≥ÿ±ÿ±ŸÖŸäŸáÿß lossy compression 
techniques   Ÿàÿ®Ÿäÿ≠ÿµŸÑ ŸÅŸÇÿØ ŸÅŸä ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑŸÖŸÖŸÑŸàŸÖÿ±ÿ±ÿßÿ™ ŸÖÿ±ÿ±ŸÜ ÿ∫Ÿäÿ±ÿ±ÿ± ŸÖÿ±ÿ±ÿß Ÿäÿ±ÿ± ÿ´ÿ± ÿπŸÑÿ±ÿ±Ÿâ ÿßŸÑÿ±ÿ±ÿ±  reconstruction   ÿ®ÿ™ÿ±ÿ±ÿßÿß ÿßŸÑÿ±ÿ±ÿ± 
signal ( ŸäŸÖŸÜŸä ŸÅŸäÿ±ÿ±Ÿá ÿ≠ÿßÿ¨ÿ±ÿ±ÿ© ŸÖŸÖŸäŸÜÿ±ÿ±Ÿá ÿßŸÜÿ±ÿ±ÿß ŸÖŸÖŸÉÿ±ÿ±ŸÜ ÿßÿ≥ÿ±ÿ±ÿ™ÿ∫ŸÜŸâ ÿπŸÜŸáÿ±ÿ±ÿß Redundant Information  )  ŸäŸÖŸÜÿ±ÿ±Ÿä ÿ≠ÿßÿ¨ÿ±ÿ±ÿ© ÿ≤Ÿäÿ±ÿ±ÿßÿØÿ©
ŸàÿßÿßŸÑÿ≥ÿ™ÿ∫ŸÜÿßÿ° ÿπŸÜŸáÿß ŸÖŸä ÿ´ÿ±ÿ¥ ŸÉÿ™Ÿäÿ± ÿπŸÑŸâ  quality ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ±ÿ≥ŸÑÿ©. 
 
‚ñ™ Operates upon one or more analog signals to produce a periodic train of symbols.  
  ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸâÿ•ÿ¥ÿßÿ±ÿ©  analog   ÿßŸà ÿßŸÉÿ™ÿ± ŸàŸáŸÜŸÖÿ±ŸÅ ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÉÿ™ÿ± ÿØŸä ÿ®ŸÜÿ™ŸÖÿßŸÖÿ±ÿ±ŸÑ ŸÖŸÖÿßŸáÿ±ÿ±ÿß ÿßÿ≤ÿßŸä ÿπÿ¥ÿ±ÿ±ÿßŸÜ ÿßÿ≠ÿµÿ±ÿ±ŸÑ ÿπŸÑÿ±ÿ±Ÿâ
periodic train of symbols   ŸàÿØÿß ÿßŸÑŸÖŸÇÿµŸàÿØ ÿ®ŸäŸá ÿßÿßŸÑÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿ≠ÿßŸäÿØ 
 
 
‚ñ™ May contain a multiplexer with channels which are used to communicate from 
more than one source at the same time.  
   ŸÑŸà ÿßŸÜÿß ÿ®ÿ™ŸÖÿßŸÖŸÑ ŸÖ System    ÿ®Ÿäÿ®ŸÖÿ™ ÿßŸÉÿ™ÿ± ŸÖŸÜÿ•ÿ¥ÿßÿ±ÿ©  ŸÅŸä ŸàŸÇÿ™ Ÿàÿßÿ≠ÿØ ,Ÿà ÿπŸÖŸÑŸäÿßŸã ŸÉŸÑ ÿßŸÑÿ± Systems  ÿ®ÿ™ÿ®ŸÖÿ±ÿ±ÿ™ ÿßŸÉÿ™ÿ±ÿ±ÿ± ŸÖÿ±ÿ±ŸÜ
ÿ•ÿ¥ÿßÿ±ÿ© ŸÅŸä ŸàŸÇÿ™ Ÿàÿßÿ≠ÿØ ÿßŸÑŸÜ ÿßŸÑÿ± Systems ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸâ ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ± users. 
 ÿ∑ÿßŸÑŸÖÿß ÿßŸÜÿß ÿ¥ÿ±ÿ±ÿ∫ÿßŸÑ ÿπŸÑÿ±ÿ±Ÿâ ÿπÿ±ÿ±ÿØÿØŸÉÿ®Ÿäÿ±ÿ±ÿ± ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±  ÿ± users  Ÿäÿ®ŸÇÿ±ÿ±Ÿâ ÿßŸÑÿ≤ŸÖ ŸäŸÉÿ±ÿ±ŸàŸÜ ŸÅŸäÿ±ÿ±Ÿá  technique ÿ®ÿ™ ŸÖŸÉŸÜÿ±ÿ±Ÿä ŸÖÿ±ÿ±ŸÜ ÿßŸÜÿ±ÿ± Ÿä ÿßÿ®ŸÖÿ±ÿ±ÿ™ 
ÿßŸÑŸÖŸÖŸÑŸàŸÖÿßÿ™ ÿßŸÑÿÆÿßÿµÿ© ÿ®ÿßŸÑÿ±  users   ŸÅŸä ŸàŸÇÿ™ Ÿàÿßÿ≠ÿØ, ÿßŸÑÿ±ÿ±ÿ±  operation    ÿØŸä ÿ®ŸÜŸÇÿ±ÿ±ŸàŸÑ ÿπŸÑŸäŸáÿ±ÿ±ÿß multiplexing    ŸäŸÖŸÜÿ±ÿ±Ÿä ÿßÿ¨ŸÖÿ±ÿ±
ŸÖŸÖŸÑŸàŸÖÿßÿ™ ŸÑŸÖÿØÿØ ŸÖŸÜ ÿßŸÑÿ±ÿ± ÿ± users  ÿ®ÿ∑ÿ±ŸäŸÇÿ±ÿ±ÿ© ŸÖÿ±ÿ±ÿß ÿ™ÿ∂ÿ±ÿ±ŸÖŸÜÿπÿ±ÿ±ÿØŸÖ ÿ≠ÿ±ÿ±ÿØŸà   interference  ŸÖÿ±ÿ±ÿß ÿ®ÿ±ÿ±ŸäŸÜ ŸÖŸÖŸÑŸàŸÖÿ±ÿ±ÿßÿ™ ÿßŸÑÿ±ÿ±ÿ± users 
Ÿàÿ®ŸÖÿ∂ŸáŸÖ. 
 
ŸàÿØŸä ÿ∑ÿ±ŸÜ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑŸÑÿ± Multiplexing : 
FDMA (Frequency Division Multiple Access)  
TDMA (Time Division Multiple Access)  
CDMA (Code Division Multiple Access)  
 
Multiple Access  ŸÖŸÖŸÜÿßŸáÿß ÿßŸÑŸàÿµŸàŸÑ ÿßŸÑŸÖÿ™ŸÖÿØÿØ ŸäŸÖŸÜŸä ÿπŸÜÿ±ÿ±ÿØŸä channel  Ÿà ÿπÿ±ÿ±ÿØÿØ ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±ÿ± users  ŸäŸÇÿ±ÿ±ÿØÿ±Ÿàÿß Ÿäÿ®ŸÖÿ™ÿ±ÿ±Ÿàÿß ŸÑÿ±ÿ±ÿ± 
Receiving station  ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß Ÿäÿ≠ÿµŸÑ interference ŸàŸäŸÇÿØÿ± ÿßŸÑÿ± Receiver  Ÿäÿ≥ÿ™ŸÇÿ®ŸÑŸáÿß ÿ®ÿ±ÿ± ÿπŸÑŸâ
ÿ¨ŸàÿØÿ©.    
 
ŸÖŸÑÿ≠Ÿàÿ∏ÿ© : ÿ£Ÿä ÿ≠ÿßÿ¨ÿ© ŸÅŸäŸáÿß ŸÉŸÑŸÖÿ©   Encoder .Ÿäÿ®ŸÇŸâ ÿßŸÉŸäÿØ ÿÆÿ±ÿ¨ÿ© ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ 
 
 
 
 
 
FDMA (Frequency Division Multiple Access)  
Ÿáÿ®ŸÖÿ™ ÿπÿ±ÿ±ŸÜ ÿ∑ÿ±Ÿäÿ±ÿ±ŸÑ ŸÅÿµÿ±ÿ±ŸÑ ÿßŸÑÿ±ÿ± ÿ± users  ŸÅÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ±ÿ± Frequency domain  ŸäŸÖŸÜÿ±ÿ±Ÿä ŸÖŸÖŸÑŸàŸÖÿ±ÿ±ÿßÿ™ŸáŸÖ ŸÅÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ±ÿ± Frequency 
domain Ÿáÿ™ŸÉŸàŸÜ ŸÖŸÜŸÅÿµŸÑÿ© Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÇÿØÿ± ÿßÿ®ŸÖÿ™Ÿáÿß Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑŸáÿß ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠. 
ÿ™ŸÖŸÑŸÖŸÜÿß ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿßŸÜ ÿ®ŸÖÿØ ŸÖÿß ÿ®ÿ≠ÿ≥ÿ® Fourier Transform  ÿ®ŸÇÿØÿ± ŸÖÿ±ÿ±ŸÜ ÿÆÿßŸÑŸÑÿ±ÿ±Ÿá ÿßÿ±ÿ≥ÿ±ÿ±ŸÖ ÿπÿßŸÑŸÇÿ±ÿ±ÿ© ŸÖÿ±ÿ±ÿß ÿ®ÿ±ÿ±ŸäŸÜ
ÿßŸÑÿ™ÿ±ÿ±ÿ±ÿØÿØ ÿπŸÑÿ±ÿ±Ÿâ ÿßŸÑŸÖÿ≠ÿ±ÿ±Ÿàÿ± ÿßÿßŸÑŸÅŸÇÿ±ÿ±Ÿä Ÿà ÿßŸÑÿ±ÿ±ÿ± amplitude spectrum  ÿπŸÑÿ±ÿ±Ÿâ ÿßŸÑŸÖÿ≠ÿ±ÿ±Ÿàÿ± ÿßŸÑÿ±ÿ£ÿ≥ÿ±ÿ±ŸäŸàŸÇŸäŸÖÿ±ÿ±ÿ© ÿßŸÑÿ±ÿ±ÿ± amplitude 
spectrum  Ÿáÿ±ÿ±Ÿä ŸÇŸäŸÖÿ±ÿ±ÿ© ÿ®ÿ™ÿ±ÿ±ÿØŸäŸÜŸä ÿßŸÑÿ±ÿ±ÿ± weight  ÿßŸàŸÜÿ≥ÿ±ÿ±ÿ®ÿ© ÿ™Ÿàÿßÿ¨ÿ±ÿ±ÿØ ŸÉÿ±ÿ±ŸÑ frequency  ŸÅÿ±ÿ±Ÿäÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ŸÖŸÇÿßÿ±ŸÜÿ±ÿ±ÿ© ÿ®ÿ±ÿ±ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ 
ÿßÿ£ŸÑÿÆÿ±Ÿâ  ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ©. 
ŸÑŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜ ÿπŸÜÿØŸÜÿß 3 users  ŸÖÿ´ÿßŸÑ Ÿàÿßÿ¥ÿßÿ±ÿ© ÿßŸÑÿµŸàÿ™ ŸÖŸÜ 300 Hz  ŸÑÿ≠ÿØ 4 KHz   ŸàŸÑŸÜŸÅÿ™ÿ±ÿ±ÿ±ÿ∂ ÿßŸÜŸáÿ±ÿ±ÿß ŸÖÿ±ÿ±ŸÜ0 Hz  ŸÑÿ≠ÿ±ÿ±ÿØ
4KHz ŸàÿßŸÑÿ±ÿ≥ŸÖ ÿßÿßŸÑÿ™Ÿä ŸáŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜŸá ÿßŸÑÿ± frequency spectrum  ÿ®ÿ™ÿßÿß ŸÉŸÑÿ•ÿ¥ÿßÿ±ÿ© (ŸÉŸÑŸÖÿ±ÿ±ÿ© spectrum  ÿØÿßŸäŸÖÿ±ÿ±ÿß ÿ®ÿ™ÿ±ÿ±ÿØŸÑ
ÿπŸÑŸâ ÿ™ÿ±ÿØÿØ) 
 
 
 
 
 
 
 
 
 
 
ŸÜŸÅÿ™ŸÉÿ± ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ùíáùüê  ŸÖÿ´ÿßŸÑÿ®ŸÜŸÇŸà ŸÑ ÿßŸÜŸáÿ±ÿ±ÿß Bandlimited to 3KHz  ŸäŸÖŸÜÿ±ÿ±Ÿä ÿ≠ÿ±ÿ±ÿØŸàÿØŸáÿß ŸÅÿ±ÿ±Ÿä
ŸÜÿ∑ÿßŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸÑÿ≠ÿØ ŸÇÿ®ŸÑ  3KHz   ÿßŸÑŸÜ ÿπŸÜÿØ 3KHz .ÿ™ÿ≠ÿØŸäÿØÿß ŸÇŸäŸÖÿ™Ÿá ÿ®ÿµŸÅÿ± 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ÿ≤Ÿä ŸÖÿß Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÖ ÿßŸÜŸáÿ±ÿ±ÿß ŸÖÿ™ÿØÿßÿÆŸÑÿ±ÿ±ÿ© ŸÅÿ±ÿ±Ÿä frequency domain  ŸÅŸÖÿ¥ÿ±ÿ±ÿßŸÜ ÿßŸÇÿ±ÿ±ÿØÿ± ÿßÿ®ŸÖÿ™Ÿáÿ±ÿ±ÿß ÿ®ÿ±ÿ±ÿ± FDMA 
ÿßŸÑÿ≤ŸÖ ÿßŸÅÿµŸÑŸáŸÖ ÿπŸÜ ÿ®ŸÖÿ∂ ŸàŸáŸÜÿß ÿ™Ÿäÿ¨Ÿä ŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ± Modulation techniques  ŸàŸáŸà ÿßŸÜŸá ÿ®ŸäÿßÿÆÿ±ÿ±ÿØ ŸÉÿ±ÿ±ŸÑÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ŸàŸäÿ≠ŸÖŸÑŸáÿ±ÿ±ÿß 
ÿπŸÑŸâ carrier    ŸÖÿÆÿ™ŸÑŸÅÿ®ÿ™ÿ±ÿØÿØ ÿπÿßŸÑŸä ŸàÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ ÿπŸÑŸâ ÿßŸÑ ÿ±  carrier   ÿ®ŸäŸÖŸÖŸÑshift  ŸÑŸÑÿ± spectrum  ÿ®ÿ™ÿßÿßÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©.  
 
 
 
 
 
 
 
 
 
 amp spectrum  
ùíá 
1KHz 0 Hz 3KHz 4KHz  ùíáùüè 
ùíáùüê 
ùíáùüë 
ùíá 
ùíáùë™ùüè amp spectrum  
ùíáùë™ùüê ùíáùë™ùüë 
 
ŸàŸÖŸÜ ÿÆÿßŸÑŸÑ ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑŸÑŸä ŸÅÿßÿ™ Ÿàÿßÿ∂ÿ≠ ÿßŸÜŸä ŸáŸÇÿØÿ± ÿßÿ®ŸÖÿ™ŸáŸÖ ÿπŸÑŸâ ŸÜŸÅÿ≥ ÿßŸÑÿ± channel  ŸÖŸÜ ÿ∫Ÿäÿ± ÿ™ÿ±ÿ±ÿØÿßÿÆŸÑ ÿßŸÑŸÜŸÅŸäÿ±ÿ±Ÿá band 
pass filter  ŸÅŸä ÿßŸÑÿ±  receiver   ŸÖÿµŸÖŸÖ ÿßŸÜŸá ŸäŸÖÿØŸäŸÜÿ∑ÿßŸÜ ŸÖŸÖŸäŸÜ  ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸàŸÖŸäŸÖÿØŸäÿ¥ ÿßŸÑÿ®ÿßŸÇŸä ŸÅÿ®Ÿäÿ™ŸÖ ÿ™ÿµŸÖŸäŸÖ  band 
pass filter ÿπŸÜÿØ ÿßŸÑÿ± receiver ŸäŸÖÿØŸä ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßŸÑÿ±ÿ± ÿ± user ÿßÿ£ŸÑŸàŸÑ ŸàŸÅŸäÿ±ÿ±Ÿá band pass filter  ŸäŸÖÿ±ÿ±ÿØŸäÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßŸÑÿ±ÿ± ÿ± user 
 ÿßŸÑÿ™ÿßŸÜŸäŸà  band pass filter   ÿ™ÿßŸÑÿ™ ÿπÿ¥ÿßŸÜ ŸäŸÖÿØŸäÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑÿ± user ÿßŸÑÿ™ÿßŸÑÿ™. 
 
TDMA ( Time  Division Multiple Access)  
ŸáŸÜÿß ÿ®ŸäŸÇÿ≥ŸÖ ÿßŸÑÿ± channel  ÿßŸÑŸàÿßÿ≠ÿØÿ© ŸÑŸÖÿ¨ŸÖŸàÿπÿ© frames  ŸàŸÉŸÑ frame  Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿπÿØÿØ ŸÖŸÜ ÿßŸÑÿ±ÿ±ÿ± time slots  ŸàŸÉÿ±ÿ±ŸÑuser 
 ŸÑŸäŸá time slot ÿÆÿßÿµ ÿ®ŸäŸá ŸäŸÖŸÜŸä ÿ®Ÿäÿ±ÿ±ÿ™ŸÖ ÿ™ÿ≠ÿØŸäÿ±ÿ±ÿØ ŸàŸÇÿ±ÿ±ÿ™ ŸÖŸÖÿ±ÿ±ŸäŸÜ ŸÑŸÑÿ±ÿ± ÿ± user  Ÿäÿ±ÿ≥ÿ±ÿ±ŸÑ ŸÅŸäÿ±ÿ±Ÿá ÿßŸÑÿ®ŸäÿßŸÜÿ±ÿ±ÿßÿ™ ÿ®ÿ™ÿßÿπÿ™ÿ±ÿ±Ÿá ŸàŸÑŸÖÿ±ÿ±ÿß ŸäÿÆŸÑÿ±ÿ±ÿµ ÿ®Ÿäÿ±ÿ±ÿ™ŸÖ
ÿ™ÿÆÿµŸäÿµ ÿßŸÑŸàŸÇÿ™ ÿØÿß ŸÑÿ±  user    ÿ™ÿßŸÜŸäŸà ŸáŸÉÿ∞ÿß ,Ÿà ŸáŸÜÿß ÿßŸÑÿ±  carrier   ÿßŸÑŸàÿßÿ≠ÿØ Ÿáÿ®ŸÖÿ™ ÿπŸÑŸäŸá ŸÖÿ¨ŸÖŸàÿπÿ±ÿ±ÿ© ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±ÿ±  users   Ÿà ÿ®ŸÅÿµÿ±ÿ±ŸÑ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ŸÅŸä ÿßŸÑÿ± time domain ÿπÿ¥ÿßŸÜ ŸÖŸäÿ≠ÿµŸÑÿ¥ ÿ™ÿØÿßÿÆŸÑ ŸÑŸà ÿ®ŸÖÿ™ŸáŸÖ ÿπŸÑŸâ ŸÜŸÅÿ≥ ÿßŸÑÿ± carrier .ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ 
 
ŸÖÿ±ÿ±ÿ´ÿßŸÑ ŸáŸÅÿ±ÿ±ÿ±ÿ∂ ÿßŸÜ ÿπŸÜÿ±ÿ±ÿØŸÜÿß 3 users  ÿ®ÿ±ÿ±ÿ≥ ÿßŸÑŸÖÿ±ÿ±ÿ±Ÿá ÿØŸä ŸáŸÜÿ¥ÿ±ÿ±ÿ™ÿ∫ŸÑ time domain  ŸàÿπŸÜÿ±ÿ±ÿØŸä ÿ™ÿ±ÿ±ÿßŸÑÿ™ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿßÿ™ ÿßÿµÿ±ÿ±ŸÑŸáŸÖ 
analog   Ÿà  Ÿàÿ≤Ÿä ŸÖÿß ŸáŸÜÿØÿ±ÿ≥ ŸÅŸä  PCM  ÿ®ŸÜÿ®ÿØÿß ÿßŸàŸÑ ÿÆÿ∑Ÿàÿ© ÿ®ÿßŸÑÿ±  sampling  ,  ŸÖŸÜ ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä Ÿáÿ±ÿ±ŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ŸÖŸÅÿ±ÿ±Ÿäÿ¥
ÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßŸÑ samples ÿØŸä Ÿàÿ®ŸÖÿ∂Ÿáÿß ŸÅŸä ÿßŸÑÿ± time domain   ÿßŸÑŸÜ ŸÉŸÑ Ÿàÿßÿ≠ÿØ ÿ®Ÿäÿ®ŸÖÿ™ ŸÅŸä  time slot .ŸÖÿÆÿ™ŸÑŸÅ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ùíï ùíîùüè(ùíï) 
ùíï 
ùíï ùíîùüê(ùíï) 
ùíîùüë(ùíï) Sampling 
interval  
11 
12 
13 21 31 
22 32 
23 33 
 
sampling interval  ŸáŸä ÿßŸÑŸÅÿ™ÿ±ÿ© ÿßŸÑÿ≤ŸÖŸÜŸäÿ±ÿ±ÿ© ÿ®ÿ±ÿ±ŸäŸÜ ŸÉÿ±ÿ±ŸÑ sample  ŸàÿßŸÑÿ™ÿßŸÜŸäÿ±ÿ±Ÿá ŸÑÿ±ÿ±ŸÜŸÅÿ≥ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ŸàŸÖŸÇŸÑŸàÿ®Ÿáÿ±ÿ±ÿß Ÿáÿ±ÿ±Ÿà ÿßŸÑÿ™ÿ±ÿ±ÿ±ÿØÿØ ùëìùë† 
( sampling frequency  ) ŸàŸÜŸÅÿ™ŸÉÿ± ÿßŸÜ ÿ¥ÿ±ÿ∑ ÿßŸÑÿ™ÿ±ÿØÿØ ÿØÿß ÿßŸÜŸá ŸäŸÉŸàŸÜ ÿßŸÉÿ®ÿ± ŸÖŸÜ ÿßŸà Ÿäÿ≥ÿßŸàŸä ÿ∂ŸÖŸÅ ÿßÿπŸÑŸâ ÿ™ÿ±ÿØÿØ ŸÖŸàÿ¨ŸàÿØ
ŸÅŸä ÿßŸÑÿ± information signal   
 
  ÿ® ÿ®ÿ≥ÿ∑ÿßÿ£ŸÑŸÅŸÉÿßÿ±  ŸÅŸäŸá ÿ≤Ÿä  switch    ÿ®ŸäŸÇŸÅŸÑ ÿßŸàŸÑ ŸÖÿ±Ÿá ÿπŸÜÿØ ÿßŸàŸÑ sample    ÿ®ÿ™ÿßÿπÿ© ÿßŸàŸÑuser    Ÿàÿ®ŸÖÿØ ŸÉÿØÿßÿßŸÑÿ±  switch   Ÿäÿ™ŸÜŸÇŸÑ
ŸÑŸÑÿ±  sample ÿ®ÿ™ÿßÿπÿ© ÿßŸÑÿ± user ÿßŸÑÿ™ÿßŸÜŸä Ÿàÿ®ŸÖÿØŸáÿß Ÿäÿ™ŸÜŸÇŸÑ ŸÑŸÑÿ± sample  ÿ®ÿ™ÿßÿπÿ±ÿ±ÿ© ÿßŸÑÿ±ÿ±ÿ± user  ÿßŸÑÿ™ÿßŸÑÿ±ÿ±ÿ™, ÿßŸÑÿ±ÿ±ÿ± switch  ŸÉÿßŸÜÿ±ÿ±Ÿá ÿ®ŸäŸÑÿ±ÿ±ŸÅ
ÿπŸÑŸâ ÿßŸÑÿ± users 3  ŸàŸäÿßÿÆÿØ sample   ŸÖŸÜ ŸÉŸÑ. user 
 
 ŸÉÿØÿß ÿÆŸÑÿµÿ™ sample  Ÿàÿßÿ≠ÿØŸá ŸÖŸÜ ŸÉŸÑuser  ÿ®Ÿäÿ≠ÿ∑ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿ™ÿ≠ÿ±ÿ±ÿØÿØ ÿßŸÜ ÿßŸÑÿ±ÿ± ÿ± frame  ÿßŸÜÿ™Ÿáÿ±ÿ±Ÿâ ÿßÿ≥ÿ±ÿ±ŸÖŸáÿß synchronizing 
pulse  ÿ®ŸäŸÉÿ±ÿ±ŸàŸÜ ÿßŸÑÿ±ÿ±ÿ± level  ÿ®ÿ™ÿßÿπŸáÿ±ÿ±ÿß ÿπÿ±ÿ±ÿßÿØÿ©Ÿã  ÿßŸÉÿ®ÿ±ÿ±ÿ± ŸÖÿ±ÿ±ŸÜ ÿ®ŸÇŸäÿ±ÿ±ÿ© ÿßŸÑ samples  ÿπÿ¥ÿ±ÿ±ÿßŸÜ ÿßŸÑ receiver  ŸÑŸÖÿ±ÿ±ÿß Ÿäÿ≥ÿ±ÿ±ÿ™ŸÇÿ®ŸÑŸáÿßŸäŸÇÿ±ÿ±ÿØÿ± 
ŸäŸÖŸäÿ≤Ÿáÿß ŸàŸäŸÖÿ±ŸÅ ÿßŸÜ ÿØŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿ±  frame. 
 
 ÿπÿ¥ÿßŸÜ Ÿäÿ≠ÿµŸÑ ÿ™ÿ≤ÿßŸÖŸÜ ÿµÿ≠Ÿäÿ≠ ÿ®ŸäŸÜ ÿßŸÑÿ±ÿ±ÿ±  transmitter   ŸàÿßŸÑÿ±ÿ±ÿ±  receiver   ÿßŸÑÿ≤ŸÖ Ÿäÿ®ŸÇÿ±ÿ±Ÿâ ÿπŸÜÿ±ÿ±ÿØŸá ÿ≠ÿßÿ¨ÿ±ÿ±ÿ© ÿ™ÿ≠ÿØÿØŸÑÿ±ÿ±Ÿá ÿßŸÖÿ™ÿ±ÿ±Ÿâ ÿßŸÑÿ±ÿ±ÿ± 
frame   ÿßÿ®ÿ™ÿØŸâ ŸàÿßŸÖÿ™Ÿâ ÿßŸÜÿ™ŸáŸâ,ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿØÿß ÿßŸÑ ÿ±  frame  ÿßŸÑŸàÿßÿ≠ÿ±ÿ±ÿØ ŸÖŸÉÿ±ÿ±ŸàŸÜ ŸÖÿ±ÿ±ŸÜ ÿ™ÿßŸÑÿ™ÿ±ÿ±Ÿá samples  Ÿàÿßÿ≠ÿ±ÿ±ÿØŸá ŸÑŸÉÿ±ÿ±ŸÑuser 
ŸÅŸÑŸÖÿß Ÿäÿ¨ŸäŸä ŸäŸàÿ≤ÿß ÿπŸÜÿØ ÿßŸÑÿ±  receiver    ŸáŸäÿ≠ÿ∑ ÿßŸàŸÑ sample   ÿ™ÿÆÿ±ÿ±ÿµ ÿßŸÑÿ±ÿ±ÿ±  user  ÿßÿ£ŸÑŸàŸÑ  ŸàŸäÿ≠ÿ±ÿ±ÿ∑  ÿßŸàŸÑ  sample   ÿ™ÿÆÿ±ÿ±ÿµ
ÿßŸÑÿ± user ÿßŸÑÿ™ÿßŸÜŸä ŸàŸäÿ≠ÿ∑ ÿßŸàŸÑ sample  ÿ™ÿÆÿ±ÿ±ÿµ ÿßŸÑÿ±ÿ±ÿ± user  ÿßŸÑÿ™ÿßŸÑÿ±ÿ±ÿ™Ÿàÿ®ŸÖÿ±ÿ±ÿØŸáÿß synchronizing pulse  Ÿàÿ®ŸÖÿ±ÿ±ÿØ ŸÉÿ±ÿ±ÿØÿß ÿßŸÑÿ±ÿ±ÿ± 
switch  ŸáŸäÿßÿÆÿØ ÿ™ÿßŸÜŸä sample  ŸÖŸÜ ÿßŸàŸÑuser  Ÿàÿ™ÿßŸÜŸä  sample ŸÖŸÜ ÿ™ÿßŸÜŸäuser  Ÿàÿ™ÿßŸÜŸä  sample ŸÖŸÜ ÿ™ÿßŸÑÿ±ÿ±ÿ™   user 
 Ÿàÿ®ŸÖÿØŸáÿß synchronizing pulse  ÿπÿ¥ÿßŸÜ ŸäŸÖÿ±ÿ±ÿ±ŸÅ ÿßŸÜ ÿßŸÑÿ±ÿ±ÿ± frame  ÿÆŸÑÿ±ÿ±ÿµ Ÿàÿ∑ÿ®ŸÖÿ±ÿ±ÿß ÿßŸÑŸÖŸÖŸÑŸäÿ±ÿ±ÿ© ŸÖÿ≥ÿ±ÿ±ÿ™ŸÖÿ±ÿ© ŸÑÿ≠ÿ±ÿ±ÿØ ŸÖÿ±ÿ±ÿß ŸäÿßÿÆÿ±ÿ±ÿØ
samples  ŸÑŸÉÿßŸÖŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
 
 
 
 
 
 
 
 
 
 
ŸÖŸÖŸÉŸÜ ŸÑÿ≥ÿ±ÿ±ÿ®ÿ® ŸÖÿ±ÿ±ÿß Ÿäÿ≠ÿµÿ±ÿ±ŸÑ ÿπŸÜÿ±ÿ±ÿØ ÿßÿ≥ÿ±ÿ±ÿ™ŸÇÿ®ÿßŸÑ ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© delay  Ÿàÿ™ÿ±ÿ±ÿØÿßÿÆŸÑ ÿ®ÿ±ÿ±ŸäŸÜ ÿßŸÑÿ±ÿ±ÿ±  symbol s  Ÿàÿ®ŸÖÿ∂ÿ±ÿ±Ÿáÿß Ÿàÿ®ÿßŸÑÿ™ÿ±ÿ±ÿßŸÑŸä ŸÖÿ±ÿ±ÿ¥
Ÿáÿ™ŸàÿµŸÑ ŸÅŸä ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿØÿØ  ŸÅÿßŸÑÿ±  frame  ÿßÿ£ŸÑŸàŸÑ  ÿπÿØŸâ ÿ¥ŸàŸäŸá ŸàÿßÿÆÿ±ÿ±ÿØ ŸÖÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ± ÿ±  frame   ÿßŸÑÿ™ÿ±ÿ±ÿßŸÜŸä ŸàŸáŸÉÿ±ÿ±ÿ∞ÿß, ŸàŸÉÿ±ÿ±ÿØÿß ÿ™ÿ±ÿ™Ÿäÿ±ÿ±ÿ®
ÿßŸÑŸÖŸÖŸÑŸàŸÖÿ© ŸáŸäÿ™ŸÑÿÆÿ®ÿ∑ ŸàÿØÿß ÿßÿ≥ŸÖŸá (ISI)  Inter symbol interference. 
 
 
 
 ùë∫(ùíï) Synchronizing  
pulse  
11 12 13 21 22 23 22 31 33 ùíï 
 
 
‚ñ™ provide security by ensuring that only the intended receiver can understand the 
message.  
‚ñ™ Encryption is a means of securing data by encoding it  mathematically such that 
it can only be read, or  decrypted, by those with the correct key or cipher.  
 
Encryption   ÿßŸÑÿ™ÿ¥ÿ±ÿ±ŸÅŸäÿ±ÿ®Ÿäÿ±ÿ±ŸàŸÅÿ± security  ŸÑŸÑÿ±ÿ±ÿ± system  ÿπÿ±ÿ±ŸÜ ÿ∑ÿ±Ÿäÿ±ÿ±ŸÑ ÿßŸÜÿ±ÿ±Ÿá ÿ®Ÿäÿ∂ÿ±ÿ±ŸÖŸÜ ÿßŸÜ ÿßŸÑÿ±ÿ±ÿ± receiver  ÿßŸÑŸÑÿ±ÿ±Ÿä ÿπÿ±ÿ±ÿßŸäÿ≤
ÿßŸàÿµŸÑŸá ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸáŸà  ÿ®ÿ≥ ÿßŸÑŸÑŸä ŸäŸÇÿØÿ± ŸäŸÅŸáŸÖŸáÿß. 
ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ™ÿ¥ŸÅŸäÿ± ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿπÿ¥ÿßŸÜ ŸÖŸÅŸäÿ¥ ÿ∫Ÿäÿ±ÿ±ÿ± ŸÖÿ¨ŸÖŸàÿπÿ±ÿ±ÿ© ŸÖÿ≠ÿ±ÿ±ÿØÿØŸá ÿßŸÑŸÑÿ±ÿ±Ÿä ÿ™ŸÇÿ±ÿ±ÿØÿ± ÿ™ŸÅŸáÿ±ÿ±ŸÖ  ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ©  ÿ≤Ÿä ÿßŸÑŸÇŸÜÿ±ÿ±Ÿàÿßÿ™ ÿßŸÑŸÖÿ¥ÿ±ÿ±ŸÅÿ±ÿ© 
ÿßŸÑŸÖÿ¥ÿ™ÿ±ŸÉŸäŸÜ ÿ®ÿ≥ ÿßŸÑŸÑŸä Ÿäÿ≥ÿ™ŸÇÿ®ŸÑŸàŸáÿß Ÿàÿ™ÿ¥ÿ™ÿ∫ŸÑ ÿπŸÜÿØŸáŸÖ. 
 
ÿßÿ®ÿ≥ÿ∑  ÿ£ŸÅŸÉÿßÿ±  ÿßŸÑÿ™ÿ¥ŸÅŸäÿ± ÿßŸÑŸÑŸä ŸÜŸÖÿ±ŸÅŸáÿß ŸáŸä ÿπŸÜ ÿ∑ÿ±ŸäŸÑ ÿπŸÖŸÑ  xor    ŸÖÿ´ÿßŸÑ ÿßŸàÿ•ÿ∂ÿßŸÅÿ©  bits    ÿ™ÿßŸÜŸäŸáŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©ÿå  ŸàŸÑŸÉŸÜ ŸÖÿ¨ŸÖŸàÿπÿ© 
ÿßŸÑÿ± bits ÿ™ŸÉŸàŸÜ ÿ®ŸáÿØŸÅ ÿßŸÑÿ™ÿ¥ŸÅŸäÿ± ŸàŸÖŸäŸÖÿ±ŸÅŸáÿßÿ¥ ÿ∫Ÿäÿ± ÿßŸÑÿ± receiver   ÿßŸÑŸÑŸä ÿπÿßŸäÿ≤ ÿßÿ®ŸÖÿ™ŸÑŸá ŸÅŸÇÿ∑ ÿ®ŸáÿØŸÅÿ•ÿÆŸÅÿßÿ° ÿßŸÑŸÖŸÖŸÑŸàŸÖÿ©. 
 
ÿÆÿ±  ÿßŸÑÿ± source encoder  ÿ®Ÿäÿ≠ŸàŸÑÿ£ŸÑÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ ŸàÿØŸàŸÑ ÿ®ŸäŸÖÿ´ŸÑÿ±ÿ±Ÿàÿß ÿßŸÑŸÖŸÖŸÑŸàŸÖÿ±ÿ±ÿ© ÿßÿßŸÑÿµÿ±ÿ±ŸÑŸäÿ© Ÿàÿ®ŸÖÿ±ÿ±ÿØ ÿßŸÑÿ™ÿ¥ÿ±ÿ±ŸÅŸäÿ± ÿßŸÑÿÆÿ±ÿ±ÿ±  
ÿ®ÿ±ÿØŸà ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ ŸàŸÑŸÉŸÜ ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßÿßŸÑÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿ≠ÿßŸäÿØ ÿ®ŸäŸÖÿ´ŸÑŸàÿß ÿßŸÑŸÖŸÖŸÑŸàŸÖÿ© ÿßÿßŸÑÿµŸÑŸäÿ©  ÿπŸÜÿØŸä Ÿàÿ¨ÿ≤ÿ° ÿ™ÿßŸÜŸä ŸÖÿ∂ÿ±ÿ±ÿßŸÅ 
ÿ®ŸáÿØŸÅ ÿßŸÑÿ™ÿ¥ŸÅŸäÿ±. 
 
ÿßŸÑÿ™ÿ¥ŸÅŸäÿ± ÿØÿß ŸÖŸàÿ∂Ÿàÿß ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿß  ŸÖÿ¥ ŸáŸÜÿØÿ±ÿ≥Ÿá ŸÅŸä ÿßŸÑŸÉÿ±ÿ±Ÿàÿ±ÿ≥  ŸàŸÖÿ∑ŸÑÿ±ÿ±Ÿàÿ® ŸÖŸÜŸÜÿ±ÿ±ÿß  ÿßŸÑŸÅŸÉÿ±ÿ±ÿ±ÿ© ÿßŸÑÿ®ÿ≥ÿ±ÿ±Ÿäÿ∑ÿ© ÿßŸÑŸÑÿ±ÿ±Ÿä ÿßÿÆÿ±ÿ±ÿØŸÜÿßŸáÿß ŸÅÿ±ÿ±Ÿä 
ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ©. 
 
 
 
 
It increases efficiency and decreases the effects of transmission errors due to noise.  
 
ŸáŸà ÿπŸÖŸÑŸäÿ© ÿ™ŸáÿØŸÅ ÿßŸÑŸâ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßÿ£ŸÑÿÆÿ∑ÿßÿ°  ÿßŸÑŸÑŸä ŸÖŸÖŸÉŸÜ ÿ™ÿ≠ÿµÿ±ÿ±ŸÑ ÿßÿ´ŸÜÿ±ÿ±ÿßÿ° ÿπŸÖŸÑŸäÿ±ÿ±ÿ© ÿßÿßŸÑÿ±ÿ≥ÿ±ÿ±ÿßŸÑ.  ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ©  ÿ®Ÿäÿ±ÿ±ÿ™ŸÖ ÿßÿ±ÿ≥ÿ±ÿ±ÿßŸÑŸáÿß ŸÅÿ±ÿ±Ÿä 
Ÿàÿ≥ÿ∑ ŸÖŸÑŸäÿßŸÜ noise  ŸàŸÖŸÖŸÉŸÜ Ÿäÿ≠ÿµŸÑ multipath propagation  ŸàŸÑŸà ÿ≠ÿµÿ±ÿ±ŸÑÿ£Ÿä error  ŸáŸäÿ®ŸÇÿ±ÿ±Ÿâ ŸÖÿ¥ÿ±ÿ±ŸÉŸÑÿ© ŸÉÿ®Ÿäÿ±ÿ±ÿ±Ÿá ÿßŸÑŸÜ
ÿØÿßÿ™ÿß ŸÉÿ™Ÿäÿ± Ÿáÿ™ÿ∂Ÿä  ŸÅŸÖÿ¥ÿßŸÜ ŸÉÿØÿß ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÖŸàÿ¨ŸàÿØ ÿπŸÜÿØŸÜÿß ÿ≠ÿßÿ¨ÿ© ÿ®ÿ™ŸÖŸÖŸÑ  Error Detection and Correction. 
 
 ÿßŸÑÿ±ÿ±ÿ± Channel Encoding  ÿ®Ÿäÿ∂ÿ±ÿ±ŸäŸÅbits  ÿ≤Ÿäÿ±ÿ±ÿßÿØÿ© ÿ®ŸÜÿ≥ÿ±ÿ±ŸÖŸäŸáÿß Redundant Bits  ÿßŸà Check bits  ÿßŸà parity bits 
  ÿπÿ¥ÿßŸÜ ŸÜŸÖŸÖŸÑ Error Detection and Correction 
 
ÿßŸÑÿ±  Redundant  Bits  ÿ®ÿ™ÿ≤ŸàÿØ ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÖŸáÿß distance  ŸÖÿß ÿ®ÿ±ÿ±ŸäŸÜ ÿßÿßŸÑŸÉÿ±ÿ±ŸàÿßÿØ ŸàÿßŸÑÿ±ÿ± ÿ± distance Ÿáÿ±ÿ±Ÿà ÿπÿ±ÿ±ÿØÿØ ÿßŸÑÿ±ÿ± ÿ± bits  ÿßŸÑŸÑÿ±ÿ±Ÿä
ÿ®ŸäÿÆÿ™ŸÑŸÅ ŸÅŸäŸáÿß ŸÉŸàÿØ ÿπŸÜ ŸÉŸàÿØ ÿßÿÆÿ± . 
 
ŸÖŸÑÿ≠Ÿàÿ∏ÿ© ŸÖŸáŸÖŸá : ÿßŸÑÿ± message ÿ®ŸÖÿØ ÿßŸÑÿ± source encoder  ÿ®ŸäŸÇŸàŸÑŸà ÿπŸÑŸäŸáÿß message word  ŸàÿßŸÑŸÑŸä ÿ®ŸäÿÆÿ±  ŸÖŸÜ
ÿßŸÑÿ± channel encoder  ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸá code word. Encryptor  
Channel Encoder  
 
If 101 is transmitted and an error occur in the third bit, 100 is received. There is no way 
for the receiver to know that 100 was not the transmitted word  
0000 , 001 1 , 010 1 , 011 0 , 100 1 , 101 0 , 110 0 , 111 1 
If 1010 is transmitted and 1011 is received, this is  not one of the received words.  
 
 ŸÖÿ´ÿßŸÑ ŸÑŸà ÿπŸÜÿØŸä 3 bits  Ÿäÿ®ŸÇŸâ ŸÉÿØÿß ÿπŸÜÿØŸä8  ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸàŸáÿ∂ŸäŸÅbit  ÿ±ÿßÿ®ŸÖŸá ÿßŸÑŸÑŸä ŸáŸä ŸÅŸä ÿßŸÑÿµÿ±ÿ±Ÿàÿ±ÿ© ÿØŸäY  Ÿäÿ®ŸÇÿ±ÿ±Ÿâ ŸÉÿ±ÿ±ÿØÿß
ŸÖŸÖÿßŸäÿß 4 bits  ŸäŸÖŸÜŸä16  ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ®ÿ≥ ŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÖŸÜŸáŸÖ8  ÿ®ÿ≥ Ÿäÿ®ŸÇŸâ ŸÅŸäŸá ŸÉÿØÿß8  ÿ™ÿßŸÜŸäŸäŸÜ ŸÖÿ¥ ŸÖÿ≥ÿ±ÿ±ÿ™ÿÆÿØŸÖŸäŸÜ ŸàŸáÿ±ÿ±ŸÖ ÿØŸàŸÑ ÿßŸÑŸÑÿ±ÿ±Ÿä
ŸáŸäÿØŸàÿß ŸÅÿ±ÿµÿ© ÿßŸÜ ŸÑŸà ÿ≠ÿµŸÑ ÿÆÿ∑  ŸÅŸä  bit  ÿ®ÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÉŸàÿØ ÿ™ÿßŸÜŸä ŸÖÿ¥ Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿßŸÑÿ±  8    ÿßŸÑŸÖÿ™ŸÖÿßÿ±ŸÅ ÿπŸÑŸäŸáŸÖ ŸÖÿß ÿ®ŸäŸÜÿßŸÑÿ±ÿ±ÿ± 
transmitter   ŸàÿßŸÑÿ±  receiver    ŸàŸáŸÜÿßÿßŸÑÿ±  receiver   ŸäŸÇÿ±ÿ±ÿØÿ± ŸäŸÉÿ™ÿ¥ÿ±ÿ±ŸÅ ÿßŸÑÿÆÿ∑ÿ±ÿ±  ÿØÿß, ŸàŸÉÿ±ÿ±ŸÑ ŸÖÿ±ÿ±ÿß ÿßŸÑÿ±ÿ±ÿ± distance  ÿ™ÿ≤Ÿäÿ±ÿ±ÿØ ŸÉÿ±ÿ±ŸÑ ŸÖÿ±ÿ±ÿß
ÿ•ŸÖŸÉÿßŸÜŸäÿ©  ÿ™ÿµÿ≠Ÿäÿ≠  ÿßÿ£ŸÑÿÆÿ∑ÿßÿ°  ÿ™ÿ≤ŸäÿØ. 
 
 
ŸÑŸà ÿßŸÜÿß ÿ®ÿßÿπÿ™ ŸÉŸàÿØ  101    ŸàŸàÿµŸÑ100  ŸÑŸÑÿ±  receiver   ŸÉÿØÿß  ÿßŸÑÿ± receiver   ŸÖÿ¥ ŸáŸäŸÉÿ™ÿ¥ŸÅ ÿßŸÑÿÆÿ∑ÿ£ŸÑŸÜŸá ŸÉŸàÿØ ŸÖŸÜ ÿßŸÑŸÑÿ±ÿ±Ÿä 
ŸÖÿ™ŸÖÿßÿ±ŸÅ ÿπŸÑŸäŸá ŸÑŸÉŸÜ ÿßŸÖÿ±ÿ±ÿß  ÿßŸÑŸÖÿ≥ÿ±ÿ±ÿßŸÅÿ©  ÿ≤ÿßÿ™ ÿßÿßŸÑŸÉÿ±ÿ±ŸàÿßÿØ ÿ≤ÿßÿØÿ™ ŸÅ ÿµÿ±ÿ±ÿ®ÿ≠ ŸÅÿ±ÿ±Ÿä 8  ÿßŸÉÿ±ÿ±ŸàÿßÿØ ŸÖÿ±ÿ±ÿ¥ ŸáŸäÿ≥ÿ±ÿ±ÿ™ÿÆÿØŸáŸÖ Ÿàÿ®ŸÉÿ±ÿ±ÿØŸá ŸáŸÖÿ±ÿ±ÿ±ŸÅ
ÿßŸÉÿ™ÿ¥ÿ±ÿ±ŸÅ ÿßŸÑÿÆÿ∑ÿ±ÿ± . ÿ≤Ÿä ÿßŸÜŸÜÿ±ÿ±ÿß ŸÜÿ®ŸÖÿ±ÿ±ÿ™ 1010  ŸàŸäÿ≥ÿ±ÿ±ÿ™ŸÇÿ®ŸÑŸáÿß 1011  ŸáŸäŸÉÿ™ÿ¥ÿ±ÿ±ŸÅ ÿßŸÑÿÆÿ∑ÿ±ÿ±  ÿßŸÑŸÜ ÿØÿß ŸÖÿ±ÿ±ÿ¥ ŸÖÿ±ÿ±ŸÜ ÿ∂ÿ±ÿ±ŸÖŸÜÿßÿßŸÑŸÉÿ±ÿ±ŸàÿßÿØ 
ÿßŸÑŸÖÿ™ŸÖÿßÿ±ŸÅ ÿπŸÑŸäŸáÿß 
 
Ÿàÿßÿ≥ŸÖŸá Channel Encoding ÿßŸÑŸÜ ÿØÿß ÿÆÿßÿµ ÿ®ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßÿ£ŸÑÿÆÿ∑ÿßÿ° ÿßŸÑŸÜÿßÿ™ÿ¨ÿ±ÿ±ÿ© ÿπÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ± ÿ± channel  ŸàÿØÿß ÿßŸÑŸàÿ≥ÿ±ÿ±ÿ∑ ÿßŸÑŸÑÿ±ÿ±Ÿä
ÿ®Ÿäÿ™ÿ®ŸÖÿ™ ŸÅŸäŸá ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸáŸà ÿØÿß ÿßŸÑŸÑŸä ÿ≥ÿ®ÿ® ÿßŸÜ ŸÅŸäŸá ÿ£ÿÆÿ∑ÿßÿ° ÿ™ÿ≠ÿµŸÑ ŸÅŸä ÿßŸÑÿ± signal. 
 
 ÿßŸÑŸÑŸä ŸÅÿßÿ™ ÿØÿß ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ÿßŸÑŸÉÿ™ÿ¥ÿßŸÅ ÿßÿßŸÑÿÆÿ∑ÿßÿ° ÿßŸÖÿß ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑÿ™ÿµÿ≠Ÿäÿ≠ÿßÿ£ŸÑÿÆÿ∑ÿßÿ° ÿßŸÑÿ± Receiver ÿ®Ÿäÿ¥ŸàŸÅ ÿßŸÑÿ± code word 
 ÿßŸÑŸÑŸä ŸàÿµŸÑÿ™ ŸÑŸäŸá ÿßŸÇÿ±ÿ® ŸÑŸÖŸäŸÜŸÖŸÜ ÿßŸÑÿ± code word 8 ÿßŸÑŸÖŸàÿ¨ŸàÿØŸäŸÜ ŸäŸÖŸÜŸä ŸÑŸà ŸÅŸäŸá ŸÅÿ±ŸÜ ÿ®ŸäŸÜŸáÿß Ÿàÿ®ŸäŸÜ ÿßÿ≠ÿØ ÿßŸÑÿ± 8  ÿßŸÉÿ±ÿ±ŸàÿßÿØ
ŸÅŸä  bit    Ÿàÿßÿ≠ÿØŸá ŸáŸäŸÅÿ™ÿ±ÿ∂ ÿßŸÜ ÿØŸä ŸáŸä ÿßŸÑbit   , ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ÿßŸÑŸÑŸä ŸÉÿßŸÜ ÿßŸÑŸÖŸÅÿ±Ÿàÿ∂ ÿ™ÿ™ÿ®ŸÖÿ™Ÿà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßŸÑÿÆÿ∑  ŸäŸÉŸàŸÜ 
ŸÅŸä bit  .Ÿàÿßÿ≠ÿØŸá ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿßŸÜ Ÿäÿ≠ÿµŸÑ ÿÆÿ∑ÿß ŸÅŸä ÿßÿ™ŸÜŸäŸÜ ÿßŸà ÿ™ÿßŸÑÿ™Ÿá   
 
ÿπÿØÿØ ÿßŸÑÿ± parity bits ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÖÿ™ŸÜÿßÿ≥ÿ® ŸÖ  ÿßŸÑÿ± propagation problem  ÿßŸÑŸÜ ŸÅŸäŸá ÿØÿ±ÿßÿ≥ÿ© ÿ®ÿ™ŸÖ ŸÑŸÑŸÜÿ∏ÿßŸÖ Ÿàÿ®ŸÜŸÖÿ±ŸÅ
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸà  ÿÆÿ∑  ÿßŸà ÿßÿ™ŸÜŸäŸÜ ÿßŸà ÿ™ÿßŸÑÿ™Ÿá ŸàŸÖŸÜŸáÿß ŸÜŸÖŸÖŸÑ  design ŸÑŸÑÿ± channel encoder  ÿ®ŸÖÿØÿØbits .ŸÖŸÜÿßÿ≥ÿ® 
 

 
 
is the process of converting digital data to baseband signal
 
 
 ÿ≤Ÿä ŸÖÿß ÿ™ŸÖŸÑŸÖŸÜÿß ÿßŸÑÿ≥ŸÜÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿπÿ¥ÿßŸÜ ŸÜÿ®ŸÖÿ±ÿ±ÿ™
ÿ£Ÿä
 
ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ©
 
ÿ®Ÿäÿ®ŸÇÿ±ÿ±Ÿâ ŸÖŸÖÿßŸÜÿ±ÿ±ÿß 
ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ©
 
ÿßÿ£ŸÑÿµÿ±ÿ±ŸÑŸäÿ© 
information signal
 
 Ÿà
carrier
 
 Ÿàÿ®ÿ™ÿ≠ÿµÿ±ÿ±ŸÑ ÿπŸÖŸÑŸäÿ±ÿ±ÿ©
modulation
 
.
 
 ÿßŸÑÿ±ÿ±
ÿ±
 
information signal
 
 Ÿáÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ±
ÿ±
 
Baseband
 
 Ÿà
ÿßŸÑÿ±ÿ±ÿ± 
carrier
 
 Ÿáÿ±ÿ±Ÿà
 
ÿ•ÿ¥ÿßÿ±ÿ©
 
sinusoidal signal
 
.ÿ®ÿ™ÿ±ÿØÿØ ÿπÿßŸÑŸä
 
 
ÿÆÿ±  ÿßŸÑ
ÿ±
 
Line Encoder
 
 ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäÿ±ÿ±Ÿá
Baseband signal
 
 Ÿàÿ®ŸÜÿ≥ÿ±ÿ±ŸÖŸäŸáÿß ŸÉÿ±ÿ±ÿØÿß ÿπÿ¥ÿ±ÿ±ÿßŸÜ ŸÜŸÖŸäÿ≤Ÿáÿ±ÿ±ÿß ÿπÿ±ÿ±ŸÜ ÿßŸÑÿ±ÿ±
ÿ±
 
analog
 
 ÿßŸÑŸÜ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿØŸä ŸäŸÖÿ™ÿ®ÿ± ŸÑÿ≥Ÿá 
analog
 
  ŸàŸÑŸÉŸÜ ÿ®ÿ™ÿßÿÆÿØ ŸÇŸäŸÖÿ™ŸäŸÜ ŸÅŸÇÿ∑
ŸàŸÖÿ±ÿ±ÿ´ÿßŸÑ ŸáŸÜŸÖÿ™ÿ®ÿ±ÿ±ÿ± ÿßŸÜ 
binary 1
 
 Ÿáÿ±ÿ±Ÿà
5
 
v
 
 Ÿà
binary 0
 
 Ÿáÿ±ÿ±Ÿà
  
-
5
v
 
ŸàÿØÿß ÿßÿ≠ÿØ ÿßÿ¥ŸÉÿßŸÑ ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑ
ÿ±
 
 
Baseband Signal
 
 ŸàŸÑŸÉŸÜ ŸÖÿ¥ ÿØÿß ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑŸàÿ≠ŸäÿØ ŸàŸÅŸäŸá ÿπŸäŸàÿ®
ÿπÿ±ŸÅŸÜÿßŸáÿß ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ±ÿ±ÿ© 
ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™  
ÿ≤Ÿä ÿßŸÜ ŸÑŸà ÿ®ŸÖÿ™ ŸÖÿ´ÿßŸÑ ÿπÿØÿØ ÿßÿµŸÅÿßÿ± ŸÉÿ®Ÿäÿ± Ÿàÿ±ÿß ÿ®ŸÖÿ∂ ÿßŸà ÿπÿØÿØ Ÿàÿ≠ÿßŸäÿØ ŸÉÿ®Ÿäÿ± ŸÖÿ±ÿ±ÿ¥ ŸáŸäÿ≠ÿµÿ±ÿ±ŸÑ 
transition
  
 ŸÑŸÑÿ±ÿ±
ÿ±
 
signal
  
ŸÑŸÅÿ™ÿ±ÿ© ŸàÿØÿß ŸáŸäÿ≥ÿ®ÿ® ŸÖÿ¥ÿßŸÉŸÑ ŸÅŸä ÿßŸÑ
ÿ±
 
timing
 
 Ÿà
ÿßŸÑÿ± 
ÿ± 
 
synchronization
 
 ÿßŸÑŸÜ ÿ®ŸäŸÉŸàŸÜ ŸÅŸäŸá
clock
 
 ŸÅÿ±ÿ±Ÿä
transmitter
 
 Ÿà
clk
 
 ŸÅÿ±ÿ±Ÿä
receiver
 
 ÿßÿßŸÑÿ™ŸÜÿ±ÿ±ŸäŸÜ ÿØŸàŸÑ ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜÿ±ÿ±Ÿàÿß
synchronized
 
 ŸÖÿ±ÿ±  ÿ®ŸÖÿ±ÿ±ÿ∂ ŸÅŸÑÿ≠ÿ∏ÿ±ÿ±ÿßÿ™ ÿßÿßŸÑŸÜÿ™ŸÇÿ±ÿ±ÿßŸÑ ÿØŸä Ÿáÿ±ÿ±Ÿä ÿßŸÑŸÑÿ±ÿ±Ÿä
ÿ®ÿ™ÿ≥ÿßÿπÿØŸÜÿß ŸÅŸä ÿ™ÿ≠ŸÇŸäŸÑ ÿßŸÑ  
synchronization
 
 
  ÿßŸÑŸÖŸäÿ® ÿßŸÑÿ™ÿßŸÜŸä ÿßŸÜ ŸÑŸà ÿßŸÑŸä ÿ≥ÿ®ÿ® ŸÖŸÜ ÿßÿ£ŸÑÿ≥ÿ®ÿßÿ® ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™
180
  
 ÿØÿ±ÿ¨ÿ© ŸÉŸÑ ÿßÿßŸÑÿµŸÅÿßÿ± Ÿáÿ™ÿ®ŸÇŸâ Ÿàÿ≠ÿßŸäÿ±ÿ±ÿØ ŸàŸÉÿ±ÿ±ŸÑ ÿßŸÑŸàÿ≠ÿßŸäÿ±ÿ±ÿØ
Ÿáÿ™ÿ®ŸÇŸâ  
ÿßÿµŸÅÿßÿ±
 
ŸàÿßŸÑÿØÿßÿ™ÿß ŸÉŸÑŸáÿß ÿ®ŸÇÿ™ ÿ∫ŸÑÿ∑
 
 
ÿßÿ≥ŸÖ
  
ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑŸÑŸä ŸÅÿßÿ™  
non return to zero 
(
NRZ
)
  
,Ÿàÿ£ÿ≠ÿØ ÿßŸÑ
ÿ±
  
forms
  
  ÿßŸÑŸÑŸä ŸáŸÜÿØÿ±ÿ≥Ÿáÿß
ÿßŸÑ
ÿ±
  
binary 1
 
 ŸáŸÖÿ®ÿ±ÿπŸÜŸá ÿ®
ÿ±
5
 
v
 
 ŸÅŸä ÿßŸÑŸÜÿµ ÿßÿ£ŸÑŸàŸÑ Ÿà
 
-
5
 
v
 
ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÜŸä ŸàÿßŸÑ
ÿ±
 
binary 0
 
 ŸáŸÖÿ®ÿ±ÿπŸÜŸá ÿ®
ÿ±
-
5
 
v
 
 
 ŸÅÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ±ŸÜÿµ ÿßÿ£ŸÑŸàŸÑ Ÿà
 
5 v
 
 ŸÅÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ±ŸÜÿµ
ÿßŸÑÿ™ÿßŸÜŸä ŸàÿßŸÑÿµŸàÿ±ÿ© ÿØŸä ÿ®ÿ™Ÿàÿ∂ÿ≠ ÿßŸÑŸÅÿ±ŸÜ ÿ®ŸäŸÜ ÿßŸÑŸÜŸàÿπŸäŸÜ
.
 
 
 
ÿßŸÑ 
ÿ¥ŸÉŸÑ ÿßŸÑŸÑŸä ŸÅÿ±ÿßÿ™ ÿ≠ÿ±ŸÑ ŸÖÿ¥ÿ±ŸÉŸÑÿ© ÿßŸÑÿ± 
ÿ± 
  
synchronization
  
 ÿßŸÑŸÜ ÿßŸÑÿ±ÿ±
ÿ±
  
transitions
  
 ÿØÿßŸäŸÖÿ±ÿ±ÿß ŸÖŸàÿ¨ÿ±ÿ±ŸàÿØ ŸÅÿ±ÿ±Ÿä ŸÜÿ±ÿ±ÿµ ÿßŸÑÿ±ÿ±
ÿ±
  
bit
ÿå
  
ŸàŸÑŸÉÿ±ÿ±ŸÜ 
ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑÿ™ÿßŸÜŸäŸá ŸÑÿ≥Ÿá ŸÖŸàÿ¨ŸàÿØ
Ÿá
  
ŸàŸáŸä ÿßŸÜ ŸÑŸà ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™ 
180
 
ÿØÿ±ÿ¨ÿ©
 
ÿßŸÑÿØÿßÿ™ÿß ŸÉŸÑŸáÿß Ÿáÿ™ÿ®Ÿàÿ∏
.
 
ŸáŸÜÿØÿ±ÿ≥ ŸÅŸä ÿßŸÑÿ¥ÿßÿ®ÿ™ÿ± ÿØÿß
 
ÿßÿßŸÑÿ¥ŸÉÿßŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑŸÑ
ÿ±
Baseband Signals 
 
 ŸàŸÖÿ≤ÿßŸäÿß ŸàÿπŸäÿ±ÿ±Ÿàÿ® ŸÉÿ±ÿ±ŸÑ ÿ¥ÿ±ÿ±ŸÉŸÑ ŸÅŸäŸáÿ±ÿ±ÿß ŸàŸáŸÜÿ±ÿ±ÿØÿ±ÿ≥
ÿ®ŸÖÿ∂ ÿßŸÑÿØŸàÿßÿ¶ÿ± ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© ÿßŸÑŸÑŸä ŸÖŸÜ ÿÆÿßŸÑŸÑŸáÿß ŸÜŸÖŸÖŸÑ  
generation
 
ŸÑÿ£ŸÑÿ¥ŸÉÿßŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©
  
ŸÑŸÑ
ÿ±
Baseband Signals 
.
 
Baseband Signal
 
 Ÿáÿ±ÿ±Ÿä ÿØŸä
ÿßŸÑÿ±ÿ± 
ÿ±
 
information signal
 
 ÿßŸÑŸÑÿ±ÿ±Ÿä ÿ®ŸÜÿ≠ŸÖŸÑŸáÿ±ÿ±ÿß ÿπŸÑÿ±ÿ±Ÿâ
ÿßŸÑÿ±ÿ± 
ÿ±
 
carrier
 
ŸàŸÜÿ®ŸÖÿ™Ÿáÿ±ÿ±ÿß ŸÑ
ŸÑÿ±ÿ± 
ÿ±
 
digital 
system
 
 
Line
 
Encoder
 
 
 
‚ñ™ Baseband signals cannot be transmitted over radio links or satellites , because this 
would require large antennas to efficien tly radiate the low frequency spectrum of 
the signal.  
 
‚ñ™ A spectrum shift to higher frequencies is also required to  transmit several 
messages simultaneously, by sharing the  large bandwidth of the transmission 
medium (FDM) . 
 
 ÿØŸä ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÑŸä ÿ®Ÿäÿ™ŸÖ ŸÅŸäŸáÿß ÿπŸÖŸÑŸäÿ©ÿßŸÑÿ± Modulation  .ÿ®ŸÖÿØ ŸÖÿß ÿ≠ÿµŸÑŸÜÿß ÿπŸÑŸâÿßŸÑÿ± information signal  ŸàÿßŸÑŸÑŸä ÿ®ŸÜÿ≥ŸÖŸäŸáÿß
Baseband Signal   ŸàÿØŸäÿ•ÿ¥ÿßÿ±ÿ© ÿ®ÿ™ÿ±ÿØÿØÿßÿ™ ŸÖŸÜÿÆŸÅÿ∂ÿ© ÿßŸÑ ÿ™ÿµÿ±ÿ±ŸÑÿ≠ ÿßŸÜŸáÿ±ÿ±ÿß ÿ™ÿ™ÿ®ŸÖÿ±ÿ±ÿ™ ÿπŸÑÿ±ÿ±Ÿâ communication channel 
ŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿ®ŸÖŸäÿØŸá ŸàÿØÿß ŸÑŸÉÿ∞ÿß ÿ≥ÿ®ÿ® ŸÖŸÜŸáÿß ÿßŸÜ  ŸÅŸä  ÿßŸÑÿ±  antenna    ÿßÿ≠ÿØ ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿ®ÿ™ŸÇŸàŸÑ ÿßŸÜŸá ŸÑŸà ÿπŸÜÿØŸäÿ•ÿ¥ÿßÿ±ÿ©  ÿ®ÿ™ÿ±ÿØÿØ ŸÖÿ±ÿ±ŸÜÿÆŸÅÿ∂ 
ÿ≠ÿ¨ŸÖ  ÿßŸÑÿ± antenna  ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿπÿ¥ÿßŸÜÿ™ ÿ± radiate ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿØŸä ŸÉÿ®Ÿäÿ± ÿßŸÖÿß ŸÑŸÖÿß ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ÿ®ÿ™ŸÖŸÑŸâ ÿßŸÑÿ≠ÿ¨ŸÖ ÿØÿß ÿ®ŸäŸÇŸÑ. 
 
ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑÿ™ÿßŸÜŸä ÿπÿØŸÖ ÿ≠ÿØŸà  ÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ŸÅŸä ÿßÿµŸÑŸáÿß ŸÖÿ™ÿØÿßÿÆŸÑÿ© ŸÅŸä ÿßŸÑÿ± ÿ±  frequency domain 
 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿπŸÖŸÑŸäÿ©ÿßŸÑÿ± ÿ±  modulation  ÿ®ŸÜÿ≠ŸÖŸÑ ŸÉŸÑÿ•ÿ¥ÿßÿ±ÿ© ÿπŸÑŸâ carrier ŸÖÿÆÿ™ŸÑŸÅ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÇÿØÿ± ÿßÿ®ŸÖÿ™ ÿßŸÉÿ™ÿ± ŸÖŸÜ 
ÿ•ÿ¥ÿßÿ±ÿ© ŸÅŸä ŸàŸÇÿ™ Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß Ÿäÿ≠ÿµŸÑ ÿ®ŸäŸÜŸáŸÖ ÿ™ÿØÿßÿÆŸÑ. 
 
 ÿßŸÑŸÖŸÅÿ±ÿ±ÿ±Ÿàÿ∂ ŸÜŸÖŸÖÿ±ÿ±ŸÑ modulation ŸÖÿ±ÿ±ŸÜ ÿÆÿ±ÿ±ÿßŸÑŸÑ ÿßŸÜŸÜÿ±ÿ±ÿß ŸÜÿ≠ŸÖÿ±ÿ±ŸÑ ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿπŸÑÿ±ÿ±Ÿâ carrier  Ÿàÿ®Ÿäÿ±ÿ±ÿ™ŸÖ ÿßŸÑÿ™ŸÖÿ±ÿ±ÿØŸäŸÑ ÿπŸÑÿ±ÿ±Ÿâ ÿßÿ≠ÿ±ÿ±ÿØÿßŸÑÿ±ÿ± ÿ± 
Parameter  ÿ®ÿ™ÿßÿπÿ©ÿßŸÑÿ± carrier ÿ∑ÿ®ŸÇÿß ŸÑŸÑÿ± information signal  Ÿàÿ≤Ÿä ŸÖÿß ŸÇŸàŸÑŸÜÿß ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÜ ŸÅŸäÿ±ÿ±Ÿá ÿ¥ÿ±ÿ±ÿßÿ®ÿ™ÿ±
ŸáŸÜÿØÿ±ÿ≥ ÿ£ŸÜŸàÿßÿß ÿßŸÑÿ± modulation  ŸàŸÖŸÜ ÿ∂ŸÖŸÜÿßÿ£ŸÑŸÜŸàÿßÿß ÿØŸä ŸáŸà Amplitude Shift Key (ASK)  Ÿàÿ≤Ÿä ŸÖÿß Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ
ÿßŸÑÿµŸàÿ±ÿ© ÿßŸÜŸá    ÿ®Ÿäÿ™ŸÖ ÿπŸÜ ÿ∑ÿ±ŸäŸÑ ÿ∂ÿ±ÿ® ÿßŸÑÿ± Baseband  ŸÅŸäÿßŸÑÿ± carrier  ÿ≠Ÿä  Ÿàÿßÿ≠ÿØ ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸáŸäÿØŸä ŸÜŸÅÿ≥ ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© 
ŸàÿµŸÅÿ± ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸáŸäÿØŸä ÿµŸÅÿ± ŸàÿØÿß technique  ŸÖŸÜASK  ÿßÿ≥ŸÖŸá on‚Äìoff keying (OOK) 
 
 
 
Carrier Modulator  
1 1 1 0 
Carrier  
signal  
Modulated  
signal  Modulating  
signal  0 
 
 
CDMA (Code Division Multiple Access) is the access  technique in the 3rd 
generation cellular mobile network, that  uses spread spectrum which enables 
multiple users to  transmit at the same time and the same frequency, but with  
different spreading code (sequence).  
 
 ÿßŸÑÿ±  Spread spectrum  ÿØÿßÿßŸÑ ÿ±  technique  ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä CDMA  ŸÅŸä ÿßŸÑÿ¨ŸäŸÑ ÿßŸÑÿ™ÿßŸÑÿ™ ŸÖŸÜ ÿßŸÑŸÖŸàÿ®ÿßŸäŸÑ3G  ,ŸàŸÅŸÉÿ±ÿ™Ÿá
ŸÖÿ®ŸÜŸäŸá ÿπŸÑŸâ ŸÅÿ±ÿØ ÿßŸÑ ÿ±   Spectrum   ÿ®ÿ™ÿßÿßÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
ŸÑŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜ ÿπŸÜÿØŸÜÿß  ÿ•ÿ¥ÿßÿ±ÿ© ÿ≠ÿ≥ÿ®ŸÜÿß ÿßŸÑ ÿ±   Fourier transform  :ÿ®ÿ™ÿßÿπŸáÿß Ÿàÿ∑ŸÑ  ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
 
 
 
 
 
 
 
 
ÿ®ŸÖÿØ ÿπŸÖŸÑŸäÿ©  ÿßŸÑ ÿ±   modulation  ÿ¥ŸÉŸÑÿßŸÑ ÿ±   spectrum   ÿ®ÿ™ÿßÿßÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑ ÿ±   modulated  : ŸáŸäÿ∑ŸÑ  ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
 
 
 
 
 
 
 
 
ŸáŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ÿßŸÑ ÿ±   spectrum ÿ®ŸÇŸâ ŸÖŸÅÿ±Ÿàÿ∂ ŸàÿßŸÑÿ± amplitude spectrum  ŸÇŸÑ  ÿ¨ÿØÿß. 
 
ŸÅŸÉÿ±ÿ© ÿßŸÑŸÖŸàÿ∂Ÿàÿß ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ÿßŸÜ ÿπŸÜÿØŸä  data  ŸÖŸÖŸäŸÜŸá  ÿπÿßŸäÿ≤ ÿßÿ®ŸÖÿ™Ÿáÿß ŸÖŸÉŸàŸÜŸá ŸÖŸÜ ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿ±ÿ±ÿØ Ÿáÿ∂ÿ±ÿ±ÿ±ÿ®ŸáŸÖ ŸÅÿ±ÿ±Ÿä code 
 ŸÖŸÖŸäŸÜ ŸÖŸÉŸàŸÜ ŸÖŸÜ ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ ÿ®ÿ±ÿØŸà ŸàÿßŸÑ ÿ±  code ÿØÿß ŸÑŸäÿ±ÿ±Ÿá ŸÖŸàÿßÿµÿ±ÿ±ŸÅÿßÿ™ ŸÖŸÖŸäŸÜÿ±ÿ±Ÿá ÿßŸÑŸÜ ŸÅÿ±ÿ±Ÿä ÿßÿ£ŸÑÿµÿ±ÿ±ŸÑ ŸÖÿ±ÿ±ÿ¥ ÿ£Ÿä ÿπŸÖŸÑŸäÿ±ÿ±ÿ© 
ÿ∂ÿ±ÿ® ÿ™ÿ≠ÿµŸÑ Ÿäÿ®ŸÇŸâ spreading  ŸàÿßŸÑŸÑŸä ŸáŸäÿ≠ÿØÿØ ŸáŸàÿßŸÑ ÿ±  rate  ÿ®ÿ™ÿßÿßÿßŸÑ ÿ±  code . ŸÑŸà ŸÉÿ±ÿ±ÿßŸÜÿßŸÑÿ± ÿ±  rate  ÿ®ÿ™ÿ±ÿ±ÿßÿßÿßŸÑÿ± ÿ±   code 
  ÿßÿπŸÑŸâ ÿ®ŸÉÿ™Ÿäÿ± ŸÖŸÜÿßŸÑ ÿ±   rate    ÿ®ÿ™ÿßÿßÿßŸÑÿ± ÿ±   data    Ÿäÿ®ŸÇÿ±ÿ±Ÿâ spreading    Ÿàÿ≠ÿßÿµÿ±ÿ±ŸÑ ÿ∂ÿ±ÿ±ÿ±ÿ®ŸáŸÖ ŸáŸäÿ∑ŸÑÿ±ÿ±ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßŸÑÿ± ÿ±  rate  ÿ®ÿ™ÿßÿπŸáÿ±ÿ±ÿß
ÿπÿßŸÑŸä ŸàÿØÿß ÿ®ŸäŸÅÿ≥ÿ± ŸÑŸäŸá ÿ≠ÿµŸÑ  spreading .ÿßŸÑŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ ÿ≤ÿßÿØ 
 
 
 Spread Spectrum Modulator  
ùíá Amp spectrum  
ùüè ùë≤ùëØùíõ ùüèùüé ùë≤ùëØùíõ 
ùíá Amp spectrum  
 
It‚Äôs used to increase the signal bandwidth and decrease its  amplitude, to 
provide additional security.  
The signal is harder to be interpreted as the need of the code (sequence) to get 
the message.  
 
 ŸÖŸÜ ŸÖŸÖŸäÿ≤ÿßÿ™ÿßŸÑ ÿ±  Spread spectrum  ÿßŸÜŸá ÿ®ŸäŸàŸÅÿ± security  ŸÅŸäÿßŸÑÿ± system  ÿßŸÑŸÜ ŸÅŸäŸá code  ÿÆÿßÿµ ÿ®ŸÉÿ±ÿ±ŸÑuser 
Ÿà ÿßŸÑÿ± spectrum ŸÑŸÖÿß ÿ®Ÿäÿ™ŸÅÿ±ÿØ ŸÉÿØÿß ÿ®ŸäŸÉŸàŸÜ ÿ∫Ÿäÿ± ŸÖŸÖÿ±ÿ±ÿ±ŸàŸÅ ŸÑŸÑÿ±ÿ± ÿ± unintended receiver  ÿßŸÑŸÜ ÿßŸÑÿ¥ÿ±ÿ±ŸÉŸÑ ÿØÿß Ÿäÿ®ÿ±ÿ±ÿßŸÜ ÿßŸÑŸä
receiver  ŸÉÿßŸÜÿ±ÿ±Ÿá noise  ŸàŸÑŸÉÿ±ÿ±ŸÜÿßŸÑÿ±ÿ± ÿ± receiver  ÿßŸÑŸÖŸÇÿµÿ±ÿ±ŸàÿØ Ÿáÿ±ÿ±Ÿà ÿπÿ±ÿ±ÿßÿ±ŸÅ ÿßŸÜ ÿØŸä ŸÖÿ±ÿ±ÿ¥  noise ÿßŸÜŸÖÿ±ÿ±ÿß ÿßÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßŸÑÿ±ÿ±ÿ± 
spectrum    ÿ®ÿ™ÿßÿπŸáÿß ŸÖŸÅÿ±ŸàÿØ, ŸàŸÖŸäÿ≤Ÿá ÿ™ÿßŸÜŸäŸá ŸàŸáŸä ÿßŸÜÿ±ÿ±Ÿä ÿßŸÇÿ±ÿ±ÿØÿ± ÿßÿÆŸÑÿ±ÿ±Ÿä ŸÉÿ±ÿ±ÿ∞ÿßuser  ŸÅÿ±ÿ±Ÿä ŸÜŸÅÿ±ÿ±ÿ≥ ÿßŸÑŸàŸÇÿ±ÿ±ÿ™ ÿπŸÑÿ±ÿ±Ÿâ ŸÜŸÅÿ±ÿ±ÿ≥ ÿßŸÑÿ± ÿ±  
carrier    ÿ®ÿ≥ ŸÉŸÑuser   ÿ®ŸäÿßÿÆÿØ code   ŸÖÿÆÿ™ŸÑŸÅ ŸàÿßÿßŸÑŸÉŸàÿßÿØ ÿØŸä ÿ®ŸäŸÉŸàŸÜ ŸÖÿ≠ŸÇŸÑ ŸÅŸäŸáÿß ÿ¥ÿ±ÿ∑ÿßŸÑ ÿ±   orthogonality 
 
Orthogonal  ŸÖŸÖŸÜÿßŸáÿß  ŸÖÿ™ŸÖÿßŸÖÿØ ŸäŸÖŸÜŸä ÿßÿßŸÑŸÉŸàÿßÿØ ŸÖÿÆÿ™ŸÑŸÅÿ© ÿπŸÜ ÿ®ŸÖÿ∂  ŸàÿØŸä ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑŸÑŸä  ÿßŸÑÿ±ÿ± ÿ±  receiver   ŸáŸäŸÅÿ±ŸÇŸÜÿ±ÿ±ÿß ŸÅŸäŸáÿ±ÿ±ÿß
ÿπŸÜ ÿ®ŸÖÿ∂ ŸàÿØÿß ŸÖŸÖŸÜÿßŸá ÿßŸÜ ÿßŸÑÿ± cross correlation  ŸÖÿß ÿ®ŸäŸÜ ÿßÿßŸÑŸÉŸàÿßÿØ Ÿäÿ≥ÿ±ÿ±ÿßŸàŸä ÿµÿ±ÿ±ŸÅÿ± ŸàŸáÿ±ÿ±Ÿà ÿ™ŸÉÿßŸÖÿ±ÿ±ŸÑ ÿ≠ÿßÿµÿ±ÿ±ŸÑ ÿ∂ÿ±ÿ±ÿ±ÿ®
ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ. 
 
 correlation    ŸäŸÖŸÜŸäÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ  ÿßÿßŸÑŸÉŸàÿßÿØ  ÿßŸà ÿßÿßŸÑÿ¥ÿßÿ±ÿßÿ™, ŸÑŸà ÿπŸÜÿ±ÿ±ÿØŸÜÿß ÿßÿ¥ÿ±ÿ±ÿßÿ±ÿ™ŸäŸÜ   sinewave  ÿ≤Ÿä ÿ®ŸÖÿ±ÿ±ÿ∂ ÿ®ÿßŸÑÿ∂ÿ±ÿ±ÿ®ÿ∑
ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ™ÿ±ÿØÿØ ŸàÿßŸÑŸÇŸäŸÖÿ© Ÿàÿ≠ÿ≥ÿ®ŸÜÿß ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ®ŸáŸÖ ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑŸÖŸàÿ¨ÿ® ŸÅŸä ÿßŸÑŸÖŸàÿ¨ÿ® ŸáŸäÿØŸä ŸÖŸàÿ¨ÿ® ŸàÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ≥ÿßŸÑÿ® ŸÅŸä 
ÿßŸÑÿ≥ÿßŸÑÿ® ŸáŸäÿØŸä ŸÖŸàÿ¨ÿ® Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸÖÿß ŸÜŸÖŸÖŸÑ ÿ™ŸÉÿßŸÖŸÑ ŸáŸäÿØŸäŸÜŸä ŸÜÿßÿ™ÿ¨ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸÇŸäŸÖÿ© ŸÉÿ®Ÿäÿ±Ÿá ŸÖŸÖÿß ŸäÿØŸÑ ÿßŸÜ ÿßŸÑÿ± ÿ±  correlation  
 ŸÉÿ®Ÿäÿ± ŸàÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ŸÉÿ®Ÿäÿ±. ŸàŸÑŸà ÿ¨ÿ®ŸÜÿß ŸÖÿ´ÿßŸÑsin  Ÿàcos  ŸàÿØŸàŸÑ ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸáŸÖ orthogonal signal  ÿßŸÑŸÜ ŸÑŸÖÿß Ÿàÿßÿ≠ÿØŸá ŸÅÿ±ÿ±ŸäŸáŸÖ
max  ÿßŸÑÿ™ÿßŸÜŸäŸámin  ŸàÿßŸÑŸÖŸÉÿ≥ ŸÅŸÑŸÖÿß ŸÜÿ≠ÿ≥ÿ® ÿ™ŸÉÿßŸÖŸÑ ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ®ŸáŸÖ ÿßŸÑŸÜÿßÿ™ÿ¨ ŸáŸäÿ®ŸÇŸâ ÿµÿ±ÿ±ŸÅÿ±Ÿàÿ®ÿßŸÑÿ™ÿ±ÿ±ÿßŸÑŸä ÿßŸÑÿ± ÿ±  correlation  
ÿ®ÿµŸÅÿ± ŸäŸÖŸÜŸä ŸÖŸÅŸäÿ¥ ÿßŸä ÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜŸáŸÖ  ,Ÿà ÿØÿß ÿ®ŸäŸÅÿ≥ÿ±ÿ±ÿ± ŸÑŸäÿ±ÿ±Ÿá ÿßÿßŸÑŸÉÿ±ÿ±ŸàÿßÿØ ÿ®ÿ™ÿßÿπÿ±ÿ±ÿ©  CDMA    ÿßŸÑÿ≤ŸÖ ÿ™ŸÉÿ±ÿ±ŸàŸÜ orthogonal   ŸäŸÖŸÜÿ±ÿ±Ÿä
ŸÖÿÆÿ™ŸÑŸÅŸäŸÜ ÿπŸÜ ÿ®ŸÖÿ∂ ÿπÿ¥ÿßŸÜ ÿßŸÑ ÿ±   receiver .ŸäŸÇÿØÿ± ŸäŸÅÿ±ŸÇŸáŸÖ ÿπŸÜ ÿ®ŸÖÿ∂ 
 
ÿ®ÿ™ŸÖ ÿπŸÖŸÑŸäÿ© ÿ∂ÿ±ÿ® ŸÅŸä ÿßŸÑ ÿ±   transmitter ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿßŸÜŸá ÿ®ŸäÿßÿÆÿØ ÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© ŸÉÿ±ÿ±ŸÑ user  ŸàŸäÿ∂ÿ±ÿ±ÿ±ÿ®Ÿáÿß ŸÅÿ±ÿ±Ÿä code  ŸàŸÜŸÅÿ±ÿ±ÿ≥ÿßŸÑÿ± ÿ±  
code    ŸÖŸàÿ¨ŸàÿØ ŸÖŸÜŸá ŸÜÿ≥ÿÆÿ© ŸÅŸäÿßŸÑ ÿ±   receiver  ÿπÿ¥ÿßŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑ ÿ±  modulated  ÿ™ÿ™ÿ∂ÿ±ÿ® ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÉÿ±ÿ±ŸàÿØ ÿßŸÑŸÖŸàÿ¨ÿ±ÿ±ŸàÿØ
ŸÅŸä ÿßŸÑ ÿ±   receiver   Ÿàÿ™ÿ≠ÿµŸÑ ÿπŸÖŸÑŸäÿ© ÿπŸÉÿ≥ŸäŸá( dispreading ) ŸàŸÜŸÇÿØÿ± ŸÜÿ≥ÿ™ÿ±ÿ¨   ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
 
ÿ®ÿßÿÆÿ™ÿµÿßÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ≠ÿµŸÑ  spreading  Ÿà dispreading : ŸÅŸäŸá ÿ¥ÿ±ÿ∑ŸäŸÜ 
1.  ÿßŸÑÿ±   transmitter    ŸàÿßŸÑ ÿ±   receiver   ŸäŸÉŸàŸÜ ŸÑŸäŸáŸÖ ŸÜŸÅÿ≥ÿßŸÑ ÿ±   code 
2.   ÿßŸÑÿ∑ÿ±ŸÅŸäŸÜ ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜŸàÿß synchronize .ŸÖ  ÿ®ŸÖÿ∂ 
 
ŸÅŸäŸá ŸÜŸàÿπŸäŸÜ ŸÖŸÜ  ÿßŸÑ ÿ±   correlation : 
Autocorrelation  :   ŸäŸÇŸäÿ≥ ŸÖÿØŸâ ÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜÿ•ÿ¥ÿßÿ±ÿ© Ÿà shifted version ŸÖŸÜ ŸÜŸÅÿ≥  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© 
cross  correlation     : ŸäŸÇŸäÿ≥ ŸÖÿØŸâ ÿ™ÿ¥ÿßÿ®Ÿá ŸÖÿß ÿ®ŸäŸÜ ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ ŸÖÿÆŸÑŸÅÿ™ŸäŸÜ 
 
 
 
 ŸÅŸä ÿßŸÑÿ±ÿ±ÿ± CDMA  ÿßŸÑÿ±ÿ±ÿ± Autocorrelation  ŸÖÿ±ÿ±ÿß ÿ®ÿ±ÿ±ŸäŸÜÿ•ÿ¥ÿ±ÿ±ÿßÿ±ÿ© Ÿà shifted version  ŸÖŸÜÿ±ÿ±Ÿáÿµÿ±ÿ±ÿ∫Ÿäÿ± ÿ¨ÿ±ÿ±ÿØÿß ŸàÿØÿß ŸÖŸÖŸÜÿ±ÿ±ÿßŸá ÿßŸÜÿ±ÿ± Ÿá 
ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ©  ŸÑŸÑÿ± receiver  ÿßŸÑÿ±ÿ±ÿ± shifted version  ŸÖÿ±ÿ±ŸÜ ŸÜŸÅÿ±ÿ±ÿ≥ ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ŸÅŸäÿ±ŸÅÿ∂ÿ±ÿ±Ÿá Ÿàÿ®ŸÉÿ±ÿ±ÿØÿß ÿ®ŸäŸÖŸÖÿ±ÿ±ŸÑ reject  ŸÑŸÑÿ±ÿ± ÿ± delay 
component  Ÿàÿ®ŸÉÿØÿßÿ≠ŸÑ  ŸÖÿ¥ŸÉŸÑÿ© ISI 
 
  ŸÅŸäÿßŸÑÿ±  CDMA  ÿßŸÑÿ±  Autocorrelation  ÿµÿ∫Ÿäÿ± ÿ¨ÿØÿß Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸáŸäÿ≠ÿ±ÿ±ŸÑ ŸÖÿ¥ÿ±ÿ±ŸÉŸÑÿ©ISI  Ÿà cross-correlation  ÿµÿ±ÿ±ÿ∫Ÿäÿ±
ÿ¨ÿØÿß Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä  ÿßŸÑÿ± receiver ŸáŸäŸÖÿ±ŸÅ ŸäŸÅÿ±ŸÜ ÿ®ŸäŸÜ ÿßÿßŸÑŸÉŸàÿßÿØ. 
 
Ÿàÿßÿ±ÿØ ÿßŸÑÿ± system  ŸäŸÉŸàŸÜ ŸÅŸäŸá spread spectrum ŸàŸàÿßÿ±ÿØ ŸÖŸäŸÉŸàŸÜÿ¥ ŸÅŸäŸá. 
 
 
 
 
Receiver  is simply a mirror image of transmitter . 
the only difference is that the carrier modulator of the transmitter has been replaced 
by carrier demodulator and the symbol synchronizer .   
 
  ÿßŸÑÿ± Receiver    ÿ®Ÿäÿ™ŸÖ ŸÅŸäŸá ÿπŸÉÿ≥ ÿßŸÑŸÑŸä ÿ™ŸÖ ŸÅŸäÿßŸÑÿ±  transmitter    ŸàŸÉŸÑ ÿ®ŸÑŸàŸÉ ŸÅŸä  ÿßŸÑÿ±  Receiver    ŸäŸÜÿßÿ∏ÿ± ÿ®ŸÑŸàŸÉ ŸÅÿ±ÿ±ŸäÿßŸÑÿ±ÿ±ÿ± 
transmitter   ŸÖÿß ÿπÿØÿßÿ®ŸÑŸàŸÉ ÿ≤ŸäÿßÿØÿ© ÿßÿ≥ŸÖŸá symbol synchronizer  ÿπÿ¥ÿßŸÜ ŸÖŸàÿ∂Ÿàÿß ÿßŸÑÿ± synchronization  
 
 ÿßŸÑÿ≤ŸÖÿßŸÑÿ±ÿ± ÿ± digital system  ŸÅÿ±ÿ±ŸäÿßŸÑÿ±ÿ± ÿ± stages  ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ±ÿ±ÿ© Ÿäÿ±ÿßÿπÿ±ÿ±Ÿä ŸÖŸàÿ∂ÿ±ÿ±ŸàÿßÿßŸÑÿ±ÿ± ÿ± synchronization  ÿ®ÿ±ÿ±ŸäŸÜÿßŸÑÿ±ÿ± ÿ± 
transmitter    ŸàÿßŸÑÿ±  receiver   ŸàÿßŸÑÿ™ÿ≤ÿßŸÖŸÜ ŸÖŸÖŸÜÿßŸáÿß ÿßŸÜ ŸäŸÉŸàŸÜ ŸÅŸä ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿß ÿ™ÿ≠ÿØÿØ ÿßŸÖÿ™ŸâÿßŸÑÿ± bit  ÿßŸÜÿ™Ÿáÿ™ Ÿàÿ®ÿßŸÑÿ™ÿ±ÿ±ÿßŸÑŸä ŸÅŸäÿ±ÿ±Ÿá
bit    ÿ¨ÿØŸäÿØŸáÿ®ÿØÿ£ÿ™  ŸàÿßŸÖÿ™Ÿâ  ÿßŸÑÿ±  frame    ÿßŸÜÿ™ŸáŸâ Ÿàÿ®ÿØÿß ŸàÿßŸÖÿ™ŸâÿßŸÑÿ±  message    ÿØŸä ÿÆŸÑÿµÿ™ ŸàÿßŸÖÿ™ŸâÿßŸÑÿ±  message   ÿßŸÑÿ™ÿßŸÜŸäŸá
ÿßÿ™ÿ®ŸÖÿ™ÿ™ ŸàŸáŸÉÿ∞ÿß. 
 
The symbol synchronizer  partitions the overall signal into  segments 
corresponding to each symbol and to each  messag e. 
  ÿ®ŸäŸÇÿ≥ŸÖÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿßŸÑÿµŸÑŸäÿ© ŸÑÿ± segments  ŸàŸÅŸä symbols ŸàŸäÿ≠ÿØÿØ ÿ®ÿØÿßŸäÿ™Ÿáÿß ŸàŸÜŸáÿßŸäÿ™Ÿáÿß. 
 
 
 
 
 
 
 
 Receiver and Symbol Synchronizer  
 
 
 ÿ≤Ÿä ŸÖÿß ÿπÿ±ŸÅŸÜÿß ŸÅÿ±ÿ±Ÿä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ±ÿ±ÿ© ÿßŸÑŸÑÿ±ÿ±Ÿä ŸÅÿßÿ™ÿ±ÿ±ÿ™ ÿßŸÜ ÿßŸÑŸáÿ±ÿ±ÿØŸÅ ŸÖÿ±ÿ±ŸÜ Fourier transform  ÿßŸÑÿ≠ÿµÿ±ÿ±ŸàŸÑ ÿπŸÑÿ±ÿ±Ÿâ ŸÖÿ≠ÿ™ÿ±ÿ±ŸàŸâ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸàŸÖŸÜ ÿÆÿßŸÑŸÑŸáÿß ŸÜŸÖÿ±ŸÅ ÿÆÿµÿßÿ¶ÿµ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÅÿ±ÿ±Ÿä ÿßŸÑÿ±ÿ± ÿ± frequency domain  ÿ≥ÿ±ÿ±Ÿàÿßÿ° ŸÉÿßŸÜÿ±ÿ±ÿ™
periodic  ÿßŸà nonperiodic  ,ŸáŸÜŸÅŸáŸÖ ÿ≥ÿ±ŸäŸÖÿß ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿØŸä ŸÇÿßŸÜŸàŸÜ ÿ™ÿ≠ŸàŸäŸÑ ŸÅŸàÿ±Ÿäÿ± 
 
ùë≠(ùùé)= ‚à´ùíá(ùíï) ùíÜ‚àíùíãùùéùíï ùíÖùíï‚àû
‚àí‚àû 
 
  ŸÇŸäŸÖÿ© magnitude    ÿßŸÑŸÖŸÖÿßÿØŸÑÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© |ùë≠(ùùé)|    ŸäÿØŸÑ ÿπŸÑŸâ ŸÜÿ≥ÿ®ÿ© Ÿàÿ¨ŸàÿØ ÿßŸÑÿ™ÿ±ÿØÿØ ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ®ŸÉÿ±ÿ±ÿßŸÖ ŸàŸÜŸÖÿ±ÿ±ÿ±ŸÅ Ÿáÿ±ÿ±Ÿà 
ŸÖŸàÿ¨ŸàÿØ  ÿ®ŸÜÿ≥ÿ®Ÿá ŸÉÿ®Ÿäÿ±Ÿá  ÿßŸà   ŸÇŸÑŸäŸÑŸá ÿßŸà ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØ ÿßÿµÿßŸÑ. 
 ŸÑŸäŸá  ÿ±Ÿäÿßÿ∂Ÿäÿß  ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ®  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÑŸä ÿπÿßŸäÿ≤ ÿßÿ¨Ÿäÿ® ÿ™ÿ±ÿØÿØÿßÿ™Ÿáÿß ŸÅŸä  ùíÜ‚àíùíãùùéùíï    ŸàÿßŸÉÿßŸÖŸÑ ŸàÿßÿÆÿØÿßŸÑÿ± magnitude 
 ÿßŸÑŸÜÿßÿ™ÿ¨ ŸäÿØŸäŸÜŸä ÿ±ŸÇŸÖ ŸäÿØŸÑ ÿπŸÑŸâ ŸÜÿ≥ÿ®ÿ© Ÿàÿ¨ŸàÿØ ÿßŸÑÿ™ÿ±ÿØÿØ ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©ÿü 
 
ŸÅŸäŸá ÿ≠ÿßÿ¨ÿ© ŸÖŸáŸÖÿ©  ÿØÿ±ÿ≥ŸÜÿßŸáÿß ÿßÿ≥ŸÖŸáÿß  correlation   ŸàŸáŸä ÿ™ŸÉÿßŸÖŸÑ ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ® ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ Ÿàÿ®ŸÜŸÖÿ±ŸÅ ŸÖŸÜŸá ÿßŸÑÿ™ÿ¥ÿ±ÿ±ÿßÿ®Ÿá
ÿ®ŸäŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ÿå ŸàŸÑŸÉŸÜ  ÿ£Ÿä ÿπÿßŸÑŸÇÿ© ÿØÿß ÿ®ÿßŸÜŸÜÿß ŸÜŸÖÿ±ŸÅ ÿ™ÿ±ÿØÿØÿßÿ™ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©ÿü 
 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùíá(ùíï)    ŸÅŸäŸáÿß ÿ™ÿ±ÿ±ÿ±ÿØÿØÿßÿ™ ŸÉÿ™Ÿäÿ±ÿ±ÿ± ÿßŸÜÿ±ÿ±ÿß ŸÖÿ±ÿ±ÿ¥ ÿπÿßÿ±ŸÅŸáÿ±ÿ±ÿß Ÿà ùíÜ‚àíùíãùùéùíï    ŸÑŸÖÿ±ÿ±ÿß ÿ®ŸÜŸÅŸÉŸáÿ±ÿ±ÿß ÿ®ÿ™ÿ±ÿ±ÿØŸäŸÜŸäsin   Ÿàcos  ŸàÿßŸÑÿ™ŸÉÿßŸÖÿ±ÿ±ŸÑ
ÿπŸÜÿØŸä ŸÖŸÜ ŸÖÿßÿßŸÑŸÜŸáÿßŸäÿ© ŸÑÿ≥ÿßŸÑÿ® ŸÖÿßÿßŸÑŸÜŸáÿßŸäÿ© ŸäŸÖŸÜŸä Ÿáÿ∂ÿ±ÿ®  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùíá(ùíï)  ŸÅŸä ŸÉŸÑ ÿßŸÑÿ™ÿ±ÿ±ÿ±ÿØÿØÿßÿ™  ŸÅŸÖŸÜÿ±ÿ±ÿØ ŸÉÿ±ÿ±ŸÑ ÿ™ÿ±ÿ±ÿ±ÿØÿØ ŸÑÿ±ÿ±Ÿà 
ŸÖŸàÿ¨ÿ±ÿ±ŸàÿØ ŸÅÿ±ÿ±Ÿä ÿßÿ•ŸÑÿ¥ÿ±ÿ±ÿßÿ±ÿ© ÿßÿßŸÑÿµÿ±ÿ±ŸÑŸäÿ© ùíá(ùíï)   ÿ®ÿ¥ÿ±ÿ±ŸÉŸÑ ŸÖŸÑÿ≠ÿ±ÿ±Ÿàÿ∏ ŸáŸäÿ™ÿ∑ÿ±ÿ±ÿßÿ®ŸÑ ŸÖÿ±ÿ±sin  Ÿàcos  Ÿàÿ®ÿßŸÑÿ™ÿ±ÿ±ÿßŸÑŸä ÿ™ŸÉÿßŸÖÿ±ÿ±ŸÑ ÿ≠ÿßÿµÿ±ÿ±ŸÑ
ÿ∂ÿ±ÿ®ŸáŸÖ ŸáŸäÿØŸä  correlation  ÿ®ŸÇŸäŸÖÿ© ÿπÿßŸÑŸäÿ© ÿπŸÜÿØ ÿßŸÑÿ™ÿ±ÿØÿØ ÿØÿßÿå  ŸàŸÑŸÉŸÜ ŸÑŸà  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸÖŸÅŸäŸáÿßÿ¥ ÿ™ÿ±ÿØÿØ ŸÖŸÖŸäŸÜ ÿßÿ∞ÿß ŸÜÿßÿ™ÿ¨  ÿßŸÑÿ±ÿ± ÿ± 
correlation  ÿ®ÿ™ÿßÿπÿ±ÿ±Ÿá ÿµÿ±ÿ±ŸÅÿ±ŸàŸÑÿ±ÿ±Ÿà ŸÖŸàÿ¨ÿ±ÿ±ŸàÿØ ÿ®ŸÜÿ≥ÿ±ÿ±ÿ®ÿ© ÿµÿ±ÿ±ÿ∫Ÿäÿ±Ÿá ŸáŸäÿ±ÿ±ÿØŸä  correlation ÿ®ŸÇŸäŸÖÿ±ÿ±ÿ© ŸÇŸÑŸäŸÑÿ±ÿ±ÿ©, ŸàŸÑÿ±ÿ±Ÿà ŸÑŸÇŸäÿ±ÿ±ÿ™  ÿßŸÑÿ±ÿ± ÿ± 
magnitude  ŸÉÿ®Ÿäÿ± Ÿäÿ®ŸÇŸâ ÿßŸÑÿ™ÿ±ÿØÿØ ŸÖŸàÿ¨ŸàÿØ ÿ®ŸÜÿ≥ÿ®Ÿá ÿßŸà weight   ŸÉÿ®Ÿäÿ± ŸàÿßŸÑŸÖŸÉÿ≥. 
 
ÿ®ÿßÿÆÿ™ÿµÿßÿ± ÿ™ÿ≠ŸàŸäŸÑ ŸÅŸàÿ±Ÿäÿ± ÿ±Ÿäÿßÿ∂Ÿäÿß ŸáŸà correlation  ŸäŸàÿ¨ÿ±ÿ±ÿØ ŸÖÿ±ÿ±ÿØŸâ ÿßŸÑÿ™ÿ¥ÿ±ÿ±ÿßÿ®Ÿá ŸÖÿ±ÿ±ÿß ÿ®ÿ±ÿ±ŸäŸÜ ÿßŸÑÿ™ÿ±ÿ±ÿ±ÿØÿØÿßÿ™ÿßŸÑŸÖŸàÿ¨ÿ±ÿ±ŸàÿØÿ© ŸÅÿ±ÿ±Ÿä 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸÖÿß ÿ®ŸäŸÜ ŸÖÿ¨ŸÖŸàÿπÿ© ÿØŸàÿßŸÑ  sin  Ÿàcos .ÿ®ÿ™ÿ±ÿØÿØÿßÿ™ ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© 
 
 Fourier Transform  
 
 
 
 
 
 
¬© Basem Hesham Lecture 
3
 
 
Probabilty & Channel Modelling 
 
 
 
ùë∑ùíì{ùë®}= ùê•ùê¢ùê¶
ùëµ‚Üí‚àûùíèùë®
ùëµ 
ùíèùë® : number of times that the event A occurs in N performances of the experiment  
 ŸáŸà ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿ∏ŸáŸàÿ± ÿßŸÑÿ≠ÿØÿ´A  ŸÅŸä ÿ™ÿ¨ÿ±ÿ®ÿ© ÿ™ŸÖ ÿßÿ¨ÿ±ÿßÿ§Ÿáÿß ÿπÿØÿØN ŸÖŸÜ ÿßŸÑŸÖÿ±ÿßÿ™  
 
In the coin tossing experiment, we may expect that out of a million tosses of a fair 
coin, about one half of them will show up heads.  
  ŸÅŸäÿ™ÿ¨ÿ±ÿ®ÿ© ÿßŸÑŸÇÿßÿ° ÿßŸÑÿπŸÖŸÑÿ©ÿ©ÿ© ŸÑÿ©ÿ©Ÿà ÿ¨ÿ±ÿ®ÿ™Ÿáÿ©ÿ©ÿß ŸÖÿ©ÿ©     10     ŸÖÿ©ÿ©ÿ±ÿßÿ™ Ÿàÿßÿ±ÿØ ŸÑÿπŸÑÿ©ÿ©Ÿá ŸÖÿ©ÿ©ÿßŸÑŸáŸÖ ŸÖÿ©ÿ©8 Head   Ÿàÿ®ÿßŸÑÿ™ÿ©ÿ©ÿßŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑÿ©ÿ©8
10 
  Ÿàÿßÿ™ÿßŸÑŸÑŸÜtail    Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ2
10  ŸÅŸáŸÑ ŸÖÿπÿßŸÑŸâ ŸÉÿØÿß ÿßŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ∏ŸáŸàÿ±  ÿ£Ÿä Ÿàÿßÿ≠ÿØ ŸÖÿßŸÑŸáŸÖ  1
2   ÿ∫ŸÑÿπÿü 
ÿπÿ®ÿπÿß ÿßŸÑ  ÿßŸÑŸÜ ÿπÿ¥ÿßŸÜ ÿßŸÑŸÇÿßÿßŸÑŸàŸÜ ŸÑŸÉŸàŸÜ ÿµÿ≠ŸÑÿ≠  ÿ®ÿßŸÑŸÅÿ™ÿ±ÿ∂  ÿßŸÜ  ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿßÿ¨ÿ±ÿßÿ° ÿßŸÑÿ™ÿ¨ÿ±ÿ®ÿ© ŸÑÿ©ÿ©ÿ§ŸàŸÑ ÿßŸÑÿ©ÿ©Ÿâ ŸÖÿßÿßŸÑÿßŸÑŸáÿßŸÑÿ©ÿ©ÿ©  ŸÑÿπÿßŸÑÿ©ÿ©Ÿä 
ŸÑÿ©ÿ©Ÿà ŸÉÿ±ÿ±ÿßŸÑÿ©ÿ©ÿß ÿ™ÿ¨ÿ±ÿ®ÿ©ÿ©ÿ© ÿßŸÑŸÇÿ©ÿ©ÿßÿ° ÿßŸÑÿπŸÖŸÑÿ©ÿ©ÿ© ÿπÿ©ÿ©ÿØÿØ ÿßŸÑ ÿßŸÑŸáÿ©ÿ©ÿßÿ≠Ÿä ŸÖÿ©ÿ©ŸÜ ÿßŸÑŸÖÿ©ÿ©ÿ±ÿßÿ™ ÿßÿ≠ÿ™ŸÖÿ©ÿ©ÿßŸÑ ÿ∏Ÿáÿ©ÿ©Ÿàÿ± ÿ£Ÿä Ÿàÿßÿ≠ÿ©ÿ©ÿØ ŸÖÿ©ÿ©ÿßŸÑŸáŸÖ Ÿáÿ©ÿ©ÿ©Ÿà 1
2 
 
ùë∑ùíì{ùë® ùíêùíì ùë©}=ùë∑ùíì{ùë®}+ùë∑ùíì{ùë©} 
ùë∑ùíì{ùë® ùíÇùíèùíÖ ùë©}=ùë∑ùíì{ùë®}‚àôùë∑ùíì{ùë©} 
 
or  ŸÖÿπÿßŸÑÿßŸáÿß ÿ¨ŸÖŸá two event  Ÿàand  ŸÖÿπÿßŸÑÿßŸáÿß ÿ∂ÿ±ÿ® two event  .ŸàŸáÿßŸÑŸÅŸáŸÖ ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿßÿ≠ŸÑ 
 
 
 
 
If the random variable is X ,then the distribution function ùë≠(ùíôùê®) is  
ùë≠(ùíôùê®)=ùë∑ùíì { ùëø ‚â§ùíôùê® } 
 
where ùë∑ùíì { ùëø ‚â§ùíôùê® } is the probability that the value taken by the random 
variable X is less than or equal to a real number ùíôùê® 
 
Distribution Function ŸáŸä ÿØÿßŸÑÿ© ÿ™ÿπÿπŸä ÿ™Ÿàÿ≤ŸÑŸá ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿπÿ¥Ÿàÿßÿ≠Ÿä Ÿà ÿ®ŸÇÿØÿ± ÿßÿ≠ÿ≥ÿ® ŸÖŸÜ ÿÆ ŸÑŸáÿ©ÿ©ÿß 
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿ™ŸÉŸàŸÜ ŸÇŸÑŸÖÿ©   ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä X  ÿßŸÇŸÑ ŸÖŸÜ ÿßŸà ÿ™ÿ≥ÿßŸàŸä ŸÇŸÑŸÖÿ© ŸÖÿπŸÑÿßŸÑ  ŸÖŸÜ ÿßŸÑŸÇÿ©ÿ©ŸÑŸÖ ÿßŸÑŸÖŸÖŸÉÿßŸÑÿ©ÿ©ÿ©ŸÑŸÇÿ©ÿ©ŸÑŸÖ ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ©ÿ©ÿ±  
ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä. 
 
 Random VariableÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä X  ŸáŸàÿ£Ÿä ŸÖÿ™ÿ∫ŸÑÿ± ŸÇŸÑŸÖÿ™  ŸÖÿ¥  ÿßÿ®ÿ™  ÿπÿßŸÑÿØ ŸÇŸÑŸÖÿ©ÿ©ÿ© ŸÖÿ≠ÿ©ÿ©ÿØÿØ  ŸÑÿ©ÿ©ŸÉŸÑÿ≥ ÿ≥ÿ©ÿ©ÿßŸÑŸÇŸàŸÖ 
ÿ®ÿØÿ±ÿßÿ≥ÿ© ÿßÿ≠ÿ™ŸÖÿßŸÑŸÑÿ© ÿßŸÜ ÿ™ŸÉŸàŸÜ ŸÇŸÑŸÖÿ© ŸáŸÉÿß ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿπÿßŸÑÿØ  range  ŸÖÿπÿ©ÿ©ŸÑŸÜ ŸÖÿ©ÿ©ŸÜ ÿßŸÑŸÇÿ©ÿ©ŸÑŸÖ.  ÿ≤Ÿä ŸÖÿ©ÿ©   ÿØÿ±ÿ¨ÿ©ÿ©ÿ© ÿßŸÑÿ≠ÿ©ÿ©ÿ±ÿßÿ±  ŸÅÿ©ÿ©Ÿä 
ŸÖÿßŸÑÿπŸÇÿ© ŸÖÿπŸÑÿßŸÑ  ŸàŸÖÿßŸÑÿπÿ±ŸÅÿ¥ ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ≠ÿ±ÿßÿ±  Ÿáÿ™ÿπŸÑŸá ÿ®ÿßŸÑÿ∂ÿ®ÿπ ŸÉÿßŸÖ ŸÅÿ®ÿßŸÑÿßÿÆÿØ  range   ŸÖÿ≠ÿØÿØŸÖŸÜ ÿßŸÑÿØÿ±ÿ¨ÿßÿ™ . Probability  
Distribution Function     ùë≠(ùíôùê®) 
 
 ŸÖ ÿßŸÑ ÿ™ÿßÿßŸÑŸä : ŸÑŸà ÿπÿßŸÑÿØŸä baseband signal  ÿßŸÑŸÑŸä ÿßÿßŸÑÿß ÿ®ÿßÿπÿ™Ÿáÿß ŸÅÿ©ÿ©ŸäÿßŸÑÿ©ÿ© ÿ© system  ŸàÿßŸÑŸÅÿ™ÿ©ÿ©ÿ±ÿ∂ ÿßÿßŸÑÿßŸÑÿ©ÿ©ÿß ŸáÿßŸÑÿπÿ®ÿ©ÿ©ÿ± ÿπÿ©ÿ©ŸÜ 1 
binary  ÿ®ÿ© 5 V  ŸàŸÖÿπÿ®ÿ± ÿπŸÜ0 binary  ÿ®ÿ© 0 V  ŸàÿßÿßŸÑÿ¥ÿßÿ±  ÿπŸÖŸÑÿ™ŸÑŸáÿß modulation  Ÿàÿ®ÿπÿ™Ÿáÿ©ÿ©ÿßÿß ÿßÿßÿ¥ÿ©ÿ©ÿßÿ±  ÿØŸä Ÿàÿßÿ±ÿØ
ÿ™ÿ™ÿπÿ±ÿ∂ ŸÑÿ© noise  ŸàŸàÿßÿ±ÿØ ÿ™ÿ™ÿπÿ±ÿ∂ŸÑÿ© propagation problems  Ÿàÿ®ÿ≥ÿ®ÿ® ÿØÿßÿßÿßÿ¥ÿßÿ±  ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÖÿ™Ÿàÿµÿ©ÿ©ŸÑÿ¥ 5 
V  ÿßŸà0 V  ÿ®ÿßŸÑÿ∂ÿ®ÿπ ŸàŸàÿßÿ±ÿØ ŸÑÿ®ŸÇŸâ ŸÅŸÑ  ÿ≤ŸÑÿßÿØ  ÿßŸà ÿßŸÑŸÇÿµÿßŸÜ ÿπŸÜ ÿßŸÑŸÇŸÑŸÖÿ© ÿßŸÑŸÖÿ≠ÿØÿØ  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÇÿ©ÿ©ÿØÿ±ÿ¥ ÿßÿ≠ÿ©ÿ©ÿØÿØ ÿ®ÿßŸÑÿ∂ÿ©ÿ©ÿ®ÿπ
Ÿáÿ™ŸàÿµŸÑ ŸÉÿßŸÖ ŸÑŸÑÿ© Receiver ÿß ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä ŸáÿßŸÑÿß ŸáŸà ÿßŸÑÿ© level voltage ( ŸÇŸÑŸÖÿ© ÿßŸÑÿ©ÿ©ÿ© volt  ÿßŸÑŸÑÿ©ÿ©Ÿä Ÿàÿßÿµÿ©ÿ©ŸÑ ŸÑŸÑÿ©ÿ©ÿ© 
Receiver  ŸÑŸÑÿ™ÿπÿ®ŸÑÿ± ÿπÿ©ÿ©ŸÜ binary  0  ÿßŸà binary  1) ÿß ÿ®ÿßŸÑÿ™ÿ©ÿ©ÿßŸÑŸä ÿßŸÑŸÇŸÑŸÖÿ©ÿ©ÿ© ÿßŸÑŸÑÿ©ÿ©Ÿä Ÿàÿßÿµÿ©ÿ©ŸÑ  ŸÑŸÑÿ©ÿ© ÿ© Receiver  Ÿáÿ©ÿ©Ÿä ŸÇŸÑŸÖÿ©ÿ©ÿ©
random  Ÿàÿ∫ŸÑÿ± ŸÖÿ≠ÿØÿØ  ŸÅÿ®ŸÑÿ≠ÿπ threshold  ŸÖÿπŸÑŸÜ ŸÑŸÇÿßÿ±ŸÜ ÿ®ŸÑ  ŸàŸáÿßŸÑŸÅÿ™ÿ±ÿ∂ ÿßÿßŸÑ  ŸáÿßŸÑÿß 2.5 V  ŸàÿßŸä ŸÇŸÑŸÖÿ© ÿßÿπŸÑŸâ ŸÖŸÜ
2.5    ŸáÿßŸÑŸÅÿ™ÿ±ÿ∂ ÿßÿßŸÑŸáÿß ŸÖÿπÿ®ÿ±  ÿπŸÜ1    ŸàÿßŸä ŸÇŸÑŸÖÿ©ÿßŸÇŸÑ  ŸÖŸÜ 2.5  ŸáÿßŸÑŸÅÿ™ÿ±ÿ∂ ÿßÿßŸÑŸáÿß ŸÖÿπÿ®ÿ©ÿ©ÿ±  ÿπÿ©ÿ©ŸÜ0ÿß Ÿàÿ®ÿßŸÑÿ™ÿ©ÿ©ÿßŸÑŸä ŸÑÿ©ÿ©Ÿà ÿπÿ©ÿ©ÿßŸÑÿ≤ 
ÿßÿ≠ÿ≥ÿ®  ŸÖ    ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸàÿµŸàŸÑ ÿßŸÑÿµŸÅÿ±  Ÿáÿ¨ŸÑÿ® ÿßŸÑÿ©  probability  ÿßŸÜ ÿßŸÑÿ¨ŸáÿØ ÿßŸÑŸàÿßÿµŸÑ ŸÑŸÑÿ©  Receiver   ŸÇŸÑŸÖÿ™  ÿ™ŸÉÿ©ÿ©ŸàŸÜ
ÿßŸÇŸÑ ŸÖÿ©ÿ©ŸÜ 2.5  ÿßÿßŸÑŸÑÿ©ÿ©Ÿä ÿ®ŸÑÿ≠ÿ≥ÿ©ÿ©ÿ® ÿßŸÑŸÉÿ©ÿ© ŸÖ ÿØÿß ÿØÿßŸÑÿ©ÿ©ÿ© ÿßÿ≥ÿ©ÿ©ŸÖŸáÿß distribution function  ŸàŸÑÿ©ÿ©Ÿà ÿ¨ŸÑÿßŸÑÿ©ÿ©ÿß ÿßŸÑÿπÿ®ÿ©ÿ©ŸÑ ÿØÿß ÿπŸÑÿ©ÿ©Ÿâ
ÿßŸÑŸÇÿßÿßŸÑŸàŸÜ 
 
ùë≠(ùíôùê®)=ùë∑ùíì { ùëø ‚â§ùíôùê® } 
 ùëøŸáŸä ÿßŸÑÿ© level voltage  )(ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠ŸäÿßŸÑŸÖÿπÿ®ÿ± ÿπŸÜ binary 1  Ÿà binary 0  
 ùíôùê®  ŸÅŸä ÿßŸÑŸÖ ÿßŸÑ ÿßŸÑŸÑŸä ŸÇŸàŸÑÿßŸÑÿß ÿπŸÑŸÑ  ŸáŸä 2.5 V 
 ùë≠(ùíôùê®) ŸáŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿ¨ŸáÿØ ÿßŸÑŸàÿßÿµŸÑ ŸÑŸÑÿ© Receiver  ŸÑŸÉŸàŸÜ ÿßŸÇŸÑ ŸÖŸÜ2.5 
 
Notes : we use capital letters for the random variables and lowercase letters for 
the values they take.  
 
The distribution function ùë≠(ùíôùê®) has the following properties:  
ùë≠(‚àí‚àû)=ùë∑ùíì { ùëø ‚â§‚àí‚àû }=ùüé     
 ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßŸÑÿ© random variable ÿ£ŸÑÿß ŸÉÿßÿßŸÑÿ™ ŸÇŸÑŸÖÿ™  ÿßŸÇŸÑ ŸÖŸÜ  ‚àí‚àû   Ÿàÿπÿ®ÿπÿß ŸÖŸÅŸÑÿ¥ ŸÇŸÑŸÖÿ© ÿßŸÇŸÑ ŸÖŸÜ‚àí‚àû 
 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÇŸÑŸÖÿ™  ÿ®ÿµŸÅÿ± 
ùë≠(+‚àû)=ùë∑ùíì { ùëø ‚â§‚àû }=ùüè        
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßŸÑÿ© random variable   ÿßŸÇŸÑ ŸÖŸÜ‚àû   ÿ®Ÿàÿßÿ≠ÿØ ÿßŸÑŸÜ ÿßŸÉŸÑÿØ ŸÖŸáŸÖÿß ŸÉÿßÿßŸÑÿ™ ŸÇŸÑŸÖÿ™  Ÿáÿ™ŸÉŸàŸÜ ÿßŸÇŸÑ ŸÖŸÜ‚àû  ŸàÿØÿß
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÉŸÑÿØ ŸàÿßÿßŸÑÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ£ŸÑŸÉŸÑÿØ ŸÇŸÑŸÖÿ™  ÿ®Ÿàÿßÿ≠ÿØ.   
ùüé‚â§ùë≠(ùíôùê®)‚â§ùüè  
ÿ£Ÿä ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÇŸÑŸÖÿ™  ŸÅŸä ÿßŸÑÿ© range  ÿ®ŸÑŸÜ0  Ÿà1 
 
ÿßŸÑÿ©  probability    ŸÖŸÜÿßŸÑÿ≠ÿßÿ¨ÿßÿ™  ÿßŸÑŸÖŸáŸÖÿ©ÿ©ÿ© ÿ¨ÿ©ÿ©ÿØÿß ŸÅÿ©ÿ©Ÿä ŸÖÿ¨ÿ©ÿ©ÿßŸÑ ÿß ŸÑÿ©ÿ© ÿ©  communications   ÿ≠ŸÑÿ©ÿ©ÿ´ ÿßÿßŸÑÿ©ÿ©  ŸÑÿ©ÿ©ÿ™ŸÖ ÿßÿ≥ÿ©ÿ©ÿ™ÿÆÿØÿßŸÖŸáÿß
ŸÑŸÖÿπÿ±ŸÅÿ© Ÿàÿ¨ŸàÿØ noise  ÿßŸàÿ£ÿÆÿπÿßÿ° ŸÅŸä ÿßÿßŸÑÿ±ÿ≥ÿßŸÑÿß Ÿà ŸÖŸÜ ÿßŸÑŸÖÿµÿπŸÑÿ≠ÿßÿ™ ÿßŸÑŸÖŸáŸÖÿ© ŸÅŸä ÿßŸä digital system Ÿáÿ©ÿ©Ÿä ÿßŸÑÿ©ÿ© ÿ© 
probability of error  ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿπÿ£ ŸÑÿπÿßŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™1 ŸÅÿ©ÿ©Ÿä ÿßŸÑÿ©ÿ© ÿ© transmitter  ŸàŸÑ Ÿàÿµÿ©ÿ©ŸÑ 0 ŸÅÿ©ÿ©Ÿä ÿßŸÑÿ©ÿ© ÿ© 
receiver  
 
 
 
probability density function
 
  ŸáŸä ÿØÿßŸÑÿ© ÿ™ŸàŸÅÿ± ÿßÿ≠ÿ™ŸÖÿßŸÑŸÑÿ© ŸàŸÇŸàÿπ ŸÇŸÑŸÖÿ© ŸÖÿ™ÿ∫ŸÑÿ± ÿπÿ¥Ÿàÿßÿ≠Ÿä ÿ®ŸÑŸÜ ÿßŸÑÿπÿßŸÇ ŸÖÿπŸÑŸÜ ŸÖŸÜ
ÿßŸÑŸÇŸÑŸÖ.
 
The 
derivative of the distribution function is :
 
 
ùë∑
(
ùíô
)
=
ùíÖùë≠
(
ùíô
ùê®
)
ùíÖùíô
 
ùë≠
(
ùíô
ùê®
)
=
ùë∑
ùíì
 
{
 
ùëø
 
‚â§
ùíô
ùê®
 
}
=
 
‚à´
ùë∑
(
ùíô
)
 
ùíÖùíô
ùíô
ùê®
‚àí
‚àû
 
 
Where 
ùë∑
(
ùíô
)
 
is known as the 
probability density function
 
of the random variable X
 
 
The probability that 
ùëã
 
is between any two limits is found by integrating the density 
function between these two limits
 
 ÿ™ÿ≥ŸÖŸâ ÿßŸÑÿØÿßŸÑÿ©
ùë∑
(
ùíô
)
 
 ÿ®
ÿ©
 
probability density function
  
  ŸàŸáŸä ÿØÿßŸÑÿ© ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ© ÿ™ÿ≠ÿ™ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿßŸÑÿÆÿßÿµ ÿ®Ÿáÿß ÿ®ŸÑŸÜ
ÿßŸÑŸÇÿπÿ™ŸÑŸÜ ŸáŸà 
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿ´ ÿßŸÑ
ÿ©
 
X
 
 .ÿ®ŸÑŸÜ Ÿáÿßÿ™ŸÑŸÜ ÿßŸÑÿßŸÑŸÇÿπÿ™ŸÑŸÜ
 
 
ÿßŸÑ
ÿ©
 
probability density function
 
 ŸáŸä ÿØŸàÿßŸÑ
ÿ•ÿ≠ÿµÿßÿ≠ŸÑÿ©
 
ŸÑŸÑŸáÿß ÿßÿ¥ŸÉÿßŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸàŸÖÿßŸÑŸáÿß 
Gaussian distribution
  
 )(ÿ¥ŸÉŸÑ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿßŸÑÿ¨ÿ±ÿ≥Ÿä
 
ŸàÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿØÿß ÿπ ŸÇÿ© ÿ®ŸÑŸÜ ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä 
X
  
  ÿπŸÑŸâ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßÿßŸÑŸÅŸÇŸä Ÿàÿ®ŸÑŸÜ
ùë∑
(
ùíô
)
 
.ÿπŸÑŸâ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿ±ÿßÿ≥Ÿä
 
ŸÑÿ≠ÿ≥ÿßÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßŸÑŸÖÿ™ÿ∫ŸÑÿ± ÿßŸÑÿπÿ¥Ÿàÿßÿ≠Ÿä 
X
 
  ŸÑŸÇŸá ŸÅŸä ÿßŸÑŸÖÿØŸâ ÿ®ŸÑŸÜ
ùíô
ùüè
 
  Ÿà
ùíô
ùüê
 
  ŸÑÿ¨ÿ® ÿ≠ÿ≥ÿßÿ®
ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ© ÿßÿ≥ŸÅŸÑ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿ≤Ÿä ŸÖÿß Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÖ ÿßŸÑÿ®ŸÑÿßÿßŸÑŸä ÿßÿßŸÑÿ™Ÿä: 
 
 
 
 
 
 
 
 
 
ùë∑
ùíì
 
{
 
ùíô
ùüè
 
‚â§
ùëø
‚â§
ùíô
ùüê
 
}
=
 
‚à´
ùë∑
(
ùíô
)
 
ùíÖùíô
ùíô
ùüê
ùíô
ùüè
 
 
 
 
 
probability density function
  
ùë∑
(
ùíô
)
 
ùíô
ùüè
 
ùíô
ùüê
 
ùë∑
(
ùíô
)
 
ùë∑
ùíì
 
{
 
ùíô
ùüè
 
‚â§
ùëø
‚â§
ùíô
ùüê
 
}
 
 
  ŸÑŸà ŸÖ  ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿØÿß ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿØÿ±ÿ¨ÿßÿ™ ÿßŸÑÿ≠ÿ±ÿßÿ±  ŸÅŸä ŸÖÿßŸÑÿπŸÇÿ© ŸÖÿπŸÑÿßŸÑ   Ÿà ÿßŸÑŸÅÿ™ÿ±ÿ∂ ÿßŸÜ   ùíôùüè=25¬∞ C     Ÿà  
ùíôùüê=30¬∞ C    ŸàÿπÿßŸÑÿ≤  ÿßÿ¨ŸÑÿ®  ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿØÿ±ÿ¨ÿ©ÿ©ÿ© ÿßŸÑÿ≠ÿ©ÿ©ÿ±ÿßÿ±  ŸÅÿ©ÿ©Ÿä ÿßŸÑŸÖÿßŸÑÿπŸÇÿ©ÿ©ÿ© ÿØŸä ÿ™ŸÇÿ©ÿ©Ÿá ŸÖÿ©ÿ©ÿß ÿ®ÿ©ÿ©ŸÑŸÜ 25  Ÿà 30 ÿßÿßŸÑÿ≠ÿ©ÿ©ŸÑ ÿπÿ©ÿ©ŸÜ 
ÿπÿ±ŸÑÿØ ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ© ÿ™ÿ≠ÿ™ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿ®ŸÑŸÜ ÿßŸÑÿßŸÑŸÇÿπÿ™ŸÑŸÜ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ  probability density function 
 
‚à´ùë∑(ùíô) ùíÖùíô‚àû
‚àí‚àû=1 
  ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿØÿß ŸÖÿπÿßŸÑÿß  ÿßÿßŸÑŸä ÿ®ÿ¨ŸÑÿ® ŸÖÿ≥ÿßÿ≠ÿ© ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ŸÉŸÑŸáÿß Ÿà ŸÉŸÑ ÿ¨ÿ≤ÿ° ÿ™ÿ≠ÿ™ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÑÿ®ŸÇŸâ ÿßŸÉŸÑÿØ
ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿØÿß ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ŸÖÿ¨ŸÖŸàÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑŸÉŸÑŸä Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸáŸÑÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ  
 
 
 
 
Is the most common density encountered in the real world  
  ŸÑÿπÿ™ÿ®ÿ± ŸáŸÉÿß ÿßŸÑÿ™Ÿàÿ≤ŸÑŸá ŸÖŸÜ ÿßŸÉ ÿ± ÿßŸÑÿ™Ÿàÿ≤ŸÑÿπÿßÿ™ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßŸÑŸÑÿ© ÿßÿ≥ÿ™ÿÆÿØÿßŸÖÿß  ŸàŸÑŸÑŸáÿß ÿ™ÿπÿ®ŸÑŸÇÿßÿ™ ŸÉÿ™ŸÑÿ± ÿ¨ÿØÿß   
ÿ®ÿßŸÑŸÉÿ™ÿ®Ÿáÿß Density ÿßÿÆÿ™ÿµÿßÿ± ŸÑÿ© probability density function 
 
ùë∑(ùíô)= ùüè
‚àöùüêùùÖ ùùà ùíÜ‚àí(ùíô‚àíùíé)ùüê
ùüê ùùàùüê  
m , ùùà  are constants.  
m (mean) : indicates the center position or symmetry  point of the density.  
 ŸÖÿßŸÑÿ≠ÿßŸÑŸâ ŸáŸÉÿß ÿßŸÑÿ™Ÿàÿ≤ŸÑŸá ŸÖÿ™ŸÖÿß ŸÑ ÿ≠ŸàŸÑ ÿßŸÑŸÇÿπÿ© ÿßŸÑŸàÿ≥ÿπ ÿßŸÑÿ≠ÿ≥ÿßÿ®Ÿä( mean) ÿß ŸàÿØÿß ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßŸÑÿ© symmetry point 
ŸàŸáŸä ŸÇŸÑŸÖÿ© ŸÑŸÑÿ© random variable  ÿ®ÿ™ÿÆŸÑŸä ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿ≠ŸàÿßŸÑŸÑŸáÿß symmetry. 
 
ùùàùüê (Variance) : indicates the spread of the density.  
 ÿ®ÿ™ÿ≠ÿØÿØÿßŸÑÿ© spread ÿßÿßŸÑÿßŸÑÿ™ÿ¥ÿßÿ± ÿßŸà ÿßÿßŸÑÿ™ÿ≥ÿßÿπ ÿ®ÿ™ÿßÿπ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâÿß Ÿàÿ®ŸÑŸÇŸÑÿ≥ ŸÖŸÇÿØÿßÿ± ÿ™ÿ¥ÿ™ÿ™ ÿßŸÑŸÇŸÑŸÖ ÿπŸÜ ÿßŸÑŸàÿ≥ÿπ ÿßŸÑÿ≠ÿ≥ÿßÿ®Ÿä ŸàŸÑŸà  
ŸÉÿßÿßŸÑÿ™ ŸÇŸÑŸÖÿ™  ŸÉÿ®ŸÑÿ±  ÿØÿß ŸÖÿπÿßŸÑÿß  ÿßŸÜ ÿßŸÑŸÇŸÑŸÖ ŸÖÿ™ÿ®ÿßÿπÿØ  ÿπŸÜ ÿ®ÿπÿ∂Ÿáÿß ŸàÿπŸÜ ÿßŸÑŸàÿ≥ÿπ ÿßŸÑÿ≠ÿ≥ÿßÿ®Ÿä ŸÑÿπÿßŸÑŸä ÿßÿ™ÿ≥ÿßÿπ ÿßŸÑŸÖÿßŸÑÿ≠ÿßŸÑŸâ ÿ®ŸÑÿ≤ŸÑÿØ  
ŸàŸÑŸà ŸÉÿßÿßŸÑÿ™ ŸÇŸÑŸÖÿ™  ÿµÿ∫ŸÑÿ±  ÿØÿß ŸÖÿπÿßŸÑÿß  ÿßŸÜ ÿßŸÑŸÇŸÑŸÖ ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ© ŸÖŸÜ ÿ®ÿπÿ∂Ÿáÿß ŸàŸÖŸÜ ÿßŸÑŸàÿ≥ÿπ ÿßŸÑÿ≠ÿ≥ÿßÿ®Ÿä.  
ŸÇŸÑŸÖÿ© ÿßŸÑŸÖÿ™Ÿàÿ≥ÿπ ÿßŸÑÿ≠ÿ≥ÿßÿ®Ÿä ŸÑŸàÿ≠ÿØ  ŸÖÿ¥ ŸÉÿßŸÅŸä ÿßŸÑÿ≤ŸÖ ÿßŸÑÿ¨ŸÑÿ® ŸÇŸÑŸÖÿ© ÿ™Ÿàÿ∂ÿ≠ ŸÖÿØŸâ ÿ®ŸèÿπÿØ ÿßŸÑŸÇŸÑŸÖ ÿπŸÜ  ÿßŸÑÿ© mean    
 
 
 
 
 
 Gaussian Density  
 
 
 
 
 
 
 
 
 
 
 
 
 
 ÿßŸÑÿ™ŸÉÿßŸÖŸÑ
ÿßŸÑŸÑŸä ŸÅÿßÿ™ 
 
ÿ±
ŸÑ
ÿ™
ŸÉ
 
ŸÖ
ÿØ
ÿÆ
ÿ™
ÿ≥
ŸÑ
ÿ®
 
 
ÿßŸÑ
ÿß
 
 
ÿß
ÿ±
ÿ∏
ÿßŸÑ
ÿπŸÖŸÑŸàŸÑ  
ÿ¨ÿØÿßŸàŸÑ ŸàÿπÿßŸÑÿØŸä ÿØÿßŸÑÿ™ŸÑŸÜ ÿ®ŸÑÿ≥ŸáŸÑŸàÿß ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸàŸáŸÖ : 
 
1.
 
Error function   
ùíÜùíìùíá
(
ùíô
)
 
ùíÜùíìùíá
(
ùíô
)
=
ùüê
‚àö
ùùÖ
 
‚à´
ùíÜ
‚àí
 
ùíñ
ùüê
 
ùíÖùíñ
ùíô
ùüé
 
 
2.
 
Error function complementary  
ùíÜùíìùíáùíÑ
(
ùíô
)
 
ùíÜùíìùíáùíÑ
(
ùíô
)
=
ùüê
‚àö
ùùÖ
 
‚à´
ùíÜ
‚àí
 
ùíñ
ùüê
 
ùíÖùíñ
‚àû
ùíô
=
ùüè
‚àí
ùíÜùíìùíá
(
ùíô
)
 
 
Notes:
 
ùüê
‚àö
ùùÖ
 
‚à´
ùíÜ
‚àí
 
ùíñ
ùüê
 
ùíÖùíñ
ùíô
ùüê
ùíô
ùüè
=
ùíÜùíìùíá
(
ùíô
ùüê
)
‚àí
ùíÜùíìùíá
(
ùíô
ùüè
)
 
ùíÜùíìùíá
(
‚àû
)
=
ùüè
 
 
 
 
 
 
 
,
 
 
 
 
 
ùíÜùíìùíá
(
ùüé
)
=
ùüé
 
 
 
 
 
 
 
,
 
 
 
 
 
ùíÜùíìùíá
(
‚àí
ùíô
)
=
‚àí
ùíÜùíìùíá
(
ùíô
)
 
 
 
 
 
   ÿØŸàŸÑ ÿØŸàÿßŸÑ ÿ™ŸÉÿßŸÖŸÑÿ™ŸáŸÖ ŸÖÿ≠ÿ≥Ÿàÿ®ÿ© Ÿàÿ¨ÿßŸáÿ≤
ŸàÿßŸÑÿ¨ÿØŸàŸÑ ŸÖ   ŸÖŸàÿ¨ŸàÿØ ŸÅŸÑ  ŸÇŸÑŸÖ 
X
  
  Ÿà
ùíÜùíìùíá
(
ùíô
)
 
 ŸàÿπÿßŸÑÿØ
X
  
  ÿ®ŸÉŸÉÿß ÿ®ÿ¨ŸÑÿ®
ŸÇŸÑŸÖÿ©  
ùíÜùíìùíá
(
ùíô
)
 
 ŸàŸÅŸÑ  ÿ¨ÿØŸàŸÑ ÿ™ÿßÿßŸÑŸä ÿÆÿßÿµ
ÿ® 
ÿ©
 
ùíÜùíìùíáùíÑ
(
ùíô
)
 
 
ÿßŸÑŸÑŸä ŸáÿπŸÖŸÑ  ÿßÿßŸÑŸä ŸáŸÉÿßŸÖŸÑ ÿßŸÑ
ÿ©
 
density function
 
 ŸàŸáŸàÿµŸÑ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿßŸÑÿ≠ÿØ ÿßŸÑÿµŸàÿ±ÿ™ŸÑŸÜ ÿßŸÑŸÑŸä ŸÅŸàŸÇ ÿ≠ÿ≥ÿ®
ÿßŸÑŸÖÿ≥ÿßŸÑÿ©
  
ŸÑÿπÿßŸÑŸä ÿ®ÿ¥ŸàŸÅ ÿßŸÑŸÑŸä Ÿáÿ™ÿ≥ŸáŸÑ ÿßŸÑÿ≠ŸÑ ŸàÿßŸàÿµŸÑ 
ÿßŸÑ
ÿ©
 
Gaussian
 
 .ÿßŸÑÿ≠ÿØ ÿßŸÑÿµŸàÿ±ÿ™ŸÑŸÜ ÿØŸàŸÑ
 
 
ÿØÿß ŸÑŸÑÿßŸÑÿ≥ ÿßŸÑÿ¨ÿØŸàŸÑ :    
 
https://bit.ly/3M6ou8H
 
 
Error Function
 
 
ùë∑
(
ùíô
)
 
ùë∑
(
ùíô
)
 
ùë∑
(
ùíô
)
 
ùë∑
(
ùíô
)
 
ùëö
=
2
 
ùëö
=
2
 
ùëö
=
‚àí
2
 
ùëö
=
‚àí
2
 
ùëã
 
ùëã
 
ùëã
 
ùëã
 
 
 
A binary communication system is one that sends only two possible messages. 
The simplest form of binary system is one in which either zero or one volt is sent. 
Consider such a system in which the transmitted voltage is corrupted by additive 
atmospheric noi se. If the receiver receives anything above 0.5 volt is assumes that 
a one was sent. If it receives anything below 0.5 volt it assumes that a zero was 
sent. Measurements have shown that if one volt is transmitted the received signal 
level is random and has  a Gaussian density with m = 1 and  ùùà=ùüé.ùüì ,find the 
probability that a transmitted one will be interpreted as a zero at the 
receiver (i.e., a bit error ) 
Solution  
ÿßŸÑÿ±ÿ±ÿ±   system   ÿØÿß ÿ®Ÿäÿ®ÿπÿ™1 V  ÿßŸà 0 V   ŸàÿßŸÅÿ™ÿ±ÿ∂ ÿßŸÜŸá ÿ®Ÿäÿ™ÿπÿ±ÿ∂ŸÑÿ±ÿ±ÿ±   noise   ÿ¨ÿßŸäŸá ŸÖŸÜ ÿßŸÑŸáŸàÿß, ŸàŸÑŸàÿßŸÑÿ±ÿ±ÿ±  Receiver  
 ÿßÿ≥ÿ™ŸÇÿ®ŸÑ ŸÇŸäŸÖÿ© ÿßÿπŸÑŸâ ŸÖŸÜV 1
2   ŸáŸäÿπÿ™ÿ®ÿ±Ÿáÿß Ÿàÿßÿ≠ÿØ ŸàŸÑŸà ÿßŸÇŸÑ ŸÖŸÜV 1
2  ŸáŸäŸÅÿ™ÿ±ÿ∂ ÿßŸÜŸá ÿ®ÿßÿπÿ™0. 
  ÿßŸÑŸÇŸäÿßÿ≥ÿßÿ™ ŸàÿßŸÑÿ™ÿ¨ÿßÿ±ÿ® Ÿàÿ∂ÿ≠ÿ™ ÿßŸÜ ŸÑŸàÿßŸÑÿ±  system    ÿØÿßŸäŸÖÿß  ÿ®Ÿäÿ®ÿπÿ™ 1 V   ÿπÿ¥ÿßŸÜ Ÿäÿ¥ŸàŸÅÿ£Ÿä  ÿßŸÑŸÑŸä ÿ®Ÿäÿ≠ÿµŸÑ  ŸÑÿ•ŸÑÿ¥ÿßÿ±   
ÿßŸÑŸÑŸä ŸàÿµŸÑÿ™ ŸÑŸÑÿ±  Receiver   ŸàŸáŸä random   ÿ∫Ÿäÿ± ŸÖÿ≠ÿØÿØŸá ŸàÿßŸä ÿ≠ÿßÿ¨ÿ© random    Ÿäÿ®ŸÇŸâ ÿßŸÑÿ≤ŸÖ ŸÑŸäŸáÿß probability 
density function    ÿ®ÿ™ÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿ±ÿßÿßŸÑÿ™ ÿ≠ÿ±ÿØŸàÿ´Ÿáÿ±ÿß Ÿàÿ¥ÿ±ÿ±ÿ±ÿ±ÿ±ŸÉÿ±ŸÑ ÿßŸÑÿ±ÿØÿßŸÑÿ±ÿ©ŸÅŸä ÿßŸÑŸÖÿ´ÿ±ÿßŸÑ ÿØÿß   ÿßŸÑŸÑŸä ÿ®ÿ™ÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿ±ÿßÿßŸÑÿ™  
ÿßŸÑŸàÿµŸàŸÑ  ŸáŸä Gaussian . 
  ÿ®ŸäÿπŸÖŸÑ ÿ™ÿ¨ÿßÿ±ÿ®ÿ•ŸÑÿ±ÿ≥ÿßŸÑ  ÿßŸÑŸàÿßÿ≠ÿØ Ÿàÿ®Ÿäÿ¥ŸàŸÅ  ÿßŸÑÿ±ÿ±   system    ŸáŸäÿ≥ÿ™ŸÇÿ®ŸÑŸáÿßÿ£Ÿä  ŸàŸÖŸÜ ÿßŸÑÿ™ÿ¨ÿßÿ±ÿ® ÿßŸÑŸÉÿ™Ÿäÿ± ÿØŸä ÿ±ÿ≥ŸÖ ŸÖŸÜÿ≠ŸÜŸâ   
density function ÿ®Ÿäÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸàÿµŸàŸÑ ÿßŸÑŸàÿßÿ≠ÿØ. 
 
ŸÖÿ∑ŸÑŸàÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÑŸÖÿß ÿßÿ®ÿπÿ™  1    ŸÖŸÜÿßŸÑÿ±ÿ±ÿ±   Transmitter    Ÿàÿ™ŸàÿµŸÑ0   ÿπŸÜÿØÿßŸÑÿ±ÿ±ÿ±   Receiver , ŸàŸÖŸÖŸÉŸÜ ŸäÿµŸäÿ∫ ÿßŸÑÿ≥ÿ§ÿßŸÑ  
ÿ®ÿ¥ÿ±ŸÉŸÑ ÿ™ÿßŸÜŸä ŸàŸäŸÇŸàŸÑ Ÿáÿßÿ™  ÿßŸÑÿ±ÿ±ÿ±ÿ±ÿ±ÿ±   bit error   ÿßŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿßŸÑÿ∑ÿ£ probability of error ŸÅŸä ÿßÿ±ÿ≥ÿ±ÿßŸÑ ÿßŸÑŸàÿßÿ≠ÿØ. ŸÉÿ£ŸÜŸä  
ÿ®ŸÇŸàŸÑŸá Ÿáÿßÿ™ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜŸä  ÿßÿ®ÿπÿ™ Ÿàÿßÿ≠ÿØ ŸàŸäŸàÿµŸÑ ÿµŸÅÿ± ŸÑŸÑÿ± Receiver. 
 
 ÿßŸàŸÑ ÿßŸÑÿ∑Ÿà  ÿπÿ¥ÿßŸÜ ÿßÿ≠ÿ≥ÿ®ÿßŸÑÿ± probability   ŸáŸÉÿßŸÖŸÑÿßŸÑÿ± density function  ŸàÿßŸÑŸÑŸä ŸáŸä ŸáŸÜÿß Gaussian  
ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸÖŸÜ  ‚àû-  ÿßŸÑŸâ0.5   ÿπÿ¥ÿ±ÿ±ÿßŸÜÿßŸÑÿ±ÿ±ÿ±ÿ±ÿ±ÿ±ÿ±   Receiver  ŸáŸäŸÇÿ±ÿ£ ÿßŸÑŸàÿßÿ≠ÿØ ÿßŸÑŸÑŸä ÿßÿ™ÿ®ÿπÿ™ ÿßŸÜŸá ÿµÿ±ÿ±ŸÅÿ± ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÜ
ÿßŸÑÿ¨ŸáÿØ ÿßŸÇŸÑ ŸÖŸÜ  0.5  
ùëÉùëü(ùëíùëüùëüùëúùëü)=‚à´ùëÉ(ùë•) ùëëùë•0.5
‚àí‚àû 
ùëÉ(ùë•)= 1
‚àö2ùúã ùúé ùëí‚àí(ùë•‚àíùëö)2
2 ùúé2= 1
‚àö2ùúã √ó 1
2 ùëí‚àí(ùë•‚àí1)2
2 (1
2)2
 
ùëÉ(ùë•)= ‚àö2
‚àöùúã ùëí‚àí2(ùë•‚àí1)2 Example  1 
 
ùëÉùëü(ùëíùëüùëüùëúùëü)=‚àö2
‚àöùúã  ‚à´ùëí‚àí2(ùë•‚àí1)2 ùëëùë•0.5
‚àí‚àû 
 
 ÿßŸÑÿßŸÑÿ∑Ÿà  ÿßŸÑŸÑŸä ÿ®ÿπÿØ ŸÉÿØÿß ÿßŸÜŸä  ÿ£ŸàÿµŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÑÿ≠ÿØ ÿßÿ¥ŸÉÿßŸÑ ŸÖÿπÿßÿØÿßŸÑÿ™ ÿßŸÑÿ± error: 
ùëíùëüùëì(ùë•)=2
‚àöùúã ‚à´ùëí‚àí ùë¢2 ùëëùë¢ùë•
0        ,        ùëíùëüùëìùëê(ùë•)=2
‚àöùúã ‚à´ùëí‚àí ùë¢2 ùëëùë¢‚àû
ùë• 
  ÿßŸÑÿ≤ŸÖ ŸÜÿ∫Ÿäÿ±ÿßŸÑÿ± variable x  ÿßŸÑŸâu  ŸàŸÜÿ∫Ÿäÿ± ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ 
 
To reduce this to a form that can be found in a table of error functions. we make the 
change of variable  
Let  ùë¢2=2(ùë•‚àí1)2 
ùë¢=‚àö2 (ùë•‚àí1) 
ùëëùë¢= ‚àö2 ùëëùë•  
  ŸÜÿ∫Ÿäÿ±ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ :   
ùë•=1
2  ‚Üíùë¢=‚àö2 (1
2‚àí1)=‚àí1
‚àö2  
ùë•=‚àí‚àû  ‚Üíùë¢=‚àí‚àû   
Then we get  
ùëÉùëü(ùëíùëüùëüùëúùëü)=‚àö2
‚àöùúã  ‚à´ùëí‚àíùë¢2 ùëëùë¢
‚àö2‚àí1
‚àö2
‚àí‚àû=1
‚àöùúã ‚à´ùëí‚àíùë¢2 ùëëùë¢‚àí1
‚àö2
‚àí‚àû 
ùëÉùëü(ùëíùëüùëüùëúùëü)=  1
2 2
‚àöùúã ‚à´ùëí‚àíùë¢2 ùëëùë¢‚àí1
‚àö2
‚àí‚àû 
 
 Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÜ ÿßÿßŸÑÿ≥ŸáŸÑÿßŸÜŸä ÿ£ŸàÿµŸÑ ŸÑŸÑÿ¥ŸÉŸÑ ÿßŸÑÿ™ÿßŸÜŸä  ‚Üê ùëíùëüùëìùëê(ùë•)=2
‚àöùúã ‚à´ùëí‚àí ùë¢2 ùëëùë¢‚àû
ùë•   ŸàŸÑŸÉŸÜ
ŸáŸÜÿ≠ÿ™ÿßÿ¨ ŸÜÿπÿØŸÑ ŸÅŸä ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ  ŸàÿßŸÑÿ´Ÿàÿßÿ®ÿ™.  
 
ÿßŸÑŸÅŸÉÿ±  ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ÿßŸÜŸÜÿß ŸáŸÜÿ®ÿØŸÑ ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸàŸÜÿ∫Ÿäÿ± ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ŸàÿØÿß ÿßŸÑŸÜ ÿßŸÑÿØÿßŸÑÿ©  even  ŸäÿπŸÜŸä ŸÖÿ™ŸÖÿßÿ´ŸÑÿ© ÿ≠ŸàŸÑ ŸÖÿ≠Ÿàÿ±y  
 ŸäÿπŸÜŸä ŸÑŸà ÿπŸÖŸÑŸÜÿß ÿ™ŸÉÿßŸÖŸÑ ŸÖŸÜ‚àí‚àû  ÿßŸÑŸâ‚àíùüè
‚àöùüê   ŸáŸà ŸÜŸÅÿ≥ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸÑŸà ÿπŸÖŸÑŸÜÿßŸá ŸÖŸÜùüè
‚àöùüê  ÿßŸÑŸâ‚àû 
 
 ÿßÿ≤ÿßŸä ÿßŸÑÿØÿßŸÑÿ© ÿØŸä symmetry  ŸàŸÇŸäŸÖÿ©m=1  ŸÖÿ¥0  ŸäÿπŸÜŸä ŸÖÿ¥ ŸÖÿ™ŸÖÿßÿ´ŸÑÿ© ÿ≠ŸàŸÑ ŸÖÿ≠Ÿàÿ±y   ÿü 
ŸáŸÜÿß ŸÖÿ¥ ÿ®ŸÜŸÉÿßŸÖŸÑ ÿßŸÑÿØÿßŸÑÿ© ÿßŸÑÿ±  gaussian    ÿßÿ£ŸÑÿµŸÑŸäÿ© Ÿàÿ®ŸÜŸÉÿßŸÖŸÑ ÿßŸÑÿØÿßŸÑÿ© ÿßŸÑÿ¨ÿØŸäÿØ  ÿßŸÑŸÑŸä ŸÅŸäŸáÿß ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ùë¢   ŸàŸÑŸà ÿ±ÿ≥ŸÖŸÜÿß
ùëí‚àíùë¢2  ŸáŸÜÿßŸÑŸÇŸäŸáÿß ŸÖÿ™ŸÖÿßÿ´ŸÑÿ© ÿ≠ŸàŸÑ ŸÖÿ≠Ÿàÿ±y 
 
ùëÉùëü(ùëíùëüùëüùëúùëü)= 1
‚àöùúã ‚à´ùëí‚àíùë¢2 ùëëùë¢‚àû
1
‚àö2 
 
the second equality is true since the integrand is even. this is now seen related to the 
complementary error function  
ùëÉùëü(ùëíùëüùëüùëúùëü)=0.5 ùëíùëüùëìùëê(1
‚àö2)=0.5√ó0.322=0.16 
 
 
 
thus, on the average, one would expect 16 out of every 100 transmitted 1's to be 
misinterpreted as 0's at the receiver.   
 
 
 ŸÖŸÖŸÉŸÜ ÿ≠ŸÑŸáÿß ÿ®ÿßÿ≥ÿ™ÿßŸÑÿØÿßŸÖerf : 
ùëÉùëü(ùëíùëüùëüùëúùëü)=  1
2 2
‚àöùúã ‚à´ùëí‚àíùë¢2 ùëëùë¢‚àí1
‚àö2
‚àí‚àû=1
2(erf(‚àí1
‚àö2)‚àíerf(‚àí‚àû)) 
=1
2(‚àíerf(1
‚àö2)+erf(‚àû))=1
2(‚àí0.67780+1)=0.16 
 
 
 
 
ùíô=ùüè
‚àöùüê=ùüé.ùüïùüéùüï 
 
 
Channel Modelling  ŸáŸäÿπÿ±ŸÑŸÇÿ© ŸáÿßŸÑÿØÿ≥ŸÑÿ© ŸÑÿ™ŸÖ ŸÑŸÑ ÿßŸÑ ÿ© Channel  ÿ®ÿ¥ŸÉŸÑ ŸÖÿ®ÿ≥ÿπ ŸÑÿ≥ŸáŸÑ ŸÇÿ±ÿßÿ°ÿ™  ŸàÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ   
ÿ±ŸÑÿßÿ∂ŸÑÿß . 
The channel is memoryless when ùë∫ùíêùíñùíï(ùíè) depends only  on ùë∫ùíäùíè(ùíè)  and not on 
ùë∫ùíäùíè(ùíè‚àíùüè) or any other input  sample values  
 
 ÿßŸÑÿ™ŸàÿµŸÑŸÅÿßŸÑŸÑŸä ŸáÿßŸÑÿØÿ±ÿ≥  ÿπŸÑŸâ  memoryless channel  ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿØÿÆŸÑ ÿßŸÑÿ≠ÿßŸÑŸä ŸÅŸÇÿπ 
 
 
 
 
ÿπÿßŸÑÿØŸä ÿπÿ±ŸÑŸÇÿ™ŸÑŸÜ ÿßÿπŸÖŸÑ ÿ®ŸÑŸáŸÖ Modelling 
 
1. Transition  Diagram  
An alternative way of displaying transition probabilities is  by use of the Transition 
Diagram.  
 ÿ¥ŸÉŸÑ ÿ®ÿ≥ŸÑÿπ ŸÑŸàÿ∂ÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ÿßŸÑŸÖŸÖŸÉÿßŸÑÿ© ŸÅŸä ÿßŸÑÿ© System 
For the binary channel  
 
 
 
ùë∑ùüèùüé 
 
 ÿ™ŸÇÿ±ÿ£ ŸÉÿßÿßŸÑÿ™Ÿä: ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™  0   Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ1   
ùë∑ùüéùüé+ ùë∑ùüèùüé=ùüè    ,     ùë∑ùüéùüè+ ùë∑ùüèùüè=ùüè  
 
The summation of probabilities leaving any node is unity .  
ŸÖÿ¨ŸÖŸàÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑÿÆÿßÿ±ÿ¨ÿ© ŸÖŸÜ ÿßŸÑŸÅÿ≥ ÿßŸÑÿ© node   ŸÑÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ 
Channel Modelling  
Channel  ùë∫ùíäùíè(ùíè) ùë∫ùíêùíñùíï(ùíè) 
 ÿßŸÑŸÖÿ±ÿ≥ŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÇÿ®ŸÑ  
 
Binary Symmetric Channel (BSC)  
A channel in which the two conditional error probabilities are equal  
  ŸáŸä ÿßŸÑŸÖŸàŸÉÿ¨ÿ±ŸÑÿßÿ∂Ÿä ŸÑŸè ÿ≥ÿ™ÿÆÿØŸÖ ŸÑŸàÿµŸÅ ŸÇÿßŸÑÿß  ÿßÿ™ÿµÿßŸÑ  ÿßŸÑÿßÿ≠ŸÑÿ© ÿßŸÑÿ≠ÿßŸÑÿ© ŸÑŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ≠ÿØÿ´ ÿ®Ÿáÿß ÿ£ÿÆÿπÿßÿ° ŸÅŸä ÿßŸÑŸÇŸÑ ÿßŸÑÿ®ÿ™ÿßÿ™.  
ÿ®ŸÑÿ™ŸÖ ÿßŸÑŸÇŸÑŸáŸÖ ŸÖŸÜ ÿßŸÑÿ© transmitter  ÿßŸÑŸâÿßŸÑÿ© receiver  ÿπÿ®ÿ±ÿßŸÑÿ©  channel  ŸàŸÖŸÖŸÉŸÜ ŸÑÿ≠ÿµŸÑ ÿÆÿπÿ£ ŸÅŸä ŸÇŸÑŸÖÿ© ÿßŸÑÿ®ÿ™
ÿßŸÑŸÖÿ±ÿ≥ŸÑÿ© ŸÖŸÜ  0 ÿßŸÑŸâ1  ÿßŸà ŸÖŸÜ1  ÿßŸÑŸâ0 .   ÿ®ŸÑÿ™ŸÖ ÿ™ŸÖ ŸÑŸÑ ÿßŸÑÿ™ÿ∫ŸÑŸÑÿ± ÿßŸÑŸÑŸä ÿ®ŸÑÿ≠ÿµŸÑ ÿØÿß ÿπŸÜ ÿπÿ±ŸÑÿØ ŸÇŸÑÿßÿ≥ ÿßÿ≠ÿ™ŸÖÿßŸÑŸÑÿ© ÿ≠ÿØŸàÿ´
ÿßŸÑÿÆÿπÿ£ ŸÅŸä ŸÉŸÑ ÿ®ÿ™ ÿ™ŸÖ ÿßŸÑŸÇŸÑ  ÿπÿ®ÿ± ÿßŸÑÿ©  channel 
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ ÿµŸÅÿ±  Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ Ÿàÿßÿ≠ÿØ ÿ™ÿ≥ÿßŸàŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ Ÿàÿßÿ≠ÿØ Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ ÿµŸÅÿ±ÿß Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™  
Ÿàÿßÿ≠ÿØ Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ Ÿàÿßÿ≠ÿØ ÿ™ÿ≥ÿßŸàŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ ÿµŸÅÿ± Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ ÿµŸÅÿ±.  
ùë∑ùüèùüé= ùë∑ùüéùüè= ùë∑    ,  ùë∑ùüéùüé= ùë∑ùüèùüè=ùüè‚àí ùë∑   
ùë∑  : ÿßÿ≠ÿ™ŸÖÿßŸÑŸÑÿ© ÿ≠ÿØŸàÿ´ error    
 
 
Tandem Connections of BSCs  
Suppose in transmitting a digital signal over a long distance , the signal path 
includes a number of repeaters.  
Further, suppose that the path between each repeater and  the following repeater 
can be viewed as BSC.  The overall channel can be viewed as a tandem  connection  
of BSCs.  
 ŸáŸà ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÜ BSCs Ÿàÿ±ÿß ÿ®ÿπÿ∂Ÿáÿß   
BSC ÿØÿß ÿ™ÿπÿ®ŸÑÿ± ÿπŸÜ ÿßŸÑÿ© Channel Ÿàÿ®ÿßŸÑÿ≠ÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿπŸÑŸâ ÿßŸÑÿ© diagram ÿØÿß. Ÿáÿ≠ÿπ repeater   ŸÅŸä ÿßŸÑÿßŸÑÿµ ÿ®ŸÑŸÜ
ÿßŸÑÿ© transmitter  ŸàÿßŸÑÿ© receiver   
 

 
 ÿ®ŸÑŸÜÿßŸÑÿ© transmitter  ŸàÿßŸÑÿ© repeater   ŸáÿπŸÖŸÑ modelling   ÿπŸÑŸâ ÿßÿßŸÑBSC ÿß ÿßŸÑÿ© repeater ÿØÿß ŸÉÿ£ÿßŸÑ  
transmitter ŸÑŸÑÿ© receiver  ÿßÿ£ŸÑÿÆŸÑÿ± ŸàŸáÿπŸÖŸÑ  ÿ®ÿ±ÿØŸà modelling   ÿπŸÑŸâ ÿßÿßŸÑBSC   ÿß ŸàÿßŸÑŸÉ ŸÖ ÿØÿß ÿ®ÿßŸÑÿ≥ÿ™ŸÅŸÑÿØ ŸÖÿßŸÑ  ŸÅŸä
ÿßÿßŸÑÿßŸÑÿß ÿßŸÑÿ≠ÿ≥ÿ® ÿßŸÑÿ© total probability of error ŸÑŸÑÿ© system 
 
 ŸÑŸà ÿπŸÑÿ≤ ÿßÿ≠ÿ≥ÿ®ÿßŸÑÿ© total probability of error   Ÿáÿ≠ÿ≥ÿ® ÿπÿßŸÑÿØ ÿ≠ÿßŸÑÿ© ŸÅŸÑŸáÿß repeater  Ÿàÿßÿ≠ÿØ ŸÅŸÇÿπ Ÿàÿ®ÿπÿØ ŸÉÿØÿß ŸÖŸÜ
ÿßŸÑÿπ ŸÇÿ© ÿßŸÑŸÑŸä ŸáŸàÿµŸÑŸáÿß ŸáÿπŸÖŸÖŸáÿß ÿπŸÑŸâ ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑÿ© repeaters . 
 
 ŸÑŸà ÿπÿßŸÑÿ≤ ÿßÿ≠ÿ≥ÿ® total probability of error  ŸáŸÖÿ¥Ÿä ŸÖŸÜÿßŸÑÿ© diagram   ÿπŸÑŸâ ŸÉŸÑ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÑŸä ÿ™ŸàÿµŸÑÿßŸÑŸä ŸÖŸÜ
1  ŸÅŸäÿßŸÑÿ© transmitter ŸÑÿ© 0  ŸÅŸäÿßŸÑÿ© receiver  ÿßŸàÿßŸÖÿ¥Ÿä ÿπŸÑŸâ ŸÉŸÑ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÑŸä ÿ™ŸàÿµŸÑÿßŸÑŸä ŸÖŸÜ0  ŸÅŸäÿßŸÑÿ© 
transmitter ŸÑÿ© 1  ŸÅŸäÿßŸÑÿ©  receiver  .Ÿàÿ®ÿπÿØ ŸÉÿØÿß ÿßÿ¨ŸÖŸá ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿØŸä 
 
ŸÑÿ≠ÿ≥ÿßÿ® total probability of error  : ŸÑŸÑŸÖ ÿßŸÑ ÿØÿß ŸáÿßŸÑÿπŸÖŸÑ ÿßÿßŸÑÿ™Ÿä 
 
ùë∑ùíÜ=ùë∑ùüèùüé ùë∑(ùüé)+ùë∑ùüéùüè ùë∑(ùüè)  
 
  ùëÉ10  ÿßŸÑŸÖÿ¥Ÿä ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÑŸä ÿ®ÿπÿ™ÿßŸÑÿß ŸÅŸÑŸáÿß0  Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑÿßŸÑÿß ŸÅŸÑŸáÿß1          ùëÉ(0)   ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ±ÿ≥ÿßŸÑ0 
  ùëÉ01  ÿßŸÑŸÖÿ¥Ÿä ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÑŸä ÿ®ÿπÿ™ÿßŸÑÿß ŸÅŸÑŸáÿß1  Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑÿßŸÑÿß ŸÅŸÑŸáÿß0          ùëÉ(1)  ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ±ÿ≥ÿßŸÑ1    
 
ùë∑ùíÜ=[ùë∑ùüéùüéùë∑ùüèùüé+ùë∑ùüèùüéùë∑ùüèùüè ] ùë∑(ùüé)+[ùë∑ùüéùüèùë∑ùüéùüé+ùë∑ùüèùüè ùë∑ùüéùüè] ùë∑(ùüè)  
=[(ùüè‚àíùë∑)ùë∑+ùë∑(ùüè‚àíùë∑) ] ùë∑(ùüé)+[ùë∑(ùüè‚àíùë∑)+(ùüè‚àíùë∑)ùë∑ ] ùë∑(ùüè)  
 
ÿπÿ¥ÿßŸÜ ÿßŸÑÿ¨ŸÑÿ® ÿßŸàŸÑ ŸÖÿ≥ÿßÿ±  (ùüè‚àíùë∑)ùë∑   ÿ®ÿπÿ™ÿßŸÑÿß ŸÅŸÑ0    Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑÿßŸÑÿß ŸÅŸÑ1   ŸáÿßŸÑŸÖÿ¥Ÿä ÿßŸÑŸÖÿ≥ÿßÿ± ÿßÿßŸÑŸàŸÑ  (ùüè‚àíùë∑)   Ÿàÿ®ÿπÿØŸáÿß
ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ™ÿßÿßŸÑŸä ùë∑   ŸàŸáÿßŸÑÿß ŸáÿßŸÑÿ∂ÿ±ÿ®ŸáŸÖ ŸÅŸä ÿ®ÿπÿ∂ÿßŸÑŸÜ ÿØÿß ŸÖÿ≥ÿßÿ± ŸÖŸÉÿ™ŸÖŸÑ ÿπŸÑŸâ ÿ®ÿπÿ∂   ÿßŸÑÿ≤ŸÖ ŸÑÿ≠ÿµŸÑ ÿßÿßŸÑÿ™ÿßŸÑŸÑŸÜ  ŸÖÿπ ÿ®ÿπÿ∂   
ŸÑÿπÿßŸÑŸä and . 
 
 ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ™ÿßÿßŸÑŸä ùë∑(ùüè‚àíùë∑)   ÿßŸÑŸÑŸäÿ®ÿπÿ™ÿßŸÑÿß ŸÅŸÑ  0   Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑÿßŸÑÿß ŸÅŸÑ1    ŸáÿßŸÑÿ¨ŸÖÿπ  ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ£ŸÑŸàŸÑ   (ùüè‚àíùë∑)ùë∑   ÿßŸÑŸÜ
ÿØÿß ŸÖÿ≥ÿßÿ± ÿ¨ÿØŸÑÿØ ŸàÿßÿßŸÑÿ≠ÿ™ŸÖÿßŸÑ ÿπÿßŸÑÿØŸä ÿßÿßŸÑŸä ÿßŸÖÿ¥Ÿä ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑŸÑŸä ŸÅŸàŸÇ ÿßŸà or   ŸÑŸÖÿ¥Ÿä ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ŸÑÿ≠ÿØ ŸÖÿß
ŸÑŸàÿµŸÑ 1 
 
 ŸàŸÉŸÑ ÿØÿß ŸáÿßŸÑÿ∂ÿ±ÿ®  ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™0  ùë∑(ùüé)  ÿ£ÿµ ŸÖŸÜ ÿßŸÑÿ© transmitter    ÿßŸÑŸÜ ÿßŸÑŸÑŸä ŸÅŸàŸÇ ŸÖÿ¥ÿ±Ÿàÿπ ÿßÿßŸÑŸä
ÿ£ŸÉŸàŸÜ ÿ®ÿßÿπÿ™ 0  ÿ£ÿµ ŸàŸÖÿßÿØÿßŸÖ condition  ÿßŸÑÿ≤ŸÖ ŸÑÿ≠ÿµŸÑ ŸÖŸá ÿ®ÿπÿ∂ ŸÑÿ®ŸÇŸâ ÿßŸÑÿ≤ŸÖ ŸàŸÇÿ™ŸáÿßÿßŸÑÿ© and ÿØŸä ÿ™ÿ™ÿ±ÿ¨ŸÖ ŸÑÿ∂ÿ±ÿ® 
Ÿàÿ®ÿßŸÑŸÖ ŸÑ ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ 1   ŸàŸÑŸàÿµŸÑ0   Ÿáÿ∂ÿ±ÿ®ÿ© ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÜ ÿßÿ®ÿπÿ™1 ùëÉ(1)  
 
  ŸáÿßŸÑÿßÿÆÿØ ùë∑(ùüè‚àíùë∑)   : ÿπÿßŸÖŸÑ ŸÖÿ¥ÿ™ÿ±ÿ≥ ŸàÿßŸÑÿ®ÿ≥ÿπ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ŸÉÿßÿßŸÑÿ™Ÿä 
ùë∑ùíÜ=[ùë∑(ùüè‚àíùë∑)+ùë∑(ùüè‚àíùë∑) ] [ùë∑(ùüé)+ùë∑(ùüè)] 
 ÿßŸÉŸÑÿØ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ ÿµŸÅÿ± ÿßŸà Ÿàÿßÿ≠ÿØ= 1           ùë∑(ùüé)+ùë∑(ùüè)=1  
 
ùë∑ùíÜ=[ùë∑(ùüè‚àíùë∑)+ùë∑(ùüè‚àíùë∑) ]=ùüêùë∑(ùüè‚àíùë∑)‚âÖùüêùë∑ 
ùë∑  ŸÇŸÑŸÖÿ© ÿµÿ∫ŸÑÿ±  ŸÖŸÖŸÉŸÜ ÿßŸÑŸáŸÖŸÑŸáÿß ÿ®ÿßŸÑÿßŸÑÿ≥ÿ®ÿ© ŸÑŸÑŸàÿßÿ≠ÿØ 
 
For n BSC   ‚Üí  ùêèùêû=ùêßùë∑  
 ŸÑŸÖÿß ŸÉÿßŸÜ ÿπÿßŸÑÿØŸäBSC 2 ÿßŸÑÿßÿ™ÿ¨ ÿßŸÑÿ© total probability of error  ÿπŸÑŸá ÿ™ŸÇÿ±ŸÑÿ®ÿßùüêùë∑   ŸàŸÖÿπÿßŸÑŸâ ŸÉÿØÿß ÿßŸÜ ŸÑŸà ÿπÿßŸÑÿØŸä
BSC 10  ÿßŸÑÿ© total probability of error  ŸáŸÑÿ≥ÿßŸàŸä10ùë∑  
 
  ŸáŸÑ ŸÖÿπÿßŸÑŸâ ŸÉÿØÿß ŸÑŸÖÿß ÿ®ÿ≤ŸàÿØ repeaters  ÿßŸÑÿ© total probability of error   ÿ®ÿ™ÿ≤ŸÑÿØ ÿü 
ÿßŸÉŸÑÿØ ÿ£ŸÑ  ÿßŸÑŸÜ ŸÇŸÑŸÖÿ© ÿßŸÑÿ© ùë∑  ŸÅŸäBSC 1  ÿ∫ŸÑÿ± ŸÅŸäBSC  2  ÿßŸà 10 BSC  ÿßŸÑŸÜ ŸÉŸÑ ŸÖÿß ÿßŸÑÿ≤ŸàÿØ ÿπÿØÿØÿßŸÑÿ© repeaters 
 ŸÇŸÑŸÖÿ©ÿßŸÑÿ© ùë∑  ÿ®ÿ™ŸÇŸÑ ÿ¨ÿØÿß  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÖÿ≠ÿµŸÑÿ© total probability of error Ÿáÿ™ŸÇŸÑ   
 
 
2. Transition Matrix  
The memoryless channel can be characterized by a Transition Matrix composed 
of conditional probabilities.  
[ùëª]= [ùë∑ùüéùüéùë∑ùüéùüè
ùë∑ùüèùüéùë∑ùüèùüè] 
 
 ŸÅŸä ÿßŸÑÿπÿ±ŸÑŸÇÿ© ÿØŸä ŸáÿßŸÑÿ≠ÿπ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸÅŸä ÿ¥ŸÉŸÑ ŸÖÿµŸÅŸàŸÅÿ© ÿ®ÿØŸÑÿßŸÑÿ© diagram   ŸàŸÉŸÑ ÿπŸÖŸàÿØ ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ
ÿßÿ±ÿ≥ÿßŸÑ bit  Ÿàÿßÿ≠ÿØ  Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑŸáÿß ŸÅŸä ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿ≠ÿßŸÑ.  
ŸÖ   ÿßŸÑÿπŸÖŸàÿØ ÿßÿ£ŸÑŸàŸÑ ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ±ÿ≥ÿßŸÑ 0   Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ0  ùë∑ùüéùüé  ÿßÿßŸà ÿßÿ±ÿ≥ÿßŸÑ0  Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ1 ùë∑ùüèùüé   ÿß ŸàÿßŸÑÿπŸÖŸàÿØ
ÿßŸÑÿ™ÿßÿßŸÑŸä ÿ®ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ±ÿ≥ÿßŸÑ  1  Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ0   ùë∑ùüéùüè  ÿßÿßŸà ÿßÿ±ÿ≥ÿßŸÑ1  Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ1  ùë∑ùüèùüè 
 ÿßŸÑÿ≥ÿ™ÿßŸÑÿ™ÿ¨ ŸÖŸÜ ŸÉÿØÿß ÿßŸÜ ŸÖÿ¨ŸÖŸàÿπ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑÿπŸÖŸàÿØ ÿßŸÑŸàÿßÿ≠ÿØ ŸÑÿ≥ÿßŸàŸä1    ÿßŸÑŸÜ ŸÉŸÑ ÿπŸÖŸàÿØ ŸÑÿπÿ®ÿ± ÿπŸÜ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™
ÿßÿ±ÿ≥ÿßŸÑ bit   Ÿàÿßÿ≠ÿØ 
 
With no noise nor distortion  
 ŸÅŸä ÿ≠ÿßŸÑÿ© ÿπÿØŸÖ Ÿàÿ¨ŸàÿØ noise  ÿßŸà distortion  ÿ™ŸÉŸàŸÜ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿßŸÑÿßÿ™ÿ¨ÿ© ŸáŸä ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑŸàÿ≠ÿØ 
ÿßŸÑŸÜ ŸÅŸä ÿßŸÑÿ≠ÿßŸÑÿ© ÿØŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™ 0    Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ0  ŸáŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÉŸÑÿØ Ÿà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑŸä ÿßÿ®ÿπÿ™1   Ÿàÿßÿ≥ÿ™ŸÇÿ®ŸÑ1 
  ŸáŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÉŸÑÿØŸàÿ®ÿßŸÑÿ™ÿßŸÑŸä  ÿ®ÿßŸÇŸä ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™  ùë∑ùüéùüè  ÿßùë∑ùüèùüé   ÿßŸÑŸÑŸä ÿ®ÿ™ÿØŸÑ ÿßŸÜ ÿ≠ÿµŸÑ ÿÆÿπÿ£ ŸÅŸä ÿßÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸÇŸÑŸÖÿ™ŸáŸÖ
ÿ®ÿµŸÅÿ±   
[ùëª]= [ùüèùüé
ùüéùüè] 
 
summation of probabilities in any column is unity.  
 = ŸÖÿ¨ŸÖŸàÿπ ÿπÿßŸÑÿßÿµÿ± ÿßŸÑÿπŸÖŸàÿØ ÿßŸÑŸàÿßÿ≠ÿØ1 
 
 
A digital communication  system has a symbol alphabet composed of four 
entries and the transition matrix is given by  
 
[ùëª] =
[     ùüèùüí‚ÅÑùüèùüê‚ÅÑùüèùüî‚ÅÑùüèùüî‚ÅÑ
ùüèùüí‚ÅÑùüèùüî‚ÅÑùüèùüê‚ÅÑùüèùüî‚ÅÑ
ùüèùüí‚ÅÑùüèùüî‚ÅÑùüèùüî‚ÅÑùüèùüë‚ÅÑ
ùüèùüí‚ÅÑùüèùüî‚ÅÑùüèùüî‚ÅÑùüèùüë‚ÅÑ]     
 
 
(a) Find the probability of a  single transmitted symbol being in error assuming 
that all four input symbols are equally probable at any time  
Solution  
 
[ùëª] =[ùë∑ùüéùüéùë∑ùüéùüèùë∑ùüéùüêùë∑ùüéùüë
ùë∑ùüèùüéùë∑ùüèùüèùë∑ùüèùüêùë∑ùüèùüë
ùë∑ùüêùüéùë∑ùüêùüèùë∑ùüêùüêùë∑ùüêùüë
ùë∑ùüëùüéùë∑ùüëùüèùë∑ùüëùüêùë∑ùüëùüë] 
 
ùë∑ùíÜ|ùüé ùíîùíÜùíèùíï= ùë∑ùüèùüé+ ùë∑ùüêùüé+ ùë∑ùüëùüé=ùüè
ùüí+ùüè
ùüí+ùüè
ùüí=ùüë
ùüí 
ùë∑ùíÜ|ùüè ùíîùíÜùíèùíï= ùë∑ùüéùüè+ ùë∑ùüêùüè+ ùë∑ùüëùüè=ùüè
ùüê+ùüè
ùüî+ùüè
ùüî=ùüì
ùüî 
ùë∑ùíÜ|ùüê ùíîùíÜùíèùíï= ùë∑ùüéùüê+ ùë∑ùüèùüê+ ùë∑ùüëùüê=ùüè
ùüî+ùüè
ùüê+ùüè
ùüî=ùüì
ùüî 
ùë∑ùíÜ|ùüë ùíîùíÜùíèùíï= ùë∑ùüéùüë+ ùë∑ùüèùüë+ ùë∑ùüêùüë=ùüè
ùüî+ùüè
ùüî+ùüè
ùüë=ùüê
ùüë 
  
The total probability of error is given by the average of four quantities  
ÿ®ÿπÿØ ŸÖÿß ÿ≠ÿ≥ÿ®ÿßŸÑÿß ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿπÿ£ ŸÅŸä ÿßÿ±ÿ≥ÿßŸÑ ŸÉŸÑ bit  ŸÑŸàÿ≠ÿØ ŸáÿßŸÑÿ≠ÿ≥ÿ® ÿßŸÑÿ© average probability of error 
 ŸáŸà ŸÇÿßŸÑŸä ÿßŸÜÿßŸÑÿ© four symbols  ÿØŸàŸÑ equally probable  ÿπÿ¥ÿßŸÜ ÿßÿ¨ŸÑÿ®ÿßŸÑÿ©  average Ÿáÿ¨ŸÑÿ® ŸÖÿ¨ŸÖŸàÿπŸáŸÖ  
ŸàÿßŸÇÿ≥ŸÖŸáŸÖ ÿπŸÑŸâ ÿπÿØÿØŸáŸÖ ÿßŸÑŸÑŸä ŸáŸà 4 
 
ùë∑ùíÜ=(ùüë
ùüí+ ùüì
ùüî+ ùüì
ùüî+ ùüê
ùüë)ùüè
ùüí=ùüëùüï
ùüíùüñ Example  2 
 
ŸÑŸà ÿßŸÑÿ© symbols  ŸÖŸÉÿßÿßŸÑÿ™ÿ¥ equally probable  ŸáÿßŸÑÿ≠ÿ≥ÿ®ÿßŸÑÿ© average   ÿ®ÿßÿßŸÑÿßŸÑÿß ÿßŸÑÿ∂ÿ±ÿ® ŸÉŸÑ ŸÇŸÑŸÖÿ© ŸÅŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ
ÿ≠ÿØŸà Ÿáÿß Ÿàÿßÿ¨ŸÖŸá ÿßŸÑÿßŸÑŸàÿßÿ™ÿ¨  ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿπ ŸÇÿ©:  
ùëÉùëí=‚àë(ùëÉùëí|ùëñ ùë†ùëíùëõùë°)3
ùëñ=0 ùëÉ(ùëñ) 
 ùëÉ(ùëñ)  ŸáŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ±ÿ≥ÿßŸÑ ŸÉŸÑ symbol  ŸàŸÅŸä ÿßŸÑŸÖ ÿßŸÑ ÿØÿß ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ±ÿ≥ÿßŸÑ ŸÉŸÑ symbol   ŸáŸà1
4 
=3
4√ó1
4+ 5
6√ó1
4+ 5
6√ó1
4+ 2
3 √ó1
4=37
48 
ÿßŸÑŸÇÿßÿßŸÑŸàŸÜ ÿØÿß ŸÑÿµŸÑÿ≠ ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßÿßŸÑŸáŸÖ equally probable  ÿßŸà nonequally probable 
 
(b) Find the probability of correct symbol transmission  
  ŸÖÿπŸÑŸàÿ® ÿ≠ÿ≥ÿßÿ® ÿßÿßŸÑÿ±ÿ≥ÿßŸÑÿßŸÑÿµÿ≠ŸÑÿ≠ ŸÑŸÑÿ© symbol 
ùë∑ùíÜ+ùë∑ùíÑ=ùüè 
ùë∑ùíÑ=ùüè‚àíùë∑ùíÜ=ùüè‚àí ùüëùüï
ùüíùüñ= ùüèùüè
ùüíùüñ 
 
 ŸÖŸÖŸÉŸÜ ÿßŸÑÿ≠ŸÑ ŸÖŸÜ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿπŸÜ ÿπÿ±ŸÑÿØ ÿßŸÑÿ¨ŸÑÿ® ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßÿßŸÑÿ±ÿ≥ÿßŸÑ ÿßŸÑÿµÿ≠ŸÑÿ≠ ŸàÿßŸÑÿ∂ÿ±ÿ® ŸÉŸÑ ŸÇŸÑŸÖÿ© ŸÅŸä ÿßŸÑÿ≥ÿ®ÿ™Ÿáÿß
ÿßŸà ÿßŸÑÿ∂ÿ±ÿ®ŸáŸÖ ŸÉŸÑŸáŸÖ ŸÅŸä  ùüè
ùüí ÿßŸÑÿßŸÑŸáŸÖ equally probable 
 
ùë∑ùíÑ=ùë∑ùüéùüé ùë∑(ùüé)+ ùë∑ùüèùüè ùë∑(ùüè)+ ùë∑ùüêùüê ùë∑(ùüê)+ ùë∑ùüëùüë ùë∑(ùüë) 
=ùüè
ùüí√óùüè
ùüí+ ùüè
ùüî√óùüè
ùüí+ ùüè
ùüî√óùüè
ùüí+ ùüè
ùüë √óùüè
ùüí=ùüèùüè
ùüíùüñ 
 
 
 
(c) If the symbols are denoted A , B , C, and D ,find the probability BADCAB  that 
the transmitted sequence will be received as DADDAB   
 
Transmitted                                 Received  
B A D C A B                                  D A D D A B 
1  0 3  2  0 1                                   3  0  3 3  0  1 
 
 
 0 1 2 3 
 
 ŸÖÿπŸÑŸàÿ® ÿ≠ÿ≥ÿßÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ±ÿ≥ÿßŸÑ BADCAB (103201)  Ÿàÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ DADDAB  (303301) 
 
ùë∑ùíÜ|ùüèùüéùüëùüêùüéùüè ùíîùíÜùíèùíï ùíÇùíèùíÖ ùüëùüéùüëùüëùüéùüè ùíìùíÜùíÑùíÜùíäùíóùíÜùíÖ= 
 ùë∑ùüëùüè ùë∑ùüéùüé  ùë∑ùüëùüë ùë∑ùüëùüê  ùë∑ùüéùüé  ùë∑ùüèùüè = 
=ùüè
ùüî √ó ùüè
ùüí √ó ùüè
ùüë √ó ùüè
ùüî √ó ùüè
ùüí √ó ùüè
ùüî= ùüè
ùüèùüéùüëùüîùüñ 
 ÿ™ŸÖ ÿ∂ÿ±ÿ® ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ£ŸÑÿßŸÑŸáÿß ÿ™ÿ≠ÿØÿ´ ŸÖÿπÿß  ŸÅŸä ÿßŸÑŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ŸÉ ÿ© sequence Ÿàÿßÿ≠ÿØ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ÿØÿß ÿ™ŸÉŸÖŸÑÿ© ŸÑŸÑŸÖÿ≠ÿßÿ∂ÿ±  ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ŸàŸáÿßŸÑÿßŸÑŸÉÿ™ŸÅŸä ÿ®ÿßÿßŸÑÿßŸÑÿß ÿßŸÑÿπÿ±ŸÅ ŸÖŸÑÿ≤ÿ™ŸÑŸÜ ŸàÿπŸÑÿ®ŸÑŸÜ ŸÅŸÇÿπ
 
1. Errors often can be corrected
 
The only decision at the 
receiver is the selection between two possible pulses, 
not the details of the pulse shape.
 
ÿ•ŸÖŸÉÿßÿßŸÑŸÑÿ©
  
ÿ™ÿµÿ≠ŸÑÿ≠  
ÿßÿ£ŸÑÿÆÿπÿßÿ°
  
ŸÅŸä ÿßŸÑ
ÿ©
 
digital
 
  ÿßŸÇŸàŸâ ÿ®ŸÉÿ™ŸÑÿ± ŸÖŸÜ
ÿßŸÑ
ÿ©
 
analog
 
ÿ£ŸÑÿßŸÑÿßŸÑÿß
 
ÿ®ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖŸá ŸÇŸÑŸÖÿ™ŸÑŸÜ ŸÅŸÇÿπ 
 
 
 
 
 
 
 
 
 
 
 
The regenerative repeater not only performs the 
function of amplification, but 
also cleans up the signal
 
ÿßŸÑ
ÿ©
 
Repeater
 
 ŸáŸà
amplifier
  
 
Ÿä
ŸÅ
 
ŸÖ
ÿØ
ÿÆ
ÿ™
ÿ≥
Ÿè
ŸÖ
ÿßŸÑ
ÿ©
 
system
 
digital
  
 ŸàŸÖÿ¥ ÿ®ŸÑŸÉÿ®ÿ±
ÿßÿßÿ¥ÿßÿ± 
 
ŸÅŸÇÿπ ŸàŸÑŸÉŸÜ ÿ®ŸÑÿπŸÖŸÑ  
regenerate
 
 ŸÑÿ•ŸÑÿ¥ÿßÿ±
 
ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠ŸÑÿ≠ ŸÑÿπÿßŸÑŸä ÿ®ŸÑÿßÿÆÿØ 
ÿßÿßÿ¥ÿßÿ± 
 
ŸàŸÅŸÑŸáÿß 
noise
 
 ŸàŸÑÿÆÿ±ÿ¨Ÿáÿß
ÿ•ÿ¥ÿßÿ± 
 
cleaned
  
 
ÿØ
ÿØ
ÿ≠
ŸÑ
 
ŸÅ
ÿ±
ÿπ
ŸÑ
ÿ®
 
ŸÜ
ÿßŸÑ
 
ÿß
Ÿá
ÿ±
ÿ®
ŸÉ
ŸÑ
 
ÿß
ŸÖ
 
ÿØ
ÿπ
ÿ®
 
 
ÿß
ŸÖ
ÿß
ŸÖ
ÿ™
ÿßŸÑ
ÿ©
 
binary 0
 
 Ÿà
binary 1
 
 ÿπŸÜ ÿπÿ±ŸÑÿØ ÿßŸÑŸÖŸÇÿßÿ±ÿßŸÑÿ© ÿ®
ÿßŸÑ
ÿ©
 
threshold
 
 ÿßŸà
ÿ£Ÿä
 
ÿ£ŸÅŸÉÿßÿ±
 
ÿ™ÿßÿßŸÑŸÑÿ©. ÿ®ŸÑŸÜ 
ÿßŸÑ 
ÿ©
 
Transmitter
 
 Ÿà
ÿßŸÑ
ÿ©
 
Receiver
 
  ÿ®ŸÑŸÉŸàŸÜ ŸÅŸÑ
Repeaters
 
   ÿπŸÑŸâ ŸÖÿ≥ÿßŸÅÿßÿ™ ŸÖÿπŸÑÿßŸÑ
 
ŸáÿßŸÑ ÿ≠ÿ∏ ŸÖŸÜ ÿßŸÑÿµŸàÿ±  ÿßŸÜ 
ÿßŸÑ
ÿ©
 
amplifier
 
 ÿ®ŸÑŸÉÿ®ÿ±
ÿßŸÑ
ÿ©
 
signal
 
 ÿ®
ÿßŸÑ
ÿ©
 
noise
 
ÿ®ÿ™ÿßÿπÿ™Ÿáÿß
 
 
 
 
 
 
 
 
 
 
 
 
Ÿàÿ≠ÿ™Ÿâ ŸÑŸà 
ÿßÿßÿ¥ÿßÿ± 
 
ŸàÿµŸÑÿ™ ŸàŸÅŸÑŸáÿß ÿ®ÿπÿ∂ 
ÿßÿ£ŸÑÿÆÿπÿßÿ°
  
ŸÅŸÑ  ÿπÿ±ŸÇ ŸÑ
ŸÑ
ÿ©
 
error detection and correction
 
  ÿßŸÑŸÇÿØÿ±
ÿßŸÑÿ≠ÿØÿØ ÿßŸÑÿÆÿπÿ£ ŸàÿßŸÑÿµŸÑÿ≠  
 
Advantages of Digital Systems
 
 
 
2. Signal manipulation (e.g. encryption) is simple to perform  
Digital systems deal with numbers, rather than waveforms. These numbers can be 
manipulated by simple logic circuits. Analog operations require complex hardware.  
 ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖŸáÿßÿßÿ¥ÿßÿ±  ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ© digital  ÿ≥ŸáŸÑ ÿ™ÿßŸÑŸÅŸÑŸÉ  ÿ£ŸÑÿßŸÑ  ÿ®ŸÑÿ™ÿπÿßŸÖŸÑ ŸÖŸá ÿßÿ±ŸÇÿßŸÖ ŸàÿØŸä ÿ≥ŸáŸÑ ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπÿßŸáÿßÿ£ŸÑÿßŸÑÿßŸÑÿß  
ÿ®ÿßŸÑÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπÿßŸáÿß simple logic gates 
 
 
 
 
 
1.  Generally, requires more bandwidth than analog.  
 ÿ®ŸÑÿ≠ÿ™ÿßÿ¨ bandwidth  ŸÖŸÖŸÉŸÜ ŸÑŸÉŸàŸÜ ÿßŸÉÿ®ÿ± ŸÖŸÜÿßŸÑÿ© analog  Ÿàÿπÿ®ÿπÿß ŸÅŸÑ  ÿ≠ŸÑŸàŸÑ ŸÑŸÑŸÖÿ¥ŸÉŸÑÿ© ÿØŸä ŸàŸáŸä bandwidth 
utilization techniques  ŸàŸÖÿ¥ŸÉŸÑÿ©ÿßŸÑÿ© large bandwidth ŸÑŸÑÿ© user ÿßŸÑŸàÿßÿ≠ÿØ ÿßÿßŸÑ  ÿ®ŸÑŸÇŸÑŸÑ ÿπÿØÿØ ÿßŸÑÿ© users   ŸàŸÖŸÜ
ÿßÿ¥Ÿáÿ± ÿßŸÑÿ© techniques  ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ© ŸÑÿ≠ŸÑ ŸáŸÉ  ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ŸáŸä CDMA   
 
2. Synchronization is required.  
ÿ£Ÿä Digital system    ÿßŸÑÿ≤ŸÖ ŸÑŸÉŸàŸÜ ŸÅŸÑ  synchronizationÿ®ŸÑŸÜ ÿßŸÑÿ© transmitter   ŸàÿßŸÑÿ© Receiver 
 Disadvantages of Digital Systems  
 
 
 
 
 
 
   
 
¬©
 
Basem Hesham
PCM & Companding & Counting Quantizer 
Lecture
 
5
 
 
 ÿßŸàŸÑ Block  ŸÖÿπÿßŸÜÿß ŸÅŸäÿßŸÑŸÄ Digital communication system  ÿßŸÑÿØÿßŸäÿ±ÿ© ÿßŸÑŸÑŸä ÿ®ÿ™ÿ≠ŸàŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ™ÿµŸÑÿ© ŸÑŸÑŸÖŸÉÿßŸÅÿ¶
ÿßŸÑŸÄ binary  ÿ®ÿ™ÿßÿπŸáÿß ŸàÿßŸÑŸÑŸä ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸáÿß analog to digital converter (ADC)  
 ŸÅŸä ÿßŸÑÿ¥ÿßÿ®ÿ™ÿ± ÿØÿß ŸáŸÜÿØÿ±ÿ≥ ÿ∑ÿ±ŸÇ ÿßŸÑÿ™ÿ≠ŸàŸäŸÑ ŸÖŸÜ analog  ÿßŸÑŸâ digital  ŸàŸáŸÜÿ®ÿØÿ£ ÿ®ÿ£ŸàŸÑ  ÿ∑ÿ±ŸäŸÇÿ© ŸàŸáŸä Pulse code 
modulation  
 
 
 
 
 
PCM  ŸáŸä ÿ∑ÿ±ŸäŸÇÿ© ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä ÿ™ÿ≠ŸàŸäŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸÖŸÜ analog  ÿßŸÑŸâ digital  ,ŸàÿßŸÑÿπŸÖŸÑŸäÿ© ÿ™ÿ™ŸÖ ÿπŸÑŸâ 3   ÿÆÿ∑Ÿàÿßÿ™ 
 - 1  ÿßŸàŸÑÿÆÿ∑Ÿàÿ© ÿ®ÿπŸÖŸÑ sampling ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©   
- 2  ÿ™ÿßŸÜŸä ÿÆÿ∑Ÿàÿ© ÿ®ÿπŸÖŸÑ ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÖŸáÿß Quantization  ŸàŸáŸÜÿß ÿ®ÿ≠ÿØÿØ ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÜÿßŸÑŸÄ Levels  ÿπŸÑŸâÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
Ÿàÿ®ŸÇÿ±ÿ® ŸÉŸÑ sample  ÿ£ŸÑŸÇÿ±ÿ® Level 
- 3 ÿ™ÿßŸÑÿ™ ÿÆÿ∑Ÿà ÿ© ÿ®ÿπŸÖŸÑ coding  ÿßŸà ÿßŸÑŸÖŸÉÿßŸÅÿ¶ÿßŸÑŸÄ binary  ÿ£ŸÑŸÇÿ±ÿ® Quantization Level  
 
 
 
 
States that, if the Fourier transform of a time function is zero for ùíá > ùíáùíé, and 
the values of the time function are known for ùíï = ùíèùëªùíî , for all integer values of 
ùíè, then the time function is known for all values of ùíï provided that the samples 
are close enough together.  
 
 
 
Source Coding  
1-Sampling  Pulse Code Modulation  
 
 ÿßŸàŸÑ ŸÖÿ±ÿ≠ŸÑÿ© ŸÖŸÜ ŸÖÿ±ÿßÿ≠ŸÑ ÿ™ÿ≠ŸàŸäŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑÿ™ŸÖÿßÿ´ŸÑŸäÿ© analog  ÿßŸÑŸâ ÿ±ŸÇŸÖŸäŸá digital   ŸáŸä ÿπŸÖŸÑŸäÿ© ÿßÿÆÿ∞ ÿßŸÑÿπŸäŸÜÿßÿ™
Sampling   ŸàÿßŸÑÿ™Ÿä ÿ™ÿ≠ŸàŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑÿ™ŸÜÿßÿ∏ÿ±Ÿäÿ© ÿßŸÑŸâ ÿ•ÿ¥ÿßÿ±ÿ© ŸÖŸÜŸÅÿµŸÑÿ© discrete   ŸàÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÖ ŸàŸÅŸÇÿß ŸÑŸÜÿ∏ÿ±Ÿäÿ© ÿßÿÆÿ∞
ÿßŸÑÿπŸäŸÜÿßÿ™ Sampling Theory ŸàÿßŸÑÿ™Ÿä ÿ™ŸÜÿµ ÿπŸÑŸâ:  ÿßÿ∞ÿß ŸÉÿßŸÜÿ™ ùíî(ùíï) ÿ•ÿ¥ÿßÿ±ÿ© ÿ™ŸÖÿßÿ´ŸÑŸäÿ© ÿ∞ÿßÿ™ ŸÜÿ∑ÿßŸÇ ÿ™ÿ±ÿØÿØŸä ŸÖŸÜ  
ÿµŸÅÿ± ÿßŸÑŸâ ùíáùíé (ÿ£Ÿä ÿßÿπŸÑŸâ ÿ™ÿ±ÿØÿØ ÿ™ÿ≠ÿ™ŸàŸäŸá ŸáŸà  ùíáùíé  ŸàŸÖŸÖŸÉŸÜ ŸÜŸÇŸàŸÑ ÿßŸÜŸá  ÿßŸÑŸÄ Bandwidth ŸÑŸÑŸÄ signal  ŸàŸÇŸäŸÖÿ© 
Fourier transform  ŸÑŸáÿ∞Ÿáÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸä ÿ™ÿ±ÿØÿØ ÿßŸÉÿ®ÿ± ŸÖŸÜ  ùíáùíé  )ÿ™ÿ≥ÿßŸàŸä ÿµŸÅÿ±ŸÅŸäŸÖŸÉŸÜ ÿ™ŸÖÿ´ŸäŸÑŸáÿß ÿ®Ÿàÿßÿ≥ÿ∑ÿ© ÿπŸäŸÜÿßÿ™  
ŸÖŸÜŸáÿß ÿ™ÿ§ÿÆÿ∞ ÿπŸÑŸâ ŸÅÿ™ÿ±ÿßÿ™ ŸÖÿ™ÿ≥ÿßŸàŸäÿ©  ùíï = ùíèùëªùíî   (ÿ®ŸÜÿ®ÿπÿ™ ŸÖŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸÇŸäŸÖ ŸÖÿ≠ÿØÿØŸá ŸÅŸÇÿ∑ ÿπŸÜÿØ  ùëõùëáùë†   ÿ≠Ÿäÿ´ùëõ  ÿ±ŸÇŸÖ
ÿµÿ≠Ÿäÿ≠ )  
 
ŸÑŸÉŸä ŸÜÿ≥ÿ™ÿ∑Ÿäÿπ ÿ™ÿ≠ŸàŸäŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸâ ÿπŸäŸÜÿßÿ™ Ÿäÿ≥ŸÖÿ≠ ÿ®ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπŸáÿß ÿ®ÿπÿØ ÿ∞ŸÑŸÉ ŸÅŸä ÿßŸÑŸÄ Receiver  ÿ®ÿßŸÑÿ¥ŸÉŸÑŸàÿßŸÑÿ¨ŸàÿØÿ©   
ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© Ÿäÿ¨ÿ® ÿßŸÜ ŸäŸÉŸàŸÜ ÿ™ÿ±ÿØÿØ ÿßÿÆÿ∞ ÿßŸÑÿπŸäŸÜÿßÿ™ ŸàŸÅŸÇÿß ŸÑŸÑÿπÿßŸÑŸÇÿ© ÿßÿ£ŸÑÿ™Ÿäÿ© :  
ùíáùíî‚â•ùüê ùíáùíé 
 : ÿ≠Ÿäÿ´ 
 ùíáùíî : ŸÖÿπÿØŸÑ ÿßŸà ÿ™ÿ±ÿØÿØ ÿßÿÆÿ∞ ÿßŸÑÿπŸäŸÜÿßÿ™ ( sampling frequency or sampling rate)   ŸàŸäÿ≥ÿßŸàŸäùüè
ùëªùíî  
 ùëªùíî : ŸáŸä ÿßŸÑŸÄ sampling period  ÿ≠Ÿäÿ´ Ÿäÿ™ŸÖ ÿßÿÆÿ∞ ÿπŸäŸÜÿßÿ™ ŸÖŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßÿßŸÑÿµŸÑŸäÿ© ŸÉŸÑ  ùëªùíî  ÿ´ÿßŸÜŸäŸá 
 
ÿßŸÑÿ™ÿ±ÿØÿØ ùëìùë†=2 ùëìùëö   Ÿäÿπÿ±ŸÅ ÿ®ŸÄ Nyquist rate   ŸàŸáŸà ŸÇŸäŸÖÿ© ÿßŸÇŸÑ ÿ™ÿ±ÿØÿØ ŸÖÿ≥ŸÖŸàÿ≠ ÿ®ŸäŸá ŸàŸàÿ≠ÿØÿ™Ÿá samples/sec  
 Ÿäÿ™ŸÖ ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßÿßŸÑÿµŸÑŸäÿ© ŸÖŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ¨ÿ≤ÿ¶ÿ© ŸÅŸä ÿßŸÑŸÄ Receiver  ÿ®Ÿàÿßÿ≥ÿ∑ÿ© Low pass filter 
 
 ÿßŸÇŸÑ ÿπÿØÿØ samples  ŸÖŸÖŸÉŸÜ ÿßÿÆÿØŸá ÿÆÿßŸÑŸÑ cycle  ŸÉÿßŸÖŸÑŸá ŸáŸà 2 ùëìùëö  ŸàŸÑŸà ŸÇŸÑ ÿπŸÜ ŸÉÿØÿß ŸÑŸÜ Ÿäÿ™ŸÖ ÿßŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
ŸÅŸä ÿßŸÑŸÄ Receiver  ,ŸàŸÉŸÑ ŸÖÿß ŸÜÿ≤ŸäÿØ ŸáŸäŸÉŸàŸÜ ÿßŸÅÿ∂ŸÑ ŸÖŸÜ ŸÜÿßÿ≠Ÿäÿ© ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠ Ÿà  ÿßŸÑŸÄ bit rate  
ŸáŸäÿ≤ŸäÿØ ŸäÿπŸÜŸä ÿßŸÑÿØÿßÿ™ÿß Ÿáÿ™ÿ™ÿ®ÿπÿ™ ÿßÿ≥ÿ±ÿπ ŸàŸÑŸÉŸÜ ÿØÿß ŸáŸäÿ≤ŸàÿØ ÿßŸÑŸÄ Bandwidth 
 
ŸÖŸÖŸÉŸÜ ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑŸÄ bit rate  ( bits/sec )ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿ∂ÿ±ÿ® ÿßŸÑÿ™ÿ±ÿØÿØ  ùíáùíî  ŸÅŸä ÿπÿØÿØ ÿßŸÑÿ®ÿ™ÿßÿ™ ŸÑŸÉŸÑ sample 
 (ÿ®ŸÜÿ≠ÿØÿØŸáÿß ÿπŸÑŸâ ÿ≠ÿ≥ÿ®  A/D Converter ÿßŸÑŸÑŸäŸáÿ≥ÿ™ÿÆÿØŸÖŸá )  
ùíÉùíäùíï ùíìùíÇùíïùíÜ  ùëπùíÉ=ùíáùíî(ùíîùíÇùíéùíëùíçùíÜùíî
ùíîùíÜùíÑ) √ó ùíè(ùíÉùíäùíïùíî
ùíîùíÇùíéùíëùíçùíÜ)‚ÜíùíÉùíäùíïùíî
ùíîùíÜùíÑ 
 
 
 
 
 
 
 
 
 
 ŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜ ÿπŸÜÿØŸÜÿßÿ•ÿ¥ÿßÿ±ÿ©  ŸÅŸä ÿßŸÑŸÄ time domain   ÿßÿ≥ŸÖŸáÿß ùë†(ùë°)  Ÿàÿπÿ¥ÿßŸÜ ÿßÿπŸÖŸÑ sampling  Ÿáÿ∂ÿ±ÿ®ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
ùë†(ùë°)  ŸÅŸä train of pulses ùëù(ùë°)  periodic ŸàŸáŸä ÿØÿßŸÑÿ© Even   
 
 
Using Fourier series  
Time domain  
ùíë(ùíï)=ùíÇùíê+‚àëùíÇùíèùêúùê®ùê¨(ùíè ùùéùíî ùíï)‚àû
ùíè=ùüè 
ùë∫ùíî(ùíï)=ùë∫(ùíï) (ùíÇùíê+‚àëùíÇùíèùêúùê®ùê¨(ùíè ùùéùíî ùíï)‚àû
ùíè=ùüè) 
=ùíÇùíêùë∫(ùíï)+‚àëùíÇùíè ùë∫(ùíï)ùêúùê®ùê¨( ùüêùùÖùíè  ùíáùíîùíï)‚àû
ùíè=ùüè 
 
Frequency  domain  
ùë∫ùíî(ùíá)=ùíÇùíêùë∫(ùíá)+‚àë ùíÇùíè
ùüê [ùë∫(ùíá‚àíùíèùíáùíî)+ùë∫(ùíá+ùíèùíáùíî)]‚àû
ùíè=ùüè 
 
  ÿ¨ÿ≤ÿ°ùíÉùíè  ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÜ ÿßŸÑÿØÿßŸÑÿ© Even   
period ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùëù(ùë°)   ŸáŸàùëáùë†  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÄ Fundamental ŸáŸÜÿß ŸáŸà ŸÖŸÇŸÑŸàÿ® ÿßŸÑŸÄ  ùëìùë†=1
ùëáùë† ‚Üêùëáùë†  
 ŸÅŸÖŸáŸÖ ÿßŸÜŸÜÿß ŸÜŸÉÿ™ÿ®Ÿá ŸÅŸä ÿßŸÑŸÖÿπÿßÿØŸÑÿ©ùëìùë†   ŸÖÿ¥ùëìùëú 
Proof  
 
ÿßŸÑŸÄ Amplitude Spectrum  ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿßŸÑÿµŸÑŸäÿ©  ùëÜ(ùëì) 
 
 
ÿßŸÑŸÄ Amplitude Spectrum ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùëÜùë†(ùëì)  ÿ®ÿπÿØ ÿπŸÖŸÑŸäÿ© ÿßŸÑŸÄ sampling   
 
 ÿ∂ÿ±ÿ®ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ùëÜ(ùë°)  ŸÅŸä ŸÖÿ¨ŸÖŸàÿπ ÿßÿ¥ÿßÿ±ÿßÿ™ cos(2ùúãùëõ ùëìùë†ùë°)  ŸÅŸä ÿßŸÑŸÄ time domain ŸäŸÉÿßŸÅÿ¶ ŸÅŸä ÿßŸÑŸÄ 
frequency  domain   ÿπŸÖŸÑshift  ÿßŸÑŸâ ÿßŸÑŸäŸÖŸäŸÜ ŸàÿßŸÑŸâ ÿßŸÑŸäÿ≥ÿßÿ± ÿ®ŸÖÿ≥ÿßŸÅÿßÿ™ ÿ™ÿ≥ÿßŸàŸä ùíè ùíáùíî   ÿ≠Ÿäÿ´ùíè  ÿ£Ÿä ÿπÿØÿØ 
ÿµÿ≠Ÿäÿ≠ ( integer)   
 
A low pass filter with a cut off frequency of ùêüùê¶ can be used to recover the 
information signal  
 
 ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖLPF ŸÅŸä ÿßŸÑŸÄ Receiver  ŸäÿπÿØŸä ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸÑÿ≠ÿØùíáùíé ŸàŸäŸÑÿ∫Ÿä ÿ®ÿßŸÇŸä ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ ŸÉŸÑŸáÿß 
 
 
 

 
 ÿ£ŸÑÿÆÿ∞ ÿßŸÑÿπŸäŸÜÿßÿ™ ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ®ÿØ ÿßŸÜ Ÿäÿ™ÿ≠ŸÇŸÇ ÿßŸÑÿ¥ÿ±ÿ∑ 
ùíáùíî‚â•ùüê ùíáùíé      ùíêùíì        ùëªùíî‚â§ùüè
ùüêùíáùíé    
 
 ÿßÿ∞ÿß ŸÑŸÖ Ÿäÿ™ÿ≠ŸÇŸÇ ŸÅÿ≥ŸàŸÅ ŸäŸÜÿ™ÿ¨ ÿÆÿ∑ÿ£ Ÿäÿ≥ŸÖŸâ Aliasing error   ÿ≠Ÿäÿ´ ÿßŸÑ ŸäŸÖŸÉŸÜ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßÿßŸÑÿµŸÑŸäÿ© 
ŸÉÿßŸÖŸÑŸá Ÿàÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠ ŸÜÿ™Ÿäÿ¨ÿ© Ÿàÿ¨ŸàÿØ ÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™. ŸàÿßÿßŸÑÿ¥ŸÉÿßŸÑ ÿßŸÑÿ™ÿßŸÑŸä  ÿ© ÿ™Ÿàÿ∂ÿ≠ ÿßŸÑÿ≠ÿßÿßŸÑÿ™ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© : 
 
 
 
 
 
 
Is a technique for rounding -off the amplitudes of a waveform.  
 ŸáŸä ÿπŸÖŸÑŸäÿ© ÿ™ŸÇÿ±Ÿäÿ® ŸÉŸÑ sample  ŸÖŸÜÿßŸÑŸÄ samples  ÿßŸÑŸÖÿ£ÿÆŸàÿ∞ÿ© ÿ∂ŸÖŸÜ ŸÖÿ≥ÿ™ŸàŸâ ŸÖÿπŸäŸÜ ŸÖŸÜ ÿßŸÑŸÄ Quantization 
Levels (L) ŸàŸäÿ™ŸÖ ÿ™ŸÇÿ±Ÿäÿ® ŸÉŸÑ ÿπŸäŸÜŸá ÿßŸÑŸâ ÿßŸÇÿ±ÿ® Quantization Level  ÿ™ÿ®ÿπÿß ŸÑŸÇŸäŸÖÿ©ÿßŸÑŸÄ Amplitude  ŸÑŸáÿ∞Ÿá
ÿßŸÑÿπŸäŸÜÿ©.  
ÿßŸÑ ŸäŸÖŸÉŸÜ ÿπŸÖŸÑ coding  ŸÑŸÑŸÄ samples ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿßŸÑŸÜ A/D Converter   ŸáŸäÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿß ŸÖŸÜ
ÿßŸÑÿπŸäŸÜÿßÿ™ ŸÑŸÉŸÜ ÿ®ÿπÿØ ÿπŸÖŸÑŸäÿ© ÿßŸÑŸÄ Quantization  ÿ®Ÿäÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿπÿØÿØ ŸÖÿ≠ÿØŸàÿØ ŸÖŸÜ Quantization Levels 
 
 
 
2-Quantization   
 
The rounding -off operation is known as quantization, and the round -off error is 
known as quantization error . 
quantization error  .ŸáŸà ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ© ŸÑŸÑÿπŸäŸÜŸá ŸàÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿäÿ© ŸÑŸáÿß 
ŸäŸÜÿ™ÿ¨ ÿπŸÜ ÿßŸÑŸÄ Quantization   ÿÆÿ∑ÿ£ ŸÜÿ™Ÿäÿ¨ÿ© ŸÑÿπŸÖŸÑŸäÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®ŸàÿØÿß ÿ®ÿ≥ÿ®ÿ®  ÿßŸÜ ÿ®ÿπÿØ ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ® ÿ®ŸäŸÉŸàŸÜ ŸÅŸäŸá 
ŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ©  ŸÑŸÑŸÄ sample  ŸàÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿäÿ©  ŸÑŸäŸáÿß ,ŸÖÿ´ÿßŸÑ ŸÑŸà ŸÇŸäŸÖÿ© ÿßŸÑÿπŸäŸÜÿ© 2.1   ŸÅŸàŸÑÿ™ ÿ®ÿπÿØ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®
ÿ®ŸÇÿ™ 2 ŸÅŸàŸÑÿ™.  
 
ŸÑÿ™ŸÇŸÑŸäŸÑ Ÿáÿ∞ÿß ÿßŸÑÿÆÿ∑ÿ£ ŸÜÿ≤ŸàÿØ ÿπÿØÿØ  Quantization Levels (L)  ÿ≠Ÿäÿ´ ÿßŸÜŸáÿß ÿ™ÿ≤ŸäÿØ ŸÖŸÜ ŸÉŸÅÿßÿ°ÿ© ÿπŸÖŸÑŸäÿ©ÿßŸÑŸÄ 
Quantization  ŸàŸÑŸÉŸÜ ÿØÿß ÿ®Ÿäÿ≤ŸàÿØ ÿπÿØÿØÿßŸÑŸÄ bits  ŸÅŸä ÿßŸÑŸÉŸàÿØ ŸÖŸÖÿß ÿ≥Ÿäÿ§ÿØŸä ÿßŸÑŸâ ÿ≤ŸäÿßÿØÿ©ÿßŸÑ ŸÄ bit rate    Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸäÿ≤ŸäÿßÿØÿ© 
ÿßŸÑŸÄ Bandwidth  :ÿ≠Ÿäÿ´ 
ùíÉùíäùíï ùíìùíÇùíïùíÜ  ùëπùíÉ=ùíáùíî √ó ùíè  ‚Üë          ,    ùë©ùëæùíéùíäùíè =ùëπùíÉ
ùüê   ‚Üë 
 ŸÑÿ∞ŸÑŸÉ ŸÜÿ≠ÿßŸà ŸÑ ŸÇÿØÿ± ÿßÿ•ŸÑŸÖŸÉÿßŸÜ ÿßŸÜ Ÿäÿ™Ÿàÿßÿ¨ÿØ ÿßÿ™ÿ≤ÿßŸÜ ( trade off ) ÿ®ŸäŸÜ ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿßÿ™ (ùë≥)  ŸàÿπÿØÿØÿßŸÑŸÄ bits  (ùíè) 
 
 Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖÿßŸÑŸÄ  DR  ŸÑŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÜÿßŸÑŸÄ  Levels  ŸàÿπÿØÿØ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ÿ≥ŸàŸÅ Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿπÿØÿØÿßŸÑŸÄ  bits   ÿßŸÑÿ™Ÿä
ŸÜÿ±ŸäÿØ ÿßŸÜ ŸÜÿπÿ®ÿ± ÿ®Ÿáÿß ÿπŸÜ ÿßŸÑŸÄ sample 
Quantization Levels  ùë≥=ùüêùíè  
ùíè‚Üínumber of bits  required to represent each sample  
ùë≥‚Üínumber of Quantization Levels     
 
 ÿ®ÿπÿØ ÿ≠ÿ≥ÿßÿ® ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ŸÜŸÇŸàŸÖ ÿ®ÿ≠ÿ≥ÿßÿ® ŸÇŸäŸÖÿ©ÿßŸÑŸÄ  step  ÿßŸÑÿ™Ÿä ÿ™ŸÅÿµŸÑ ÿ®ŸäŸÜ ŸÉŸÑ ŸÖÿ≥ÿ™ŸàŸäŸäŸÜ 
‚àÜùíî= ùë´ùëπ
ùë≥ 
 
‚àÜùíî‚Üí separation between two quantization level.  
 
 ùê∑ùëÖ  ŸáŸà ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßÿπŸÑŸâ ŸàÿßŸÇŸÑ amplitude  ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
ùë´ùëπ =ùíÇùíéùíëùíçùíäùíïùíñùíÖ ùíÜùíéùíÇùíô ‚àíùíÇùíéùíëùíçùíäùíïùíñùíÖ ùíÜùíéùíäùíè 
 
 
 
 
 
 
PCM codes the various levels into binary numbers and  sends the binary code 
corresponding to the particular round -off level.  
 ŸÉŸÑ ŸÖÿ≥ÿ™ŸàŸâ Ÿäÿ£ÿÆÿ∞ ŸÉŸàÿØ ŸÖÿπŸäŸÜŸàŸäÿ™ŸÖ ÿ™ŸÖÿ´ŸäŸÑ ŸÉŸÑ sample  ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑŸàÿßŸÇÿπÿ©ŸÅŸäŸá  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä Ÿäÿ™ŸÖ ÿ™ÿ≠ŸàŸäŸÑ  
ÿßŸÑÿπŸäŸÜÿßÿ™ ÿßŸÑŸâ ÿßŸÉŸàÿßÿØ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3-Encoding   
 
 
‚ñ™ The processing pair (compression and expansion) is  called companding.  
‚ñ™ The most common form of non -uniform quantization is known as companding.  
Companding  ŸáŸä ÿßÿÆÿ™ÿµÿßÿ± ŸÑÿØŸÖÿ¨ ŸÖÿµÿ∑ŸÑÿ≠ŸäŸÜ ŸÖÿπ ÿ®ÿπÿ∂ Compression and Expansion    ŸàŸáŸä ŸÖŸÜ
ÿßÿ¥Ÿáÿ± ÿµŸàÿ± non-uniform quantization 
 
‚ñ™ The uniform quantization provides the same resolution at high levels as at low.  
‚ñ™ For some signals like voice signals it is desirable to use small quantization 
steps at lower levels and larger steps at higher levels.  
 
ÿßŸÑŸÑŸä ÿ™ŸÖ ÿ¥ÿ±ÿ≠Ÿá ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ŸáŸà uniform quantization   ŸàŸÅŸä ÿßŸÑŸÜŸàÿπ ÿØÿß ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ÿ®ŸäŸÜÿßŸÑŸÄ Levels  (‚àÜùíî) 
 ÿ´ÿßÿ®ÿ™Ÿá, ŸàÿØÿß ŸÖŸÖŸÉŸÜ ŸäÿπŸÖŸÑ ŸÖÿ¥ŸÉŸÑÿ© ŸÅŸä ÿ®ÿπ ÿ∂ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ ÿ≤Ÿä ÿßÿ¥ÿßÿ±ÿ© ÿßŸÑÿµŸàÿ™ ŸÖÿ´ÿßŸÑ ŸÖÿπÿ∏ŸÖ ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä 
ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÖŸÜÿÆŸÅÿ∂ÿ© ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© Ÿàÿ®ŸÉÿØÿß ŸÉŸÑ ÿßŸÑŸÄ samples  ŸÅŸä ÿßŸÑŸÖŸÜÿ∑ŸÇÿ© ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ŸáŸäÿ™ŸÖ ÿ™ŸÇÿ±Ÿäÿ®ŸáŸÖ  ŸÑŸÜŸÅÿ≥ ÿßŸÑŸÄ Level  Ÿà  
ÿßŸÑŸÄ Quantization Error   ŸáŸäŸÉŸàŸÜ ŸÉÿ®Ÿäÿ± 
 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿØŸä ÿπÿ¥ÿßŸÜ ŸÜŸÇÿØÿ± ŸÜÿπŸÖŸÑŸáÿß Quantization  Ÿà Coding   ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠ ÿßŸÑŸÖŸÅÿ±Ÿàÿ∂ ÿßÿ≤ŸàÿØ ÿπÿØÿØ
ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑŸÑŸä ŸÅŸäŸá ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ŸÜÿ≤ŸàÿØ ÿπÿØÿØ ÿßŸÑŸÄ Levels  ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸÉŸÑŸáÿß Ÿà  ÿßŸÑŸÄ  
Bandwidth Ÿäÿ≤ŸäÿØ.  
 
ÿßŸÑÿ≠ŸÑ ŸáŸÜÿß ŸÅŸä ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ non-uniform quantization  ŸàŸÖŸÜ ÿÆÿßŸÑŸÑŸá ÿßŸÇÿØÿ± ÿßÿ≤ŸàÿØ ÿπÿØÿØÿßŸÑŸÄ Levels  ŸÅŸä
ÿßÿ£ŸÑÿ¨ÿ≤ÿßÿ° ÿßŸÑŸÑŸä ŸÅŸäŸáÿß ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÉÿ™ÿ±  ŸäÿπŸÜŸä ÿßÿ≥ÿ™ÿÆÿØŸÖ  ‚àÜùíî   ÿµÿ∫Ÿäÿ±Ÿá ŸàÿßÿßŸÑÿ¨ÿ≤ÿßÿ° ÿßŸÑŸÑŸä ŸÅŸäŸáÿß ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÇŸÑ ÿßÿ≥ÿ™ÿÆÿØŸÖ‚àÜùíî 
ŸÉÿ®Ÿäÿ±Ÿá Ÿàÿ®ŸÉÿØÿß ÿßŸÑŸÄ Quantization Error  Average ŸáŸäŸÇŸÑ.  
 
ŸÉÿØÿß ÿπŸÖŸÑŸÜÿß ÿ™Ÿàÿßÿ≤ŸÜ ŸÖÿß ÿ®ŸäŸÜ ÿßŸÜŸÜÿß ŸÖŸÅŸÇÿØŸÜÿßÿ¥ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸáŸÖÿ© ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ Ÿàÿ®ŸäŸÜ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ  
ÿßŸÅÿ∂ŸÑ ŸÑŸÑŸÄ  Bandwidth 
 
Companding  
 
‚ñ™ The average quantization error may well dec rease using this approach.  
‚ñ™ Prior to quantization, the signal is compressed  by a function similar to the one 
show below that.  
 ÿßŸÑÿØŸàÿßÿ¶ÿ± ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ÿπŸÜÿØŸÜÿß uniform quantiz er    ŸäÿπŸÜŸä ÿ®ÿ™ÿπŸÖŸÑ quantization  ÿ®ŸÄ  ‚àÜùíî  ÿ´ÿßÿ®ÿ™Ÿá.  
ÿßÿ≤ÿßŸä ŸáŸÜÿ≥ÿ™ŸÅŸäÿØ ŸÖŸÜ ÿßŸÑÿØŸàÿßŸäÿ± ÿØŸä ŸàŸÜÿπŸÖŸÑ ÿ®ŸäŸáÿß non-uniform quantization ÿü 
ÿßŸÑÿ≠ŸÑ ÿßŸÜŸÜÿß ŸÜÿ∂ÿ∫ÿ∑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  compression  ŸÖŸÜ ÿÆÿßŸÑŸÑÿØÿßŸäÿ±ÿ© ÿßÿ≥ŸÖŸáÿß  ÿßŸÑŸÄ compressor   ŸàŸÜÿ∂ÿ∫ÿ∑Ÿáÿßÿ®ÿ¥ŸÉŸÑ  
ŸäŸÜÿßÿ≥ÿ® ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑŸÑŸä ŸáŸÜÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸäŸá ŸàÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑŸÑŸä ÿπŸÜÿØŸÜÿß ÿØÿß ÿßŸÑŸÄ compressor   ÿ®Ÿäÿ∂ÿ∫ÿ∑ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸàŸäÿ≠ÿßŸÅÿ∏
ÿπŸÑŸâ ÿßŸÑŸÇŸäŸÖ ÿßŸÑÿµÿ∫Ÿäÿ±ÿ© Ÿàÿ®ÿπÿØŸáÿß ŸÜÿßÿÆÿØ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿ∂ÿ∫Ÿàÿ∑ÿ© ŸàŸÜÿØÿÆŸÑŸáÿß ÿπŸÑŸâ uniform quantiz er   Ÿàÿ®ŸÉÿØÿß ÿπŸÖŸÑŸÜÿßnon-
uniform quantization 
 ÿÆÿµÿßÿ¶ÿµÿßŸÑŸÄ compressor  ÿ™ÿÆÿ™ŸÑŸÅ ÿπŸÑŸâ ÿ≠ÿ≥ÿ® ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑŸÑŸäŸáÿ≥ÿ™ÿÆÿØŸÖŸá   ÿßŸÑŸÜ ŸÖŸÖŸÉŸÜ ÿ™ŸÉŸàŸÜ ÿ™ŸÅÿßÿµŸäŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© 
ŸÅŸä ÿßŸÑŸÇŸäŸÖ ÿßŸÑÿπÿßŸÑŸäŸá ŸÖÿ´ÿßŸÑ ÿßŸà ÿ™ŸÉŸàŸÜ ŸÖÿ™ÿ∫Ÿäÿ±Ÿá ŸÅÿ®ŸÜÿ¥ŸàŸÅ ÿ™ŸÅÿßÿµŸäŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÅŸäŸÜ  ÿ®ÿßŸÑÿ∂ÿ®ÿ∑  ŸàÿßÿπŸÖŸÑ enhance   ŸÑÿ™ŸÅÿßÿµŸäŸÑ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© Ÿà  compress ŸÑÿ£ŸÑÿ¨ÿ≤ÿßÿ° ÿßŸÑŸÑŸä ŸÖÿ¥ ŸÖŸáŸÖÿ© ÿßŸàŸä ŸÅŸä ÿßÿßŸÑÿ¥ÿßÿ±ÿ© 
 
 
ÿßŸÑÿ¥ŸÉŸÑ ÿØÿß ŸäŸàÿ∂ÿ≠ ÿπÿßŸÑŸÇÿ© ÿßŸÑÿØÿÆŸÑ ŸàÿßŸÑÿÆÿ±ÿ¨ ŸÑŸÑŸÄ compressor   ÿØÿß 
 
 
This operation compresses the extreme values of the wave form while enhancing 
the small values  
 Ÿàÿßÿ∂ÿ≠ ŸÖŸÜÿßŸÑÿ±ÿ≥ŸÖ ÿßŸÜ ÿßŸÑŸÇŸäŸÖ ÿßŸÑÿµÿ∫Ÿäÿ±ÿ© ÿ®ÿ™ÿ∑ŸÑÿπ ÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ®ŸÜŸÅÿ≥ ÿßŸÑŸÇŸäŸÖÿ©  ŸàÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ÿ®ÿ™ÿÆÿ±ÿ¨ ŸÇÿ±Ÿäÿ®Ÿá ŸÖŸÜ ÿ®ÿπÿ∂ 
ŸäÿπŸÜŸä ÿπŸÖŸÑ ÿ∂ÿ∫ÿ∑ ŸÑŸÑŸÇŸäŸÖ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸÅŸÇÿ∑ ŸäÿπŸÜŸä ÿ®Ÿäÿ∂ÿ∫ÿ∑  ÿßŸÑŸÇŸäŸÖ ÿßŸÑÿπÿßŸÑŸäÿ© ŸÖŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸäÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÇŸÑŸäŸÑÿ© 
 
Compression   characteristic  
 
‚ñ™ The analog signal forms the input to the compressor, and the output is 
uniformly quantized. The result is equivalent to quantizing with steps that start 
out small and get larger with higher signal values.  
‚ñ™ At the receiver, expansion is applied so that the overall transmission is not 
distorted.  
 ŸÅŸäÿßŸÑŸÄ Receiver ÿ®ŸÜÿπŸÖŸÑ  ÿπŸÉÿ≥ ÿßŸÑÿπŸÖŸÑŸäÿ© ÿØŸä ŸàŸáŸä  Expansion 
 
North America and Japan have adopted a standard compression curve known 
as ùùª-low companding. Europe has adopted another standard known as A-law 
companding.  
ÿØŸä ŸÇŸàÿßŸÜŸäŸÜ ÿßŸà  standards ŸÑŸÑŸÄ  compression ùùª-low  ŸÑŸÑŸÜÿ∏ÿßŸÖÿßÿ£ŸÑŸÖÿ±ŸäŸÉŸä   Ÿà  A-law  ŸÑŸÑŸÜÿ∏ÿßŸÖ ÿßÿßŸÑŸàÿ±Ÿàÿ®Ÿä 
 
ŸÑÿßŸÑÿ∑ÿßŸÑÿπ ŸÅŸÇÿ∑ 
ÿßŸÑŸÇÿßŸÜŸàŸÜ Ÿà ÿßŸÑŸÄ curve  ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ©ŸäŸàÿ∂ÿ≠Ÿàÿß ÿÆÿµÿßÿ¶ÿµ  ùùª-low    
ùë≠(ùíî)=ùíîùíàùíè  (ùíî) ùíçùíè(ùüè+ùùÅ|ùíî|)
ùíçùíè(ùüè+ùùÅ) 
 
 ÿØÿ±ÿ¨ÿ© ÿßŸÜÿ≠ŸÜÿßÿ° ÿßŸÑŸÖŸÜÿ≠ŸÜŸâ ÿ®Ÿäÿ≠ÿØÿØŸáÿß parameter ÿßÿ≥ŸÖŸá ùùÅ  
 ŸÑŸà ùùÅ=ùüé   ŸÅÿØÿß ŸÖÿπŸÜÿßŸá ÿßŸÜ ŸÖŸÅŸäÿ¥ compression ŸàŸÑŸà ÿπŸàÿ∂ŸÜÿß ŸÅŸä ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿ®ÿßŸÑŸÇŸäŸÖÿ© ÿØŸä ŸáŸÜÿßŸÑŸÇŸä ÿßŸÑÿØÿÆŸÑ ÿ≤Ÿä  
ÿßŸÑÿÆÿ±ÿ¨ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑, ŸàÿßŸÇÿµŸâ ŸÇŸäŸÖÿ© ŸÖŸÖŸÉŸÜŸá   ùùÅ=ùüêùüìùüì   
  ÿπŸÑŸâ ÿ≠ÿ≥ÿ® ŸÇŸäŸÖÿ©ùùÅ ÿ®ŸÜÿ≠ÿØÿØ ÿ¥ŸÉŸÑ ÿßŸÑÿπÿßŸÑŸÇÿ© ÿ®ŸäŸÜ ÿßŸÑÿØÿÆŸÑ ŸàÿßŸÑÿÆÿ±ÿ¨ ŸàÿßŸä ÿßŸÑŸÑŸä ŸáŸäÿ™ÿ∂ÿ∫ÿ∑  ŸàŸÜÿ≥ÿ®ÿ© ÿßŸÑŸÄ  
compression  ŸàÿπŸÑŸâ ÿ≠ÿ≥ÿ®ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  Ÿà ŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™Ÿáÿß ÿ®ŸÜÿ≠ÿØÿØ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© 
 
 

 
 
A PCM modulator is nothing more than an analog -to digital converter. The 
converter first samples the waveform, and then quantizes each sample value.  
 
There are three generic forms for the quantizer:  
‚û¢ Counting Quantizer  
‚û¢ Serial Quantizer  
‚û¢ Parallel quantizer  
 
 
 
 
 
Sample and hold  ÿ®ÿßÿ®ÿ≥ÿ∑ ÿßÿ£ŸÑŸÖÿ´ŸÑÿ© ÿπÿ®ÿßÿ±ÿ© ÿπŸÜ switch ŸäÿπŸÖŸÑ sampl ing    ŸäÿπŸÜŸä ŸäÿßÿÆÿØ ŸÇŸäŸÖÿ©ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿπŸÜÿØ  
ŸÑÿ≠ÿ∏ÿ© ŸÖÿπŸäŸÜŸá  Ÿà ŸÖŸÉÿ´ŸÅ ŸäÿπŸÖŸÑ Hold  ŸäÿπŸÜŸä Ÿäÿ≠ÿ™ŸÅÿ∏ ÿ®ÿßŸÑŸÇŸäŸÖÿ© ŸÑŸÖÿØÿ© sampling period  ùëáùë†   Ÿàÿ®ÿπÿØŸäŸÜ Ÿäÿ±ÿ¨ÿπ ŸäÿßÿÆÿØ
sample  ÿ™ÿßŸÜŸäÿ© ŸàŸáŸÉÿ∞ÿß 
 
Comparator ÿ™ŸÇÿßÿ±ŸÜ ÿÆÿ±ÿ¨ S/H   Ÿà ramp generator  ŸàÿπŸÜÿØ ÿ™ÿ≥ÿßŸàŸä ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ÿ™ÿÆÿ±ÿ¨ÿ•ÿ¥ÿßÿ±ÿ© stop 
 
ŸÅŸä ÿßŸÑÿ®ÿØÿßŸäÿ© ÿ®ÿ™ÿÆÿ±ÿ¨ ÿ•ÿ¥ÿßÿ±ÿ© start  ÿ®ÿ™ÿ¥ÿ∫ŸÑ binary counter   Ÿà S/H   ÿ®ÿßŸÑÿ™ÿ≤ÿßŸÖŸÜ ŸÖÿπ ÿ®ÿπÿ∂ Ÿàÿ™ÿØÿÆŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
ÿßŸÑÿ™ŸÜÿßÿ∏ÿ±Ÿäÿ©  ùëì(ùë°)  ÿπŸÑŸâS/H Ÿàÿ™ÿßÿÆÿØ ÿßŸàŸÑ sample   Ÿàÿ™ÿπŸÖŸÑHold   ŸÑŸäŸáÿß ŸäÿπŸÜŸäŸäÿ´ÿ®ÿ™Ÿáÿß ÿÆÿßŸÑŸÑ ŸÅÿ™ÿ±ÿ© ÿ≤ŸÖŸÜŸäŸá   ùëáùë† 
 ŸàŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ ÿØÿßŸäÿ±ÿ©ÿßŸÑŸÄ ramp  ÿ®ÿ™ÿ∑ŸÑÿπÿ•ÿ¥ÿßÿ±ÿ© ÿ®ŸÖŸäŸÑ ÿ´ÿßÿ®ÿ™ ŸÑÿ≠ÿØ ŸÖÿß ÿßŸÑŸÄ comparator   ŸäÿßŸÑŸÇŸä ÿÆÿ±ÿ¨
ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäŸÜ S/H   Ÿà ramp generator  ÿ™ÿ≥ÿßŸàŸàÿß ŸáŸäÿÆÿ±ÿ¨ÿ•ÿ¥ÿßÿ±ÿ© stop ŸÑŸÑŸÄ binary counter  Ÿà  ŸäŸàŸÇŸÅ ÿπÿØ .  
 
ÿßŸÑÿπÿØŸá ÿßŸÑŸÑŸä ŸáŸäŸÇŸÅ ÿπŸÜÿØŸáÿß ÿßŸÑŸÄ counter  ÿ™ÿπÿ®ÿ± ÿπŸÜÿßŸÑŸÄ sample  ÿ®ÿßŸÑŸÄ binary  Ÿàÿ®ÿπÿØŸáÿß ÿßŸÑÿØÿßŸäÿ±ÿ© ÿ™ÿπŸÖŸÑ reset ŸÑŸÑŸÄ  
binary counter  Ÿà ramp generator  ŸàŸÜÿπŸÖŸÑstart  ÿπŸÜÿØ ÿ®ÿØÿßŸäÿ© sample   ÿ¨ÿØŸäÿØŸáŸàŸáŸÉÿ∞ÿß ŸÅŸä ÿ®ÿßŸÇŸä ÿßÿßŸÑÿ¥ÿßÿ±ÿ© .  
PCM Modulators  
Counting Quantizer  
 
 
The ramp generator starts at each sampling point, and
 
a binary counter is 
simultaneously started.
 
ŸÖÿπ ÿ®ÿØÿßŸäÿ© ŸÉŸÑ
 
sample
  
 Ÿäÿ®ÿØÿ£
ÿßŸÑ 
ŸÄ
 
ramp generator
 
 &
S/H
 
 &
binary counter
 
ÿßŸÑÿπŸÖŸÑ ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™
 
 
 
ŸÅŸäŸá 
ÿ¥ÿ±ÿ∑ŸäŸÜ 
 
ÿπÿ¥ÿßŸÜ ÿßŸÑÿØÿßŸäÿ±ÿ© ÿ™ÿ¥ÿ™ÿ∫ŸÑ ÿµÿ≠ 
ÿßŸÑÿ¥ÿ±ÿ∑ 
ÿßÿ£ŸÑŸàŸÑ
  
ÿÆÿßÿµ ÿ® 
ÿßŸÑ
ŸÄ
 
 
ramp
 
slope
 ŸàÿßŸÑÿ™ÿßŸÜŸä ÿÆÿßÿµ ÿ®
ÿßŸÑ
ŸÄ
 
 
clock frequency
 
 :
 
‚ñ™
 
The time duration of the ramp, and therefore the
 
duration of the count is
 
proportional to the sample
 
value 
(ramp slope is constant).
 
 ÿßŸÑÿ≤ŸÖ ŸÖŸäŸÑ
ÿßŸÑ
ŸÄ
 
ramp generator
 
 ŸäŸÉŸàŸÜ ÿ´ÿßÿ®ÿ™ ÿπÿ¥ÿßŸÜ ÿ™ÿ∑ŸÑÿπ ÿßŸÑÿÆÿ±ÿ¨ ÿµÿ≠ ÿßŸÑŸÜ ŸÖŸäŸÜŸÅÿπÿ¥ ÿπŸÜÿØ ŸÉŸÑ
sample
 
ŸäŸÉŸàŸÜ ŸÅŸäŸá ŸÖŸäŸÑ ŸÖÿÆÿ™ŸÑŸÅ
.
 
ÿ≤ŸÖŸÜ 
ÿßŸÑ
ŸÄ
 
ramp
 
 
ÿ©
ŸÖ
Ÿä
ŸÇ
 
ÿπ
ŸÖ
 
Ÿã
ÿß
Ÿä
ÿØ
ÿ±
ÿ∑
 
ÿ®
ÿ≥
ÿß
ŸÜ
ÿ™
Ÿä
ÿßŸÑ
ŸÄ
 
sample
 
 ŸäÿπŸÜŸä ŸÉŸÑ ŸÖÿß ŸÉÿßŸÜÿ™ ŸÇŸäŸÖÿ©
ÿßŸÑ
ŸÄ
  
sample
 
 ÿßŸÉÿ®ÿ± ÿ≤ŸÖŸÜ
ÿßŸÑ
ŸÄ
 
ramp
 
ÿßŸÑŸÑŸä ŸáŸäÿ≠ÿ™ÿßÿ¨ÿ© ÿπÿ¥ÿßŸÜ ŸäŸàÿµŸÑ ŸÑŸÑŸÇŸäŸÖÿ© ÿØŸä ŸáŸäÿ≤ÿØÿßÿØ
 
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπŸÜÿØ ÿßŸÇÿµŸâ ŸÇŸäŸÖÿ©  
ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ŸáŸäŸÉŸàŸÜ ÿßŸÉÿ®ÿ± ÿ≤ŸÖŸÜ 
ŸÑŸÑ
ŸÄ
 
ramp
 
ŸàŸÖŸÜ ÿØÿß ÿ®ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÖŸäŸÑ ÿ´ÿßÿ®ÿ™ ŸÖŸÜ ÿßŸÑÿπÿßŸÑŸÇÿ©
 
ÿßÿ£ŸÑÿ™Ÿäÿ©: 
 
ùíìùíÇùíéùíë
 
ùíîùíçùíêùíëùíÜ
=
ùë´ùëπ
ùëª
ùíìùíÇùíéùíë
  
 
  
ùê∑ùëÖ
 ŸáŸà ÿßÿπŸÑŸâ
ŸÇŸäŸÖÿ© ŸÅŸä 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
Ÿà  
ùëá
ùëüùëéùëöùëù
 
 ŸáŸà ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑÿßŸÑÿ≤ŸÖ ŸÑŸÑŸàÿµŸàŸÑ ÿßŸÑÿπŸÑŸâ ŸÇŸäŸÖÿ© ŸÅŸä ÿßÿßŸÑÿ¥ÿßÿ±ÿ©
 
 
‚ñ™
 
The ramp slope must be 
sufficient to reach the
 
maximum possible values 
within one sampling period.
 
 ŸÖŸäŸÑ
ÿßŸÑ
ŸÄ
ramp 
 
 ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÉÿßŸÅŸä ÿßŸÜŸá ŸäŸàÿµŸÑ ÿ£ŸÑŸÇÿµŸâ ŸÇŸäŸÖÿ© ŸÅŸä
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ŸÅŸä ŸÅÿ™ÿ±ÿ©
 
ÿßŸÑ
ŸÄ
  
ùëá
ùë†
 
 ŸäÿπŸÜŸä ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÑŸä
Ÿáÿ™ÿ≠ÿ™ÿßÿ¨ÿ© 
ÿ•ÿ¥ÿßÿ±ÿ©
 
ÿßŸÑ
ŸÄ
 
ramp
 
 ÿßŸÑÿ≤ŸÖ ŸäŸÉŸàŸÜ ÿßŸÇŸÑ ŸÖŸÜ ÿßŸà Ÿäÿ≥ÿßŸàŸä
ùëá
ùë†
 
  ÿßŸÑŸÜ ÿ®ÿπÿØ ÿ≤ŸÖŸÜ
ùëá
ùë†
 
 ŸÅÿ™ÿ±ÿ©
ÿßŸÑ
ŸÄ
 
Hold
 
Ÿáÿ™ŸÜÿ™ŸáŸä
  
Ÿà 
  
S/H
 
 Ÿáÿ™ÿπŸÖŸÑ
 
sample
 
ÿ¨ÿØŸäÿØŸá
 
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸà ŸÉÿßŸÜ
 
ÿ≤ŸÖŸÜ
 
ÿßŸÑ
ŸÄ
ramp 
 
  ÿßŸÉÿ®ÿ± ŸÖŸÜ
ùëá
ùë†
 
  ŸÖÿ¥ ŸáŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÇŸäŸÖÿ©
digital
 
ŸÖŸÇÿßÿ®ŸÑÿ© ÿ£ŸÑÿπŸÑŸâ ÿ¨ŸáÿØ
 
ÿßŸÑŸÜŸá ŸáŸäÿßÿÆÿØ 
sample
 
ÿ¨ÿØŸäÿØŸá ŸÇÿ®ŸÑ ŸÖÿß ŸäŸàÿµŸÑ ŸÑŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©
.
 
ŸÅÿßŸÑÿ≤ŸÖ 
ÿ•ÿ¥ÿßÿ±ÿ©
 
ÿßŸÑ
ŸÄ
ramp 
 
ÿ™ŸàÿµŸÑ ÿ£ŸÑÿπŸÑŸâ ŸÇŸäŸÖÿ©
 
ŸÅŸä ÿ≤ŸÖŸÜ
 
ÿßŸÇŸÑ ŸÖŸÜ ÿßŸà Ÿäÿ≥ÿßŸàŸä  
ùëá
ùë†
 
 
ùëª
ùíìùíÇùíéùíë
‚â§
 
ùëª
ùíî
 
S/H o/p
 
ramp o/p
 
 ŸáŸÜÿß ÿ™ÿ≥ÿßŸàŸâ ÿßÿßŸÑÿ´ŸÜŸäŸÜ ŸÅŸäŸÇŸÅ
ÿßŸÑŸÄ  
counter
 
  ÿπŸÜ ÿßŸÑÿπÿØ
ÿπŸÜÿØ ŸÇŸäŸÖÿ© 
ÿßŸÑŸÄ 
sample
 
 
‚ñ™ The clock frequency is such that the counter has enough time to count to its 
highest count for a ramp duration corresponding to the maximum possible 
sample.  
 ÿßŸÑÿ≤ŸÖÿßŸÑŸÄ clock frequency ÿ™ŸÖŸÉŸÜ ÿßŸÑŸÄ counter   ÿßŸÜŸá Ÿäÿ®ŸÇŸâ ÿπŸÜÿØŸá ÿ≤ŸÖŸÜ ŸÉÿßŸÅŸä ÿπÿ¥ÿßŸÜ ŸäŸàÿµŸÑÿ£ŸÑŸÇÿµŸâ  ÿπÿØŸá 
ŸàÿßŸÑŸÑŸä ÿ® ÿ™ŸÉŸàŸÜ ŸÖŸÜÿßÿ∏ÿ±ÿ© ÿ£ŸÑÿπŸÑŸâ ŸÇŸäŸÖÿ© ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÜ ÿßŸÑŸÉŸàÿØ ÿßŸÑŸÑŸä ÿÆÿßÿ±ÿ¨ ŸÖŸÜ ÿßŸÑŸÄ binary counter   Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ
ŸÇŸäŸÖÿ© ÿßŸÑŸÄ sample  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÇÿµŸâ ÿπÿØŸá ÿ™ŸÉŸàŸÜŸÖŸÜÿßÿ∏ÿ±ÿ© ÿ£ŸÑŸÇÿµŸâ  ŸÇŸäŸÖÿ© ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  
 
The ending counts on the counter will correspond binary  equivalent value of the 
input.  
 ŸÅŸäŸÜŸáÿßŸäÿ© ÿßŸÑÿπÿØ   ÿÆÿ±ÿ¨ ÿßŸÑŸÄ binary counter  ŸáŸäÿπÿ®ÿ± ÿπŸÜ ÿßŸÑŸÖŸÉÿßŸÅÿ¶ÿßŸÑŸÄ binary  ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©   
 
Counting Quantizer  ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑÿπŸÑŸâ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÖŸàÿ¨ÿ®ÿ© ŸÅŸÇÿ∑ ŸàŸÑŸà  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÉÿßŸÜ ŸÅŸäŸáÿß ŸÇŸäŸÖ ÿ≥ÿßŸÑÿ®ÿ© ŸÖÿ´ÿßŸÑ ŸÖŸÜ -10 ÿßŸÑŸâ 
10  ŸáŸÜÿπŸÖŸÑshift  ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ®ŸÖŸÇÿØÿßÿ±  10 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä  ÿßŸÑŸÄ range ÿßŸÑÿ¨ÿØŸäÿØ: 0‚Üí20  
 
 : ŸÇŸàÿßŸÜŸäŸÜ ŸÖŸáŸÖÿ© ŸÅŸä ÿ≠ŸÑ ÿßŸÑŸÖÿ≥ÿßÿ¶ŸÑ 
time of all counts =ùëªùíìùíÇùíéùíë  
 ùëáùëüùëéùëöùëù  ŸáŸà ÿ≤ŸÖŸÜ ÿßŸÑŸàÿµŸàŸÑ ÿßŸÑŸÇÿµŸâ ŸÇŸäŸÖÿ©ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©  ŸàŸäŸÜÿßÿ∏ÿ±  ÿßŸÇÿµŸâ ÿπÿØŸá   
ŸÖÿ´ÿßŸÑ ŸÑŸà ÿßŸÇÿµŸâ ŸÇŸäŸÖÿ© ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© 10 V  ŸàÿπŸÜÿØŸÜÿß 4-bit counter  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπŸÜÿØ ÿ™ÿ≥ÿßŸàŸä ŸÇŸäŸÖÿ©ÿßŸÑŸÄ ramp   ŸÖÿπ ŸÇŸäŸÖÿ©
ÿßŸÑŸÄ sample = 10  ÿßŸÑÿ≤ŸÖ ŸäŸàÿµŸÑÿßŸÑŸÄ counter  ÿ¢ŸÑÿÆÿ± ÿπÿØŸá‚Üê 1111 
 
time of one count =ùëªùíìùíÇùíéùíë
ùêßùêÆùê¶ùêõùêûùê´  ùê®ùêü ùêúùê®ùêÆùêßùê≠ùê¨=ùëªùíìùíÇùíéùíë
ùüêùêß 
 ÿ≤ŸÖŸÜ ÿßŸÑÿπÿØŸá ÿßŸÑŸàÿßÿ≠ÿØŸá ŸàÿßŸÑŸÑŸäŸáŸÜÿ≥ÿ™ŸÅŸäÿØ ŸÖŸÜŸá ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ±ÿØÿØ  
clock frequency of counter =ùüè
ùê≠ùê¢ùê¶ùêû  ùê®ùêü ùê®ùêßùêû  ùêúùê®ùêÆùêßùê≠   
clock frequency  ŸáŸà ÿπÿØÿØ ÿßŸÑÿπÿØÿßÿ™ counts ŸÅŸä ÿßŸÑÿ´ÿßŸÜŸäŸá ÿßŸÑŸàÿßÿ≠ÿØÿ©  ŸàŸáŸà ŸÖŸÇŸÑŸàÿ® ÿ≤ŸÖŸÜ ÿßŸÑÿπÿØŸá ÿßŸÑŸàÿßÿ≠ÿØÿ© 
 
 
 
 
 
 
Give the ramp slope is ùüèùüéùüî V/s, signal amplitude's range from 0 to 10 V, and a 
4-bit counter is used, what should the clock frequency be for a voice signal, 
that is a signal with maximum frequency of 3KHz.  
Solution  
 
ùëüùëéùëöùëù  ùë†ùëôùëúùëùùëí =ùê∑ùëÖ
ùëáùëüùëéùëöùëù‚Üí106=10
ùëáùëüùëéùëöùëù 
ùëáùëüùëéùëöùëù =0.01 ùëöùë† 
0.01 ùëöùë†                  counter  counts  (24) 
1 ùë†                             ùëìùëêùëôùëúùëêùëò  
ùëìùëêùëôùëúùëêùëò =24
0.01 ùëöùë†=1.6 ùëÄùêªùëß  
 
 ÿ≠ŸÑ ÿßÿÆÿ± 
time of all counts =ùëáùëüùëéùëöùëù =0.01 ùëöùë† 
time of one count =ùëáùëüùëéùëöùëù
number  of counts=0.01 ùëöùë†
24=0.625  ùúáùë† 
ùëìùëêùëôùëúùëêùëò =1
0.625  ùúáùë†=1.6 ùëÄùêªùëß  
 
  0.01 ùëöùë†  ÿØÿß ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÑŸä Ÿáÿ™ÿßÿÆÿØŸáÿßŸÑŸÄ ramp   ÿπÿ¥ÿßŸÜ ÿ™ŸàÿµŸÑÿ£ŸÑÿπŸÑŸâ ŸÇŸäŸÖÿ© ŸàŸáŸä  10 V   Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ≤ŸÖÿßŸÑŸÄ  
counter   ŸäÿπÿØ ÿßŸÇÿµŸâ ÿπÿØŸá ŸÑŸäŸá ŸÅŸä ÿ≤ŸÖŸÜ ÿßŸÇŸÑ ŸÖŸÜ ÿßŸà Ÿäÿ≥ÿßŸàŸäùëáùë† 
 
 ŸáÿπŸÖŸÑ check  ÿπÿ¥ÿßŸÜÿ£ÿ™ÿ£ŸÉÿØ ÿßŸÜ ramp  slope    ŸÉÿßŸÜ ŸÉÿßŸÅŸä ÿßŸÜŸá ŸäŸàÿµŸÑŸÜŸäÿ£ŸÑŸÇÿµŸâ  ŸÇŸäŸÖÿ© ŸÅŸä ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿÆÿßŸÑŸÑ  
ÿ≤ŸÖŸÜ ÿßŸÇŸÑ ŸÖŸÜ  ùëáùë† 
ùëìùë† ùëöùëñùëõ =2 ùëìùëö=6ùêæùêªùëß  
ùëáùë†=1
6 ùêæùêªùëß=0.1667  ùëöùë† 
‚à¥ùëáùëüùëéùëöùëù < ùëáùë†  
 Example  
 
The only reason for worrying about maximum frequency of the signal is to see if the 
ramp slope is sufficient to reach the maximum possible value within one sampling 
period.  With a maximum signal frequency of 3KHz the minimum sampling rate for 
recovery is 6KHz ,so the maximum sampling period is 1
6 ùëöùë†. 
 
Since the ramp can reach the 10 V maximum in 0.01 .it is sufficient fast to avoid 
problems.  
 
 
 
 
 
 
 
   
 
 
 
¬©
 Basem HeshamLinear Block Coding & Line Coding
Error Detection and Correction & 
Lecture
 
9
 
 
Error Detection and Correction ŸáŸä ÿπŸÖŸÑŸäÿ© ÿ™ŸáÿØŸÅ ÿßŸÑŸâŸâÿß ÿßÿ¥ÿ™ŸÅŸâŸâ ŸÅ Ÿàÿ™ÿµŸâŸâÿ≠Ÿä  ÿßÿ£ŸÑÿÆÿ∑ŸâŸâ   ÿßŸÑŸÑŸâŸâŸä ŸÖŸÖÿ¥ŸâŸâÿµ ÿ™ÿ≠ÿµŸâŸâ  
ÿßÿ´ŸÜ   ÿπŸÖŸÑŸäÿ© ÿßÿßŸÑÿ±ÿ≥  . ÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿ®Ÿäÿ™ŸÖ ÿßÿ±ÿ≥ ŸÑŸá  ŸÅŸâŸâŸä Ÿàÿ≥ŸâŸâÿ∑ ŸÖŸÑŸäŸâŸâ ÿµ noise   ŸàŸÅŸäŸâŸâ  Propagation problems Ÿàÿ® ŸÑÿ™ŸâŸâ ŸÑŸä
Ÿàÿßÿ±ÿØ Ÿäÿ≠ÿµ   error   ŸÅŸä ÿßŸÑŸâbits   ÿßÿ´ŸÜ   ÿßÿßŸÑÿ±ÿ≥. 
 
ÿßŸÑŸâŸâ Ÿâ Channel Encoding  ÿ®Ÿäÿ∂ŸâŸâŸäŸÅbits  ÿ≤ŸäŸâŸâ ÿØÿ© ÿ®ŸÜÿ≥ŸâŸâŸÖŸäŸá ÿßŸà parity bits   ÿπŸÅŸâŸâ ÿµ ŸÜŸÑŸÖŸâŸâ Error Detection and 
Correction 
 
ÿßŸÑŸâ  parity bits    ÿ®ÿ™ÿ≤ŸàÿØ ÿ≠ ÿ¨ÿ© ÿßÿ≥ŸÖŸá minimum distance   ŸÖ  ÿ®Ÿäÿµ ÿßÿßŸÑÿ¥ŸàÿßÿØ ŸàÿßŸÑŸâ distance ŸáŸà ÿπÿØÿØ ÿßŸÑŸâ bits  ÿßŸÑŸÑŸâŸâŸä
ÿ®ŸäÿÆÿ™ŸÑŸÅ ŸÅŸäŸá  ÿ¥ŸàÿØ ÿπÿµ ÿ¥ŸàÿØ ÿßÿÆÿ± Ÿàÿ¥  ŸÖ  ÿ≤ÿßÿØÿ™ ÿßŸÑŸâ distance ÿ≤ÿßÿØÿ™ ÿ•ŸÖÿ¥ ŸÜŸäÿ© ÿßÿ¥ÿ™ŸÅ ŸÅ Ÿàÿ™ÿµÿ≠Ÿä  ÿßÿ£ŸÑÿÆÿ∑  . 
 
ŸáŸÜÿ™ŸÑŸÑŸÖ ŸÅŸâŸâŸä ÿßŸÑŸÖÿ≠ ÿ∂ŸâŸâÿ±ÿ© ÿØÿØ ÿßÿ≠ŸâŸâÿØ ÿßŸÑÿ∑ŸâŸâÿ±ŸÜ ÿßŸÑŸÑŸâŸâŸä ÿ®ŸÜŸÑŸÖŸâŸâ  ŸÖŸâŸâÿµ ÿÆ ŸÑŸáŸâŸâ  Error Detection and Correction  ŸÑÿ¥ŸâŸâÿµ
ŸáŸÜÿ™ŸÑŸÑŸÖ ÿßÿ£ŸÑŸà  ŸÖŸÅŸáŸâŸâŸàŸÖ ŸÖŸáŸâŸâŸÖ ŸàŸáŸâŸâŸà ÿßÿµ ÿπŸâŸâÿØÿØ ÿßÿ£ŸÑÿÆÿ∑ŸâŸâ   ÿßŸÑŸÑŸâŸâŸä ŸÖŸÖÿ¥ŸâŸâÿµ ŸÜÿ¥ÿ™ŸÅŸâŸâŸÅŸá  ŸàŸÜÿµŸâŸâÿ≠ÿ≠Ÿá  ŸÖŸâŸâÿ±ÿ™ÿ®ÿ∑ ÿ® ŸÑŸâŸâ Ÿâ minimum 
distance .ŸàŸÅŸäŸÖ  ŸäŸÑŸä ŸÅÿ±ÿ≠ ŸÑŸÑŸÖŸÅŸáŸàŸÖ ÿØÿß 
 
‚ñ™ In general, if the minimum distance between code words is ùê∑ùëöùëñùëõ ,errors involving up to 
ùê∑ùëöùëñùëõ‚àí1  bits can be detected.  
‚ñ™ If ùê∑ùëöùëñùëõ is an even number, errors up to ùê∑ùëöùëñùëõ
2‚àí1 can be corrected  
‚ñ™ If ùê∑ùëöùëñùëõ is an odd number, errors up to ùê∑ùëöùëñùëõ
2‚àí1
2 can be corrected  
 
 ŸÑŸà ÿπŸÜÿØÿØÿ£ÿØ code words   Ÿàÿ≠ÿ≥ÿ®ÿ™ ÿßŸÇ distance  ŸÖŸâŸâ  ÿ®ŸäŸâŸâŸÜŸáŸÖ ŸÇŸäŸÖÿ™ŸâŸâ  ÿ®ŸÜÿ≥ŸâŸâŸÖŸäŸá  (ùê∑ùëöùëñùëõ)  ŸáŸÇŸâŸâÿØÿ± ÿßÿ¥ÿ™ŸÅŸâŸâŸÅ errors 
ÿπÿØÿØŸá  Ÿäÿ≥ ŸàÿØ  (ùê∑ùëöùëñùëõ‚àí1)   ,    Ÿàÿ® ŸÑŸÜÿ≥ÿ®ÿ© ŸÑÿ™ÿµŸâŸâÿ≠Ÿäÿßÿ£ŸÑÿÆÿ∑ŸâŸâ    ŸáŸÇŸâŸâÿØÿ± ÿßÿµŸâŸâÿ≠   errors   ÿπŸâŸâÿØÿØŸá  (ùê∑ùëöùëñùëõ
2‚àí1)    ŸÑŸâŸâŸà
ÿ¥ ŸÜÿ™  (ùê∑ùëöùëñùëõ)  ÿ±ŸÇŸÖ ÿ≤Ÿàÿ¨Ÿä Ÿà( ùê∑ùëöùëñùëõ
2‚àí1
2)     ŸÑŸà ÿ¥ ŸÜÿ™(ùê∑ùëöùëñùëõ) ÿ±ŸÇŸÖ ŸÅÿ±ÿØÿØ. 
 
 ùê∑ùëöùëñùëõ   ŸÜŸÇÿØÿ± ŸÜÿ≠ÿ≥ÿ®Ÿá  ŸÖÿµ ÿÆ   ŸÖŸÇ ÿ±ŸÜÿ© ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿ®ÿ®ŸÑÿ∂ŸàŸÜŸÅŸà ŸÅ  ÿ£ÿØ  ÿßŸÇ  ŸÅÿ±ŸÜ ŸÖ  ÿ®Ÿäÿµ ÿ¥ŸàÿØ  ŸàÿßŸÑÿ™ ŸÜŸä  ŸàÿØÿß ŸáŸäŸàÿ∂ŸâŸâ  
ÿßÿ¥ÿ™ÿ± ŸÖÿπ  ÿßŸÑŸÖÿ≥ ÿ¶  
 
For example ,by combining groups of 3 bits, we can form eight possible message words :  
000 , 001 , 010 , 011 , 100 , 101 , 110 and 111  
Since every possible 3 -bit combination is used as a message, the received 3 -bit 
combination will be identical to one of the code words.  If 101 is transmitted and an error 
occur in the third bit, 100 is received. There is no way for the receiver to know that 100 
was not the transmitted word . 
 Error Detection and Correction  
 
 ŸÖÿ´  ŸÑŸà ÿπŸÜÿØÿØ 3 bits Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿπŸÜÿØÿØ 8 ÿßÿ¥ŸàÿßÿØ Ÿàÿ®ŸÑÿ™ ŸÖÿ´  101   Ÿàÿ≠ÿµ error  ŸÅŸä ÿ™ ŸÑÿ™bit  ŸàŸàÿµŸÑÿ™100  ŸÖÿ¥
ŸáŸÇÿØÿ± ÿßÿ¥ÿ™ŸÅŸÅ ÿßŸÑÿÆÿ∑ÿ£ ÿßŸÑÿµ  100   ÿØÿß ÿßÿ≠ÿØ ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿßŸÑŸÖŸèÿ™ŸÑ ÿ±ŸÅ ÿπŸÑŸäŸáŸÖ ÿ®ŸäÿµÿßŸÑ Ÿâ transmitter  ŸàÿßŸÑŸâŸâ Ÿâ Receiver  ŸàŸÖŸÜŸÇŸâŸâÿØÿ±ÿ¥
ŸÜŸÑÿ±ŸÅ Ÿá  ÿØÿß ŸáŸà ÿßŸÑÿ¥ŸàÿØ  ÿßÿ£ŸÑÿµŸÑŸä ÿßŸÑŸÑŸä ÿßÿ™ÿ®ŸÑÿ™ ŸàÿßŸÑ ÿ≠ÿµ  ŸÅŸä  ÿÆÿ∑ÿ£. 
 
ŸÅŸä ÿßŸÑÿ≠ ŸÑÿ© ÿØÿØ ÿ¥  ÿßÿ™ŸÜŸäÿµ ÿ¥ŸàÿØ ÿ®ŸäÿÆÿ™ŸÑŸÅŸàÿß ÿπÿµ ÿ®ŸÑÿ∂ ÿπŸÑÿß ÿßÿßŸÑŸÇ  ŸÅŸä 1 bit  Ÿàÿ® ŸÑÿ™ŸâŸâ ŸÑŸä ùê∑ùëöùëñùëõ=1  ŸàŸÑŸâŸâŸà ÿπŸàÿ∂ŸâŸâŸÜ  ŸÅŸâŸâŸä
ÿßŸÑŸÑ ŸÇÿ© ÿ®ÿ™ ÿπÿ© ÿ≠ÿ≥ ÿ® ÿπÿØÿØ ÿßŸÑŸâ errors    ÿßŸÑŸÑŸä ŸÖŸÖÿ¥ÿµ ÿßÿ¥ÿ™ŸÅŸÅŸá 
 
ùê∑ùëöùëñùëõ=1 
 
 error can be detected  =ùê∑ùëöùëñùëõ‚àí1  ‚Üí1‚àí1  =0 
 
error can be corrected  =ùê∑ùëöùëñùëõ
2‚àí1
2  ‚Üí1
2‚àí1
2  =0 
 
No errors  can be detected  or corrected . 
 ÿßÿ∞ÿßŸã ŸÖŸÇÿØÿ±ÿ¥ ÿßÿ¥ÿ™ŸÅŸÅ ÿßŸà  ÿßÿµÿ≠  ÿ£ÿØ ÿÆÿ∑ÿ£ 
 
ŸÅŸä ÿßŸÑŸÖÿ´   ÿßŸÑŸÑŸä ŸÅ ÿ™ ŸáŸÜÿ∂ŸäŸÅ Even parity bit  ŸàÿØÿØ ÿ®ÿ™ÿÆŸÑŸä ÿπÿØÿØ ÿßŸÑŸàÿ≠ ŸäÿØ ŸÅŸä ÿßŸÑÿ¥ŸâŸâŸàÿØ ÿ≤Ÿàÿ¨ŸâŸâŸäŸà ÿØÿØ ÿßÿßŸÑÿ¥ŸâŸâŸàÿßÿØ ÿßŸÑŸÑŸâŸâŸä 
ŸáŸÜÿ≠ÿµ   ÿπŸÑŸäŸá : 
0000 , 001 1 , 010 1 , 011 0 , 100 1 , 101 0 , 110 0 , 111 1 
The minimum distance between code words is 2   
 
ùê∑ùëöùëñùëõ=2 
error can be detected  =ùê∑ùëöùëñùëõ‚àí1  ‚Üí2‚àí1  =1   
error can be corrected  =ùê∑ùëöùëñùëõ
2‚àí1 ‚Üí2
2‚àí1   =0  
errors up to 1 can be detected, and no errors can be corrected  
  ŸáŸÇÿØÿ± ÿßÿ¥ÿ™ŸÅŸÅ ÿÆÿ∑ÿ£ Ÿàÿßÿ≠ÿØ ŸÑÿ¥ÿµ ŸÖŸÇÿØÿ±ÿ¥ ÿßÿµÿ≠ÿ£ÿØ ÿÆÿ∑ÿ£. 
 
Suppose that 0101 is transmitted and it is received as 0111. This is not one of the eight 
acceptable words, and the system can tell that an error was made  
 
 ŸÑŸà ÿ®ŸÑÿ™ŸÜ 0101  ŸÅŸä ÿßŸÑŸâ transmitter  Ÿà ÿßŸÑŸâ  receiver ÿßÿ≥ÿ™ŸÇÿ®ŸÑŸá 0111 ÿßÿ∞ÿßŸã ÿßŸÑŸâ  receive ŸáŸä ŸÇŸâŸâÿØÿ± Ÿä ÿ¥ÿ™ŸÅŸâŸâŸÅ ÿßŸÑÿÆÿ∑ŸâŸâÿ£ ÿßŸÑÿµ 
ÿØÿß ŸÖÿ¥ ŸÖÿµ ÿ∂ŸÖÿµ ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿßŸÑŸÖÿ™ŸÑ ÿ±ŸÅ ÿπŸÑŸäŸá . ŸÑÿ¥ÿµ ÿßŸÑŸâ  receive r ŸÖÿ¥ ŸáŸäŸÇÿØÿ± Ÿä ÿµÿ≠  ÿßŸÑÿÆÿ∑ÿ£ ÿßŸÑÿµ ŸÅŸäŸâŸâ  ÿ¥ŸâŸâÿ∞ÿß ÿßÿ≠ÿ™ŸÖŸâŸâ   
ŸÑŸÑÿ¥ŸàÿØ ÿßŸÑÿµÿ≠Ÿä  ÿßŸÑŸÑŸä ÿ™ŸÖ ÿßÿ±ÿ≥ ŸÑ  ŸÖÿ´   ÿ¥ ÿµ  1111  ŸàÿßŸÑÿÆÿ∑ÿ£  ÿ¥ ÿµ ŸÅŸä ÿßŸà  ÿ®ÿ™ ÿßŸà  0011   ŸàÿßŸÑÿÆÿ∑ÿ£ ÿ¥ ÿµ ŸÅŸä ÿ™ ŸÜŸä ÿ®ÿ™ ÿßŸà
0101    ŸàÿßŸÑÿÆÿ∑ÿ£ ÿ¥ ÿµ ŸÅŸä ÿ™ ŸÑÿ™ ÿ®ÿ™ ÿßŸà 0110   ŸàÿßŸÑÿÆÿ∑ÿ£ ÿ¥ ÿµ ŸÅŸä ÿ±ÿßÿ®ÿπ ÿ®ÿ™ Ÿàÿ®ŸÖ  ÿßÿµ ÿßŸÑÿ™ÿµŸâŸâÿ≠Ÿä  ŸÖŸÖÿ¥ŸâŸâÿµ ŸäŸâŸâÿ™ŸÖ ÿ®ŸâŸâ ÿ¥ÿ™ÿ± ŸÖŸâŸâÿµ
ÿ∑ÿ±ŸäŸÇÿ© ÿßÿ∞ÿßŸã ŸÖŸÇÿØÿ±ÿ¥ ÿßÿπŸÖ   correction  
 
 
As an example, suppose that the system consisted of three code words, 01111 , 10011 
and 01000. The minimum distance between code words is 3  
ùê∑ùëöùëñùëõ=3 
 
ùê∑ùëöùëñùëõ‚àí1  ‚Üí3‚àí1  =2  error can be detected  
ùê∑ùëöùëñùëõ
2‚àí1
2 ‚Üí3
2‚àí1
2   =1   error can be corrected  
ŸÅŸä ÿßŸÑŸÖÿ´   ÿØÿß ŸáŸÜŸÇÿØÿ± ŸÜÿ¥ÿ™ŸÅŸÅ ŸÑÿ≠ÿØ  2 error  ŸàŸáŸÜŸÇÿØÿ± ŸÜÿµÿ≠   error   .Ÿàÿßÿ≠ÿØ ŸÅŸÇÿ∑ 
 
If 01111 were transmitted and the second bit were in error ,00111 would be received  
 
00111   ŸÖÿ¥ ŸÖÿµ ÿ∂ŸÖÿµ ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿßŸÑŸÖÿ™ŸÑ ÿ±ŸÅ ÿπŸÑŸäŸá  Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸáŸÜŸÇÿØÿ± ŸÜÿ¥ÿ™ŸÅŸÅ ÿßŸÑÿÆÿ∑ÿ£, ŸàŸÜŸÇÿØÿ± ŸÜÿµŸâŸâÿ≠  ÿßŸÑÿÆÿ∑ŸâŸâÿ£ ÿØÿß ŸÖŸâŸâÿµ
ÿÆ   ÿßÿµ ÿßŸÑŸâ  receiver     ÿ®ŸäŸÅŸàŸÅ ÿßŸÑÿ¥ŸàÿØ ÿßŸÑŸÑŸä Ÿàÿµ 00111     ÿßŸÇÿ±ÿ® ŸÑŸÖŸäÿµ ŸÖÿµ ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿßŸÑŸÖŸèÿ™ŸÑ ÿ±ŸÅ ÿπŸÑŸäŸá  ŸàŸÅŸä ÿßŸÑŸÖÿ´
ÿØÿß ŸáŸà ÿßŸÑÿ¥ŸàÿØ  01111. 
    ÿßÿ≠ÿ™ŸÖ   ÿßÿµ Ÿäÿ≠ÿµ error    Ÿàÿßÿ≠ÿØ ÿßÿπŸÑÿß ŸÖÿµ ÿßÿ≠ÿ™ŸÖ   ÿ≠ÿµŸà  ÿßÿ¥ÿ™ÿ± ŸÖÿµ error    Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿßŸÑŸâ receiver   ŸàÿßŸÅÿ™ÿ±ÿ∂ ÿßÿµ
error   Ÿàÿßÿ≠ÿØ ÿ®ÿ≥ ÿßŸÑŸÑŸä ÿ≠ÿµ ŸàÿßÿÆÿ™ ÿ±  01111. 
 
 ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ŸÖŸâŸâÿµ ÿßŸÑŸÑŸâŸâŸä ŸÅŸâŸâ ÿ™ ÿßÿµ ÿ¥ŸÑŸÖŸâŸâ  ÿ≤ÿßÿØÿ™ minimum distance ùê∑ùëöùëñùëõ    ÿ¥ŸÑŸÖŸâŸâ  ÿ≤ÿßÿØÿ™ ÿßŸÖÿ¥ ŸÜŸäŸâŸâ  ÿßÿ¥ÿ™ŸÅŸâŸâ ŸÅ Ÿàÿ™ÿµŸâŸâÿ≠Ÿä
ÿßÿ£ŸÑÿÆÿ∑    ŸàÿØÿß ŸÖÿµ ÿÆ   ÿ≤Ÿä ÿØÿ© ÿßŸÑŸâ parity bits  ÿßŸà check bits. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Algebraic codes involve the production of check bits by adding together different groups 
of information bits. The type of addition used is normal binary addition without carr y. 
 
Linear Block Coding  ŸáŸä ÿßÿ≠ÿØÿ£ŸÜŸàÿßÿπ ÿßŸÑŸâ coding   ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ© ŸÅŸä ÿßÿ¥ÿ™ŸÅŸâŸâ ŸÅ Ÿàÿ™ÿµŸâŸâÿ≠Ÿäÿßÿ£ŸÑÿÆÿ∑ŸâŸâ   ÿ≠ŸäŸâŸâ  ŸäŸâŸâÿ™ŸÖ 
ÿ•ÿ∂ ŸÅÿ© check  bits  ÿπÿµ ÿ∑ÿ±ŸäŸÇ ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑŸâ information bits  ŸàÿßŸÑÿ¨ŸÖÿπ ÿßŸÑŸÖŸÇÿµŸàÿØ ŸáŸÜ  ŸáŸà ÿ¨ŸÖŸâŸâÿπ binary  ÿπŸâŸâ ÿØÿØ
ŸàŸÑÿ¥ÿµ ÿ®ÿØŸàÿµ  .carry    ŸäŸÑŸÜŸä ŸÖÿ´  ŸÑŸà ÿ®ŸÜÿ¨ŸÖÿπ1+1  ÿßŸÑŸÜ ÿ™ÿ¨10   ŸàŸÑÿ¥ÿµ ŸáŸÜ ÿÆÿØ0  ŸÅŸÇÿ∑ ŸàŸáŸÜŸÅŸä  ÿßŸÑŸâ carry  ÿßŸÑŸÑŸä ŸáŸà ÿ®Ÿâ1  
 
check bits  ŸáŸä ŸÜ ÿ™ÿ¨ ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry    ŸàŸÖŸÖÿ¥ÿµ ŸÜŸÑÿ±ŸÅŸáŸâŸâ  ÿ®ÿ∑ÿ±ŸäŸÇŸâŸâÿ© ÿ™ ŸÜŸäŸâŸâÿ© ŸàŸáŸâŸâŸä ÿπŸÖŸâŸâ XOR  ÿ®ŸâŸâŸäÿµ ÿßŸÑŸâŸâŸâ
information bits  Ÿàÿ®ŸÑÿ∂Ÿá  ŸàŸÖŸÖÿ¥ÿµ ŸÜŸÑÿ±ŸÅŸá  ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ™ ŸÑÿ™  ŸàŸáŸä ÿßÿµ ÿßŸÑŸâbit  ÿßŸÑŸÖŸåÿ∂ ŸÅÿ© Ÿáÿ™ÿÆŸÑŸä ÿπŸâŸâÿØÿØ ÿßŸÑŸàÿ≠ ŸäŸâŸâÿØ ŸÅŸâŸâŸä
ÿßŸÑÿ¥ŸàÿØ ÿ≤Ÿàÿ¨Ÿä  (ŸáŸÜŸÑŸÖ   Even parity  ŸàÿØÿØ ÿßÿ®ÿ≥ÿ∑ ÿ∑ÿ±ŸäŸÇÿ© ÿ•ŸÑÿ∂ ŸÅÿ© parity check bits) 
0 ‚äï0 =0 
0 ‚äï1 =1 
1 ‚äï0 =1 
1 ‚äï1 =0 
 
 
ŸÖŸÖÿ¥ÿµ ŸÜŸÑÿ±ŸÅŸá  ÿ®ÿ∑ÿ±ŸäŸÇ  ÿ±ÿßÿ®ŸÑ  ŸàŸáŸä ÿßŸÜŸÜ  ŸÑŸà ÿ¨ŸÖŸÑŸÜ  ÿßŸÑŸâ message bits  ŸÖÿπ ÿßŸÑŸâ check bit  ÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry 
.ÿßŸÑÿ≤ŸÖ ÿßŸÑŸÜ ÿ™ÿ¨ Ÿäÿ∑ŸÑÿπ ÿµŸÅÿ± ŸàÿØÿØ ŸÇ ÿπÿØŸá ŸÖŸáŸÖÿ© ŸáŸÜÿ≠ÿ™ ÿ¨Ÿá  ŸÇÿØÿßŸÖ   
 0+0+0=0      ,     0+1+1 =0 
1+0+1 =0     ,     1+1+0 =0 
 
 ŸÅŸä ÿ≠ ŸÑÿ© ÿßŸÜŸÜ  ÿπ Ÿäÿ≤Ÿäÿµ ŸÜÿ∂ŸäŸÅ ÿßÿ¥ÿ™ÿ± ŸÖÿµ parity bit    Ÿàÿ∑ÿ®ŸÑ  ŸÖŸäŸÜŸÅŸÑÿ¥ ŸÜÿ≠ÿ∑Ÿá  ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿπŸÅŸàÿßÿ¶Ÿäÿ© Ÿàÿ® ŸÑÿ™ ŸÑŸä Ÿäÿ¨ÿ®
ÿßÿµ ŸäŸàÿ¨ÿØ ÿπ ŸÇÿ© ÿ®ŸäŸÜŸá  Ÿàÿ®Ÿäÿµ ÿßŸÑŸâ Information bits   :ŸÖÿµ ÿÆ   ÿßÿßŸÑÿ™Ÿä 
 
We can generalize this type of coding. Suppose that the message word consists of m 
bits ‚áí2ùëö distinct message word.  
We add ùëõ parity bits to the ùëö message bits to end up with code words of length (ùëö+
ùëõ) bits. ‚áí number of code words become 2ùëö+ùëõ. 
‚áí Note that of the 2ùëö+ùëõ possible code words only 2ùëö are used  
 
 ŸÜŸÅÿ™ÿ±ÿ∂ ÿßÿµ ÿßŸÑŸâ information bits   ÿßÿßŸÑÿµŸÑŸäÿ© (ÿ®ŸÜŸÇŸà  ÿπŸÑŸäŸá message word  Ÿàÿ®ŸÑÿØÿ•ÿ∂ ŸÅÿ© parity bit s    ŸáŸÜŸÇŸà
ÿπŸÑŸäŸá  code word  ) ŸàŸÜŸÅÿ™ÿ±ÿ∂ ÿßÿµ ÿπÿØÿØbits   ÿßŸÑŸâ message   ŸáŸàùëö   Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿπŸÜÿØŸÜ message words 2ùëö 
 ŸÖÿÆÿ™ŸÑŸÅŸäÿµ ,ŸàÿπÿØÿØ ÿßŸÑŸâbits   ÿßŸÑŸÑŸä Ÿáÿ∂ŸäŸÅŸáŸÖùëõ  Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿπŸÜÿØŸÜ  code words   2ùëö+ùëõ   ŸÖÿÆÿ™ŸÑŸÅŸäÿµ ŸÖŸÜŸáŸÖ2ùëö   ŸÅŸÇÿ∑
ŸÖÿ≥ÿ™ÿÆÿØŸÖŸäÿµ ŸäŸÑŸÜŸä ŸáŸÖ ÿØŸà   ŸÅŸÇÿ∑ ÿßŸÑŸÑŸä ŸÖÿ™ŸÑ ÿ±ŸÅ ÿπŸÑŸäŸáŸÖ ÿ®Ÿäÿµ ÿßŸÑŸâ transmitter  ŸàÿßŸÑŸâ receiver   ŸàŸáŸÖ ÿØŸà  ÿßŸÑŸÑŸä
ŸáŸÑÿ±ŸÅ ÿßÿπŸÖ  ŸÖÿµ ÿÆ ŸÑŸáŸÖ error detection and error correction 
 
 Linear Block Coding  
Information bits  Check bit  
 
 ÿßŸÑŸâ Code Word  ŸáŸäÿ®ŸÇÿß ÿ® ÿßŸÑŸÅÿ¥  ÿßÿßŸÑÿ™Ÿä ÿ≠Ÿä  ŸÖÿµùëé1  ÿßŸÑÿßùëéùëö  ŸáŸä message bits  Ÿà ŸÖÿµùëê1  ÿßŸÑŸâŸâÿßùëêùëõ  ŸáŸâŸâŸä
ÿßŸÑŸâ parity bits 
[ùëé1  ùëé2 ùëé3‚Ä¶‚Ä¶ùëéùëö  ùëê1  ùëê2 ùëê3‚Ä¶‚Ä¶ùëêùëõ] 
Each check bit  is chosen to achieve even parity when combined with specific message 
bits. 
 ÿßŸÑŸâ check bit  ÿßŸÑŸÑŸä ÿ®ŸÜÿ∂ŸäŸÅŸá  ÿπŸÅ ÿµ ÿ™ÿ≠ŸÇŸÇ even parity  ŸÖÿπ ÿπÿØÿØ ŸÖŸÑŸäÿµ ŸÖÿµ ÿßŸÑŸâŸâŸâ message bits  . ŸÖŸâŸâÿ¥ ÿ¥ŸÑŸáŸâŸâ
ŸÖÿ´  ŸÑŸà ÿßŸÑÿ¥ŸàÿØ 1011  Ÿàÿπ Ÿäÿ≤ ÿßÿ∂ŸäŸÅ 2 parity bit  ŸÖŸÖÿ¥ÿµ ÿßÿÆÿ™ ÿ± ÿßŸÑŸâbit ÿßÿ£ŸÑŸàŸÑŸâŸâÿß ÿ™ÿ≠ŸÇŸâŸâŸÇ even parity   ŸÖŸâŸâÿπ ÿßŸà
Ÿàÿ™ ŸÑÿ™  bit   ŸÅŸä ÿßŸÑÿ¥ŸàÿØŸÖÿ´  ŸàŸÖŸÖÿ¥ÿµ ÿßÿÆÿ™ ÿ± ÿßŸÑŸâ  bit   ÿßŸÑÿ™ ŸÜŸä  ÿ™ÿ≠ŸÇŸÇ even parity  ŸÖÿπÿ™ ŸÑÿ™ Ÿàÿ±ÿßÿ®ÿπ   bit   
 ÿßŸÑŸâ combination  ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ÿØÿØ ÿπŸÑÿßÿ£ÿ≥ ÿ≥ ÿßÿµ ÿßÿßŸÑÿ¥ŸâŸâŸàÿßÿØ ÿßŸÑŸÑŸâŸâŸä ŸáŸÜÿ≠ÿµŸâŸâ  ÿπŸÑŸäŸáŸâŸâ  ŸÅŸâŸâŸä ÿßÿßŸÑÿÆŸâŸâÿ± ŸÜ ŸÇŸäŸáŸâŸâ  ÿ®ÿ™ÿ≠ŸÇŸâŸâŸÇ  
ùê∑ùëöùëñùëõ  ŸÖŸÑŸäŸÜ  ÿ™ŸàÿßŸÅŸÇ ÿßÿ≠ÿ™Ÿä ÿ¨ ÿ™Ÿä ŸÅŸä ÿßŸÑŸâ system  ŸÅŸä ÿßŸÑŸâ Error detection and correction 
 
ÿ•ÿ∂ ŸÅÿ© ÿßŸÑŸâ  parity bits : ŸÖÿ¥ ŸáŸäÿ™ŸÖ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸäÿØŸàŸä  ŸàŸÑÿ¥ÿµ ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅ ÿ™ ŸÖÿµ ÿÆ   ÿßŸÑŸÑ ŸÇÿ© ÿßÿ£ŸÑÿ™Ÿäÿ© 
 
The check bits is chosen to satisfy:  
[ùêª] ùëáÃÖ=0 
 
 ÿ®ŸÜÿÆÿ™ ÿ± ÿßŸÑŸâ check bits    ŸÖÿµ ÿÆÿ∂ÿ±ÿ® ÿßŸÑŸÖÿµŸÅŸàŸÅÿ™Ÿäÿµ  [ùêª]   ŸàùëáÃÖ ŸàŸÜÿ≥ ŸàÿØ ÿ≠ ÿµ  ÿ∂ÿ±ÿ®ŸáŸÖ ÿ®ÿµŸÅÿ±  ŸàŸáŸÜŸÅŸáŸÖ  
ÿ£ÿØ ŸáŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅ ÿ™ ÿØÿØ ŸàŸÑŸä  ÿ®ŸÜÿ≥ ŸàÿØ ÿ≠ ÿµ  ÿ∂ÿ±ÿ®ŸáŸÖ ÿ®ÿµŸÅÿ±.  
 
ùëáÃÖ    ÿßÿ≥ŸÖŸá transmitted vector  ŸàŸáŸä ÿπÿ® ÿ±ÿ© ÿπÿµ ŸÖÿµŸÅŸàŸÅÿ©  ÿ™ŸÖÿ´  ÿßŸÑŸâ Code Word   ŸàŸáŸà ÿπÿ® ÿ±ÿ© ÿπÿµ ÿßŸÑŸâ
message bits  ÿπÿØÿØŸáŸÖùëö (ŸÖÿµ ùëé1 ÿßŸÑÿßùëéùëö )   ŸÖŸèÿ∂ ŸÅ ÿπŸÑŸäŸá parity bits   ÿπÿØÿØŸáŸÖùëõ (ŸÖÿµ ùëê1  ÿßŸÑÿßùëêùëõ  )
 Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿπÿØÿØ ÿµŸÅŸàŸÅ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©ùëö+ùëõ  .Ÿàÿ™ÿ™ÿ¥Ÿàÿµ ŸÖÿµ ÿπŸÖŸàÿØ Ÿàÿßÿ≠ÿØ 
 
ùëáÃÖ=
[            ùëé1
ùëé2
ùëé3
.
.
.
ùëéùëö
ùëê1
ùëê2
.
.
.
ùëêùëõ]            
 
 
 
[ùêª] is ùëõ√ó(ùëõ+ùëö) matrix  
[ùêª]=[‚Ñé11‚Ñé12‚Ä¶‚Ñé1ùëö1000‚Ä¶0
‚Ñé21‚Ñé22‚Ä¶‚Ñé2ùëö0100‚Ä¶0
..‚Ä¶.....‚Ä¶0
‚Ñéùëõ1‚Ñéùëõ2‚Ä¶‚Ñéùëõùëö0000‚Ä¶1] 
 
 [ùêª]   ÿßÿ≥ŸÖŸá parity check  matrix    Ÿàÿßÿ®ŸÑ ÿØŸá   ‚Üê ùëõ√ó(ùëõ+ùëö)   ÿ≠Ÿä  ÿπÿØÿØ ÿßŸÑÿµŸÅŸàŸÅùëõ  ŸàÿπÿØÿØ
ÿßÿßŸÑÿπŸÖÿØÿ© ùëõ+ùëö , ŸàÿßŸÑŸÖÿµŸÅŸàŸÅÿ©  [ùêª]    ÿ™ÿ™ÿ¥Ÿàÿµ ŸÖÿµ ÿ¨ÿ≤ÿ¶Ÿäÿµ ÿßŸÑÿ¨ÿ≤ÿßÿ£ŸÑŸà  ÿπÿ® ÿ±ÿ© ÿπÿµ ŸÖÿµŸÅŸàŸÅÿ© ŸÖÿµ ŸÇŸäŸÖ ÿßŸÑŸâ ‚Ñé 
ÿ≠ÿ¨ŸÖŸá   ùëõ√óùëö  ŸàÿßŸÑÿ¨ÿ≤  ÿßŸÑÿ™ ŸÜŸä ÿπÿ® ÿ±ÿ© ÿπÿµ ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑŸàÿ≠ÿØÿ© Identity matrix    ÿ≠ÿ¨ŸÖŸáùëõ√óùëõ 
 
 ŸÇŸäŸÖ‚Ñé ÿπÿ® ÿ±ÿ© ÿπÿµ ÿßÿµŸÅ ÿ± ŸàŸàÿ≠ ŸäÿØ  ŸàÿØÿØ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÑŸä ÿ®ÿÆÿ™ ÿ± ŸÖÿµ ÿÆ ŸÑŸá  ÿ¥  parity bit   Ÿáÿ™ŸÑŸÖ even  parity  ŸÖÿπ
ÿ£ÿØ bits  ŸÅŸä ÿßŸÑŸâ message  ŸàŸáŸÜŸÅŸáŸÖ ÿØÿß ÿ®Ÿäÿ™ŸÖ ÿßÿ≤ÿßÿØ.  
 
ŸÖÿµ ÿÆ   ÿßŸÑŸÑ ŸÇÿ© ÿßÿßŸÑÿ™Ÿä  ÿ®Ÿäÿ¥Ÿàÿµ ŸÖŸÑ ŸÜ  ŸÖÿ¨ŸáŸà  ŸàŸáŸà ŸÇŸäŸÖ ÿßŸÑŸâ  check bits  (ŸÖÿµ ùëê1 ÿßŸÑÿßùëêùëõ)     ŸáŸÜÿ≠ÿ≥ÿ®ŸáŸÖ ŸÖÿµ ÿÆ
ÿ∂ÿ±ÿ® ÿßŸÑŸÖÿµŸÅŸàŸÅÿ™Ÿäÿµ ŸÅŸä ÿ®ŸÑÿ∂ ÿ® ŸÜŸÜ  ŸÜÿ∂ÿ±ÿ® ÿ¥  ÿµŸÅ ŸÖÿµ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©   [ùêª]  ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©ùëáÃÖ   ÿßŸÑŸÑŸä ÿ®ÿ™ÿ™ÿ¥Ÿàÿµ
ŸÖÿµ ÿπŸÖŸàÿØ Ÿàÿßÿ≠ÿØ ŸàŸáŸÜÿ≠ÿµ  ÿπŸÑÿß ÿπÿØÿØ  ùëõ  ŸÖÿµ ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ŸÖÿµ ÿÆ   ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ ÿØÿØ ŸáŸÜŸÇÿØÿ± ŸÜÿ≠ÿ≥ÿ® ŸÇŸäŸÖ ÿßŸÑŸâ  ùëê 
ÿßŸÑŸÑŸä ŸÅŸä ÿ¥  ŸÖŸÑ ÿØŸÑÿ© 
 
[ùêª] ùëáÃÖ=[‚Ñé11‚Ñé12‚Ä¶‚Ñé1ùëö1000‚Ä¶0
‚Ñé21‚Ñé22‚Ä¶‚Ñé2ùëö0100‚Ä¶0
..‚Ä¶.....‚Ä¶0
‚Ñéùëõ1‚Ñéùëõ2‚Ä¶‚Ñéùëõùëö0000‚Ä¶1] 
[            ùëé1
ùëé2
ùëé3
.
.
.
ùëéùëö
ùëê1
ùëê2
.
.
.
ùëêùëõ]            
=0  
 
  ÿ¥  ÿµŸÅ ŸÅŸä  ŸÇŸäŸÖ‚Ñé ÿπÿØÿØŸá  ùëö  ŸáŸäÿ™ÿ∂ÿ±ÿ®Ÿà ŸÅŸä ÿßŸÑŸâ message bits ŸÖÿµ ùëé1 ÿßŸÑÿßùëéùëö  ŸàŸÅŸä ÿ¥  ŸÖŸÑ ÿØŸÑÿ© ŸáŸäÿ∑ŸÑÿπ
ŸÖŸÑ ŸÜ  parity bit   Ÿàÿßÿ≠ÿØ ÿ®ÿ≥ ÿßŸÑÿµ ÿßŸÑŸâ parity bits  ŸÖÿµ ùëê1  ÿßŸÑÿßùëêùëõ .ŸáŸäÿ™ÿ∂ÿ±ÿ®Ÿà ŸÅŸä ÿµŸÅ ŸÖÿµ ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑŸàÿ≠ÿØÿ©   
 
 
 
 
If the number of ones is even, the sum is zero:  
‚Ñé11 ùëé1+‚Ñé12 ùëé2+‚ãØ+‚Ñé1ùëö ùëéùëö+ùëê1=0 
‚Ñé21 ùëé1+‚Ñé22 ùëé2+‚ãØ+‚Ñé2ùëö ùëéùëö+ùëê2=0 
  
  
‚Ñéùëõ1 ùëé1+‚Ñéùëõ2 ùëé2+‚ãØ+‚Ñéùëõùëö ùëéùëö+ùëêùëõ=0 
 
 Ÿàÿßÿ∂  ÿßÿµ ÿ¥  ŸÖŸÑ ÿØŸÑÿ© ÿ®ÿ™ÿ∑ŸÑÿπ ÿßÿ≠ÿØ ÿßŸÑŸâŸâŸâùëê  ÿßŸÑŸÑŸâŸâŸä ŸáŸâŸâŸä ÿßŸÑŸâŸâŸâ parity bit  Ÿàÿ≤ÿØ ŸÖŸâŸâ  ŸÇŸàŸÑŸÜŸâŸâ  ÿßÿµ ŸÇŸâŸâŸäŸÖ ÿßŸÑŸâŸâŸâ ‚Ñéÿπÿ®ŸâŸâ ÿ±ÿ© ÿπŸâŸâÿµ  
ÿßÿµŸÅ ÿ± ŸàŸàÿ≠ ŸäÿØ ŸàŸÖÿ´  ŸÅŸä ÿßŸà  ŸÖŸÑ ÿØŸÑÿ© ŸÑŸà ÿ¥ ŸÜÿ™ ‚Ñé11=1  ,   ‚Ñé12=0   ,  ‚Ñé13=1  ,  Ÿàÿ® ŸÇŸä ŸÇŸâŸâŸäŸÖÿßŸÑŸâŸâŸâ  
‚Ñé ŸÑÿ≠ÿØ ‚Ñé1ùëö   :Ÿäÿ≥ ŸàŸàÿß ÿµŸÅÿ± ŸáŸÜÿ≠ÿµ  ÿπŸÑÿß ÿßŸÑŸÖŸÑ ÿØŸÑÿ© ÿßÿ£ŸÑÿ™Ÿäÿ© 
ùëé1+ ùëé3+ùëê1=0 
ŸÖŸÑŸÜÿß ÿßŸÑŸÖŸÑ ÿØŸÑŸâŸâÿ© ÿØÿØ ÿßÿµ ÿßŸà  ùëê1 parity bit  ÿ®ÿ™ÿ≠ŸÇŸâŸâŸÇ Even parity  ÿ®ŸâŸâŸäÿµ ÿßŸà  Ÿàÿ™ ŸÑŸâŸâÿ™ bit (ùëé3 Ÿà ùëé1)  ŸÅŸâŸâŸä ÿßŸÑŸâŸâŸâ
message  
 
 ÿ≤ÿØ ŸÖ  ŸÇŸàŸÑŸÜ  ÿßŸÜŸÜ  ŸÑŸà ÿ¨ŸÖŸÑŸÜ ÿßŸÑŸâ message bits  ŸÖÿπ ÿßŸÑŸâ check bit  ÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry  ÿßŸÑÿ≤ŸÖ ÿßŸÑŸÜ ÿ™ÿ¨ Ÿäÿ∑ŸÑÿπ ÿµŸÅÿ±
ŸàÿØÿß ÿ≥ÿ®ÿ® ÿßŸÜŸÜ  ÿ®ŸÜÿ≥ ŸàÿØ ŸÜ ÿ™ÿ¨ ÿ∂ÿ±ÿ® ÿßŸÑŸÖÿµŸÅŸàŸÅÿ™Ÿäÿµ ÿ®ÿµŸÅÿ±. 
  
ÿßŸÑŸÅÿ¥ÿ±ÿ© ÿ® ÿÆÿ™ÿµ ÿ± ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖŸÜ  ŸÑŸÑŸÑ ŸÇÿ© [ùêª] ùëáÃÖ=0   ŸÑŸÖ  ÿ®ŸÜŸÅÿ¥Ÿá  ÿ®ÿ™ÿ∑ŸÑŸâŸâÿπ ŸÖÿ¨ŸÖŸàÿπŸâŸâÿ© ŸÖŸÑŸâŸâ ÿØÿßŸÑÿ™ ÿ¥ŸâŸâ  ŸÖŸÑ ÿØŸÑŸâŸâÿ© ŸÖŸâŸâŸÜŸáŸÖ
ÿÆ ÿµÿ© ÿ®ÿ•Ÿäÿ¨ ÿØ ÿßÿ≠ÿØ ÿßŸÑŸâ parity bit  Ÿàÿ¥  ŸÖŸÑ ÿØŸÑÿ© ŸÅŸäŸáŸÖ ÿπÿ® ÿ±ÿ© ÿπÿµ ÿ¨ŸÖÿπ ÿßŸÑŸâŸâŸâ message bits  ŸÖŸâŸâÿπ ÿßŸÑŸâŸâŸâ check bit 
 ÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry  Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿπŸÅ ÿµ ÿßŸÑŸâ Even parity ÿ™ÿ™ÿ≠ŸÇŸÇ ÿßŸÑÿ≤ŸÖ ÿßŸÑŸÜ ÿ™ÿ¨ Ÿäÿ≥ ŸàÿØ ÿµŸÅÿ± Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÜŸÇŸâŸâÿØÿ± ŸÜÿ≠ÿ≥ŸâŸâÿ® 
ÿßŸÑŸâ  ùëê  ÿ® ŸÜŸÜ  ŸÜÿÆÿ™ ÿ± ŸÇŸäŸÖÿ™Ÿá  ÿßŸÑŸÑŸä ÿ™ÿÆŸÑŸä ŸÜ ÿ™ÿ¨ ÿßŸÑŸÖŸÑ ÿØŸÑÿ© Ÿäÿ≥ ŸàÿØ ÿµŸÅÿ±  ÿßŸà ŸÜÿÆÿ™ ÿ± ŸÇŸäŸÖÿ© ÿßŸÑŸâ  ùëê   ÿßŸÑŸÑŸä ÿ™ÿÆŸÑŸä ÿπÿØÿØ ÿßŸÑŸàÿ≠ ŸäŸâŸâÿØ
ÿ≤Ÿàÿ¨Ÿä ŸÅŸä ÿßŸÑŸÖŸÑ ÿØŸÑÿ©. 
 
ÿßŸÑŸÑŸä ŸÅÿ±ÿ≠ŸÜ Ÿá ÿØÿß ÿßÿ≤ÿßÿØ ÿßŸÑŸâ transmitter  ÿ®Ÿäÿ¥Ÿàÿµ ÿßŸÑŸâ parity bits ÿßÿ≤ÿßÿØ ŸàŸáŸÜŸÅŸàŸÅ ÿßÿ≤ÿßÿØ ÿßŸÑŸâ receiver  ŸáŸäÿ≥ŸâŸâÿ™ÿÆÿØŸÖ
ÿßŸÑÿ¥ ŸÖ ÿØÿß ŸÅŸä ÿßŸÜ  Ÿäÿ¥ÿ™ŸÅŸÅ ŸàŸäÿµÿ≠  ÿßÿßŸÑÿÆÿ∑    ÿ®ŸÑÿØ ÿßŸÑŸÖÿ≥ÿ£ŸÑÿ© ÿßŸÑÿ¨ Ÿä  ÿßÿµ ŸÅ   ŸÑŸÑÿß . 
 
 
 
 
 
 
 
 
 .
.
. 
 
 
Given the algebraic code with 4 -bit message words and 3 parity -check bits , [ùëØ] is 
defined as  
[ùëØ]=[ùüèùüèùüéùüèùüèùüéùüé
ùüèùüéùüèùüèùüéùüèùüé
ùüéùüèùüèùüèùüéùüéùüè] 
Find the code words corresponding to 16 possible message words.  
Solution  
  ÿπŸÜÿØŸÜ message  ÿπÿØÿØ ÿßŸÑŸâbits  Ÿäÿ≥ ŸàÿØ4  (ùëö=4  ) ŸàÿπÿØÿØ ÿßŸÑŸâbits parity  Ÿäÿ≥ ŸàÿØ 3(ùëõ=3)   ŸàŸÖŸÑÿ∑ÿß
ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© [ùêª]  ÿßŸÑŸÑŸä Ÿáÿ≠ÿ≥ÿ® ŸÖÿµ ÿÆ ŸÑŸá  ÿßŸÑŸâbits parity  
 ŸÖÿ∑ŸÑŸàÿ® ŸÜÿ¨Ÿäÿ® ÿßŸÑŸâ 16 code words ŸäŸÑŸÜŸä ŸáŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸâ bits parity   ŸÑÿ¥ message word  ŸÖÿµ 0000   ÿßŸÑÿß
1111 
[ùêª] ùëáÃÖ=[1101100
1011010
0111001] 
[      ùëé1
ùëé2
ùëé3
ùëé4
ùëê1
ùëê2
ùëê3]      
=0 
The three equations corresponding to the matrix equation [ùêª] ùëáÃÖ=0 are  
ùëé1+ùëé2+ùëé4+ùëê1=0 
ùëé1+ùëé3+ùëé4+ùëê2=0 
ùëé2+ùëé3+ùëé4+ùëê3=0 
 
  ŸÜ ÿ≠ÿ∏ ÿßÿµùëê1  ÿ®ÿ™ÿ≠ŸÇŸÇ Even parity  ÿ®Ÿäÿµ ÿßŸà  Ÿàÿ™ ŸÜŸä Ÿàÿ±ÿßÿ®ÿπbit   ,Ÿà ùëê2  ÿ®ÿ™ÿ≠ŸÇŸÇ Even parity  ÿ®Ÿäÿµ ÿßŸà  Ÿàÿ™ ŸÑÿ™
Ÿàÿ±ÿßÿ®ÿπ bit   ,Ÿàùëê3  ÿ®ÿ™ÿ≠ŸÇŸÇ Even parity  ÿ®Ÿäÿµ ÿ™ ŸÜŸä Ÿàÿ™ ŸÑÿ™ Ÿàÿ±ÿßÿ®ÿπbit 
 
 ŸáŸÜŸÑŸàÿ∂ ÿ®ŸÇŸäŸÖ ÿßŸÑŸâ message bits (ùëé1 ùëé2 ùëé3 ùëé4)   ŸÅŸä ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ ÿßŸÑÿ≥ ÿ®ŸÇÿ© ŸàŸÖŸÜŸá  ŸÜÿ≠ÿ≥ÿ®bits parity    ŸÑÿ¥
ÿ¥ŸàÿØ  
 
For message 0000  
0+0+0+ùëê1=0‚Üí  ùëê1=0 
0+0+0+ùëê2=0‚Üí  ùëê2=0 
0+0+0+ùëê3=0‚Üí  ùëê3=0 
 Example  1 
 
For message 0001  
0+0+1+ùëê1=0‚Üí  ùëê1=1 
0+0+1+ùëê2=0‚Üí  ùëê2=1 
0+0+1+ùëê3=0‚Üí  ùëê3=1 
  ŸÅŸä ÿ¥  ŸÖŸÑ ÿØŸÑÿ© ŸÅŸä ÿßŸÑÿ¥ŸàÿØ 0001  ÿπŸÅ ÿµ Ÿäÿ™ÿ≠ŸÇŸÇ Even parity  ÿßŸÑÿ≤ŸÖ ÿπÿØÿØ ÿßŸÑŸàÿ≠ ŸäÿØ Ÿäÿ¥Ÿàÿµ ÿ≤Ÿàÿ¨Ÿä ŸÅŸáŸÜÿ≠ÿ∑1   ŸÅŸä
ÿ¥  ŸÖŸÑ ÿØŸÑÿ© ŸàŸÖŸÖÿ¥ÿµ ŸÜŸÇŸàŸÑŸá  ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ™ ŸÜŸä  ŸàŸáŸä ÿßŸÑÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry .Ÿäÿ≥ ŸàÿØ ÿµŸÅÿ± 
 
For message 1010 
1+0+0+ùëê1=0‚Üí  ùëê1=1 
1+1+0+ùëê2=0‚Üí  ùëê2=0 
0+1+0+ùëê3=0‚Üí  ùëê3=1 
 
 ŸàŸáÿ¥ÿ∞ÿß ŸÖÿπ ÿ® ŸÇŸä ÿßÿßŸÑÿ≠ÿ™ŸÖ ÿßŸÑÿ™ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©ŸàŸáŸÜÿ≠ÿµ  ŸÖŸÜŸáŸÖ ÿπŸÑÿß ÿßŸÑŸâ  code words  ÿßÿßŸÑÿ™Ÿä  ÿ®ŸÑÿØÿ•ÿ∂ ŸÅÿ© ÿßŸÑŸâ parity bit  
  ŸÑÿ¥ message word  ŸàÿßŸÑÿ¨ÿØŸà  ÿßÿßŸÑÿ™Ÿä ŸäŸàÿ∂  ÿ¥  ÿßŸÑŸâ code words  ÿ®ŸÑÿØÿ•ÿ∂ ŸÅÿ© ÿßŸÑŸâ parity bits 
 
 
Code Word s Message Word s 
0000  000 0000  
0001  111  0001  
0010  011 0010  
0011  100 0011  
0100  101 0100  
0101  010 0101  
0110  110 0110  
0111  001 0111  
1000  110 1000  
1001  001 1001  
1010  101 1010  
1011  010 1011  
1100  011 1100  
1101  100 1101  
1110 000 1110 
1111  111 1111  
 
 
Suppose that transmitted vector   ùëáÃÖ. We define an error vector ùê∏ÃÖ which contains a ‚Äú1‚Äù in 
each bit position in which an error occurs. The  received vector is therefore of the form:  
ùëÖÃÖ=ùëáÃÖ+ùê∏ÃÖ 
 
transmitted vector  ùëáÃÖ   ÿπÿ® ÿ±ÿ© ÿπÿµ ÿßŸÑŸâ message   ŸÖÿ∂Ÿå ŸÅ ÿπŸÑŸäŸá  ÿßŸÑŸâ parity bits  Ÿàÿ®ŸÜÿ≥ŸÖŸä  ÿßŸÑŸâ code word  
 ùëÖÃÖ Received vector   ŸàÿØÿß ŸáŸàùëáÃÖ  ÿ®ÿ≥ ÿ®ŸÑÿØ ŸÖ  Ÿàÿµ  ŸÑŸÑŸâ Receiver  ŸàÿßŸÑŸÑŸä Ÿàÿßÿ±ÿØ Ÿäÿ¥Ÿàÿµ ŸÅŸä  ÿÆÿ∑ÿ£ÿßÿ´ŸÜ   ÿßÿßŸÑÿ±ÿ≥  . 
 ùê∏ÃÖ Error vector  ŸáŸä ŸÖÿµŸÅŸàŸÅÿ© ÿ™ÿ≠ÿ™ŸàÿØ ÿπŸÑÿß ÿßÿµŸÅ ÿ± ŸÖ  ÿπÿØÿßÿßŸÑŸÖÿ¥ ÿµ ÿßŸÑŸÑŸä ŸÅŸä  ÿÆÿ∑ÿ£ ŸáŸÜ ŸÇŸä ŸÅŸä  Ÿàÿßÿ≠ÿØ Ÿà  ŸÑŸà ÿßŸÑŸâ 
transmission  ÿµÿ≠Ÿä  ÿßŸÑŸâ ùê∏ÃÖ ŸÇŸäŸÖÿ™  ÿ¥ŸÑŸá  ÿßÿµŸÅ ÿ±. 
ÿßŸÑŸÑ ŸÇÿ© ÿ®ŸäŸÜŸáŸÖ ŸáŸä ÿßÿµ  ùëÖÃÖ Received vector   ŸáŸä ŸÜŸÅÿ≥Ÿá transmitted vector  ùëáÃÖ   ŸàŸÑÿ¥ÿµ ŸÖŸèÿ∂ ŸÅ ÿπŸÑŸä  ÿßŸÑŸâ error  
 ÿßŸÑŸÑŸä ÿ≠ÿµ , ŸàŸÖŸÜŸÜÿ≥ ÿ¥ ÿßÿµ ÿßŸÑÿ¨ŸÖÿπ ŸáŸÜ  ŸáŸà ÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry   ŸäŸÑŸÜŸä ÿ¥ ŸÜŸÜ  ÿ®ŸÜŸÑŸÖ XOR operation 
 
For Example  
[     1
1
1
0
1
1]     
=
[     1
1
1
1
1
1]     
+
[     0
0
0
1
0
0]     
 
 
 
 ŸÅŸä ÿßŸÑŸÖÿ´   ÿØÿß ÿ≠ÿµ  ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑŸâbit  ÿ±ŸÇŸÖ4   Ÿàÿ®ÿØ  ŸÖ  ÿ™Ÿàÿµ1  ŸàÿµŸÑÿ™0  Ÿàÿ® ŸÑÿ™ ŸÑŸäùê∏ÃÖ  ÿ¥ŸÑ  ÿßÿµŸÅ ÿ± ŸÖ  ÿπÿØÿß ÿßŸÑŸâbit 
ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸäŸá  ÿÆÿ∑ÿ£ ŸàŸáŸä bit  ÿ±ŸÇŸÖ4   
 
 ŸÅŸä ÿßŸÑŸâ Transmitter    ÿ∂ÿ±ÿ®ŸÜ[ùêª]  ŸÅŸäùëáÃÖ  Ÿàÿ≥ ŸàŸäŸÜ  ÿßŸÑŸÜ ÿ™ÿ¨ ÿ®ÿµŸÅÿ± Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸáŸÜŸÑŸÖ  ŸÜŸÅÿ≥ ÿßŸÑÿ¥ ŸÖ ŸÅŸä ÿßŸÑŸâ Receiver 
  ŸáŸÜÿ∂ÿ±ÿ®[ùêª]  ŸÅŸäùëÖÃÖ 
 
[ùêª] ùëÖÃÖ=[ùêª] [ùëáÃÖ+ùê∏ÃÖ]=[ùêª] ùëáÃÖ+[ùêª] ùê∏ÃÖ 
‚à¥[ùêª] ùëÖÃÖ=[ùêª] ùê∏ÃÖ 
Syndrome ( ùëÜÃÖ) =[ùêª] ùëÖÃÖ=[ùêª] ùê∏ÃÖ 
 
The syndrome characterizes the specific bit error.  The result is a vector that is identical to 
one column of  ùêª, that column being the one corresponding to the bit  position in error.  
 
Syndrome ( ùë∫ÃÖ)   ŸáŸà vector   ÿ®ŸÇÿØÿ± ÿßÿπŸÖ  ÿ®Ÿä error detection and correction   ÿ≠Ÿä  ÿßÿµ ŸÜ ÿ™ÿ¨ ÿßŸÑŸâ
Syndrome  ŸäŸÜ ÿ∏ÿ± ÿßÿ≠ÿØÿ£ÿπŸÖÿØÿ© ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© [ùêª]  Ÿàÿ±ŸÇŸÖ ÿßŸÑŸÑŸÖŸàÿØ ÿØÿß ŸáŸà ÿ±ŸÇŸÖ ÿßŸÑŸâbit   ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸäŸá.  error 
 ŸÑŸà ŸÖŸÅŸäÿ¥ÿ£ÿØ  error ÿßŸÑŸâ syndrome ŸáŸäÿ≥ ŸàÿØ ÿµŸÅÿ±  ùëÜÃÖ=0 Decoding at the receiver  
ùëÖÃÖ ùëáÃÖ ùê∏ÃÖ 
0 
 
[ùëØ] defined as:  
[ùëØ]=[ùüèùüèùüéùüèùüèùüéùüé
ùüèùüéùüèùüèùüéùüèùüé
ùüéùüèùüèùüèùüéùüéùüè] 
A message word 1010 is transmitted. A bit error occurs in the fourth bit position. 
Find the syndrome.  
Solution  
 ÿπŸÅ ÿµ ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸâ Syndrome   ŸáŸÜÿ¨Ÿäÿ®ùëÖÃÖ    Ÿàÿ≤ÿØ ŸÖ  ŸÇŸàŸÑŸÜ  ÿßŸÜŸáùëáÃÖ  ŸàŸÑÿ¥ÿµ ŸÖŸèÿ∂ ŸÅ ÿπŸÑŸä  ÿßŸÑŸâ error   ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸàŸÅŸä
ÿßŸÑŸÖÿ´   ÿØÿß ÿ≠ÿØÿØ ÿßÿµ ÿßŸÑŸâ error   ÿ≠ÿµ  ŸÅŸäÿ±ÿßÿ®ÿπ bit . 
 ùëáÃÖ ÿπÿ® ÿ±ÿ© ÿπÿµ ÿßŸÑŸâ message word 1010  ŸÖŸèÿ∂ ŸÅ ÿπŸÑŸäŸá  ÿßŸÑŸâ parity bits  ŸàÿßŸÑŸÑŸä ŸÖŸÖÿ¥ÿµ ŸÜÿ≠ÿ≥ÿ®Ÿá  ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÑ ŸÇÿ© 
[ùêª] ùëáÃÖ=0 
[ùêª] ùëáÃÖ=[1101100
1011010
0111001] 
[      1
0
1
0
ùëê1
ùëê2
ùëê3]      
=0 
 
1+0+0+ùëê1=0‚Üí  ùëê1=1 
1+1+0+ùëê2=0‚Üí  ùëê2=0 
0+1+0+ùëê3=0‚Üí  ùëê3=1 
 
 ÿßÿ∞ÿßŸã ÿßŸÑŸâ code word  ÿßŸÑŸÑŸä ŸáŸÜÿ®ŸÑÿ™  ŸáŸà 1010 101   Ÿàÿ®ŸÖ  ÿßŸÜ  ÿ≠ÿØÿØ ÿßÿµ ÿ≠ÿµ error  ŸÅŸä ÿ±ÿßÿ®ÿπbit   ŸáŸäŸàÿµ  ŸÑŸÑŸâ
Receiver 1011101 
 
The transmitted code word is 1010101, and the received word is 1011101. We form the 
product [ùêª] ùëÖÃÖ to get  
[ùêª] ùëÖÃÖ=[1101100
1011010
0111001] 
[      1
0
1
1
1
0
1]      
=[1
1
1] 
 Example  2 
 
 ÿ≠ÿ≥ÿ®ŸÜ  ÿßŸÑŸâ Syndrome  ÿ®ÿ∂ÿ±ÿ® ÿ¥  ÿµŸÅ ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©[ùêª]  ŸÅŸä ÿßŸÑŸÑŸÖŸàÿØ ÿ®ÿ™ ÿπ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©ùëÖÃÖ   ŸàŸÖŸÜŸÜÿ≥ ÿ¥ ÿßŸÜŸÜ
ÿ®ŸÜÿ¨ŸÖÿπ binary   ÿ®ÿØŸàÿµ carry 
1+0+0+1+1+0+0=1 
1+0+1+1+0+0+0=1 
0+0+1+1+0+0+1=1 
 
We note that the result is equal to the fourth column of [ùêª],thus identifying an error as 
having occurred in the fourth bit position.  
ŸÜ ÿ≠ÿ∏ ÿßÿµ ŸÜ ÿ™ÿ¨  ÿßŸÑŸâ Syndrome  ŸáŸà ÿ±ÿßÿ®ÿπ ÿπŸÖŸàÿØ ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©[ùêª]   ŸàÿØÿß ŸÖŸÑŸÜ Ÿá ÿßÿµ ÿßŸÑŸâ error  ÿ≠ÿµ  ŸÅŸä ÿ±ÿßÿ®ÿπ
bit 
 
 
 ŸÑŸàŸÖŸÇÿßŸÑÿ¥ ÿßŸÜ ÿßŸÑÿÆÿ∑ÿ£ ÿ≠ÿµŸÑ ŸÅŸä ÿßŸÑŸÄ bit  ÿ±ŸÇŸÖ4  ŸÉŸÜÿß ŸáŸÜÿπÿ±ŸÅ ÿßÿ≤ÿßŸäÿü 
 
ŸÖÿµ ÿÆ   ŸÖŸÇ ÿ±ŸÜÿ© ŸÇŸäŸÖÿ© ÿßŸÑŸâ Syndrome  ŸÖÿπÿ£ÿπŸÖÿØÿ© ÿßŸÑŸâ  [ùêª]  ÿßŸÇÿØÿ± ÿßŸÜŸä ÿßÿπÿ±ŸÅ ŸÖÿ¥ ÿµ ÿßŸÑŸâ error  .ŸÅŸäÿµŸàÿØÿß ŸÖÿµ 
ÿÆ   ÿßŸÑŸÑ ŸÇÿ©   
Syndrome (ùëÜÃÖ)=[ùêª] ùëÖÃÖ=[ùêª] ùê∏ÃÖ  
 ÿπÿ±ŸÅŸÜ  ÿßÿµ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©ùê∏ÃÖ   ÿ¥ŸÑŸá  ÿßÿµŸÅ ÿ± ŸÖ  ÿπÿØÿß ÿßŸÑŸÖÿ¥ ÿµ ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸä error   ŸáŸÜÿ≠ÿ∑ ŸÅŸä  Ÿàÿßÿ≠ÿØ ŸÅŸÖŸÑŸÜÿß ÿßÿµ
[ùêª] ùëÖÃÖ   Ÿäÿ≥ ŸàÿØ[1
1
1]    ÿßÿ∞ÿßŸã[ùêª] ùê∏ÃÖ   ÿ®ÿ±ÿØŸà Ÿäÿ≥ ŸàÿØ[1
1
1]   ŸàÿØÿß ŸÖŸÑŸÜ Ÿá ÿßÿµùê∏ÃÖ   ŸÅŸäŸábit  ŸÇŸäŸÖÿ™  ÿ™ÿ≥ ŸàÿØ1 Ÿàÿ™ÿ≠ÿØŸäÿØÿß ŸÅŸä ÿßŸÑŸâ  
bit   ÿßŸÑÿ±ÿßÿ®ÿπÿßŸÑÿµ ÿßŸÑŸÜ ÿ™ÿ¨ ŸÅÿ®  ÿßŸÑŸÑŸÖŸàÿØ ÿßŸÑÿ±ÿßÿ®ÿπ   
[ùêª] ùê∏ÃÖ=[1101100
1011010
0111001] 
[      0
0
0
1
0
0
0]      
=[1
1
1]   
 
 ŸàŸÑŸà ŸÜ ÿ™ÿ¨ ÿßŸÑŸâ Syndrome  ÿ∑ŸÑÿπ[0
0
0]  ÿØÿß ŸÖŸÑŸÜ Ÿá ÿßÿµ ŸÖŸÅŸäÿ¥ÿ£ÿØ error  ŸàŸÖŸÑŸÜ Ÿá ÿßÿµùê∏ÃÖ  ÿ¥ŸÑŸá  ÿßÿµŸÅ ÿ± 
 
ŸÅŸä ŸÖÿ≥ÿßÿ¶ŸÑ ÿßÿßŸÑŸÖÿ™ÿ≠ÿßŸÜ ŸáŸäÿ®ŸÇŸâ ŸÖÿπÿßŸÜÿß ŸÖÿπÿ∑Ÿâ ùëπÃÖ Received vector  Ÿà[ùëØ]  ŸàŸÜÿ≠ÿØÿØ ŸáŸÑ ŸÅŸäŸá ÿÆÿ∑ÿ£ŸàÿßŸÑ ÿßŸÑ ŸàŸÑŸà   
ŸÅŸäŸá ŸÜÿ≠ÿØÿØ ŸÅŸä ÿ£Ÿä bit. 
 
 
 ÿßŸÑÿ¥ÿ±ÿ≠ ÿßŸÑŸÑŸä ŸÅÿßÿ™ ŸÅŸä ÿ≠ÿßŸÑÿ© ÿ≠ÿµŸÑ error   Ÿàÿßÿ≠ÿØ ŸàŸÑŸà ÿßŸÑÿÆÿ∑ÿ£ ÿ≠ÿµŸÑ ŸÅŸä error 2 ŸÜÿ≠ÿØÿØŸá ÿßÿ≤ÿßŸäÿü 
 
ŸÇŸàŸÑŸÜ  ŸÑŸà  ÿ≠ÿµ   ÿÆÿ∑ÿ£ ŸÅŸä bit Ÿàÿßÿ≠ÿØŸá ÿßŸÑŸâ Syndrome (ùëÜÃÖ)    ŸáŸà ÿπŸÖŸàÿØŸäŸÜ ÿ∏ÿ± ÿßÿ≠ÿØ ÿßÿßŸÑÿπŸÖÿØÿ©  ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©  [ùêª] 
 Ÿàÿ±ŸÇŸÖ ÿßŸÑŸÑŸÖŸàÿØ ŸáŸà ÿ±ŸÇŸÖ ÿßŸÑŸâbit   ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸäŸá error 
 
ŸÑŸà ÿßŸÑÿÆÿ∑ÿ£ ÿ≠ÿµ  ŸÅŸä  bits 2  ÿßÿ∞ÿß ÿßŸÑŸâ Syndrome (ùëÜÃÖ)   ŸáŸäÿ∑ŸÑÿπÿπŸÖŸàÿØ Ÿäÿ≥ ŸàÿØ  ŸÖÿ¨ŸÖŸàÿπ ÿπŸÖŸàÿØŸäÿµ  ŸÖÿµ ÿ£ÿπŸÖÿØÿ©  
ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© [ùêª]  Ÿàÿ±ŸÇŸÖ ÿßŸÑŸÑŸÖŸàÿØŸäÿµ ŸáŸÖ ÿ±ŸÇŸÖÿßŸÑŸâ bits 2  ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸäŸáŸÖ ÿßŸÑÿÆÿ∑ÿ£. 
 
 
ÿßŸÑÿ≤ŸÖ ÿ™ÿµŸÖŸäŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©  [ùëØ]   ŸäŸÉŸàŸÜ ÿ®ÿ¥ŸÉŸÑ ŸÖÿπŸäŸÜ ÿßŸÑŸÜŸÅŸäŸá ÿ≠ÿßÿßŸÑÿ™ ŸÖŸÖŸÉŸÜ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿØÿß ŸäŸÅÿ¥ŸÑ ŸÅŸäŸáÿß  ŸàŸáŸÖ:  
‚ñ™  ŸÑŸà ŸÖÿ¨ŸÖŸàÿπ ÿπŸÖŸàÿØŸäÿµ ŸÖÿµÿßŸÑŸÖÿµŸÅŸàŸÅÿ© [ùêª] Ÿäÿ≥ ŸàÿØ ÿπŸÖŸàÿØ ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ŸàŸÅŸä ÿßŸÑÿ≠ ŸÑÿ© ÿØÿØ ÿ≠ÿµ  2 
error ŸàŸÑÿ¥ÿµ ÿßŸÑŸâ Receiver  ŸáŸäŸÅÿ≥ÿ±Ÿá ÿßŸÜ  error  ŸÅŸäbit  Ÿàÿßÿ≠ÿØŸá ÿ®ÿ≥ ÿßŸÑŸÜ  ÿØÿßŸäŸÖ  ÿ®Ÿäÿ±ÿ¨  ÿ≠ÿØŸà  ÿπÿØÿØ ÿßÿ£ŸÑÿÆÿ∑   
ÿßÿßŸÑŸÇ . 
 
‚ñ™ ŸÑŸà ŸÖÿ¨ŸÖŸàÿπ ÿπŸÖŸàÿØŸäÿµ ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© Ÿäÿ≥ ŸàÿØ ŸÖÿ¨ŸÖŸàÿπ ÿπŸÖŸàÿØŸäÿµ ÿ™ ŸÜŸäŸäÿµ ŸàŸáŸÜ  ŸÖÿ¥ ŸáŸäŸÇÿØÿ± Ÿäÿ≠ÿØÿØ ÿ£ÿØ  bits 
2  ÿßŸÑŸÑŸä ÿ≠ÿµ  ŸÅŸäŸáŸÖ error. 
 
‚ñ™  ŸÑŸà ÿπŸÜÿØÿØ ÿÆÿ∑ÿ£ ŸÅŸä bits 2 ŸàÿßŸÑŸÑŸÖŸàÿØŸäÿµ ÿßŸÑŸÑŸä ŸÅŸäŸáŸÖ ÿßŸÑÿÆÿ∑ÿ£ ŸÖÿ¨ŸÖŸàÿπ ŸáŸÖ  Ÿäÿ≥ ŸàÿØ ÿµŸÅÿ±  Ÿà ÿßŸÑŸâ Syndrome (ùëÜÃÖ)  
ÿ∑ŸÑÿπ ÿ®ÿµŸÅÿ±  ÿ¥ÿØÿß ŸáŸäŸÅÿ≥ÿ± ÿßÿµ ŸÖÿ≠ÿµŸÑÿ¥ error 
 
  ÿßŸÑÿ≠  ŸÅŸä ÿßŸÜŸÜ  ŸÜŸÑŸäÿØ ÿ™ÿµŸÖŸäŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©[ùêª] .ÿ®ÿ≠Ÿä  ŸÜÿ™ ŸÅÿß ÿßŸÑŸÖŸÅ ÿ¥  ÿßŸÑÿ≥ ÿ®ŸÇÿ© 
 
 
ŸÖÿßŸÑÿ≠ÿ∏ÿ© ŸÖŸáŸÖ ÿ© :  ŸÖŸÖÿ¥ÿµ ŸÜŸÑÿ±ŸÅ ÿßŸÑŸâ minimum  distance   ÿ®Ÿäÿµ ÿßŸÑŸâ code  words ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©  [ùêª] 
 ŸàÿØÿßŸÖÿµ ÿÆ    ÿ≠ ÿ≥ ÿ® ÿπÿØÿØ ÿßŸÑŸàÿ≠ ŸäÿØ  ŸÅŸä ÿ¥  ÿµŸÅ  ŸàŸÜŸÅŸàŸÅ ÿßŸÑÿµŸÅ ÿßŸÑŸÑŸä ŸÅŸä  ÿßŸÇ  ÿπÿØÿØ ŸÖÿµ ÿßŸÑŸàÿ≠ ŸäÿØ ŸàÿßŸÑŸÑÿØÿØ ÿØÿß 
ŸáŸà ÿßŸÑŸâ minimum  distance  
  
    [ùêª]=[11001000
10100100
01010010
00110001] 
 
 ŸÅŸä ÿßŸÑŸÖÿ´   ÿØÿß ÿπÿØÿØ ÿßŸÑŸàÿ≠ ŸäÿØ ŸÅŸä ÿ¥  ÿßŸÑÿµŸÅŸàŸÅ ŸáŸà3 ÿßÿ∞ÿß  ùê∑ùëöùëñùëõ=3 ‚Üê  
 ŸÅŸäÿßÿßŸÑŸÖÿ™ÿ≠ ÿµ ÿßŸÑÿ≤ŸÖ ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸâ code words ÿ¥ŸÑŸáŸÖ Ÿà ÿßŸÑŸÖ ÿ≠ÿ∏ÿ© ÿØÿØ ÿπŸÅ ÿµ ŸÜÿ™ ÿ¥ÿØ ÿßÿµ ÿßŸÑŸâ code words  Ÿà ÿßŸÑŸâ  
ùê∑ùëöùëñùëõ   ÿßŸÑŸÑŸä ÿ≠ÿ≥ÿ®ŸÜ ŸáŸÖÿµ   
 
 
 
converts digital bits into digital signal.  
 ŸáŸä ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ™ÿ≠ŸàŸä  ŸÖÿµ digital data  ÿßŸÑÿß digital signal  
 
 ŸÅŸä ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ≥ ÿ®ŸÇÿ© ÿ≠ÿµŸÑŸÜ  ÿπŸÑÿß code word    ŸàÿØÿß ÿßŸÑŸÑŸä ŸÜŸÇÿµÿØ ÿ®Ÿä digital data  ÿ£ŸÑŸÜŸÜ ŸÑÿ≠ÿØ ÿßÿßŸÑÿµ ŸÖŸÑ ŸÜ  ÿ¥ŸàÿØ  
ÿπÿ® ÿ±ÿ© ÿπÿµ ÿßÿµŸÅ ÿ± ŸàŸàÿ≠ ŸäÿØ Ÿàÿπ Ÿäÿ≤Ÿäÿµ ŸÜŸÑÿ®ÿ± ÿπŸÜ  ÿ®ŸÇŸäŸÖ ÿßŸÑŸâ volts  ÿßŸÑŸÑŸä ŸáŸÜÿ®ŸÑÿ™  ÿ®ŸäŸá  ŸàÿØÿß ÿßŸÑŸÑŸä ŸÜŸÇÿµÿØ ÿ®Ÿä  ÿßŸÑŸâ digital 
signal  Ÿàÿ¥ŸÜ  ŸÇŸàŸÑŸÜ  ÿßÿµÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿßŸÑŸÑŸä ÿ®ŸÜÿ≠ÿµ  ÿπŸÑŸäŸá  ÿ®ŸÑÿØ ÿπŸÖŸÑŸäÿ© ÿßŸÑŸâ Line Coding  ÿ®ŸÜÿ≥ŸÖŸäŸá  Baseband 
Signal  ŸàŸáŸä ÿØÿØÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿßŸÑŸÑŸä ŸáŸÜŸÑŸÖŸÑŸá  modulation   ŸÅŸä ÿßŸÑÿ®ŸÑŸàŸÉ ÿßŸÑŸÑŸä ÿßÿ≥ŸÖ Carrier Modulator 
 
  ŸäŸàÿ¨ÿØ ÿ∑ÿ±ŸÜ ŸÖÿÆÿ™ŸÑŸÅÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ© ŸÅŸä ÿ™ŸÖÿ´Ÿä  ÿßŸÑÿ®Ÿä ŸÜ ÿ™ ÿßŸÑÿ±ŸÇŸÖŸäÿ© ÿßŸÑŸÖÿ±ÿ≥ŸÑÿ© ŸàŸÑÿ¥  ŸÖŸÜŸá  ÿÆÿµ ÿ¶ÿµ ŸàŸÖŸÖŸäÿ≤ÿßÿ™ ŸàÿπŸäŸàÿ®
ŸàŸÅŸäŸÖ  ŸäŸÑŸä ÿßŸÑÿ∑ÿ±ŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ© ŸÅŸä ÿßŸÑŸâ Line Coding   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Line Coding  
Differential 
format  
Differential 
with Biphase  
 
Transmits +V volts for digital 1 and -V volts for digital 0.  
 
   ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÜŸàÿπ Ÿäÿ™ŸÖ ÿ™ŸÖÿ´Ÿä binary 1  ÿ®Ÿâ pulse  ŸÖŸàÿ¨ÿ®ÿ©+V   Ÿàÿ™ŸÖÿ´Ÿä binary 0  ÿ®Ÿâ pulse  ÿ≥ ŸÑÿ®ÿ©-V  
 
 
 
 
 
 
1. When the data is static (no change from bit interval to bit interval) , there are no 
transition the transmitted waveform. This can cause serious timing problem when we 
try to establish bit synchronization. Establishing the proper clock signals requires 
transitions in the received waveform  
 
   ŸÑŸà ÿ®ŸÑÿ™ ŸÖÿ´  ÿπÿØÿØ ÿßÿµŸÅ ÿ± ÿ¥ÿ®Ÿäÿ± Ÿàÿ±ÿß ÿ®ŸÑÿ∂ ÿßŸà ÿπÿØÿØ Ÿàÿ≠ ŸäÿØ ÿ¥ÿ®Ÿäÿ± ŸÖÿ¥ ŸáŸäÿ≠ÿµŸâŸâ transition    ŸÑŸÑŸâŸâŸâ  signal   ŸÑŸÅÿ™ŸâŸâÿ±ÿ©
ŸàÿØÿß ŸáŸäÿ≥ÿ®ÿ® ŸÖŸÅ ÿ¥  ŸÅŸä ÿßŸÑŸâ timing  ŸàÿßŸÑ Ÿâ  synchronization   ÿßŸÑÿµ ÿ®Ÿäÿ¥Ÿàÿµ ŸÅŸäŸâŸâ clock  ŸÅŸâŸâŸä transmitter  Ÿà clock 
  ŸÅŸä receiver    ÿßÿßŸÑÿ™ŸÜŸäÿµ ÿØŸà  ÿßŸÑÿ≤ŸÖ Ÿäÿ¥ŸàŸÜŸàÿß synchronized    ŸÖÿπ ÿ®ŸÑÿ∂ ŸÅŸÑÿ≠ÿ∏ ÿ™ ÿßÿßŸÑŸÜÿ™ŸÇ   ÿØÿØ ŸáŸä ÿßŸÑŸÑŸâŸâŸä ÿ®ÿ™ÿ≥ŸâŸâ ÿπÿØŸÜ
ŸÅŸä ÿ™ÿ≠ŸÇŸäŸÇ ÿß  synchronization 
 
 
2. If the levels are reversed during transmission (i.e. +V is interpreted as -V at the 
receiver), the entire data train will be inverted,  and every bit will be in error  
 
  ŸÑŸà ÿßŸÑÿØ ÿ≥ÿ®ÿ® ŸÖÿµ ÿßÿ£ŸÑÿ≥ÿ® ÿ® ÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™180   ÿØÿ±ÿ¨ÿ© ÿ¥  ÿßÿßŸÑÿµŸÅ ÿ± Ÿáÿ™ÿ®ŸÇÿß Ÿàÿ≠ ŸäÿØ Ÿàÿ¥  ÿßŸÑŸàÿ≠ ŸäÿØ Ÿáÿ™ÿ®ŸÇÿßÿßÿµŸÅ ÿ± 
ŸàÿßŸÑÿØÿßÿ™  ÿ¥ŸÑŸá  ÿ®ŸÇÿ™ ÿ∫ŸÑÿ∑ 
 
For these reasons we often choose differential forms of encoding. In such techniques, the 
data are represented as changes in levels rather than  the practical level signal.  
 
 ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ NRZ‚ÄìL  ŸáŸÜÿ≠ÿµ  ÿπŸÑÿßÿ£ŸÜŸàÿßÿπ ÿ£ÿÆÿ±Ÿâ  ŸÜÿ≠  ŸÅŸäŸá  ÿßŸÑŸÑŸäŸàÿ® ÿßŸÑÿ≥ ÿ®ŸÇÿ© ŸàŸÅŸä  ÿ£ŸÜŸàÿßÿπ Ÿáÿ™ÿ≠  ÿπŸäÿ® Ÿàÿßÿ≠ÿØ ŸÅŸÇÿ∑  
ŸàÿßÿÆÿ± ŸÜŸàÿπ ŸáŸäÿ≠  ÿßŸÑŸÖŸÅÿ¥ŸÑÿ™Ÿäÿµ.  
 
 
 
1. Non Return to Zero ‚ÄìL  (NRZ  ‚Äì L) 
Disadvantages of NRZ  ‚Äì L +V 
-V 
 
 
A data 1 is represented by a change in level between two consecutive bit times which a 
data 0 is represented by no change.  
 ŸáŸÜŸÑÿ®ÿ± ÿπÿµ1   ÿ®Ÿâ change in level   ŸäŸÑŸÜŸä ŸÑŸà ÿ¥ ŸÜÿ™ÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿπŸÜÿØ +V  ŸáŸÜŸÑÿ¥ÿ≥Ÿá  Ÿàÿ™ÿ®ŸÇÿß-V    ŸàÿßŸÑŸÑÿ¥ÿ≥ ŸäŸÑŸÜŸä ŸÑŸà
ÿ≥ ŸÑÿ® ŸáŸÜŸÇŸÑÿ®Ÿá  ŸÖŸàÿ¨ÿ® ,ŸàŸÜŸÑÿ®ÿ± ÿπÿµ 0  ÿ®Ÿâ change in level no  ŸäŸÑŸÜŸäÿßÿ•ŸÑŸÅ ÿ±ÿ© Ÿáÿ™ÿ¥ŸÖ  ÿπŸÑÿß ŸÜŸÅÿ≥ ÿßŸÑŸâ level   ŸÖÿµ
ÿ∫Ÿäÿ± ÿ™ÿ∫ŸäŸäÿ±.  
 
 
 
ŸÅŸä ÿßÿ£ŸÑŸà   ÿ®ŸÜÿ≠ÿØÿØ Level ŸÜÿ®ÿØÿ£ ÿ®Ÿä  ŸàŸÖŸÖÿ¥ÿµ ŸÜÿ®ÿØÿ£ ŸÖÿµ  +V  ÿ≤ÿØ ÿßŸÑŸÖÿ´   ÿØÿß ÿßŸà ŸÜÿ®ÿØÿ£ ŸÖÿµ-V   
 
 
 
 
We start with NRZ‚ÄìL and excl usive with the delayed NRZ‚ÄìM  
 
  ŸÑŸà ŸÖŸÑ ŸÜÿßÿ•ŸÑŸÅ ÿ±ÿ© NRZ‚ÄìL  ŸáŸÜÿ≥ÿ™ÿÆÿØŸÖŸá  ŸÅŸä ÿßŸÑÿ≠ÿµŸà  ÿπŸÑÿß NRZ‚ÄìM  :ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿØÿßŸäÿ±ÿ© ÿßŸÑŸÑŸä ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ© 
 
 
 
 T ÿØÿØ delay circuit   ÿ®ÿ™ŸÑŸÖ delay  ÿ®ŸÖŸÇÿØÿßÿ± interval  .Ÿàÿßÿ≠ÿØ 
 
ÿ®ŸÜŸÑŸÖ  XOR  ÿ®Ÿäÿµ NRZ‚ÄìL  Ÿà NRZ‚ÄìM delayed  ŸàÿßŸÑŸÖŸÇÿµŸàÿØ ÿ®Ÿâ delayed  ÿßŸÜŸá  ŸÖÿ™ ÿÆÿ±ÿ© ÿ®Ÿâ interval  Ÿàÿßÿ≠ÿØŸá
Ÿàÿ≤ÿØ ŸÖ  ŸÇŸàŸÑŸÜ  ÿßÿµ NRZ‚ÄìM  ÿ®ÿ™ÿ®ÿØÿ£ ÿ®Ÿâlevel  ŸÖŸÑŸäÿµÿßÿ≠ŸÜ  ÿ®ŸÜÿ≠ÿØÿØŸá ŸàŸÅŸä ÿßŸÑÿ∫ ŸÑÿ® ÿ®Ÿäÿ¥Ÿàÿµ  +V   
 
2.  Non Return to Zero ‚ÄìM (NRZ ‚ÄìM) 
NRZ‚ÄìM Encoder  +V 
 
 ÿ®ŸÜÿ®ÿØÿ£ ŸÖÿ´  ÿ®Ÿâ+V   ŸàŸáŸÜŸÑŸÖ XOR  ÿ®Ÿäÿµ+V  ÿßŸÑŸÑŸä ŸáŸÜŸÑÿ®ÿ± ÿπŸÜŸá  ÿ®Ÿâ1    Ÿàÿ®Ÿäÿµ ÿßŸàlevel  ŸÅŸä NRZ‚ÄìL    ŸàÿßŸÑŸÑŸä ŸáŸà ŸáŸÜ
+V   ,ÿ®ŸÑÿØ ÿ¥ÿØÿß ŸÜŸÑŸÖ XOR   ÿ®Ÿäÿµ ÿßŸà NRZ‚ÄìM  ÿßŸÑŸÑŸä ÿ≠ÿ≥ÿ®ŸÜ Ÿá ŸÅŸä ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑŸÑŸä ŸÇÿ®ŸÑŸá  Ÿàÿ®Ÿäÿµ ÿ™ ŸÜŸä  level   ŸÅŸä
NRZ‚ÄìL ŸàŸáÿ¥ÿ∞ÿß 
 
1          ‚äï        1    =    0 
 
 
 
                                           0  ‚äï   1    =    1 
 
 
ŸÖÿµ ÿÆ   ÿßŸÑÿ±ÿ≥ŸÖ ŸáŸÜ ŸÇŸä  ŸÖÿµ ÿÆ   ÿßŸÑÿØÿßŸäÿ±ÿ© ÿßŸÑÿ≥ ÿ®ŸÇÿ© ÿßŸÇÿØÿ± ÿßÿ≠ÿµ  ÿπŸÑÿß NRZ‚ÄìM  ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ NRZ‚ÄìL 
 
 
 
 
 
NRZ‚ÄìS is the same as NRZ‚ÄìM except that the two outputs are reversed.  
 ŸáŸä ŸÜŸÅÿ≥ ŸÅÿ¥ÿ±ÿ© NRZ‚ÄìM  ŸàŸÑÿ¥ÿµ ŸáŸÜŸÑÿ¥ÿ≥ ŸàŸáŸÜŸÑÿ®ÿ± ÿπÿµ1  ÿ®Ÿâ change in level no  ŸàŸáŸÜŸÑÿ®ÿ± ÿπÿµ0   ÿ®Ÿâ change 
in level  
 
 
 
 ÿ¥  ÿßŸÑÿ∑ÿ±ŸäŸÇÿ™Ÿäÿµ NRZ‚ÄìS   Ÿà NRZ‚ÄìM  ÿ®ŸÜÿ≥ŸÖŸäŸáŸÖ Differential format  ÿ£ŸÑŸÜŸÜ ŸÖÿ®ŸÜŸÑÿ®ÿ±ÿ¥ ÿπÿµ ÿßŸÑŸâ Levels   ÿ®ŸÇŸäŸÖ
ÿ´ ÿ®ÿ™  ŸàŸÑÿ¥ÿµ ÿ®ŸÜŸÑÿ®ÿ± ÿ®Ÿâ change  ÿßŸà no change 
 
NRZ‚ÄìS and NRZ‚ÄìM differential systems have solved the problem of waveform inversion 
but the not solved the problem of timing information.  
 
3.  Non Return to Zero ‚ÄìS (NRZ ‚ÄìS) NRZ‚ÄìM delayed  NRZ‚ÄìL 
NRZ‚ÄìM delayed  NRZ‚ÄìL NRZ‚ÄìM  
NRZ‚ÄìM  
+V 
 
 ÿßŸÑÿ∑ÿ±ŸäŸÇÿ™Ÿäÿµ NRZ‚ÄìS  Ÿà NRZ‚ÄìM  ÿ≠ŸÑŸà ÿßŸÑŸÖŸÅÿ¥ŸÑÿ© ÿßŸÑÿ™ ŸÜŸä  ŸàŸáŸä ŸÑŸàÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿ≠ÿµŸÑŸá  phase shift  ÿ®Ÿâ180   ÿßŸÑÿØÿßÿ™
Ÿáÿ™Ÿàÿµ  ÿµ .  
ŸÖÿ´  ŸÑŸà ÿ¨ÿ±ÿ®ŸÜ  Ÿàÿπÿ¥ÿ≥ŸÜ  ÿ±ÿ≥ŸÖÿ© ÿßŸÑŸâ NRZ‚ÄìS   ŸÅŸä ÿßŸÑŸÖÿ´   ÿßŸÑŸÑŸä ŸÅ ÿ™ Ÿàÿ®ÿ±ÿØŸà+V    ÿßŸÑŸÑŸä ÿ®ÿØÿ£ŸÜ  ÿ®ŸäŸáŸáÿ™ÿ™ŸÑÿ¥ÿ≥ ŸÖŸÑ ŸáŸÖ  
Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÖÿµ ÿßŸÑÿ±ÿ≥ŸÖÿ© ÿßŸÑŸÑŸä ÿ≠ÿµŸÑŸÜ  ÿπŸÑŸäŸá  ŸáŸäÿ∑ŸÑÿπ ŸÖŸÑ ŸÜ  ŸÜŸÅÿ≥ ÿßŸÑÿ¥ŸàÿØ ÿßŸÑÿµ 1  ŸÑÿ≥  ŸäŸÑÿ®ÿ± ÿπŸÜ  ÿ®Ÿâ change in level 
no  Ÿà0   ŸäŸÑÿ®ÿ± ÿπŸÜ  ÿ®Ÿâ change in level  Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÑŸÖ  ŸäŸàÿµ  ŸÑŸÑŸâ receiver  ŸáŸäŸÇÿ±ÿ£ ÿßŸÑÿØÿßÿ™  ÿµ ,ŸàŸÑÿ¥ÿµ ŸÖŸÅÿ¥ŸÑÿ©   
ÿßŸÑŸâ Synchronization  .ŸÑÿ≥  ŸÖŸàÿ¨ŸàÿØÿ© 
 
 
 
 
ÿßŸÑŸâ binary 1  ŸáŸÑÿ®ÿ±ÿπŸÜ  ÿ®Ÿâ +V  ŸÅŸä ÿßŸÑŸÜÿµ ÿßÿ£ŸÑŸà  Ÿà  -V ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä ŸàÿßŸÑŸâ binary 0  ŸáŸÑÿ®ÿ±ÿπŸÜ  ÿ®Ÿâ -V  ŸÅŸä
ÿßŸÑŸÜÿµ ÿßÿ£ŸÑŸà  Ÿà  +V  ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä 
 
 
 
ŸÖŸÖÿ¥ÿµ ÿßŸÑŸÑÿ¥ÿ≥ ŸàÿßŸÑŸâ binary 1  ŸáŸÑÿ®ÿ±ÿπŸÜ  ÿ®Ÿâ -V  ŸÅŸä ÿßŸÑŸÜÿµ ÿßÿ£ŸÑŸà  Ÿà +V   ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä ÿßŸÑŸÖŸáŸÖ Ÿäÿ¥Ÿàÿµ ŸÅŸä
transition  ŸÅŸä ŸÜÿµ ÿßŸÑŸâ interval    Ÿàÿ¥  ÿßŸÑÿ∑ÿ±ŸäŸÇÿ™Ÿäÿµ ÿµ  ÿ®ÿ≥ ÿ∫ ŸÑÿ®  ÿ®ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ™ ŸÜŸä 
 
 
 
This format overcome this static data and timing problem.  
The problem of signal inversion during transmission or reception will still result in 
bit reversed  
 
 ÿßŸÑŸÜŸàÿπ ÿØÿß  ÿ≠   ÿßŸÑ ŸÖŸÅÿ¥ŸÑÿ©  ÿßÿßŸÑŸàŸÑÿß  ÿßŸÑ Ÿâ   synchronization  ÿßŸÑÿµ ÿßŸÑŸâ  transitions  ÿØÿßŸäŸÖ  ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ŸÜÿµ ÿßŸÑŸâ intervalÿå ŸàŸÑÿ¥ŸâŸâÿµ 
ÿßŸÑŸÖŸÅÿ¥ŸÑÿ© ÿßŸÑÿ™ ŸÜŸä  ŸÑÿ≥  ŸÖŸàÿ¨ŸàÿØŸá  ŸàŸáŸä ÿßÿµ ŸÑŸà ÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿßÿ™ŸÇŸÑÿ®ÿ™ 180 ÿØÿ±ÿ¨ÿ© ÿßŸÑÿØÿßÿ™  ÿ¥ŸÑŸá  Ÿáÿ™ŸÇÿ±ÿ£ ÿ®ŸÅÿ¥  ÿÆ ÿ∑ÿ¶. 
 
 
4.  Biphase  ‚Äì L 
 
 
  ÿ®Ÿäÿ¥Ÿàÿµ ŸÖŸÑ ŸÜ‚Äì L  NRZ   Ÿàÿπ Ÿäÿ≤Ÿäÿµ ŸÜÿ¨Ÿäÿ® ŸÖŸÜŸá Biphase ‚Äì L     ŸàÿØÿß ŸÖÿµ ÿÆÿπŸÖ  XOR ÿ®ŸâŸâŸäÿµ ‚Äì L  NRZ  ŸàÿßŸÑŸâŸâŸâ 
clock  
 
 
 
 ÿßŸÑŸâ clock  ÿ®ÿ™ÿ¥Ÿàÿµ+ve  ŸÅŸä ÿßŸÑŸÜÿµÿßÿ£ŸÑŸà  Ÿà -ve  ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä ÿÆ   ÿßŸÑŸâ interval  Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÑŸà binary 1   ŸÖŸâŸâÿ´
ŸàÿπŸÖŸÑŸÜ   XOR  ŸÖÿπ ÿßŸÑŸâ clock  ŸáŸÜÿ≠ÿµ  ÿπŸÑÿß-ve  ŸÅŸä ÿßŸÑŸÜÿµÿßÿ£ŸÑŸà   Ÿà  +ve ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä 
1   ‚äï  1    =  0      ,     1    ‚äï    0    =   1 
 
 ŸÑŸà binary 0    ŸàÿπŸÖŸÑŸÜ XOR  ŸÖÿπ ÿßŸÑŸâ clock  ŸáŸÜÿ≠ÿµ  ÿπŸÑÿß+ve  ŸÅŸä ÿßŸÑŸÜÿµÿßÿ£ŸÑŸà   Ÿà  -ve ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ ŸÜŸä 
0    ‚äï    1 =    1          ,          0   ‚äï  0    =   0 
 
   ÿßÿ∞ÿßŸã ŸÖÿµ ÿÆ   ÿπŸÖ XOR ÿ®Ÿäÿµ  ‚Äì L  NRZ  ŸàÿßŸÑŸâ clock  ŸÇÿØÿ±ŸÜ  ŸÜÿ≠ÿµ  ÿπŸÑÿß Biphase ‚Äì L  
 
 
 
 
 
 
 
 
 
 
 
 
Biphase  ‚Äì L Encoder  
NRZ ‚Äì L 
Biphase ‚Äì L 
 
 
 
We can combine differential encoding with biphase encoding to solve both problems.  
 
In the Biphase ‚ÄìM & Biphase ‚ÄìS formats a transition occurs at the beginning of every bit 
period , in Biphase ‚ÄìM a data 1 is represented by an additional transition in the 
midperiod, while a 0 is represented by no period transition .  
 
in Biphase ‚ÄìS a data 1 results in no mid period transition, while data 0 results in a mid -
period transition  
 
  ÿ¥ ÿµ ÿπŸÜÿØŸÜ  ŸÖŸÅÿ¥ŸÑÿ™Ÿäÿµÿßÿ£ŸÑŸàŸÑÿß  ÿÆ ÿµÿ© ÿ® ŸÑŸâ  Synchronization    Ÿàÿ≠ŸÑŸäŸÜ Ÿá  ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ Biphase‚ÄìL     ŸàÿßŸÑŸÖŸÅÿ¥ŸÑÿ© ÿßŸÑÿ™ ŸÜŸäŸâŸâ
ÿ¥ ŸÜÿ™ ŸÅŸä ÿßŸÑŸâ Phase shift 180  Ÿàÿ≠ŸÑŸäŸÜ Ÿá  ÿ® ÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸâ Differential format  ( NRZ‚ÄìS NRZ‚ÄìM and) 
 
 ŸÅŸâŸâŸä ÿßŸÑŸÜŸâŸâŸàÿπ ÿØÿß ŸáŸÜŸâŸâÿØŸÖÿ¨ ÿ®ŸâŸâŸäÿµ ÿßŸÑÿ∑ŸâŸâÿ±ŸäŸÇÿ™Ÿäÿµ ÿπŸÅŸâŸâ ÿµ ŸÜŸÇŸâŸâÿØÿ± ŸÜÿ™ÿ∫ŸÑŸâŸâÿ® ÿπŸÑŸâŸâÿß ÿßŸÑŸÖŸÅŸâŸâÿ¥ŸÑÿ™Ÿäÿµ ŸÖŸâŸâÿµ ÿÆŸâŸâ   ÿßÿ≥ŸâŸâÿ™ÿÆÿØÿßŸÖ ŸÅÿ¥ŸâŸâÿ±ÿ© ÿßŸÑŸâŸâŸâ
Biphase‚ÄìL    ŸÅŸä ÿßŸÜŸÜ  ŸÜŸÑŸÖ transition  ŸÅŸäÿ¥  interval  ŸàŸÅÿ¥ŸâŸâÿ±ÿ© ÿßŸÑŸâŸâŸâ Differential format  ŸÅŸâŸâŸä ÿßŸÜŸÜŸâŸâ  ŸÖŸÜŸÑÿ®ŸâŸâÿ±ÿ¥
ÿπÿµ ÿßŸÑŸâ level  ÿ®ŸÇŸäŸÖÿ´ ÿ®ÿ™ ÿå  ŸàŸÑÿ¥ÿµ ŸáŸÜŸÇÿØÿ± ŸÜÿØŸÖÿ¨ ŸÖ  ÿ®Ÿäÿµ ÿßŸÑŸÜŸàÿπŸäÿµ ÿßÿ≤ÿßÿØÿü 
 
ŸÅŸä ÿ®ÿØÿßŸäÿ© ÿ¥   bit     ÿ®ŸÜŸÑŸÖ transition    ŸàŸÅŸä ÿßŸÑŸÜŸàÿπ Biphase ‚ÄìM     ŸáŸÜŸÖÿ´  ŸÅŸä1    ÿ®Ÿâ change    ŸÅŸä ŸÜŸâŸâÿµ ÿßŸÑŸâŸâŸâ interval 
   ÿ®ŸäŸÜŸÖ0    ŸáŸÜŸÖÿ´ŸÑ  ŸÅŸäbit    ÿ¥ ŸÖŸÑ  ŸäŸÑŸÜŸä ÿ®ŸÑÿØ ŸÖ  ŸÜŸÑŸÖ transition  ŸÅŸä ÿ®ÿØÿßŸäŸâŸâÿ© ÿßŸÑŸâŸâŸâbit   ŸÑŸâŸâŸà ŸÑŸÇŸäŸÜŸâŸâ0   ŸáŸÜÿ≥ŸâŸâŸäÿ® ÿßŸÑŸÇŸäŸÖŸâŸâÿ© ÿ´ ÿ®ÿ™ŸâŸâ
ÿßŸÑŸÑŸä ÿ¨ÿ®ŸÜ Ÿá  ÿ®ŸÑÿØ ÿßŸÑŸâ transition    ŸÖÿµ ÿ∫Ÿäÿ± ŸÖ  ŸÜŸÑŸÖ transition   ŸÅŸä ŸÜÿµ ÿßŸÑŸâ interval 
 
   ÿßŸÑŸÑÿ¥ÿ≥ ÿ® ŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑŸÜŸàÿπ Biphase ‚ÄìS     ŸáŸÜŸÖÿ´  ŸÅŸä0    ÿ®Ÿâ change   ŸÅŸâŸâŸä ŸÜŸâŸâÿµ ÿßŸÑŸâŸâŸâ interval   ÿ®ŸäŸÜŸÖŸâŸâ1  ŸáŸÜŸÖÿ´ŸÑŸâŸâ  ŸÅŸâŸâŸäbit 
 ÿ¥ ŸÖŸÑ 
 
 
 
 
 
 
ŸÅŸä ÿßŸÑŸÜŸàÿπŸäÿµ ÿØŸà  ÿ®ŸÜÿÆÿ™ ÿ± ÿ®ÿ±ÿØŸà Level  ŸÜÿ®ÿØÿ£ ÿ®Ÿä  ŸàŸÅŸä ÿßŸÑŸÖÿ´   ÿßŸÑŸÑŸä ŸÅ ÿ™ ÿ®ÿØÿ£ ÿ®Ÿâ-V 
 
 
 
 
 
 5.  Biphase  ‚Äì M  and  Biphase‚ÄìS 
 
 
 ŸÑŸà ÿßŸÑÿØÿÆ  ÿ¥ ÿµ NRZ‚ÄìM   ÿßŸÑÿÆÿ±ÿ¨ Biphase ‚ÄìS  ŸàŸÑŸà ÿßŸÑÿØÿÆ  ÿ¥ ÿµ NRZ‚ÄìS  ÿßŸÑÿÆÿ±ÿ¨ Biphase ‚ÄìM    Ÿàÿ®ŸÜŸÑŸÖ
delay  ŸÑŸÑÿØÿÆ  ÿ®ŸÖŸÇÿØÿßÿ± ŸÜÿµ ÿßŸÑŸâ Interval T/2 
 
 
 
NRZ‚ÄìM ‚ÜíBiphase‚ÄìS  
NRZ‚ÄìS ‚ÜíBiphase‚ÄìM  
 
  ŸÅŸä ÿßŸÑŸÖÿ´   ÿØÿß ÿßŸÑÿØÿÆ  NRZ‚ÄìM   ŸàÿπŸÖŸÑŸÜ delay  ŸÑŸä  ÿ®ŸÖŸÇÿØÿßÿ± ŸÜÿµ interval   ŸàÿπŸÖŸÑŸÜ XOR  ŸÑŸä  ŸÖÿπ ÿßŸÑŸâ clock  ŸáŸÜ ŸÇŸä
ÿßÿµ ÿßŸÑÿÆÿ±ÿ¨ ÿßŸÑŸÑŸä ÿ≠ÿµŸÑŸÜ  ÿπŸÑŸäŸá  ŸáŸà  Biphase ‚ÄìS    
 
 ÿßŸÑŸÅÿ¥ŸâŸâŸâÿ±ÿ© ÿßÿµ NRZ‚ÄìM   ÿ®ÿ™ŸÑŸÖŸâŸâŸâ change  ÿπŸÜŸâŸâŸâÿØ1 
   Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÑŸÖ  ŸÜŸÑŸÖ delay    ÿ®ŸâT/2    ÿ¥ÿØÿß ÿßŸÑŸâ change 
  ŸáŸäÿ≠ÿµ  ŸÅŸä ŸÜÿµ ÿßŸÑŸâ interval     ÿßŸÑŸÑŸä ŸÅŸäŸá1  ŸàŸÑÿ¥Ÿâÿµ
ÿπŸÅ ÿµ ŸÜÿ≠ÿµŸâ  ÿπŸÑŸâÿß  Biphase ‚ÄìS    ÿßŸÑÿ≤ŸÖ Ÿäÿ≠ÿµŸâ
change     ŸÅŸä ÿ®ÿØÿßŸäÿ© ÿ¥ interval    ŸÅ ŸÑÿ≠  ÿßŸÜŸÜ  ŸÜŸÑŸÖŸâ
XOR  ŸÖŸâŸâÿπ ÿßŸÑŸâŸâŸâ clock   ÿßŸÑÿµ ÿ≤ÿØ ŸÖŸâŸâ  ŸÇŸàŸÑŸÜŸâŸâ  ÿ≠ÿµŸâŸâ
delay  ŸÇŸäŸÖÿ™ŸâŸâ  ŸÜŸâŸâÿµ ÿßŸÑŸâŸâŸâ interval  ŸÖŸÑŸÜŸâŸâÿß ÿ¥ŸâŸâÿØÿß ÿßÿµ
ÿ∂ ŸÖÿµ ÿßÿµ ŸÅŸä  ŸÇŸäŸÖ  ÿ´ ÿ®ÿ™   ŸÇÿ®  ÿ®ÿØÿßŸäÿ© ÿßŸÑŸâŸâ  bit   ÿ®ŸâŸÜÿµ
interval  Ÿàÿ®ŸÑÿØŸá  ÿ®ŸÜÿµ interval 
  Ÿàÿ® ŸÑÿ™ ŸÑŸäŸÑŸÖ  Ÿäÿ™ŸÑŸÖŸÑŸá  XOR  ŸÖÿ±Ÿá ŸÖŸâÿπ1  ŸàŸÖŸâÿ±Ÿá ŸÖŸâÿπ
0    Ÿáÿ∂ŸÖÿµ ÿßÿµŸáŸäÿ≠ÿµŸÑŸá   change    ŸÅŸâŸä ÿ®ÿØÿßŸäŸâÿ© ÿ¥Ÿâ
bit  
   ŸàÿßŸÑŸÇŸâŸäŸÖ ÿßŸÑŸÑŸâŸä ÿ¥ ŸÜŸâÿ™ ÿ´ ÿ®ÿ™Ÿâ  ÿÆŸâ0 bit   ŸáŸäÿ≠ÿµŸâŸâŸÑŸá
change    ŸÅŸä ŸÜÿµ ÿßŸÑŸâ interval   ŸàÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÑŸä ÿ¥ ŸÜŸâÿ™
ÿ®Ÿäÿ≠ÿµŸÑŸá   change      ÿÆ1  bit   Ÿáÿ™ÿ´ÿ®ÿ™ÿ£ŸÑŸÜŸáŸâ  ŸÑŸÖŸâ  
Ÿäÿ™ŸÑŸÖŸÑŸáŸâ  XOR  ŸÖŸâŸâÿ±Ÿá ŸÖŸâŸâÿπ Ÿàÿßÿ≠ŸâŸâÿØ ŸàŸÖŸâŸâÿ±Ÿá ŸÖŸâŸâÿπ ÿµŸâŸâŸÅÿ±
ŸàŸáŸä ŸÜŸÅÿ≥Ÿá  ÿ®ÿ™ÿ™ÿ∫Ÿäÿ± ŸÖŸâÿ±Ÿá ÿ®Ÿàÿßÿ≠ŸâÿØ ŸàŸÖŸâÿ±Ÿá ÿ®ÿµŸâŸÅÿ± ÿßŸà 
ÿßŸÑŸÑÿ¥ÿ≥ Ÿáÿ™ÿØÿØ ÿ±ŸÇŸÖ ÿ´ ÿ®ÿ™ ŸÅŸä ÿßÿßŸÑÿÆÿ± 
 
 
 
ÿ¥ÿØÿß ÿßŸÑŸÖÿ≠ ÿ∂ÿ±ÿ© ÿÆŸÑÿµÿ™ ŸàÿßŸÑÿ¨ÿ≤  ÿßŸÑŸÑŸä ÿ¨Ÿä ÿØÿß ÿ¨ÿØÿπŸÜÿ© ŸÖÿµ ÿπŸÜÿØÿØ  
Biphase  ‚Äì M  and  Biphase  ‚Äì S  Encoder  
 Encoder  
 
 
 ÿßŸÜ  ÿπ ÿ±ŸÅ ÿßÿµ ÿßŸÑŸÖÿ≠ ÿ∂ÿ±ÿ© ŸÖŸÑŸä ŸÜÿ© ÿ±ÿ∫Ÿä ÿ¥ÿ™Ÿäÿ± ŸàŸÖÿ¥ ÿπ ÿ±ŸÅ ŸÖŸäÿµ ŸÅŸäÿµ Ÿàÿ™ÿ≠ŸÅÿ∏ ÿßŸä  ŸÅ  ÿßÿÆŸàŸÉ ŸÖÿ¥ ŸÜ ÿ≥ŸäŸÉ  Ÿàÿ¨ Ÿäÿ®ŸÑŸÉ ÿßÿ≥ÿ¶ŸÑÿ© ŸÖÿµ  
ÿßŸÖÿ™ÿ≠ ŸÜ ÿ™ ŸÇÿØŸäŸÖÿ© ÿπŸÑÿß ÿßŸÑÿ¨ÿ≤  ÿØÿß ÿ®ÿ≠Ÿä  ŸÜŸÑÿ±ŸÅ ÿßŸÑÿ¥ ŸÖ ÿØÿß ÿ®Ÿäÿ™ÿ≥ÿ£  ŸÅŸä  ÿßÿ≤ÿßÿØ Ÿàÿ¥ÿØÿß. 
 
 
 
An 8 -bit code word is formed by adding 4 parity bits to the 4 information bits. The parity bits are 
given by  
ùë™ùüè=ùíÇùüè ‚äïùíÇùüê          ùë™ùüê=ùíÇùüè ‚äïùíÇùüë             ùë™ùüë=ùíÇùüê ‚äïùíÇùüí             ùë™ùüí=ùíÇùüë ‚äïùíÇùüí  
 
i. find the minimum distance between code words.  
ii. How many errors can be detected? How many errors can be corrected?  
iii. If the received code word 10001111, find the transmitted code word?  
Solution  
 
i. find the minimum distance between code words.  
  ŸÑŸÖŸÑÿ±ŸÅÿ© ÿßŸÑŸâùê∑ùëöùëñùëõ  ŸáŸÜÿ¨Ÿäÿ® ÿßŸÑŸâ code words    ŸÖÿµ ÿÆÿ≠ÿ≥ ÿ® ÿßŸÑŸâ parity bits  ŸÖÿµ ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ ÿßŸÑŸÖŸÑÿ∑ ÿ© ŸÅŸä ÿßŸÑÿ≥ÿ§ÿß   
For message 0000  
ùê∂1=0 ‚äï0=0             ùê∂2=0 ‚äï0=0 
ùê∂3=0 ‚äï0=0             ùê∂4=0 ‚äï0=0 
 
 
For message 000 1 
ùê∂1=0 ‚äï0=0             ùê∂2=0 ‚äï0=0 
ùê∂3=0 ‚äï1=1             ùê∂4=0 ‚äï1=1 
 
For message 0010  
ùê∂1=0 ‚äï0=0             ùê∂2=0 ‚äï1=1 
ùê∂3=0 ‚äï0=0             ùê∂4=1 ‚äï0=1 
 
For message 0011  
ùê∂1=0 ‚äï0=0             ùê∂2=0 ‚äï1=1 
ùê∂3=0 ‚äï1=1             ùê∂4=1 ‚äï1=0 
 
 
  ŸàŸáÿ¥ÿ∞ÿß ŸÅŸä ÿ® ŸÇŸä ÿßŸÑŸâ message words  ÿ≠ÿ™ÿß ŸÜÿ≠ÿµ  ÿπŸÑÿß ÿßŸÑŸâ code words   ŸÅŸä ÿßŸÑÿ¨ÿØŸà  ÿßÿßŸÑÿ™Ÿä:  
  
  [Extra] Related Exam s Questions  
Question 1 (Final 2020)  
 Encoder  
 
Code Word s Message Word s 
0000  0000  0000  
0001  0011   0001  
0010  0101  0010  
0011  0110  0011  
0100  1010  0100  
0101  1001  0101  
0110  1111  0110  
0111  1100  0111  
1000  1100  1000  
1001  1111  1001  
1010  1001  1010  
1011  1010  1011  
1100  0110  1100  
1101  0101  1101  
1110  0011  1110  
1111  0000  1111  
 
‚à¥ùë´ùíéùíäùíè=ùüë  
 ŸÖÿµ ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿßŸÑŸÑŸä ÿ≠ÿµŸÑŸÜ  ÿπŸÑŸäŸá  ŸáŸÜ ŸÇŸä ÿ¥  ÿ¥ŸàÿØ ŸäÿÆÿ™ŸÑŸÅ ÿπÿµ ÿßÿßŸÑÿÆÿ± ÿπŸÑÿßÿßÿ£ŸÑŸÇ  ŸÅŸä 3 bits   
 ÿπÿ±ŸÅŸÜ  ÿØÿß ŸÖÿµ ÿÆ   ŸÖŸÇ ÿ±ŸÜÿ© ÿßÿßŸÑÿ¥ŸàÿßÿØ ÿ®ÿ®ŸÑÿ∂ ŸáŸÜ ŸÇŸä ÿßÿµ ÿßÿÆÿ± ÿßÿ™ŸÜŸäÿµ code word   ŸäÿÆÿ™ŸÑŸÅŸàÿß ŸÅŸä3-bit 
 
 
ii.  How many errors can be detected? How many errors can be corrected?  
 
error can be detected  =ùê∑ùëöùëñùëõ‚àí1  ‚Üí3‚àí1  =2 bits  
error can be corrected =ùê∑ùëöùëñùëõ
2‚àí1 ‚Üí3
2‚àí1
2   =1  bit  
 
iii. If the received code word 10001111, find the transmitted code word?  
 ÿπŸÅ ÿµ ŸÜÿ¨Ÿäÿ® ÿßŸÑÿ¥ŸàÿØ ÿßŸÑŸÑŸä ÿ®ŸÑÿ™ŸÜ Ÿá ÿßŸÑÿ≤ŸÖ ŸÜÿ¨Ÿäÿ® ÿßŸÑŸâ syndrome   ÿπŸÅ ÿµ ŸÜŸÑÿ±ŸÅ ŸÑŸà ÿ≠ÿµ error   . ÿßŸà ÿßŸÑ ŸàŸÑŸà ŸÅŸä  ŸÜÿµŸÑÿ≠ 
Syndrome ( ùëÜÃÖ) =[ùêª] ùëÖÃÖ 
 
 ùëÖÃÖ ŸÖŸÑÿ∑ÿß  ŸàŸÑÿ¥ÿµ ŸáŸÜÿ≠ÿ™ ÿ¨ ŸÜÿ¨Ÿäÿ® [ùêª]   ŸàÿØÿß ŸÜŸÇÿØÿ± ŸÜÿ¨Ÿäÿ®  ŸÖÿµ ÿÆ   ŸÖŸÑ ÿØÿßŸÑÿ™ ÿßŸÑŸâ parity bits   ŸàÿπÿØÿØ ÿßŸÑŸâ information bits    ŸàÿßŸÑŸâ
parity bits  ÿßŸÑŸÖŸÑÿ∑ ÿ© ŸÅŸäÿßŸÑÿ≥ÿ§ÿß  
 
 
ùëö=4  (message bits) , ùëõ=4   (parity bits)  
Size of [ùêª]‚Üíùëõ √ó(ùëõ+ùëö)‚Üí4 √ó8  
  ÿßÿ∞ÿßŸã ÿ≠ÿ¨ŸÖ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© [ùêª]    ŸáŸà4 √ó8    ,Ÿàÿ¥ŸÜ  ŸÇŸàŸÑŸÜ  ÿßÿµ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿ®ÿ™ÿ™ÿ¥Ÿàÿµ ŸÖÿµ ÿ¨ÿ≤ÿ¶Ÿäÿµ ÿßŸÑÿ¨ÿ≤  ÿßŸÑÿ™ ŸÜŸä ÿπÿ® ÿ±ÿ© ÿπÿµ unity 
matrix     ÿ≠ÿ¨ŸÖŸáùëõ √óùëõ    Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÅŸä ÿßŸÑŸÖÿ´   ÿØÿß ÿ≠ÿ¨ŸÖŸá4 √ó4 
 
  ÿ® ŸÇŸä ÿ¥ÿØÿß ŸÇŸäŸÖ ÿßŸÑŸâ‚Ñé ŸàÿØÿØ ŸáŸÜÿ¨Ÿäÿ®Ÿá  ŸÖÿµ ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ ÿßŸÑŸÖŸÑÿ∑ ÿ© ŸÖÿ´  ÿßŸà  ŸÖŸÑ ÿØŸÑÿ©  ùê∂1=ùëé1 ‚äïùëé2    ŸàŸÖŸÖÿ¥ÿµ ŸÜŸàÿµŸÅŸá  ÿ®ŸÅÿ¥  ÿ™ ŸÜŸä
Ÿàÿßÿµ ŸÖÿ¨ŸÖŸàÿπ ÿßŸÑŸâ information bits  ŸàÿßŸÑŸâ parity bit   ÿ¨ŸÖÿπ ÿ®ÿØŸàÿµ carry   ÿßŸÑŸÜ ÿ™ÿ¨ Ÿäÿ≥ ŸàÿØ ÿµŸÅÿ± ùëé1+ùëé2+ùê∂1=0  
  ÿßÿ∞ÿßŸã ŸÖÿµ ÿÆ   ÿßŸÑŸÑ ŸÇÿ© [ùêª]ùëáÃÖ=0   ŸáŸÜÿ≠ÿ∑ ŸÇŸäŸÖ‚Ñé ÿßŸÑŸÑŸä ÿ™ŸÜ ÿ∏ÿ± ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä ÿ¥  ŸÖŸÑ ÿØŸÑÿ© ŸäŸÑŸÜŸä ŸÖÿ´  ÿßŸà  ŸÖŸÑ ÿØŸÑÿ© ŸáŸÜÿ≠ÿ∑  
ÿßŸà  ŸÇŸäŸÖÿ™Ÿäÿµ ŸÅŸä ÿßŸÑÿµŸÅ ÿßÿ£ŸÑŸà   ÿ®Ÿàÿßÿ≠ÿØ ŸàÿßŸÑŸÇŸäŸÖÿ™Ÿäÿµ ÿßŸÑÿ™ ŸÜŸäŸäÿµ ÿ®ÿµŸÅÿ±  ÿπŸÅ ÿµ ŸÑŸÖ  ŸÜÿ∂ÿ±ÿ®Ÿá  ŸÅŸä ŸÖÿµŸÅŸàŸÅÿ©  ùëáÃÖ     ŸÜÿ¨Ÿäÿ®ùëé1   Ÿàùëé2  , ŸàŸáÿ¥ÿ∞ÿß
ŸÅŸä ÿ® ŸÇŸä ÿßŸÑŸÖŸÑ ÿØÿßŸÑÿ™ ŸÑÿ≠ÿØ ŸÖ  ŸÜÿ¨Ÿäÿ® ÿ® ŸÇŸä ŸÇŸäŸÖ ÿßŸÑŸâ  ‚Ñé   
[ùêª]ùëáÃÖ=[‚Ñé11‚Ñé12‚Ñé13‚Ñé141000
‚Ñé21‚Ñé22‚Ñé23‚Ñé240100
‚Ñé31‚Ñé32‚Ñé33‚Ñé340010
‚Ñé41‚Ñé42‚Ñé43‚Ñé440001]
[       ùëé1
ùëé2
ùëé3
ùëé4
ùëê1
ùëê2
ùëê3
ùëê4]       
=0 
  ŸÖÿµ ÿßŸÑŸÖŸÑ ÿØŸÑÿ© ÿßŸÑÿ≥ ÿ®ŸÇÿ© ŸáŸÜŸÇÿØÿ± ŸÜŸÅŸáŸÖ ÿßÿ¥ÿ™ÿ± ÿßÿ≤ÿßÿØ ÿßÿÆÿ™ ÿ±ŸÜ  ŸÇŸäŸÖ‚Ñé   
 
 
    [ùêª]=[11001000
10100100
01010010
00110001] 
 
 
 
ùëÜÃÖ=[ùêª] ùëÖÃÖ=[11001000
10100100
01010010
00110001] 
[       1
0
0
0
1
1
1
1]       
=[0
0
1
1] 
 ùëÜÃÖ  ŸáŸä ŸÜŸÅÿ≥ ÿßŸÑŸÑŸÖŸàÿØ4   ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©[ùêª]  Ÿàÿ® ŸÑÿ™ ŸÑŸä ÿßŸÑÿÆÿ∑ÿ£ ÿ≠ÿµ  ŸÅŸä ÿßŸÑŸâbit  ÿ±ŸÇŸÖ4   Ÿàÿ® ŸÑÿ™ ŸÑŸä ŸÜŸÑÿØ  ŸÇŸäŸÖÿ™  ŸÖÿµ0  ÿßŸÑÿß1 
 
‚à¥transmitted  code= 100ùüè1111  
 
 ùëé1 ùëé2 ùëé3 ùëé4 ùëê1 ùëê2 ùëê3 ùëê4 
 
i. Explain the disadvantages of NRZ -L coding format.  
ii. State and explain a technique which overcomes these disadvantages.   
iii. Sketch an encoder circuit for the technique explain ed in (ii).  
Solution  
 
i.  Explain the disadvantages of NRZ -L coding format.  
1. When the data is static (no change from bit interval to bit interval), there are no transition the 
transmitted waveform. This can cause serious timing problem when we try to establish bit 
synchronization. Establishing the proper clock signals requires transitions in the received waveform  
 
2. If the levels are reversed during transmission (i.e. +V is interpreted as -V at the receiver), the entire 
data train will be inverted, and every bit will be in error.  
 
ii. State and explain a technique which overcomes these disadvantages.    
Biphase ‚Äì M  or  Biphase ‚ÄìS can overcome these  disadvantages.  
 We can combine differential encoding with biphase encoding to solve both problems.  
 
In the Biphase ‚ÄìM & Biphase ‚ÄìS formats a transition occurs at the beginning of every bit period , in 
Biphase ‚ÄìM a data 1 is represented by an additional transition in the midperiod, while a 0 is 
represented by no period transition.  
in Biphase ‚ÄìS a data 1 results in no mid period transition, while data 0 results in a mid -period 
transition.  
 
iii. Sketch an encoder circuit for the technique explained in (ii).  
 
 
 
NRZ‚ÄìM ‚ÜíBiphase‚ÄìS  
 
 
NRZ‚ÄìS ‚ÜíBiphase‚ÄìM  
 
Question 2 (Final 20 18) 
 Encoder  
 
Given the algebraic code with 3 bit message words and 3 parity check bits  
[ùëØ] is defined as  
[ùëØ]=[ùüèùüéùüèùüèùüéùüé
ùüéùüèùüèùüéùüèùüé
ùüèùüèùüéùüéùüéùüè] 
If the receiver receives 100011, determine the corresponding data word.  
Solution  
ùëÜÃÖ=[ùêª] √ó ùëÖÃÖ=[101100
011010
110001]√ó
[     1
0
0
0
1
1]     
=[1
1
0] 
So there error and it's in bit 3. So the correct word is 101011  
 
 
 
 
Discuss the advantages and disadvantages of NRZ -L, NRZ -M and biphase -L formats?  
Solution  
NRZ-L 
Advantage: It‚Äôs simple.  
Disadvantages:  
1- When the data is static, timing (synchronization) is lost.  
2- Data inversion: If 180 phase shift occurred in the received data , it will be translated wrong where 0 will be 
1 , and 1 will be 0.  
 
NRZ-M 
Advantage: It solves the phase shift problem  
Disadvantage: It doesn‚Äôt solve the synchronization problem.  
 
Biphase -L 
Advantage: It solves the synchronization problem.  
Disadvantage: It doesn‚Äôt solve the phase shift problem . 
 
 Question 4 (Final 20 16) 
 Encoder  Question 3 (Final 20 16) 
 Encoder  
 
A 1-kHz, 2V Pk -Pk sine wave is sampled 5000 times each second and encoding using  
3-bit PCM.  
(a) Sketch the waveform for the first 2 msec assuming that the NRZ -L format is used.  
(b) Repeat part (a) for the NRZ -M differential format.  
(c) Repeat part (a) for the biphase -L format  
Solution  
 ŸÅÿ¥ÿ±ÿ©ÿßŸÑÿ≥ÿ§ÿß  ÿßŸÜŸÜ  ŸáŸÜÿ±ÿ≥ŸÖ ÿ±ÿ≥ŸÖÿ©  sine wave   ÿ™ÿ±ÿØÿØŸá 1KHz   ŸàŸÜŸÑŸÖŸÑŸá sampling  ÿ®ÿ™ÿ±ÿØÿØ ùëìùë†=5000 ùêªùëß   ŸäŸÑŸÜŸä ŸÜ ÿÆÿØ sample    ÿ¥
0.2 ms    ŸàŸÖÿ∑ŸÑŸàÿ® ŸÜŸÑŸÖŸÑŸá encoding  ŸÅŸä 3 bits   ŸäŸÑŸÜŸä ŸáŸÜŸÇÿ≥ŸÖ ÿßŸÑÿ±ÿ≥ŸÖÿ© ŸÑŸâ 8 Levels   ŸàŸÜÿ≠ÿµ  ÿπŸÑÿß ÿßŸÑÿ¥ŸàÿØ ÿ®ÿ™ ÿπÿßÿ•ŸÑŸÅ ÿ±ÿ© ÿÆ   ŸÅÿ™ÿ±ÿ©  
2ms   ŸàŸÜÿ±ÿ≥ŸÖŸá  ÿ®Ÿâ NRZ-L   Ÿà NRZ-M  Ÿà biphase -L 
 
ùëìùëö=1000 ùêªùëß  
ùê¥=1 ùëâ 
ùë•(ùë°)=ùê¥sin(2ùúã ùëìùëö ùë°)=sin(2000ùúãùë°) 
 
ùëõ=3 
ùêø=2ùëõ=23=8 
‚àÜùë†=ùê∑ùëÖ
ùêø=2
8=0.25 
 ŸáŸÜÿ¨Ÿäÿ® ÿßŸÑÿ¥ŸàÿØ ŸÖÿµ ÿßŸÑÿ±ÿ≥ŸÖ ÿπÿµ ÿ∑ÿ±ŸäŸÇ ÿßŸÜŸÜ  ŸÜŸÅŸàŸÅ ÿ¥  sample  ÿ™ŸÇÿπ ŸÅŸäÿ£ÿØ  range 
at t=0 ‚Üí 100  ,  at t=0.2 ms ‚Üí 111  ,   at t=0.4ms ‚Üí 110  ,  at t=0.6ms ‚Üí 001  ,  at t=0.8ms ‚Üí 000 
at t=1ms ‚Üí 100   
and it‚Äôs repeated for the second period  
at t=1.2 ms ‚Üí 111  ,   at t=1.4ms ‚Üí 110  ,  at t=1.6ms ‚Üí 001  ,  at t=1.8ms ‚Üí 000  ,  at t=2ms ‚Üí 100 
 
so the binary code of the 2  msec from the signal is  
100 111 110 001 000 100 111 110 001 000 100 
 
 
 
 
 
Question 5 (Final 20 16) 
 Encoder  
000 001 010 011 100 101 110 111 
 
 
Output digital bits
 
 
 
 
 
 
Ÿáÿ™ŸÅŸÇ ŸÖÿπÿßŸÉ
 
ÿßŸÜ 
ÿß
ŸÑŸÉÿßŸÑŸÖ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿØŸä ŸÉÿßŸÜ ŸÉÿ™Ÿäÿ± ŸàŸÉŸÑ ÿ¥ŸàŸäÿ© ÿ®ŸÜŸÉÿ±ÿ± ÿ≠ÿßÿ¨ÿßÿ™ ÿ®ÿ≥ ÿµÿØŸÇŸÜŸä.. 
 
 
 
 
 
 

 
 
 
 
 
 
 
  
 
¬©
 
Basem Hesham
Matched Filter & Amplitude Shift Keying
Lecture
 
10
 
 
 ŸàÿµŸÑŸÜÿß ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ŸÑÿ≠ÿØ Line Coding  Ÿàÿπÿ±ŸÅŸÜÿßÿ£Ÿä ÿßŸÑÿ∑ÿ±ŸÇ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ÿßŸÑŸÑŸä ÿ®ŸÜŸÖÿ´ŸÑ ÿ®ŸäŸáÿß ÿßÿßŸÑÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿ≠ÿßŸäÿØ Ÿàÿ®ŸÜÿ≠ÿµŸÑŸÑ 
ŸÅŸä ÿßŸÑÿÆÿ∑Ÿàÿ© ÿØŸä ÿπŸÑŸâ ÿßŸÑŸÑŸÑ signal Baseband  Ÿàÿ®ÿπŸÑÿØ ÿØŸÑÿØÿß ÿßŸÑŸÖŸÅŸÑÿ±Ÿàÿπ ŸÜÿπŸÖŸÑŸÑ modulation ŸÑÿ•ŸÑÿ¥ŸÑÿßÿ±ÿ© ÿØŸä ÿπÿ¥ŸÑÿß  ŸÜÿØŸÑÿØÿ± ŸÜÿ®ÿπÿ™ŸáŸÑÿß 
ŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿ∑ŸàŸäŸÑÿ© ŸàŸáŸÜÿØÿ±ÿ≥ ÿßŸÑÿ∑ÿ±ŸÇ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑÿπŸÖŸÑ ÿßŸÑŸÑ modulation   ŸàŸÑÿØÿßÿ£ŸÑŸàŸÑ ŸáŸÜÿ™ÿπŸÑŸÖ ÿ≠ÿßÿ¨ÿ© ŸÖŸáŸÖŸÑÿ© ŸàŸáŸÑŸä Matched Filter 
  ŸàÿØÿß ŸÅŸÑÿ™ÿ± ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÑ Receiver    Ÿàÿ∏ŸäŸÅÿ™Ÿá ÿßŸÜŸá Ÿäÿ≠ŸÑ ŸÖÿ¥ÿØŸÑÿ© ÿßŸÑŸÑ noise    ÿßŸÑŸÑŸÑŸä ÿ®ÿ™ŸÑÿ±ÿ´ÿ± ÿπŸÑŸÑŸâÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿ©  ÿßŸÑŸÑŸÑŸä ÿ®ÿπŸÑÿØ ŸÖŸÑÿß ÿπŸÖŸÑŸÑÿ™ ŸÑŸäŸáŸÑÿß 
modulation  Ÿàÿ®ÿπÿ™Ÿáÿß ÿπŸÑŸâ ÿßŸÑŸÑ channel. 
 
ÿßŸÑŸÖÿ´ÿßŸÑ ŸàÿßŸÑÿ±ÿ≥ŸÖ ÿßÿßŸÑÿ™Ÿä ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ®ÿ¥ÿØŸÑ ŸÖÿ®ÿ≥ÿ∑ ŸÑŸÅÿØÿ±ÿ© ÿßŸÑŸÑ matched filter  
 
 
 
 
ŸÜŸÅÿ™ÿ±ÿπ ÿß  ÿπŸÜÿØŸÜÿß ÿßÿ¥ÿßÿ±ÿ© ùë†(ùë°)  ŸáŸÜŸÜÿØŸÑŸáÿß ŸÅŸä channel  ÿ®ÿ™ÿ™ÿπÿ±ÿπ ŸÑŸÑ noise ùëõ(ùë°)   ŸàŸáŸÜŸÅÿ™ÿ±ÿπ ÿßŸÜŸáŸÑÿß AWGN  ŸäÿπŸÜŸÑŸä ÿßŸÑŸÑŸÑ noise 
 ŸÖÿ™ÿ≥ÿßŸàŸäÿ© ŸÅŸä ŸÇŸäŸÖÿ© ÿßŸÑÿ®ÿßŸàÿ± ŸÅŸä ÿ¨ŸÖŸäÿπ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™ Ÿàÿ™ÿ≥ÿßŸàŸäùëÅùëú
2 . 
 
  ÿ®ÿπÿØ ŸÖÿß ÿ™ÿπÿØŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùë†(ùë°)    ÿÆÿßŸÑŸÑ ÿßŸÑŸÑ channel    Ÿàÿ®ÿπÿØ ŸÖÿß ÿ™ÿ™ÿπÿ±ÿπ ŸÑŸÑ noise    Ÿáÿ™ÿÆŸÑÿ±ÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿ© ùë†(ùë°)+ùëõ(ùë°)   ŸàŸÖŸÑ  ÿßŸÑÿ±ÿ≥ŸÑŸÖ
Ÿàÿßÿ∂ÿ≠ ÿß  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ¥ÿØŸÑŸáÿß ÿßÿ™ÿ∫Ÿäÿ± Ÿàÿµÿπÿ® ŸÜŸÅÿ±ŸÇ ŸÅŸäŸáÿß ÿ®Ÿä   1  Ÿà0  ŸàŸáŸÜÿß ŸäŸäÿ¨Ÿä ÿØŸàÿ± ÿßŸÑŸÑŸÑ matched filter  ŸàŸáŸÑŸà ŸÖŸÑÿ® ÿ®Ÿäÿ¥ŸÑŸäŸÑ ÿßŸÑŸÑŸÑ
noise    ŸàŸÑÿØ  ÿ®Ÿäÿ≤ŸàÿØ ÿ™ÿ±ÿ´Ÿäÿ± ÿßŸÑŸÑ signal    ŸÖÿØÿßÿ±ŸÜÿ© ÿ®ÿßŸÑŸÑ noise    Ÿàÿ®ŸÜÿ≠ÿµŸÑ ŸÖŸÜŸá ÿπŸÑŸâÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùë¶(ùë°)    ŸàŸáŸä ŸÖŸÑÿ® ÿ¥ŸÑÿ®Ÿá ùë†(ùë°)   ÿ®ÿßŸÑÿ∂ŸÑÿ®ÿ∑
ŸàŸÑÿØ  ŸÖŸÑ  ÿßŸÑÿ≥ŸÑŸáŸÑ ÿßŸÑÿ™ÿπŸÑÿ±ŸÑ ÿπŸÑŸÑŸâ 1  Ÿà0  ŸÅŸäŸáŸÑÿß ŸÑŸÑŸà ÿØÿÆŸÑŸÜÿßŸáŸÑÿß ÿπŸÑŸÑŸâ decision circuit   Ÿàÿ≠ŸÑÿØÿØŸÜÿßvolt threshold  ŸÑŸÑŸà ÿ≤ÿßÿØ ÿπŸÜŸÑŸá
ŸáŸÜÿπÿ™ÿ®ÿ±Ÿá ÿ®ŸÑ 1  ŸàŸÑŸà ŸÇŸÑ ÿπŸÜŸá ŸáŸÜÿπÿ™ÿ®ÿ±Ÿá ÿ®ŸÑ0 , Ÿàÿßÿ≠ŸÜÿß ÿ®ŸÜÿπŸÖŸÑŸÑ threshold  ÿ®ŸÜÿßÿÆŸÑÿØ decision  ÿØŸÑŸÑ Time period T  ŸàÿßŸÑŸÑŸÑŸä ÿ®ÿ™ŸÖÿ´ŸÑŸÑ ÿßŸÑŸÑŸÑ
duration  ÿ®ÿ™ÿßÿπÿ© ÿßŸÑŸÑ pulse. 
 
 ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿØŸä ŸáŸÜÿ™ÿπŸÑŸÖŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÑ impulse response ‚Ñé(ùë°)   ŸÑŸÑŸÑŸÑ matched filter  Ÿàÿßÿ≤ÿßŸä ŸÜÿπŸÖŸÑŸÑ implementation  ŸÑŸäŸÑŸá
ŸàŸáŸÜÿπÿ±ŸÑ ÿßÿ≤ÿßŸä ÿ®Ÿäÿ≤ŸàÿØ ÿ™ÿ±ÿ´Ÿäÿ± ÿßŸÑŸÑ signal  ŸÖÿØÿßÿ±ŸÜŸÑÿ© ÿ®ÿßŸÑŸÑŸÑ noise,  ŸàŸáŸÜŸÑÿ™ÿπŸÑŸÖ ÿßÿ≤ÿßŸä ŸÜÿ≠ÿ≥ŸÑÿ® ÿßÿ≠ÿ™ŸÖŸÑÿßŸÑ ÿßŸÑÿÆÿ∑ŸÑÿ± Probability of error 
(ùëùùëí) ŸàŸÖ  ÿÆÿßŸÑŸÑŸáÿß ŸÜÿØÿØÿ±  ŸÜÿ≠ÿ≥ÿ® ÿØŸÅÿßÿ°ÿ© Ÿàÿ£ÿØÿßÿ° ÿßŸÑŸÑ digital modulation techniques ( ASK , FSK , PSK). 
 
 ÿßŸÑŸÑŸä ŸÅÿßÿ™ ÿØÿß ÿØÿß  ŸÖÿØÿØŸÖÿ© ŸÖ  ÿπŸÜÿØŸä ŸÑÿ™Ÿàÿ∂Ÿäÿ≠ ÿßŸÑŸÅÿØÿ±ÿ© ŸÇÿ®ŸÑ ŸÖÿß ŸÜÿ®ÿØÿ£ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© 
 
 
 
 
 
 
 
 
 
 
Matched Filter  
ùë†(ùë°) ùëõ(ùë°) ùë†(ùë°)+ùëõ(ùë°) ‚Ñé(ùë°) ùë¶(ùë°) ùë°=ùëá 
 
The matched filter is one technique for processing the received signal. The criterion for designing the 
filter is that the output at a sample T is to have maximum signal to noise ratio.  
 
 
Matched Filter ŸáŸà ŸÅŸÑÿ™ÿ± ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÑ Receiver ŸàŸàÿ∏ŸäŸÅÿ™Ÿá ŸáŸà ÿ≤ŸäŸÑÿßÿØÿ© ÿ™ŸÑÿ±ÿ´Ÿäÿ± ÿßŸÑŸÑŸÑ signal  ÿ®ÿßŸÑŸÜÿ≥ŸÑÿ®ÿ© ŸÑŸÑŸÑŸÑ noise  ŸäÿπŸÜŸÑŸä ÿ≤ŸäŸÑÿßÿØÿ©
SNR    ,ŸàÿßŸÑÿ∑ÿ±ŸäÿØÿ© ÿßŸÑŸÑŸä ÿ®Ÿäÿ™ŸÖ ÿßÿßŸÑÿπÿ™ŸÖÿßÿØ ÿπŸÑŸäŸáÿß ŸÅŸä ÿ™ÿµŸÖŸäŸÖ ÿßŸÑŸÜŸàÿπ ÿØÿß ŸáŸä ÿßŸÜŸÜÿß ÿ®ŸÜÿßÿÆÿØ sample    ÿ®ÿπÿØ ÿØŸÑ ÿ≤ŸÖT  ŸàŸäÿ≠ÿØÿØ ÿßŸÑŸÑŸÑŸä ÿßÿ™ÿ®ÿπŸÑÿ™
ÿØÿß Ÿàÿßÿ≠ÿØ ŸàÿßŸÑ ÿµŸÅÿ±. 
 
ŸÜŸÅÿ™ÿ±ÿπ ÿß  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÑŸä ÿ®ÿπÿ™ŸÜÿßŸáÿß ùë†(ùë°)  ŸÖÿ∂ÿßŸÑ ÿπŸÑŸäŸáÿß noise ùëõ(ùë°)   ŸàŸàÿµŸÑÿ™ ùë†(ùë°)+ùëõ(ùë°)   ÿπŸÜÿØ ÿßŸÑŸÑŸÑ Receiver  ,ŸáŸÜÿßÿÆŸÑÿØ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ®ÿπÿØ ŸÖÿß ÿßÿ™ÿ∂ÿßŸÑ ÿπŸÑŸäŸáŸÑÿß noise  ŸàŸÜŸÑÿØÿÆŸÑŸáÿß ÿπŸÑŸÑŸâ Matched Filter ÿ®ŸÑŸÑ impulse response ‚Ñé(ùë°)   ,ŸàŸáŸÜŸÅÿ™ŸÑÿ±ÿπ ÿß  ÿßŸÑŸÑŸÑ
noise  ŸáŸä AWGN  ŸäÿπŸÜŸä ŸÑŸäŸáÿß power spectral density ÿ´ÿßÿ®ÿ™Ÿá ŸÇŸäŸÖÿ™Ÿáÿß ùëÅùëú
2 ( Ÿàÿ≠ÿØÿ™Ÿáÿß watt/Hz  ) 
 
Let us assume that ùëõ(ùë°) is white noise with power spectral density ùëÅùëú
2 , the filter impulse response then 
becomes  
‚Ñé(ùë°)=1
2 ùê∂ ùëÅùëú ùë†(ùëá‚àíùë°) 
 ùê∂  ŸáŸä ÿ´ÿßÿ®ÿ™ ŸàùëÅùëú
2  ÿ≤Ÿä ŸÇŸàŸÑŸÜÿß ÿßŸÜŸáÿß  power spectral densityŸÇŸäŸÖÿ™Ÿáÿß ÿ´ÿßÿ®ÿ™Ÿá ŸÅŸä ÿØŸÑŸÑ ÿßŸÑÿ™ŸÑÿ±ÿØÿØÿßÿ™  Ÿàÿ®ÿßŸÑÿ™ŸÑÿßŸÑŸä ŸÖŸÖÿØŸÑ  ŸÜÿπŸÑŸàÿπ ÿπŸÑ  
ÿßŸÑÿ¨ÿ≤ÿ° 1
2ùê∂ùëÅùëú  ÿ®ÿ´ÿßÿ®ÿ™ ŸáŸÜÿ≥ŸÖŸäŸá ùêæ  
‚Ñé(ùë°)=ùêæ ùë†(ùëá‚àíùë°) 
the impulse response of the optimum filter, except for the factor K,  is a time reversed and delayed 
version of the input signal, that is, it is "matched" to the input signal.  
 
 ŸÖ  ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ŸÜÿßŸÑÿ≠ÿ∏ ÿß  ÿπÿßŸÑŸÇÿ© ÿßŸÑŸÑ impulse response ‚Ñé(ùë°)   ÿ®ÿ™ÿßÿπÿ© ÿßŸÑŸÑŸÑfilter  ÿ™ÿ¥ŸÑÿ®Ÿá ÿ¨ŸÑÿØÿß ŸÖÿπÿßÿØŸÑŸÑÿ© ÿßŸÑŸÑŸÑ input signal   ŸàŸÖŸÖÿØŸÑ
ŸÜÿØŸàŸÑ ÿßŸÜŸáÿß ŸáŸä ŸÜŸÅÿ≥Ÿáÿß ŸàŸÑÿØ  time reversed   ÿßŸÑÿ•ÿ¥ŸÑÿßÿ±ÿ© ùë°  ÿ®ÿßŸÑÿ≥ŸÑÿßŸÑÿ® Ÿà  delayed ÿ®ŸÖÿØŸÑÿØÿßÿ±ùëá ,  ŸàÿØÿß ÿßŸÑŸÑŸÑŸä ŸáŸäÿÆŸÑŸÑŸä ÿßŸÑŸÅŸÑÿ™ŸÑÿ± ŸäÿπŸÖŸÑŸÑ
detect ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ®ÿ¥ÿØŸÑ ÿµÿ≠Ÿäÿ≠ Ÿàÿ®ŸÖÿß ÿßŸÜŸáŸÖ ÿ¥ÿ®Ÿá ÿ®ÿπÿπ ÿ®ŸÜÿØŸàŸÑ ÿ®ŸäŸÜŸáŸÖ matching  
 
 ÿØÿß ÿ≥ÿ®ÿ® ÿ™ÿ≥ŸÖŸäÿ™Ÿá Matched Filter  ÿßŸÑ  ÿßŸÑŸÑ impulse Response   ÿ®ÿ™ÿßÿπÿ™Ÿá ÿ®ÿ™ÿØŸà matching  ŸÖÿπ ÿßŸÑŸÑ input ÿ£ŸÑŸÜŸáÿß ŸÜŸÅŸÑÿ≥ ÿßŸÑÿ¥ŸÑÿØŸÑ 
ŸàŸÑÿØ  time reversed  ŸàŸÅŸäŸáÿßshift  ÿ®ŸÖÿØÿØÿßÿ±T 
 
 
 
 
 
 
 

 
 
 
 
ÿßŸÑÿπÿßŸÑŸÇÿ© ÿ®Ÿä  ÿßŸÑÿÆÿ±  ŸàÿßŸÑÿØÿÆŸÑ ŸÅŸä ÿ£Ÿä ŸÅŸÑÿ™ÿ± ŸÅŸä ÿßŸÑŸÑ time domain  ŸáŸä convolution  ÿ®ŸÑŸä  ÿßŸÑŸÑÿØÿÆŸÑ ùë†(ùë°)+ùëõ(ùë°)   ŸàÿßŸÑŸÑŸÑ
impulse response ‚Ñé(ùë°)  
ùë†ùëú(ùë°)+ùëõùëú(ùë°)=[ùë†(ùë°)+ùëõ(ùë°)]‚àó‚Ñé(ùë°) 
=‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùë°
0‚Ñé(ùë°‚àíùúè) ùëëùúè 
 
‚Ñé(ùë°)=ùêæ ùë†(ùëá‚àíùë°) 
‚à¥‚Ñé(ùë°‚àíùúè)=ùêæ ùë†(ùëá‚àíùë°+ùúè) 
 
ùë†ùëú(ùë°)+ùëõùëú(ùë°)=ùëò‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùë°
0ùë†(ùëá‚àíùë°+ùúè) ùëëùúè 
 
 ÿßŸÑŸÑ decision   ÿ®ÿ™ÿßÿπ ÿßŸÑŸÅŸÑÿ™ÿ± ÿ®Ÿäÿ™ŸÖ ÿπŸÜÿØ ÿØŸÑùëá  ŸÅŸáŸÜÿ¥ŸäŸÑ ÿØŸÑ ùë°  ŸÅŸä ÿßŸÑŸÖÿπÿßÿØŸÑÿ©ŸàŸÜÿπŸàÿπ ÿπŸÜŸáÿß ÿ®ŸÑ   ùëá 
At ùë°=ùëá 
ùë†ùëú(ùëá)+ùëõùëú(ùëá)=ùëò‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùëá
0ùë†(ùúè) ùëëùúè 
 
 
 
 
 
 
 
The operation being performed by the system is called correlation (i.e. multiply two time functions 
together and integrate the product).  For that reason, the matched filter is sometimes called correlator . 
 ŸÅÿØÿ±ÿ© ÿßŸÑŸÅŸÑÿ™ÿ± ÿßŸÜŸá ÿ®ŸäÿßÿÆÿØ ÿßŸÑÿØÿÆŸÑ ÿßŸÑŸÖÿ∂ÿßŸÑ ÿπŸÑŸäŸá noise  ŸàŸäÿ∂ÿ±ÿ®Ÿá ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÖ  ÿ∫Ÿäÿ± noise  ŸàŸäÿØÿßŸÖŸÑŸÑ ÿ≠ÿßÿµŸÑŸÑ ÿßŸÑÿ∂ŸÑÿ±ÿ® ŸàÿØŸÜŸÑÿß
ÿ®ŸÜÿØŸÑŸÑŸàŸÑ ÿπŸÑŸÑŸÑŸâ ÿ™ÿØÿßŸÖŸÑŸÑŸÑ ÿ≠ÿßÿµŸÑŸÑŸÑ ÿ∂ŸÑŸÑÿ±ÿ® ÿßÿ¥ŸÑŸÑÿßÿ±ÿ™Ÿä  ŸáŸÑŸÑŸà correlation  Ÿàÿπÿ¥ŸÑŸÑÿß  ÿØŸÑŸÑÿØÿß ÿßŸÑŸÑŸÑŸÑ matched filter  ŸÑŸäŸÑŸÑŸá ÿßÿ≥ŸÑŸÑŸÖ ÿ™ŸÑŸÑÿßŸÜŸä ŸàŸáŸÑŸÑŸà
correlator ŸàŸáŸÜÿ¥ŸàŸÑ ŸÇÿØÿßŸÖ ÿßÿ≤ÿßŸä ÿ®ŸÜÿ≥ÿ™ŸÅŸäÿØ ŸÖ  ÿßŸÑÿπÿßŸÑŸÇÿ© ÿØŸä ŸÅŸä ÿ≠ŸÑ ŸÖÿ¥ÿØŸÑÿ© ÿßŸÑŸÑ noise. 
 
ÿ£Ÿä ŸÅŸÑÿ™ÿ± ÿßŸÑŸÑ operation  ÿ®ÿ™ÿßÿπÿ™Ÿá ŸáŸä convolution  ŸàŸÅŸÑŸä ÿßŸÑŸÑŸÑ matched filter  ŸÑŸÖŸÑÿß ÿπŸÖŸÑŸÜŸÑÿß Implementation  ŸÑŸäŸÑŸá ŸÑÿØŸäŸÜŸÑÿß ÿß  ÿßŸÑŸÑŸÑ
convolution  ÿßŸÑŸÑŸä ÿ®Ÿäÿ™ŸÖ ÿπŸÜÿØŸá ŸäÿØÿßŸÅÿ¶ correlation . 
 
 
 
 Implementation of Matched Filter  
‚à´  ùëëùë°ùëá
0 
 
 
 
 
 
 
 
 
 
 
 
 
Binary Matched 
Filter
 
  ÿØÿß ŸÜŸàÿπ ŸÖ
ÿ£ŸÜŸàÿßÿπ
 
ÿßŸÑŸÑ 
matched filter
 
 ÿÆÿßÿµ ÿ®ÿßÿ≥ÿ™ÿØÿ®ÿßŸÑ
ÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿßÿ™ 
 
ÿßŸÑÿ±ŸÇŸÖŸäŸÑÿ© ŸäÿπŸÜŸÑŸä 
ÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿ©
 
ÿπÿ®ŸÑÿßÿ±ÿ© 
ÿπ  ÿßÿµŸÅÿßÿ± ŸàŸàÿ≠ÿßŸäÿØ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπŸÜÿØŸä ŸÜŸàÿπŸä  ŸÖ  
ùë†
(
ùë°
)
 
 Ÿàÿßÿ≠ÿØŸá ŸÑŸÑŸÑ
1
 
 ŸáŸÜÿ≥ŸÖŸäŸáÿß
ùë†
1
(
ùë°
)
  
Ÿàÿ£ÿÆÿ±Ÿâ
 
ŸÑŸÑŸÑ 
0
 
 ŸáŸÜÿ≥ŸÖŸäŸáÿß
ùë†
2
(
ùë°
)
 
.
 
ÿßŸÑŸÖÿ´ŸÑŸÑÿßŸÑ ÿßÿßŸÑÿ™ŸÑŸÑŸä ŸáŸäŸàÿ∂ŸÑŸÑÿ≠ ŸÅÿØŸÑŸÑÿ±ÿ© ÿßŸÑÿπŸÖŸÑŸÑŸÑ
 
ŸàŸÅŸÑŸÑŸä ÿßŸÑŸÖÿ´ŸÑŸÑÿßŸÑ ÿØÿß ŸÑŸÑÿ™ÿ≥ŸÑŸÑŸáŸäŸÑ ŸáŸÜŸÅÿ™ŸÑŸÑÿ±ÿπ  ÿßŸÜŸÜŸÑŸÑÿß ŸÖÿπŸÖŸÑŸÜŸÑŸÑÿßÿ® 
modulation
 
 Ÿàÿ®ÿπÿ™ŸÜŸÑŸÑÿß
ÿ•ÿ¥ŸÑŸÑÿßÿ±ÿ©
 
ÿßŸÑŸÑŸÑŸÑ 
Baseband
 
 
 ŸàŸáÿ™ŸàÿµŸÑ ÿßŸÑŸÑ
Receiver
   
 Ÿà
 
ŸÜŸÅÿ™ŸÑŸÑÿ±ÿπ ÿßŸÜŸÜŸÑŸÑÿß ÿ®ŸÜÿπÿ®ŸÑŸÑÿ± ÿπŸÑŸÑ  
binary 1
 
ÿ®ÿ•ÿ¥ŸÑŸÑÿßÿ±ÿ©
 
ÿ®ÿßŸÑÿ¥ŸÑŸÑÿØŸÑ ÿßÿßŸÑÿ™ŸÑŸÑŸä      
 
ŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÑ 
noise
  
 
 ŸàŸáÿ™ŸàÿµŸÑŸÑŸÑŸÑŸÑ ÿßŸÑŸÑŸÑŸÑŸÑŸÑ
Receiver
 
 
  ŸáŸÜÿπÿ®ŸÑŸÑÿ± ÿπŸÑŸÑ
binary 
0
 
ÿ®ÿ•ÿ¥ŸÑŸÑÿßÿ±ÿ©
 
ÿ®ÿßŸÑÿ¥ŸÑŸÑÿØŸÑ ÿßÿßŸÑÿ™ŸÑŸÑŸä       
 
 
ŸäÿπŸÜŸä ÿπŸÖŸÑŸÜÿß 
Line Coding
 
 ÿ®ŸÑ
Biphase
-
L
 
 ŸàÿßŸÑŸÑ
Receiver
 
 ÿπÿßÿ±ŸÑ ŸÜŸàÿπ ÿßŸÑŸÑ
Line Coding
 
 ÿßŸÑŸÑŸÑŸä ÿ¥ŸÑÿ∫ÿßŸÑ ÿ®ŸäŸÑŸá ÿßŸÑŸÑŸÑ
transmitter
 
 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπÿßÿ±ŸÑ ÿ¥ÿØŸÑ ÿßŸÑŸÑ
binary 1
 
 ŸàÿßŸÑŸÑ
binary 0
 
 ŸÖ  ÿ∫Ÿäÿ±
noise
 
 
 ÿßŸÑŸÑ
Receiver
 
ŸàÿµŸÑÿ™
ŸÑ
Ÿá ÿßŸÑŸÑ 
noisy signal
 
 ŸàŸáŸà ŸÖÿ® ÿπÿßÿ±ŸÑ
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿØŸä ÿπÿ®ÿßÿ±ÿ© ÿπ  
ÿ£Ÿä
 
Ÿàÿπÿ¥ÿß  Ÿäÿ≠ÿØÿØ ŸáŸä 
ÿ£Ÿä
 
ŸáŸäÿ∂ÿ±ÿ®Ÿáÿß ŸÖÿ±Ÿá 
ŸÅŸä 
ùë†
1
(
ùë°
)
 
 ÿßŸÑŸÑŸä ŸáŸä
ÿ•ÿ¥ÿßÿ±ÿ©
 
binary 1
 
 ŸÖ  ÿ∫Ÿäÿ±
noise
 
 ŸàŸäÿ∂ÿ±ÿ®Ÿáÿß ŸÅŸä ŸÅŸä
ùë†
2
(
ùë°
)
 
 ÿßŸÑŸÑŸä ŸáŸä
ÿ•ÿ¥ÿßÿ±ÿ©
 
binary 0
 
 ŸÖ  ÿ∫ŸäŸÑÿ±
noise
 
 ŸàŸäÿØÿßŸÖŸÑ ÿ≠ÿßÿµŸÑ ÿßŸÑÿ∂ÿ±ÿ®. ŸÜÿßÿ™ÿ¨ ÿßŸÑŸÑ
branch
 
 ÿßŸÑŸÑŸä ŸÅŸàŸÇ ÿπÿ®ÿßÿ±ÿ© ÿπ  ŸÖÿØŸâ ÿ™ÿ¥ÿßÿ®ÿ© ÿßŸÑŸÑ
noisy signal
 
 ŸÖÿπ
ÿ•ÿ¥ŸÑÿßÿ±ÿ©
 
ÿßŸÑŸÑŸÑ 
binary 1
 
  ŸÖŸÑ
ÿ∫Ÿäÿ± 
noise
 
Ÿà
 
ŸÜÿßÿ™ÿ¨ ÿßŸÑŸÑ 
branch
 
 ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿπÿ®ÿßÿ±ÿ© ÿπ  ŸÖÿØŸâ ÿ™ÿ¥ÿßÿ®ÿ© ÿßŸÑŸÑ
noisy signal
 
 ŸÖÿπ
ÿ•ÿ¥ÿßÿ±ÿ©
 
ÿßŸÑŸÑ 
binary 0
 
 ŸÖ  ÿ∫Ÿäÿ±
noise
 
 .ŸäÿπŸÜŸä ÿØÿßŸÜŸá ÿ®Ÿäÿ≠ÿ≥ÿ® ÿßŸÑŸÑ
correlation
 
 ŸÑŸÑŸÑŸÑ
noisy signal
 
 ŸÖŸÑÿπ
1
 
 ŸàŸÖŸÑÿπ
0
 . ÿ®ÿπŸÑÿØ ÿØŸÑÿØÿß ÿ®ŸäÿßÿÆŸÑÿØ ÿßŸÑÿØŸäŸÖŸÑÿ© ŸÅŸÑŸä ÿßŸÑŸÑŸÑ
branch
 
 ÿßŸÑŸÑŸÑŸä ŸÅŸÑŸàŸÇ
Ÿäÿ∑ÿ±ÿ≠Ÿáÿß ŸÖ 
 
ŸÅŸä ÿßŸÑŸÑ 
branch
 
 ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ŸàÿßŸÑŸÜÿßÿ™ÿ¨ ÿßŸÑŸÑŸä ÿ∑ŸÑÿπ ŸáŸÜÿØÿßÿ±ŸÜŸá ÿ®ÿØŸäŸÖÿ©
threshold 
ùë¶
ùëú
 
 ŸÑŸà ŸÜÿßÿ™ÿ¨ ÿßŸÑÿ∑ÿ±ÿ≠ ÿßÿπŸÑŸâ ŸÖŸÑ  ŸÇŸäŸÖŸÑÿ© ÿßŸÑŸÑŸÑ
threshold
  
 ÿØÿß ŸÖÿπŸÜÿßŸá ÿß  ÿßŸÑŸÑ
correlation
 
 ŸÅŸä ÿßŸÑŸÑ
branch
 
 ÿßŸÑŸÑŸä ŸÅŸàŸÇ ÿßÿØÿ®ÿ± ŸÖ  ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™
 
ŸäÿπŸÜŸä 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿ¥ŸÑÿØŸÑŸáÿß ÿßŸÇŸÑÿ±ÿ® ŸÑŸÑŸÑ 
1
 
 ÿßÿ∞ÿß
ÿßŸÑŸÑŸä 
noisy signal
 
 ÿØŸä ÿØÿßŸÜÿ™
1
 
 ŸàÿßŸÑÿπÿØÿ≥ ŸÑŸà ÿØÿß  ŸÜÿßÿ™ÿ¨ ÿßŸÑÿ∑ÿ±ÿ≠ ÿßŸÇŸÑ ŸÖ  ŸÇŸäŸÖÿ© ÿßŸÑŸÑ
threshold
 
 ÿØÿß ŸÖÿπŸÜÿßŸá ÿß  ÿßŸÑŸÑŸÑ
correlation
 
 ŸÅŸÑŸä
ÿßŸÑŸÑ 
branch
 
 ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿßÿØÿ®ÿ± ŸÖ  ÿßŸÑŸÑŸä ŸÅŸàŸÇ ŸäÿπŸÜŸä
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿ¥ÿØŸÑŸáÿß ÿßŸÇÿ±ÿ® ŸÑŸÑ 
0
 
 ÿßÿ∞ÿß ÿßŸÑŸÑŸä
noisy signal
 
 ÿØŸä ÿØÿßŸÜÿ™
0
 
 ÿØÿØÿß ŸÜÿ¨ÿ≠ ŸÅŸä ÿßŸÜŸá ŸäÿØŸÑŸÑ ÿßŸÑŸÑ
noise
 
 ÿ¨ÿØÿß Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÑ
SNR
  
Ÿáÿ™ÿ≤ŸäÿØ ÿ®ÿ¥ÿØŸÑ ÿØÿ®Ÿäÿ±
. 
 
 
 
 
 
 
Binary Matched Filter
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
‚àë
 
 
 ŸÇÿ®ŸÑ ŸÖÿß ŸÜÿ®ÿØÿ£ ŸÅŸä ÿßŸÑŸÑ Digital modulation techniques  ŸáŸÜÿ™ÿπŸÑŸÖ ÿßÿ≤ÿßŸä ŸÜÿ≠ÿ≥ÿ® ÿØŸÅÿßÿ¶ÿ™ŸáŸÖ ŸÖ  ÿÆÿßŸÑŸÑ ŸÖÿπÿßÿØŸÑŸÑÿ© ÿßŸÑŸÑŸÑ probability of 
error  ŸàŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿØÿß ŸáŸÜÿ™ÿπŸÑŸÖ ŸÖÿπÿßÿØŸÑÿ© ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ± ŸÅŸäÿ£Ÿä digital system  ÿ®ÿ¥ÿØŸÑ ÿπÿßŸÖ ÿ™ÿ≥ÿßŸàŸäÿ£Ÿä Ÿàÿ®ÿπÿØ ÿØŸÑÿØÿß ŸÜÿ∑ÿ®ÿØŸáŸÑÿß ÿπŸÑŸÑŸâ 
ÿØŸÑ ŸÜŸàÿπ ŸÖ  ÿ£ŸÜŸàÿßÿπ ÿßŸÑŸÑ  Digital modulation .ŸàŸÜÿ¥ŸàŸÑ ÿßŸÑŸÜÿßÿ™ÿ¨ ŸÅŸä ÿØŸÑ ÿ≠ÿßŸÑÿ© 
 
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÑŸä ŸàÿßÿµŸÑÿ© ŸÑŸÑŸÑ  receiver    ÿ≥Ÿàÿßÿ° ÿØÿßŸÜÿ™ÿ•ÿ¥ÿßÿ±ÿ©  ŸÖÿπÿ®ÿ±ÿ© ÿπ  ÿßŸÑÿµŸÅÿ± ÿßŸà ŸÖÿπÿ®ÿ±ÿ© ÿπ  ÿßŸÑŸàÿßÿ≠ŸÑÿØ ŸáŸÑŸä  ÿ•ÿ¥ŸÑÿßÿ±ÿ©  ŸÖÿ∂ŸÑÿßŸÑ ÿßŸÑŸäŸáŸÑÿß 
noise ŸàÿØŸÜÿß ŸÅÿ±ÿ∂ŸÜÿß ÿß  ÿßŸÑŸÑ  noise  ŸáŸä AWGN Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÑ probability density function  ÿßŸÑÿÆÿßÿµŸÑÿ©ÿ®ÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿ© ÿßŸÑŸÖÿ≥ŸÑÿ™ÿØÿ®ŸÑÿ© 
ŸáŸä Gaussian  ŸÑŸáÿß mean  Ÿà variance .ÿ®ÿØŸäŸÖÿ© ŸÖÿπŸäŸÜŸá 
 
ÿßŸÑÿ±ÿ≥ŸÖ ÿßÿßŸÑÿ™Ÿä ŸáŸà ÿßŸÑŸÑ probability density function ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπ  ÿßÿ≥ÿ™ÿØÿ®ÿßŸÑ ÿßŸÑŸàÿßÿ≠ÿØ Ÿàÿßÿ≥ÿ™ÿØÿ®ÿßŸÑ ÿßŸÑÿµŸÅÿ± Ÿà ÿßŸÅÿ™ÿ±ÿ∂ŸÜÿß ÿß  ÿßŸÑŸÑ  
threshold = 0   Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÑ receiver  ŸáŸäÿ≥ÿ™ÿØÿ®ŸÑÿ£Ÿä ŸÇŸäŸÖÿ© ÿßÿØÿ®ÿ± ŸÖ  ÿµŸÅÿ± ÿßŸÜŸáÿß 1   ,ŸàÿßŸä ŸÇŸäŸÖÿ© ÿßŸÇŸÑ ŸÖ  ÿµŸÅÿ± ÿßŸÜŸáÿß.0   
 
 
 
 
   
 
 
 
 
 
 
 
 
 ÿßÿ´ÿ®ÿßÿ™ ŸÇÿßŸÜŸà  ÿßŸÑŸÑ probability of bit error  ŸÖÿ® ÿπŸÑŸäŸÜÿß ŸàŸÑÿØ  ŸÖŸáŸÖ ŸÜŸÅŸáŸÖ ÿßŸÑÿØÿßŸÜŸà  ŸàŸÜÿπÿ±ŸÑ ÿØŸÑ parameter   ŸÅŸäŸá Ÿàÿ™ÿ±ÿ´Ÿäÿ±Ÿá ÿπŸÑŸâ
ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ±  
 
The probability of  bit error (bit error rate) is given by :  
 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú 
 
ùëÅùëú‚Üí power spectral density of noise (Watt/Hz)  
ùê∏‚Üí average energy per bit  
ùê∏=ùê∏1+ùê∏2
2 
ùê∏1‚Üí Energy of binary 1                                                         ùê∏2‚Üí Energy of binary 0  
Probability of error (ùíëùíÜ) 
 ÿßÿ≠ÿ™ŸÖÿßŸÑŸäÿ© ÿßÿ±ÿ≥ÿßŸÑ0  ÿ∫ŸÑÿ∑ ÿßÿ≠ÿ™ŸÖÿßŸÑŸäÿ© ÿßÿ±ÿ≥ÿßŸÑ 1  ÿ∫ŸÑÿ∑ Probability density 
function for zero 
binary received  Probability density 
function for one 
binary received  
Threshold  
ùë¶ùëú 
 
 
ùê∏
 
 ŸáŸä
average
 
 ÿßŸÑŸÑ
Energy
  
  Ÿà
ùëÅ
ùëú
 
 ŸáŸä ÿßŸÑŸÑ
PSD
 
 ŸÑŸÑŸÑ
noise
  
 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÖÿØ  ŸÜÿπÿ™ÿ®ÿ±
 
ÿß  
  
ùê∏
 
 
ùëÅ
ùëú
 
  ŸáŸä
SNR
 
 
 ŸÖ  ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑÿ≥ÿßÿ®ÿØÿ© Ÿàÿßÿ∂ÿ≠ ÿß  ÿßŸÑŸÑ
probability of error
  
 ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ
SNR
  
 ŸàÿØŸÑŸÖÿß ÿ≤ÿßÿØÿ™ ŸÇŸÑ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ± ÿ≠Ÿäÿ´ ÿß
 
ŸÅŸä 
  
ùëíùëüùëìùëê
(
ùë•
)
 
  ÿØŸÑŸÖÿß ÿ≤ÿßÿØ
ùë•
 
 ÿØŸÑŸÖÿß ŸÇŸÑ
 
ùëíùëüùëìùëê
(
ùë•
)
  ŸàÿßŸÑÿπÿØÿ≥
 
 
ùúå
‚Üí
 
cross correlation between 
ùë†
1
(
ùë°
)
 
 
and  
ùë†
2
(
ùë°
)
 
 
ùë†
1
(
ùë°
)
‚Üí
 
signal express digit 1
 
ùë†
2
(
ùë°
)
‚Üí
 
signal express digit 0
 
 
 
 ŸÅÿØÿ±ÿ© ÿπŸÖŸÑ ÿßŸÑŸÑ
matched filter
 
 ŸáŸà
ÿ•Ÿäÿ¨ÿßÿØ 
 
ÿ™ÿ¥ÿßÿ®Ÿá ŸÖÿß ÿ®Ÿä  ÿßŸÑŸÑ 
noisy signal
 
Ÿà
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©
 
ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπ  
1
 
binary
 
 ŸÖ  ÿ∫Ÿäÿ±
noise
 
   ŸàÿßÿßŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπ
0
  
binary
  
  ŸÖ  ÿ∫Ÿäÿ±
noise
  
 ŸàÿßŸÑŸÑŸä ŸÇŸäŸÖÿ© ÿßŸÑÿ™ÿ¥ŸÑÿßÿ®Ÿá ÿ®ÿ™ÿßÿπÿ™ŸáŸÑÿß ÿßÿπŸÑŸÑŸâ ŸáŸÑŸä ÿØŸä ÿßŸÑŸÑŸÑ
bit
 
 ÿßŸÑŸÑŸÑŸä ÿØÿßŸÜŸÑÿ™ ŸÖÿ®ÿπŸàÿ™ŸÑŸá
,Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸà ÿØÿßŸÜÿ™ ÿßÿßŸÑÿ¥ÿßÿ±ÿ™Ÿä  
ùë†
1
(
ùë°
)
 
 Ÿà
ùë†
2
(
ùë°
)
 
 ÿ¥ÿ®Ÿá ÿ®ÿπÿπ ÿßŸÑŸÑŸÑ
matched filter
 
 ŸÖŸÑÿ® ŸáŸäÿØŸÑÿØÿ± Ÿäÿ≠ŸÑÿØÿØ ÿßŸÑŸÑŸÑ
noisy signal
 
 ÿØŸä
ÿØÿß  ÿßÿµŸÑŸáÿß 
ÿ£Ÿä
 
ÿßŸÑ  ŸÇŸäŸÖÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ŸÖÿπÿßŸáŸÖ ÿ™ÿØÿ±Ÿäÿ®ÿß 
ŸÖÿ™ÿ≥ÿßŸàŸäÿ©
 
 
ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ŸÖ  ÿØÿØÿß ÿß   
ÿ¥ÿØŸÑ  
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿßÿ™ 
  
ŸäŸÅÿ±ŸÇ ŸÅŸä ŸÖÿπÿØŸÑ ÿßŸÑÿÆÿ∑ÿ± 
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπÿ¥ÿß  ŸÜÿ≠ÿµŸÑ ÿπŸÑŸÑŸâ ŸÖÿπŸÑÿØŸÑ ÿÆÿ∑ŸÑÿ± ÿßŸÇŸÑŸÑ ÿßŸÑÿ≤ŸÖ 
ÿßÿ•ŸÑÿ¥ŸÑÿßÿ±ÿßÿ™ 
 
ÿ™ÿØŸà  ŸÖÿÆÿ™ŸÑŸÅÿ© 
ŸäÿπŸÜŸä
 
ÿßŸÑŸÑ
 
cross correlation 
 
 ÿ®ŸäŸÜŸáŸÖ ÿßŸÇŸÑ ŸÖÿß ŸäÿØŸà
 
ÿØÿØÿß ŸÅŸáŸÖŸÜÿß ŸÑŸäŸá ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä ÿßŸÑÿØÿßŸÜŸà  
(
1
‚àí
ùúå
)
 
 ŸÅŸä ÿßŸÑÿ®ÿ≥ÿ∑ ÿ®ÿ≠ŸäŸÑÿ´ ÿØŸÑŸÑ ŸÖŸÑÿß ÿßŸÑŸÑŸÑ
cross correlation
 
 Ÿäÿ®ÿØŸÑŸâ ÿßÿµŸÑÿ∫ÿ± ÿØŸÑŸÖŸÑÿß ÿ≤ÿßÿØÿ™
ÿßŸÑÿØŸäŸÖÿ© ÿØÿßÿÆŸÑ 
ùëíùëüùëìùëê
 
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÇŸÑ ŸÖÿπÿØŸÑ ÿßŸÑÿÆÿ∑ÿ±
 
 
The probability of error decreases as the average energy increases, the correlation decreases, or the 
noise power decreases.
 
 Ÿáÿ∞Ÿá ÿßŸÑÿπÿßŸÑŸÇÿ© ŸÖŸÜÿ∑ÿØŸäŸá ÿ¨ÿØÿß, ŸÅŸÖ  ÿßŸÑŸÖŸÜÿ∑ÿØŸä ÿπŸÜÿØ ÿ≤ŸäÿßÿØÿ© ÿßŸÑŸÑ
power
 
 ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä
ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© 
  
ŸàŸÜÿØÿµ ÿßŸÑŸÑ 
power
 
  ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÑ
noise
 
 ÿß  ÿ™ÿØŸÑ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑÿÆÿ∑ÿ±, ŸàÿØÿ∞ŸÑŸÉ ÿπŸÜÿØŸÖÿß ÿ™ÿØŸÑ ÿßŸÑŸÑ
correlation
 
 ŸÖÿß ÿ®Ÿä  ÿßÿßŸÑÿ¥ÿßÿ±ÿ™Ÿä  ÿß  ŸäÿØŸÑ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ± ÿßŸÑ  ÿ∞ŸÑŸÉ
ŸäÿπŸÜŸä ÿß  ÿßÿßŸÑÿ¥ÿßÿ±ÿ™Ÿä  ŸÖÿÆÿ™ŸÑŸÅÿ™Ÿä  ŸÅŸä ÿßŸÑÿ¥ÿØŸÑ ŸàÿßŸÑŸÑ 
receiver
 
 .ŸÑŸÖ ŸäÿÆÿ∑ÿ¶ ŸÅŸä ÿßŸÑÿ™ŸÖŸäŸäÿ≤ ŸÖÿß ÿ®ŸäŸÜŸáŸÖ
 
 
threshold
 
ÿØÿ≠ÿßŸÑÿ© ÿπÿßŸÖŸá ÿßŸÑ Ÿäÿ¥ÿ™ÿ±ÿ∑ ÿß  ŸäÿØŸà  ÿµŸÅÿ± ŸàŸÇŸäŸÖÿ™Ÿá ÿ®ÿπÿØ ÿßÿßŸÑÿ´ÿ®ÿß
ÿ™ 
: 
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
 
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
 
 
 
 
 
 
 
 
 
 
,
 
 
 
 
 
ùëá
ùëè
0
 
 
ùê∏
2
=
‚à´
ùë†
2
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
 
 
ùúå
=
1
ùê∏
‚à´
ùë†
1
(
ùë°
)
 
ùë†
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
 
 
ùúå
 
  ÿ™ÿ™ÿ±ÿßŸàÿ≠ ŸÖ
1
 
 ÿßŸÑŸâ
-
1
 
  ÿ≠Ÿäÿ´
1
 
ŸáŸä ÿßÿØÿ®ÿ± ŸÇŸäŸÖÿ© Ÿàÿ™ÿØŸÑ ÿπŸÑŸâ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™ÿßŸÖ ÿ®Ÿä  ÿßÿßŸÑÿ¥ÿßÿ±ÿ™Ÿä  Ÿà
-
1
  
  ÿ™ÿØŸÑ ÿß  ÿßÿßŸÑÿ¥ÿßÿ±ÿ™Ÿä  ÿπÿØÿ≥ ÿ®ÿπÿπ
 
 
 
 
 
 
 
 
  ÿÆÿ∑Ÿàÿßÿ™ ÿ≠ŸÑ ÿßŸÑŸÖÿ≥ÿßÿ¶ŸÑ 
 
 
1.  ÿ±ÿ≥ŸÖ ÿ±ÿ≥ŸÖÿ© Binary Matched Filter  ÿØÿßŸÖŸÑŸá 
 
2.   ŸÜÿ¨Ÿäÿ® ŸÖÿπÿßÿØŸÑÿ© ùë†1(ùë°)    Ÿà ùë†2(ùë°)  ŸàŸÜÿ≠ÿ∑ŸáŸÖ ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÖ  ÿßŸà ŸÜÿ≥Ÿäÿ®ŸáŸÖ ÿØÿ±ŸÖŸàÿ≤ ŸàŸÜÿ≠ÿ∑ ÿßŸÑŸÖÿπÿßÿØÿßŸÑÿ™ ÿ™ÿ≠ÿ™ ÿßŸÑÿ±ÿ≥ŸÑŸÖ ŸàŸÜÿ¨ŸäŸÑÿ® ŸÇŸäŸÖŸÑÿ© 
ùëáùëè  ŸàŸÜÿ≠ÿ∑Ÿáÿß ŸÅŸä ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ÿØÿßŸÖŸÑ ÿßŸÑŸÑŸä ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÖ  
 
3.  ŸÜÿ≠ÿ≥ÿ®ùê∏1  Ÿàùê∏2  ŸàŸÖŸÜŸáÿß ŸÜÿ¨Ÿäÿ® ÿßŸÑŸÑùê∏ 
ùê∏1=‚à´ùë†12(ùë°) ùëëùë°    ,       ùê∏2=‚à´ùë†22(ùë°) ùëëùë°    ,    ùê∏=ùê∏1+ùê∏2
2 
 
4.  ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸÑùúå cross correlation  
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†2(ùë°) ùëëùë° 
 
5.  ŸÜÿ≠ÿ≥ÿ® Probability of error  
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú 
 
 
 
 
 
Given the two -baseband signals shown in the following figure. Assume that white Gaussian 
noise of power spectral density =ùüèùüé‚àíùüë W/Hz  is added  
 
 
 
 
 
i. Design a binary matched filter to choose between these baseband signals.  
ii. Find the probability of error  
Solution  
Example  1 

 
 ŸÅŸä
ÿ£Ÿä 
 
ÿ≥ÿ§ÿßŸÑ ŸÑŸà ÿ∑ŸÑÿ® 
design matched filter
 
   ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ŸáŸà ÿ±ÿ≥ŸÖ ÿßŸÑÿ±ÿ≥ŸÖÿ© ÿØÿßŸÖŸÑŸá ŸàÿØÿ™ÿßÿ®ÿ© ÿßŸÑŸÖÿπÿßÿØÿßŸÑÿ™ ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÖ ÿßŸà ŸÖŸÖÿØ
ŸÜÿØÿ™ÿ® ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÖÿ©  
  
ùë†
1
(
ùë°
)
  Ÿà
ùë†
2
(
ùë°
)
  
 Ÿàÿ™ÿ≠ÿ™ ÿßŸÑÿ±ÿ≥ŸÖÿ© ŸÜÿØÿ™ÿ® ÿßŸÑŸÖÿπÿßÿØÿßŸÑÿ™
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
1
 
 
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
8
 
msec
 
ùë†
2
(
ùë°
)
=
{
1
 
 
 
 
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
4
 
msec
‚àí
1
 
 
 
 
4
msec
‚â§
ùë°
‚â§
8
 
msec
 
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
0
=
‚à´
1
2
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
0
=
8
√ó
1
0
‚àí
3
 
Watt
.
sec
 
ùê∏
2
=
‚à´
1
2
 
ùëëùë°
4
 
ùëöùë†ùëíùëê
0
+
‚à´
(
‚àí
1
)
2
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
4
 
ùëöùë†ùëíùëê
=
8
√ó
10
‚àí
3
 
Watt
.
sec
 
ùê∏
=
ùê∏
1
+
ùê∏
2
2
=
8
√ó
10
‚àí
3
 
Watt
.
sec
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
=
0
 
ùúå
=
1
ùê∏
‚à´
ùë†
1
(
ùë°
)
 
ùë†
2
(
ùë°
)
 
ùëëùë°
=
1
8
√ó
10
‚àí
3
 
[
‚à´
1
 
ùëëùë°
4
 
ùëöùë†ùëíùëê
0
+
‚à´
‚àí
1
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
4
 
ùëöùë†ùëíùëê
]
=
0
 
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê∏
 
(
1
‚àí
ùúå
)
2
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
‚àö
8
√ó
10
‚àí
3
2
 
√ó
10
‚àí
3
=
0
.
5
 
ùëíùëüùëìùëê
(
2
)
=
2
.
34
√ó
10
‚àí
3
 
 
 
 
 
 
‚à´
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
0
 
‚à´
 
ùëëùë°
8
 
ùëöùë†ùëíùëê
0
 
ùë¶
<
0
 
 
 
 
 
 
 
 
 
0
 
ùë¶
>
0
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
=
0
 
 
 
ùëá
ùëè
=
8
 
ùëöùë†ùëíùëê
 
‚àë
 
 
Design a 
binary matched filter 
detector for the two signals shown below. 
Assume that white 
Gaussian noise is added
 
ùëµ
ùíê
=
ùüé
.
ùüè
 
ùêñùêöùê≠ùê≠
/
ùêáùê≥
. Find the probability of error.
 
 
 
 
 
 
 
 
Solution
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
sin
2
ùúãùë°
 
ùë†
2
(
ùë°
)
=
{
1
 
 
 
 
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
0
.
5
‚àí
1
 
 
 
 
0
.
5
‚â§
ùë°
‚â§
1
 
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
1
0
=
‚à´
(
sin
2
ùúãùë°
)
2
 
ùëëùë°
1
0
 
=
1
2
‚à´
[
1
‚àí
cos
4
ùúãùë°
]
 
ùëëùë°
1
0
=
1
2
[
‚à´
1
 
ùëëùë°
1
0
‚àí
‚à´
cos
4
ùúãùë°
 
ùëëùë°
1
0
]
 
=
1
2
[
 
ùë°
|
0
1
‚àí
 
ùë†ùëñùëõ
(
4
ùúãùë°
)
4
ùúã
|
0
1
]
 
=
1
2
[
1
‚àí
0
]
=
1
2
 
Watt
.
sec
 
ùê∏
2
=
‚à´
ùë†
2
2
(
ùë°
)
 
ùëëùë°
1
0
=
‚à´
1
2
 
ùëëùë°
0
.
5
0
+
‚à´
(
‚àí
1
)
2
 
ùëëùë°
1
0
.
5
=
1
 
 
Watt
.
sec
 
Example
 
2
 
ùíî
ùüê
(
ùíï
)
 
ùíî
ùüè
(
ùíï
)
 
‚à´
 
 
ùëëùë°
1
0
 
‚à´
 
 
ùëëùë°
1
0
 
ùë¶
ùëú
=
‚àí
0
.
25
 
 
 
T
b
=
1
 
ùë†ùëíùëê
 
ùë¶
<
‚àí
0
.
25
 
 
 
 
 
 
 
 
0
 
ùë¶
>
‚àí
0
.
25
 
 
 
 
 
 
 
 
 
1
 
‚àë
 
ùê∏=ùê∏1+ùê∏2
2=0.5+1
2=3
4 Watt .sec 
ùë¶ùëú=0.5‚àí1
2=‚àí0.25 Watt .sec 
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†2(ùë°) ùëëùë°ùëáùëè
0=4
3 [‚à´ 1√ósin2ùúãùë° ùëëùë°0.5
0+‚à´‚àí1√ósin2ùúãùë° ùëëùë°1
0.5] 
=4
3 [‚àí ùëêùëúùë†(2ùúãùë°)
2ùúã|
00.5
+ ùëêùëúùë†(2ùúãùë°)
2ùúã|
0.51
]=4
3√ó1
2ùúã [‚àí(‚àí1‚àí1)+ (1+1)]=4
6ùúã(4) 
=16
6ùúã=0.85 
 ŸÑŸà ÿßŸÑÿ≠ÿ∏ŸÜÿßŸÖ  ÿßŸÑÿ±ÿ≥ŸÖ ŸáŸÜÿßŸÑŸÇŸä ÿß    ùë†1(ùë°)   Ÿà ùë†2(ùë°)     Ÿäÿ¥ÿ®ŸáŸàÿß ÿ®ÿπÿπ ŸàÿØÿß ÿ≥ÿ®ÿ® ÿßùúå   ŸÇŸäŸÖÿ™Ÿáÿß ŸÇÿ±Ÿäÿ®Ÿá ŸÖ 1   Ÿàÿ∑ÿ®ÿπÿß ÿßŸÑŸÑ system  
ÿØÿß ÿ∫Ÿäÿ± ÿØŸÅÿ¶.  
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àö0.75 (1‚àí0.85)
2√ó 0.1=0.5 ùëíùëüùëìùëê (0.75)=0.5√ó.28884 =0.14442  
 
 
 
 
Is one in which amplitude of carrier is modulated by digital signal  
ÿßŸÑŸÑŸÑŸÑ Baseband Signal   ÿØŸäÿ•ÿ¥ŸÑŸÑÿßÿ±ÿ© ÿ®ÿ™ŸÑŸÑÿ±ÿØÿØÿßÿ™ ŸÖŸÜÿÆŸÅÿ∂ŸÑŸÑÿ© ÿßŸÑ ÿ™ÿµŸÑŸÑŸÑÿ≠ ÿßŸÜŸáŸÑŸÑÿß ÿ™ÿ™ÿ®ÿπŸÑŸÑÿ™ ÿπŸÑŸÑŸÑŸâ ÿßŸÑŸÑŸÑŸÑ channel  ŸàÿßŸÑŸÖŸÅŸÑŸÑÿ±Ÿàÿπ ŸÜÿπŸÖŸÑŸÑŸÑ
modulation    ŸÖ  ÿÆÿßŸÑŸÑ ÿßŸÜŸÜÿß ŸÜÿ≠ŸÖŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿπŸÑŸâ  carrier   ÿ®ÿ™ÿ±ÿØÿØ ÿπÿßŸÑŸä Ÿàÿ®ŸäŸÑÿ™ŸÖ ÿßŸÑÿ™ÿπŸÑÿØŸäŸÑ ÿπŸÑŸÑŸâ ÿßÿ≠ŸÑÿØÿßŸÑŸÑŸÑ Parameter  ÿ®ÿ™ÿßÿπŸÑÿ©ÿßŸÑŸÑŸÑ 
carrier ÿ∑ÿ®ÿØÿß ŸÑŸÑŸÑ information signal. ŸÅŸä ÿßŸÑŸÜŸàÿπ ÿßŸÑŸÑ Parameter  ÿßŸÑŸÑŸä ŸáŸäÿ™ŸÖ ÿßŸÑÿ™ÿπÿØŸäŸÑ ŸÅŸä ÿßŸÑŸÑ Amplitude 
 
ASK   Ÿäÿ¥ÿ®Ÿá ÿßŸÑŸÑ Amplitude Modulation   ÿßŸÑŸÑŸä ÿØÿ±ÿ≥ŸÜÿßŸá ŸÇÿ®ŸÑ ÿØÿØÿß ÿ≠Ÿäÿ´ ÿß  ŸÅŸäASK  ÿßŸÑÿ™ÿπÿØŸäŸÑ ÿ®Ÿäÿ™ŸÖ ŸÅŸä ÿßŸÑŸÑ amplitude   ÿ®ÿ™ÿßÿπ ÿßŸÑŸÑ
carrier  ÿ™ÿ®ÿπÿßŸã ŸÑŸÑŸÑ information signal. 
 
 ÿπÿ¥ÿß  ŸÜÿπŸÖŸÑ ÿØŸäÿ≤ÿßŸä  ÿßŸÑŸÑ matched filter  ŸÑŸÑŸÑFSK     ŸáŸÜÿ≠ÿ™ÿß  ŸÜÿ¨Ÿäÿ® ŸÖÿπÿßÿØŸÑÿ™Ÿä ùë†1(ùë°)    Ÿà ùë†2(ùë°)    ÿ≠Ÿäÿ´ ÿß ùë†1(ùë°)   ŸáŸä ÿßŸÑŸÑ
modulated signal   ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπ binary 1   Ÿà ùë†2(ùë°)  ŸáŸä ÿßŸÑŸÑ modulated signal   ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπ binary 0 
 
  ŸÜŸÅÿ™ÿ±ÿπ ÿß  ÿßÿ≠ŸÜÿß ÿπŸÜÿØŸÜÿß ùëìùëê(ùë°) carrier signal   ÿπÿ®ÿßÿ±ÿ© ÿπÿ•ÿ¥ÿßÿ±ÿ©  sinusoidal  : ÿ®ÿ™ÿ±ÿØÿØ ÿπÿßŸÑŸä 
 
Assume carrier signal   ùëìùëê(ùë°)=ùê¥
2cos(2ùúãùëìùëêùë°) 
 Amplitude Shift Keying (ASK)  
 
ùëë(ùë°)  ŸáŸä ÿßŸÑŸÑ information signal  ŸàÿØŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÑŸä ÿπÿßŸäÿ≤ ÿßÿ≠ŸÖŸÑŸáÿß ÿπŸÑŸâ carrier ŸàŸÑŸäŸáÿß ŸÇŸäŸÖÿ™Ÿä , ŸÇŸäŸÖÿ©   ùêµ   ÿπŸÜÿØ1 digital 
  ŸàŸÇŸäŸÖÿ©‚àíùêµ  ÿπŸÜÿØ0 digital   
Assume digital signal   ùëë(ùë°)={ùêµ   digital  1
‚àíùêµ   digital  0 
 
ùëÜùê¥ùëÜùêæ =(ùê¥
2+ùëë(ùë°))cos(2ùúãùëìùëêùë°)=(ùê¥
2¬±ùêµ)cos(2ùúãùëìùëêùë°) 
=ùê¥
2(1¬±2ùêµ
ùê¥)cos(2ùúãùëìùëêùë°)=ùê¥
2(1¬±ùëö)cos(2ùúãùëìùëêùë°) 
 
ùë†1(ùë°)=ùê¥
2(1+ùëö)cos(2ùúãùëìùëêùë°) 
ùë†2(ùë°)=ùê¥
2(1‚àíùëö)cos(2ùúãùëìùëêùë°) 
 
ùëö (modulation index) =ùêµ
ùê¥2‚ÅÑ=signal  amplitude  
carrier  amplitude   
 
 ŸäŸàÿ¨ÿØ ŸÇŸäŸÖÿ™Ÿä  ŸÅÿØÿ∑ ŸÑŸÑŸÑ amplitude , ŸÇŸäŸÖÿ© ÿπŸÜÿØ digital 0  Ÿàÿ™ÿ≥ÿßŸàŸä  ùê¥
2(1‚àíùëö)   ŸàŸÇŸäŸÖÿ© ÿπŸÜÿØ digital 1   Ÿàÿ™ÿ≥ÿßŸàŸäùê¥
2(1+ùëö)  
 
 
 
 
 
digital  1 
digital  0 
 
 
Is special type of ASK in which ùëö=1 
 
  ÿ≠ÿßŸÑÿ© ÿÆÿßÿµÿ© ŸÖASK    ÿπŸÜÿØŸÖÿß ŸäÿØŸà modulation index ùëö=1   Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπŸÜÿØ ÿßÿ±ÿ≥ÿßŸÑ digital 1    ÿ™ÿØŸà  ŸÇŸäŸÖÿ© ÿßŸÑŸÑ
ÿ™ÿ≥ÿßŸàŸä modulated signal ùê¥  ,ŸàÿπŸÜÿØ ÿßÿ±ÿ≥ÿßŸÑ digital 0  ÿ™ÿØŸà  ŸÇŸäŸÖÿ© ÿßŸÑŸÑ modulated signal  ÿ™ÿ≥ÿßŸàŸä ÿµŸÅÿ± ŸàÿØÿß ÿ≥ÿ®ÿ®
ÿ™ÿ≥ŸÖŸäÿ™Ÿáÿß ÿ®ÿßŸÑŸÅÿ™ÿ≠ ŸàÿßŸÑÿ∫ŸÑŸÇ  On-off Keying  ŸàÿØÿ±ŸÜŸá  ÿ®Ÿä ÿØŸÅŸÑ ÿπŸÜÿØ  digital  0 Ÿàÿ®ŸäŸÅÿ™ÿ≠  ÿπŸÜÿØ  digital 1 
 
ùë†1(ùë°)=ùê¥cos(2ùúãùëìùëêùë°) 
ùë†2(ùë°)=0  
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
On-off Keying (OOK)  
1 1 1 0 
Carrier signal   ùíáùíÑ 
Modulated  signal   ùë∫ùë®ùë∫ùë≤ Modulating signal   ùíÖ(ùíï) 0 
 
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
)
 
ùë†
2
(
ùë°
)
=
0
 
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
‚à´
[
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
)
]
2
 
ùëëùë°
ùëá
ùëè
0
 
=
ùê¥
2
 
‚à´
1
2
[
1
+
ùëêùëúùë†
(
4
ùúã
ùëì
ùëê
ùë°
)
]
 
ùëëùë°
ùëá
ùëè
0
=
ùê¥
2
2
 
[
‚à´
1
 
ùëëùë°
ùëá
ùëè
0
+
‚à´
ùëêùëúùë†
(
4
ùúã
ùëì
ùëê
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
]
 
ùê¥
2
2
[
 
ùë°
|
0
ùëá
ùëè
+
 
ùë†ùëñùëõ
(
4
ùúã
ùëì
ùëê
ùë°
)
4
ùúã
ùëì
ùëê
|
0
ùëá
ùëè
]
=
ùê¥
2
2
 
[
 
ùëá
ùëè
+
 
0
]
 
‚à¥
ùê∏
1
=
ùê¥
2
2
 
ùëá
ùëè
 
 
ùê∏
2
=
‚à´
ùë†
2
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
0
 
 
ùê∏
=
ùê∏
1
+
ùê∏
2
2
=
 
[
ùê¥
2
2
 
ùëá
ùëè
+
0
2
]
=
ùê¥
2
4
 
ùëá
ùëè
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
=
[
ùê¥
2
2
 
ùëá
ùëè
‚àí
0
2
]
=
ùê¥
2
4
 
ùëá
ùëè
 
 
 
ùíî
ùë®ùë∫ùë≤
(
ùíï
)
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
 
 
Design and Performance
 
for OOK
 
‚àë
 
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†2(ùë°) ùëëùë°=0 
 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê¥2
4 ùëáùëè (1‚àí0)
2 ùëÅùëú 
‚à¥ ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê¥2 ùëáùëè 
8 ùëÅùëú 
 
 
 
  ÿßŸÑŸÖÿ≥ÿßÿ¶ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿØÿß Ÿáÿ™ÿØŸà  ÿπŸÑŸâ ÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿÆÿßÿµÿ© OOK  ŸàŸÅŸä ÿßŸÑÿ≠ŸÑ ŸáŸÜÿ±ÿ≥ŸÖ ÿßŸÑÿ±ÿ≥ŸÖÿ© ŸàŸÜÿ≠ŸÑÿ∑ ÿßŸÑŸÖÿπŸÑÿßÿØÿßŸÑÿ™ ŸàÿßŸÑÿØŸÑŸäŸÖ ÿπŸÑŸäŸáŸÑÿß ÿ≤Ÿä
ŸÖÿ´ÿßŸÑ ŸÜÿ≠ÿ≥ÿ®  ùëáùëè  ŸàŸÜÿ≠ÿ∑Ÿáÿß ŸÅŸä ÿ≠ÿØŸàÿØ ÿßŸÑÿ™ÿØÿßŸÖŸÑ ŸàÿßŸÑŸÖÿπÿßÿØÿßŸÑÿ™ ùë†1(ùë°)  Ÿà ùë†2(ùë°)   ŸÜÿØÿ™ŸÑÿ®ŸáŸÖ ÿπŸÑŸÑŸâ ÿßŸÑÿ±ÿ≥ŸÑŸÖ ÿßŸà ŸÜÿ≥ŸÑŸäÿ®ŸáŸÖ ÿØÿ±ŸÖŸÑŸàÿ≤ ŸàŸÑÿØŸÑ
ŸÜÿØÿ™ÿ®ŸáŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿ±ÿ≥ŸÖ 
ÿØŸä ÿßŸÑÿØŸàÿßŸÜŸä  ÿßŸÑŸÑŸä ÿßÿ≥ÿ™ŸÜÿ™ÿ¨ŸÜÿßŸáÿß ŸàŸáŸÜÿ≥ÿ™ÿÆÿØŸÖŸáÿß ŸÅŸä ÿ≠ŸÑ ÿßŸÑŸÖÿ≥ÿßÿ¶ŸÑ :  
 
ùë†1(ùë°)=ùê¥ùëêùëúùë†(2ùúãùëìùëêùë°) 
ùë†2(ùë°)=0 
ùê∏1=ùê¥2
2 ùëáùëè 
ùê∏2=0 
ùê∏=ùê∏1+ùê∏2
2=ùê¥2
4 ùëáùëè 
ùë¶ùëú=ùê∏1‚àíùê∏2
2=ùê¥2
4 ùëáùëè 
ùúå=0 
 ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê¥2 ùëáùëè 
8 ùëÅùëú 
 
 
 
 
 
 
 
 
 
Binary information is transmitted at 10 kbps using OOK. The carrier frequency is 10 MHz and 
the received carrier amplitude is 
ùüèùüé
‚àí
ùüê
 
volt. The additive noise power is 
ùüì
√ó
ùüèùüé
‚àí
ùüèùüé
  
Watt/Hz. 
Design a matched filter detector and find the probability of error
 
(bit error rate)
.
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
)
=
10
‚àí
2
 
ùëêùëúùë†
(
2
√ó
10
7
 
ùúãùë°
)
 
ùë†
2
(
ùë°
)
=
0
 
 
ùëá
ùëè
=
1
ùëÖ
ùëè
=
1
10
√ó
10
3
=
0
.
1
 
msec
 
 
 
ùê∏
1
=
ùê¥
2
2
 
ùëá
ùëè
=
(
10
‚àí
2
)
2
2
 
√ó
10
‚àí
4
=
5
√ó
10
‚àí
9
 
 
Watt
.
sec
 
ùê∏
2
=
0
 
 
ùê∏
=
ùê∏
1
+
ùê∏
2
2
=
2
.
5
 
√ó
10
‚àí
9
 
 
Watt
.
sec
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
=
2
.
5
 
√ó
10
‚àí
9
 
 
Watt
.
sec
 
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê¥
2
 
ùëá
ùëè
 
8
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
(
10
‚àí
2
)
2
 
√ó
10
‚àí
4
 
8
 
√ó
 
5
√ó
10
‚àí
10
=
0
.
5
 
ùëíùëüùëìùëê
(
1
.
58
)
 
 
=
0
.
5
 
√ó
0
.
02545
=
0
.
013
 
 
Example
 
3
 
‚à´
 
 
ùëëùë°
10
‚àí
4
0
 
‚à´
 
 
ùëëùë°
10
‚àí
4
0
 
ùíî
ùë®ùë∫ùë≤
(
ùíï
)
 
ùë¶
ùëú
=
2
.
5
√ó
10
‚àí
9
 
 
 
T
b
=
0
.
1
 
ùëöùë†ùëíùëê
 
ùë¶
<
2
.
5
√ó
10
‚àí
9
 
 
 
 
 
 
 
0
 
ùë¶
>
2
.
5
√ó
10
‚àí
9
 
 
 
 
 
 
 
 
1
 
‚àë
 
 
 
 
 
Explain how you can implement a matched filter . 
 
Solution  
 
 
 
for matched filter  
‚Ñé(ùë°)=ùêæ ùë†(ùëá‚àíùë°) 
‚à¥‚Ñé(ùë°‚àíùúè)=ùêæ ùë†(ùëá‚àíùë°+ùúè) 
 
ùë†ùëú(ùë°)+ùëõùëú(ùë°)=[ùë†(ùë°)+ùëõ(ùë°)]‚àó‚Ñé(ùë°) 
=‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùë°
0‚Ñé(ùë°‚àíùúè) ùëëùúè 
 
ùë†ùëú(ùë°)+ùëõùëú(ùë°)=ùëò‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùë°
0ùë†(ùëá‚àíùë°+ùúè) ùëëùúè 
 
At ùë°=ùëá 
ùë†ùëú(ùëá)+ùëõùëú(ùëá)=ùëò‚à´[ùë†(ùúè)+ùëõ(ùúè)]ùëá
0ùë†(ùúè) ùëëùúè 
 
 
 
 
 
 
 
 
 
 
 
  [Extra] Related Exams Questions  
Question 1 (Final 2016)  
 
‚à´  ùëëùë°ùëá
0 
 
Given the two baseband signals shown in the figure. Assume that white Gaussian noise of 
power spectral density 
=
ùüèùüé
‚àí
ùüë
 
W/Hz is added 
 
 
 
 
 
 
i.
 
Design a binary matched filter detector to choose between these baseband signals.
 
ii.
 
Find the probability of bit 
error
.
 
iii.
 
Redesign the matched filter detector if 
ùíî
ùíê
(
ùíï
)
=
ùüé
 
, then find the probability of bit error.
 
Solution
 
 
i.
 
Design a binary matched filter detector to choose between these baseband signals.
 
 
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
{
1
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
1
 
msec
‚àí
1
 
 
 
 
1
 
msec
‚â§
ùë°
‚â§
2
 
msec
1
 
 
 
 
2
 
msec
‚â§
ùë°
‚â§
3
 
msec
      
    
    
   
ùë†
0
(
ùë°
)
=
{
‚àí
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
1
 
msec
 
 
1
 
 
 
 
 
 
 
 
1
 
msec
‚â§
ùë°
‚â§
2
 
msec
 
 
 
‚àí
1
 
 
 
 
 
 
 
2
 
msec
‚â§
ùë°
‚â§
3
 
msec
 
 
ii.
 
Find the probability of bit error.
 
ùê∏
1
=
‚à´
1
2
 
ùëëùë°
1
 
ùëöùë†ùëíùëê
0
+
‚à´
(
‚àí
1
)
2
 
ùëëùë°
2
 
ùëöùë†ùëíùëê
1
 
ùëöùë†ùëíùëê
+
‚à´
1
2
 
ùëëùë°
3
 
ùëöùë†ùëíùëê
2
 
ùëöùë†ùëíùëê
 
 
ùë°
|
0
1
 
ùëöùë†ùëíùëê
+
 
ùë°
|
1
 
ùëöùë†ùëíùëê
2
 
ùëöùë†ùëíùëê
+
ùë°
|
2
 
ùëöùë†ùëíùëê
3
 
ùëöùë†ùëíùëê
=
[
(
1
‚àí
0
)
+
(
2
‚àí
1
)
+
(
3
‚àí
2
)
]
√ó
10
‚àí
3
 
=
3
 
√ó
10
‚àí
3
 
 
Watt
.
sec
 
 
‚à´
 
ùëëùë°
3
 
ùëöùë†ùëíùëê
0
 
‚à´
 
ùëëùë°
3
 
ùëöùë†ùëíùëê
0
 
ùë¶
<
0
 
 
 
 
 
 
 
 
 
0
 
ùë¶
>
0
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
=
0
 
 
 
ùëá
ùëè
=
3
 
ùëöùë†ùëíùëê
 
Question 2 
(Final 2018)
 
ùë†
0
(
ùë°
)
 
‚àë
 
 
ùê∏ùëú=‚à´ (‚àí1)2  ùëëùë°1 ùëöùë†ùëíùëê
0+‚à´ 12 ùëëùë°2 ùëöùë†ùëíùëê
1 ùëöùë†ùëíùëê+‚à´ (‚àí1)2  ùëëùë°3 ùëöùë†ùëíùëê
2 ùëöùë†ùëíùëê 
ùë°|01 ùëöùë†ùëíùëê+ ùë°|1 ùëöùë†ùëíùëê2 ùëöùë†ùëíùëê+ùë°|2 ùëöùë†ùëíùëê3 ùëöùë†ùëíùëê=[(1‚àí0)+(2‚àí1)+(3‚àí2)]√ó10‚àí3 
=3 √ó10‚àí3  Watt .sec  
 
ùê∏=ùê∏1+ùê∏0
2=3 √ó10‚àí3  Watt .sec 
ùë¶ùëú=ùê∏1‚àíùê∏0
2=0 
 
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†0(ùë°) ùëëùë°=1
3√ó10‚àí3 [‚à´ ‚àí1 ùëëùë°1 ùëöùë†ùëíùëê
0+‚à´ ‚àí1 ùëëùë°2 ùëöùë†ùëíùëê
1 ùëöùë†ùëíùëê+‚à´ ‚àí1 ùëëùë°3 ùëöùë†ùëíùëê
2 ùëöùë†ùëíùëê] 
1
3√ó10‚àí3[‚àíùë°|01 ùëöùë†ùëíùëê‚àí ùë°|1 ùëöùë†ùëíùëê2 ùëöùë†ùëíùëê‚àíùë°|2 ùëöùë†ùëíùëê3 ùëöùë†ùëíùëê]=10‚àí3
3√ó10‚àí3 [‚àí(1‚àí0)‚àí(2‚àí1)‚àí(3‚àí2)] 
‚à¥ùúå=‚àí3√ó10‚àí3
3√ó10‚àí3=‚àí1 
  ŸÇŸäŸÖÿ© ÿßŸÑŸÑùúå cross correlation  ÿ™ÿ≥ÿßŸàŸä-1    ŸàÿØŸä ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ŸÖŸÖÿØŸÜŸá ŸàÿØÿß ŸÖŸÜÿ∑ÿØŸä ÿßŸÑ  ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäÿπÿØÿ≥ ÿ®ÿπÿπ    
 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê ‚àö3 √ó10‚àí3(1+1)
2 √ó10‚àí3=0.5 ùëíùëüùëìùëê (1.73)=0.5√ó0.01442   
=7.21 √ó10‚àí3 
 
 
 
 
 
 
 
 
 
 
i.
 
Redesign the matched filter detector if 
ùë†
ùëú
(
ùë°
)
=
0
 
, then find the probability of bit error.
 
 
 
 
 
 
 
 
 
 
ùë†
0
(
ùë°
)
=
0
 
 
ùê∏
1
=
3
 
√ó
10
‚àí
3
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
 
ùê∏
ùëú
=
0
 
ùê∏
=
ùê∏
1
+
ùê∏
0
2
=
1
.
5
 
√ó
10
‚àí
3
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
0
2
=
1
.
5
 
√ó
10
‚àí
3
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
 
ùúå
=
1
ùê∏
‚à´
ùë†
1
(
ùë°
)
 
ùë†
0
(
ùë°
)
 
ùëëùë°
=
0
 
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê∏
 
(
1
‚àí
ùúå
)
2
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
‚àö
1
.
5
 
√ó
10
‚àí
3
(
1
‚àí
0
)
2
 
√ó
10
‚àí
3
=
0
.
5
 
ùëíùëüùëìùëê
(
0
.
87
)
=
0
.
5
√ó
0
.
21856
 
 
=
0
.
10928
 
 
 
 
 
 
 
 
 
‚à´
 
ùëëùë°
3
 
ùëöùë†ùëíùëê
0
 
‚à´
 
ùëëùë°
3
 
ùëöùë†ùëíùëê
0
 
ùë¶
<
1
.
5
√ó
10
‚àí
3
 
 
 
 
 
 
 
 
 
0
 
 
ùë¶
>
1
.
5
√ó
10
‚àí
3
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
 
=
1
.
5
√ó
10
‚àí
3
 
  
 
ùëá
ùëè
=
3
 
ùëöùë†ùëíùëê
 
ùë†
0
(
ùë°
)
=
0
 
‚àë
 
Given the two baseband signals shown in the figure. Assume
.
  
ùëµ
ùíê
=
ùüê
√ó
ùüèùüé
‚àí
ùüë
 
W/Hz is added 
 
 
 
 
                                                            
 
 
 
 
i.
 
Design a binary matched filter detector to choose between 
these baseband signals.
 
ii.
 
Find the probability of bit error.
 
Solution
 
 
Design a binary matched filter detector to choose between these baseband signals.
 
 
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
{
1
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
1
 
‚àí
2
 
 
 
 
1
 
‚â§
ùë°
‚â§
3
 
2
 
 
 
 
3
‚â§
ùë°
‚â§
4
 
      
    
    
   
ùë†
0
(
ùë°
)
=
{
1
 
 
 
 
 
 
 
 
0
‚â§
ùë°
‚â§
1
‚àí
1
 
 
 
 
 
 
 
1
‚â§
ùë°
‚â§
2
1
 
 
 
 
 
 
 
 
 
2
‚â§
ùë°
‚â§
3
‚àí
1
 
 
 
 
 
 
 
 
3
‚â§
ùë°
‚â§
4
 
 
Find the probability of bit error.
 
ùê∏
1
=
‚à´
1
2
 
ùëëùë°
1
0
+
‚à´
(
‚àí
2
)
2
 
ùëëùë°
2
1
+
‚à´
(
‚àí
2
)
2
 
ùëëùë°
3
 
2
 
+
‚à´
2
2
 
ùëëùë°
4
3
 
 
ùë°
|
0
1
 
+
 
4
ùë°
|
1
 
2
 
+
4
ùë°
|
2
 
3
 
+
4
ùë°
|
3
 
4
 
=
[
(
1
‚àí
0
)
+
(
8
‚àí
4
)
+
(
12
‚àí
8
)
+
+
(
16
‚àí
12
)
]
 
=
13
 
 
Watt
.
sec
 
 
Question 3 
(Final 2020)
 
 
 
‚à´
 
ùëëùë°
4
0
 
‚à´
 
ùëëùë°
4
0
 
ùë¶
<
4
.
5
 
 
 
 
 
 
 
 
 
0
 
ùë¶
>
4
.
5
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
=
4
.
5
 
 
 
ùëá
ùëè
=
4
 
ùë†ùëíùëê
 
ùë†
0
(
ùë°
)
 
‚àë
 
 
 
ùê∏ùëú=‚à´12 ùëëùë°1
0+‚à´(‚àí1)2 ùëëùë°2
1+‚à´12 ùëëùë°3 
2 +‚à´(‚àí1)2 ùëëùë°4
3 
 ùë°|01 + ùë°|1 2 +ùë°|2 3 +ùë°|3 4 =[(1‚àí0)+(2‚àí1)+(3‚àí2)+(4‚àí3)] 
=4  Watt .sec  
 
ùê∏=ùê∏1+ùê∏0
2=13+4
2=8.5  Watt .sec 
ùë¶ùëú=ùê∏1‚àíùê∏0
2=13‚àí4
2=4.5  Watt .sec  
 
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†0(ùë°) ùëëùë°=1
8.5 [‚à´1 ùëëùë°1
0+‚à´2 ùëëùë°2
1+‚à´‚àí2 ùëëùë°3 
2 +‚à´‚àí2 ùëëùë°4
3] 
1
8.5 [ùë°|01 + 2ùë°|1 2 ‚àí2ùë°|2 3 ‚àí2ùë°|3 4 ]=[(1‚àí0)+(4‚àí2)‚àí(6‚àí4)‚àí(8‚àí6)] 
‚à¥ùúå=1+2‚àí2‚àí2
8.5=‚àí0.118 
 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê ‚àö8.5 (1+0.118 )
2 √ó2√ó10‚àí3=0.5 ùëíùëüùëìùëê (48) 
 48  ŸÖÿ® ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä ÿßŸÑÿ¨ÿØŸàŸÑŸÅŸáŸÜÿ≥Ÿäÿ® ÿßŸÑŸÜÿßÿ™ ÿ¨ ÿ®ÿØÿßŸÑŸÑÿ©   ùëíùëüùëìùëê 
 
 
 
 
 
 
 
 
 
Binary information is transmitted at 100 kbps using OOK. The carrier frequency is 20
 
MHz and 
the received carrier amplitude is 
ùüè
ùüé
‚àí
ùüë
 
volt. The additive noise power is 
ùüè
ùüé
‚àí
ùüèùüê
 
Watt/Hz. Design 
a matched filter detector and find the probability of error.
 
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
ùëì
ùëê
=
20
 
ùëÄùêªùëß
 
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
 
 
 
 
ùê¥
=
10
‚àí
3
 
ùë£ùëúùëôùë°
 
 
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
 
ùëÅ
ùëú
=
10
‚àí
12
 
ùëäùëéùë°ùë°
/
ùêªùëß
 
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
)
=
10
‚àí
3
 
ùëêùëúùë†
(
4
√ó
10
7
 
ùúãùë°
)
 
ùë†
2
(
ùë°
)
=
0
 
 
ùëá
ùëè
=
1
ùëÖ
ùëè
=
1
100
√ó
10
3
=
10
‚àí
5
 
ùë†ùëíùëê
 
 
 
ùê∏
1
=
ùê¥
2
2
 
ùëá
ùëè
=
(
10
‚àí
3
)
2
2
 
√ó
10
‚àí
5
=
5
√ó
10
‚àí
12
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
ùê∏
2
=
0
 
 
ùê∏
=
ùê∏
1
+
ùê∏
2
2
=
2
.
5
 
√ó
10
‚àí
12
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
=
2
.
5
 
√ó
10
‚àí
12
 
 
ùëäùëéùë°ùë°
.
ùë†ùëíùëê
 
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê¥
2
 
ùëá
ùëè
 
8
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
(
10
‚àí
3
)
2
 
√ó
10
‚àí
5
 
8
 
√ó
10
‚àí
12
=
0
.
5
 
ùëíùëüùëìùëê
(
1
.
12
)
 
 
=
0
.
5
 
√ó
0
.
11321
=
0
.
06
 
Question 
4
 
(Final 2016)
 
‚à´
 
 
ùëëùë°
10
‚àí
5
0
 
‚à´
 
 
ùëëùë°
10
‚àí
5
0
 
ùíî
ùë®ùë∫ùë≤
(
ùíï
)
 
ùë¶
ùëú
=
2
.
5
√ó
10
‚àí
9
 
 
 
T
b
=
0
.
1
 
ùëöùë†ùëíùëê
 
ùë¶
<
2
.
5
√ó
10
‚àí
12
 
 
 
 
 
 
 
0
 
ùë¶
>
2
.
5
√ó
10
‚àí
12
 
 
 
 
 
 
 
 
1
 
‚àë
 
 
 
 
 
 
   
 
¬©
 
Basem Hesham
Serial Quantizer  & Parallel Quantizer & 
Quantization noise & Delta Modulation 
Lecture
 
6
 
 
Design a counting  ADC to convert ùíî(ùíï)=ùíîùíäùíè(ùüêùùÖùíï ) into a 4-bit signal.  
Choose appropriate parameter s for ADC.   
Solution  
 
 ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ùë†(ùë°)  ŸÑŸäŸáÿß ŸÇŸäŸÖÿ©max  ÿπŸÜÿØ1  ŸàŸÇŸäŸÖÿ© min  ÿπŸÜÿØ-1  , Ÿà ÿßŸÑŸÄ Counting Quantizer   ÿ®Ÿäÿ¥ÿ™ÿ∫ŸÑ
ÿπŸÜÿØ ŸÇŸäŸÖ ŸÖŸàÿ¨ÿ®ÿ© ŸÅŸÇÿ∑ ŸÅŸáŸÜÿπŸÖŸÑ shift ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ®ŸÖŸÇÿØÿßÿ±  1  ŸàÿßŸÑŸÄ range  ŸáŸäŸÉŸàŸÜ ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
Amplitude range  0‚Üí2 ùë£ùëúùëôùë°  
ùëìùëö=1 ùêªùëß 
ùëìùë†‚â•2 ùëìùëö    ,   ùëìùë†‚â•2 
 
Let us choose a rate 25% above the Nyquist  value   
ùëìùë†=2+0.25√ó2=2.5  samples /sec 
ùëáùë†=1
ùëìùë†=1
2.5=0.4 sec/sample  
 
The ramp must reach the maximum value with one sampling period  
Assume ùëáùëüùëéùëöùëù =ùëáùë†=0.4  
ramp  slope =2
0.4=5  ùë£ùëúùëôùë° /ùë†ùëíùëê 
 
0.4 ùë†                 24 
1  ùë†                     ùëìùëêùëôùëúùëêùëò  
 
ùëìùëêùëôùëúùëêùëò =16
0.4=40 counts /sec 
 
 
 
 Example  1 
 
 
 
Serial  Quantizer  ÿ™ÿßŸÜŸä ŸÜŸàÿπ ŸÖŸÄŸÄŸÜÿ£ŸÜŸÄŸÄŸàÿßÿπ PCM Modulator  ŸàŸáŸÄŸÄŸà ÿπÿ®ŸÄŸÄÿßÿ±ÿ© ÿπŸÄŸÄŸÜ ADC  Ÿäÿ≠ŸÄŸÄŸàŸÑÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© 
ÿßŸÑÿ™ŸÜÿßÿ∏ÿ±Ÿäÿ© ÿßŸÑŸâ ÿ±ŸÇŸÖŸäÿ©  ,Ÿà ŸÅŸÉÿ±ÿ© ÿπŸÖŸÑŸá  ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ  ÿ™ŸÇÿ≥ŸäŸÖ  ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ©  ÿßŸÑÿ™ŸÜÿßÿ∏ÿ±ŸäŸÄŸÄÿ© ÿßŸÑŸÄŸÄŸâ ŸÖÿ≥ŸÄŸÄÿ™ŸàŸäÿß  ŸÖŸÄŸÄŸÜ  0  ÿßŸÑŸÄŸÄŸâ  1 
  ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿπŸÖŸÑ normalization  ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ©  (ŸÜŸÇÿ≥ŸÖ  ÿπŸÑŸâ ÿßŸÉÿ®ÿ± ŸÇŸäŸÖÿ© ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ range  ŸÖŸÜ0 
ÿßŸÑŸâ 1)  ŸàŸÑŸà ŸÉÿßŸÜ  ŸÇŸäŸÖ ÿßÿßŸÑÿ¥ÿßÿ±ÿ© ÿ®ÿßŸÑÿ≥ÿßŸÑÿ® ŸáŸÜÿπŸÖŸÑshift  ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ŸÇŸäŸÖ ŸÖŸàÿ¨ÿ®ÿ©  ŸÖŸÜ  0  ÿßŸÑŸâ1 
 ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿß  Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿπÿØÿØÿßŸÑŸÄ bits  ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ© (ŸÑŸÄŸÄŸà ÿßÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖŸÜÿß3 bit ÿ≥ŸÜÿ≠ÿµŸÄŸÄŸÑ ÿπŸÑŸÄŸÄŸâ 8  ŸÖÿ≥ŸÄŸÄÿ™ŸàŸäÿß 
ŸàŸÑŸà ÿßÿ≥ÿ™ÿÆÿØŸÖŸÜÿß  4 bit  ÿ≥ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ 16 ŸÖÿ≥ÿ™ŸàŸâ  ŸàŸáŸÉÿ∞ÿß)  ŸàŸÉŸÑ ŸÖÿ≥ÿ™ŸàŸâ ŸÑŸäŸá ŸÉŸàÿØ ŸÖÿÆÿ™ŸÑŸÅ. 
Ÿàÿ™ÿ™ŸÖ ÿßŸÑÿπŸÖŸÑŸäÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ  comparators    ÿ≠Ÿäÿ´ Ÿäÿ™ŸÖ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâÿßŸÑŸÄ  bits    ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ÿ®ÿØÿßŸäŸÄŸÄÿ© ŸÖŸÄŸÄŸÜ 
MSB  ÿßŸÑŸâLSB  ÿ®ÿ¥ŸÉŸÑ ÿ™ÿ≥ŸÑÿ≥ŸÑŸäŸÉŸÖÿß ÿ≥ŸÜŸàÿ∂ÿ≠ ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿßÿßŸÑÿ™Ÿä.  
 
 
 
‚ñ™ The serial quantizer successively divides the ordinates into two regions.  
‚ñ™ It first divides the axis in half and observes whether the sample is in the upper or lower 
half. 
‚ñ™ The result of this observation generates the most significant bit in the code word.  
 
 
 
 ÿ≥Ÿäÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ŸÇŸäŸÖÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ÿßŸÑŸÄŸÄŸâ 8  ŸÖÿ≥ŸÄŸÄÿ™ŸàŸäÿß Ÿàÿ≥ŸÄŸÄŸÜÿ≠ÿ™ÿßÿß ÿßŸÑŸÄŸÄŸâ 3 comparator  ŸÅŸÇŸÄŸÄÿ∑ ÿπŸÜŸÄŸÄÿØ1
2  Ÿà1
4  Ÿà1
8  ŸàŸÖŸÄŸÄŸÜ
ÿÆÿßŸÑŸÑŸáŸÖ ŸáŸÜŸÇÿØÿ± ŸÜÿπÿ±ŸÅ  ŸÇŸäŸÖÿ©  ÿßŸÑŸÄ sample   ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸäÿ£Ÿä range ÿ®ÿßŸÑÿ∂ÿ®ÿ∑  
ŸÅŸä ÿßŸÑÿ®ÿØÿßŸäÿ© ŸáŸäÿ¥ŸàŸÅ ŸÇŸäŸÖÿ© ÿßŸÑŸÄ sample  ÿßŸÉÿ®ÿ± ŸÖŸÜ1
2  ŸàÿßŸÑ ÿ£ŸÑ ŸÑŸà ÿßŸÇŸÑ ŸáŸäÿ≠ÿ∑0 ŸÅŸä( ùëè1) MSB  ŸàŸÑŸà ÿßŸÉÿ®ÿ± ŸáŸäÿ≠ÿ∑
1    ŸàŸäÿ∑ÿ±ÿ≠1
2    Ÿàÿ®ÿπÿØ ŸÉÿØŸá ŸáŸäÿ¥ŸàŸÅ ÿßŸÑÿ®ŸÄŸÄÿßŸÇŸä ÿ®ÿπŸÄŸÄÿØ ÿ∑ŸÄŸÄÿ±ÿ≠ ÿßŸÑŸÄŸÄŸÜÿß ŸáŸäŸÉŸÄŸÄŸàŸÜ ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ1
4   ŸàÿßŸÑ ÿ£ŸÑŸà ŸÑŸÄŸÄŸà ÿßŸÇŸÄŸÄŸÑ ŸáŸÄŸÄŸäÿ≠ÿ∑ 
ùëè2=0    ŸàŸÑŸà ÿßŸÉÿ®ÿ± ŸáŸäÿ≠ÿ∑ ùëè2=1    ŸàŸäÿ∑ÿ±ÿ≠1
4   ŸàŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ© ŸáŸäÿ¥ŸàŸÅ ÿßŸÑÿ®ŸÄŸÄÿßŸÇŸä ŸáŸäŸÉŸÄŸÄŸàŸÜ ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ1
8  ŸàÿßŸÑ ÿßŸÑ
ŸÑŸà ÿßŸÇŸÑ ŸáŸäÿ≠ÿ∑ 0 ŸÅŸä( ùëè3) LSB  ŸàŸÑŸà ÿßŸÉÿ®ÿ± ŸáŸäÿ≠ÿ∑ ùëè3=1  (ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ∑ŸÄŸÄÿ±ÿ≠ ÿ™ŸÄŸÄÿ™ŸÖ ŸÑŸÖŸÄŸÄÿß Ÿäÿ™ÿ≠ŸÇŸÄŸÄŸÇ ÿßŸÑÿ¥ŸÄŸÄÿ±ÿ∑ ŸÅŸÇŸÄŸÄÿ∑
ŸäÿπŸÜŸä ŸÑŸÖÿß ÿßŸÑŸÇŸäŸÖÿ© ÿ™ÿ®ŸÇŸâ ÿßŸÉÿ®ÿ± ŸÖŸÜ ŸÑŸÉŸÜ ŸÑŸà ŸÉÿßŸÜ  ÿßŸÇŸÑ ŸáŸÜÿ≠ÿ∑  0   ŸàŸÜÿπÿØŸäŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ŸÜÿ∑ÿ±ÿ≠)  
Serial Quantizer  
Serial 3 -bit Quantizer  
 
  ÿßŸÑÿ¨ÿØŸàŸÑ ÿßÿßŸÑÿ™Ÿä ŸäŸàÿ∂ÿ≠ ÿßŸÑŸÖŸÉÿßŸÅÿ¶ÿßŸÑŸÄ binary  ŸÑŸÉŸÑ range ŸàŸÑŸà ÿ±ŸÉÿ≤ŸÜÿß ŸÅŸÄŸÄŸä ŸÇŸÄŸÄŸäŸÖ ÿßŸÑÿ¨ŸÄŸÄÿØŸàŸÑ ŸáŸÄŸÄŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ÿßÿ£ŸÑÿ±ŸÇŸÄŸÄÿßŸÖ 
ÿßÿ£ŸÑŸÉÿ®ÿ±  ŸÖŸÜ  1
2    ŸÇŸäŸÖÿ© MSB    ÿ™ÿ≥ÿßŸàŸä1  Ÿàÿßÿ£ŸÑÿ±ŸÇÿßŸÖ  ÿßÿ£ŸÑŸÇŸÑ  ŸÖŸÜ  1
2    ŸÇŸäŸÖÿ© MSB    ÿ™ÿ≥ÿßŸàŸä0  ,Ÿàÿßÿ£ŸÑÿ±ŸÇÿßŸÖ  ÿßÿ£ŸÑŸÇŸÑ  ŸÖŸÜ  1
4   ÿßŸà
ÿßÿ£ŸÑÿ±ŸÇÿßŸÖ ÿßŸÑŸÑŸä ŸÑŸà ÿ∑ÿ±ÿ≠ŸÜÿß ŸÖŸÜŸáÿß 1
2  ŸàŸÇŸäŸÖÿ™Ÿáÿß ÿßŸÇŸÑ ŸÖŸÜ1
4  ŸáŸÜÿßŸÑŸÇŸä ÿ™ÿßŸÜŸäbit  ŸÑŸäŸáÿß ÿ™ÿ≥ÿßŸàŸä 0  ŸàŸáŸÉŸÄŸÄÿ∞ÿß ŸÅŸÄŸÄŸä ÿ®ŸÄŸÄÿßŸÇŸä ÿßŸÑŸÇŸÄŸÄŸäŸÖ
,ŸàÿØÿß ÿ®Ÿäÿ≠ŸÇŸÇ ŸÅŸÉÿ±ÿ© ÿπŸÖŸÑ  Serial Quantizer ÿßŸÑŸÑŸä ÿ™ŸÉŸÑŸÖŸÜÿß ÿπŸÜŸáÿß 
 
 
 
ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ∑ÿ±ÿ≠ ÿ®ÿπÿØ ÿ™ÿ≠ŸÇŸÇ ÿßŸÑÿ¥ÿ±ÿ∑ ÿ®ÿπÿØ ŸÉŸÑ ŸÖŸÇÿßÿ±ŸÜÿ© ŸÜÿπÿ±ŸÅ ŸÜŸàÿµŸÑ  ŸÑŸÑŸÄ  range   ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿßŸÑŸÜ ŸÖÿ´ÿßŸÑ
ŸÑŸà ŸÉÿßŸÜ  ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÉÿ®ÿ± ŸÖŸÜ  1
2   Ÿàÿ®ÿπÿØ ŸÖÿß ŸÜÿ∑ÿ±ÿ≠ ŸÖŸÜŸáÿß1
2  ŸàŸÜŸÇÿßÿ±ŸÜŸáÿß ÿ®ŸÄ  1
4  ŸÉÿßŸÜŸÜÿß ÿ®ŸÜŸÇÿßÿ±ŸÜŸáÿß ÿ®ŸÄ 6
8 
(Value greater than 1
2 ) ‚àí1
2>1
4                     (Value greater than 1
2 ) >1
4+1
2=6
8  
 
 ŸàŸÑŸà ŸÉÿßŸÜ  ÿßŸÉÿ®ÿ± ŸÖŸÜ1
4   ŸáŸÜÿ∑ÿ±ÿ≠ ŸÖŸÜŸáÿß1
4   ŸàŸÜŸÇÿßÿ±ŸÜŸáÿßÿ® ŸÄ 1
8   ŸÉÿßŸÜŸÜÿß ÿ®ŸÜŸÇÿßÿ±ŸÜŸáÿßÿ® ŸÄ 7
8 
(Value greater than 6
8 ) ‚àí1
4‚àí1
2>1
8                (Value greater than 6
8 ) >1
4+1
2+1
8=7
8  
 
ÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÖÿ¥  ŸáŸÜÿ≠ÿ™ÿßÿß  ŸÜÿ≥ÿ™ÿÆÿØŸÖ  comparator    ÿπŸÜŸÄŸÄÿØ6
8    Ÿà7
8  ÿ£ŸÑŸÜŸÜŸÄŸÄÿß  ŸÖŸÖŸÉŸÄŸÄŸÜ ŸÜÿ¨ŸäŸÄŸÄÿ® ÿßŸÑŸÖŸÉŸÄŸÄÿßŸÅÿ¶ ÿ®ÿ∑ŸÄŸÄÿ±ÿ≠  1
2    ÿßŸà1
4   ŸÖŸÄŸÄŸÜ
ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÑŸä ÿπŸÜÿØŸä Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä Ÿáÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© 
ÿ®ÿßŸÑŸÖÿ´ŸÑ ŸÑŸà ÿπŸÜÿØŸÜÿß Serial 4-bit Quantizer  ŸáŸÜÿ≠ÿ™ÿßÿß 4 comparator  ÿπŸÜÿØ1
2  Ÿà1
4  Ÿà1
8  Ÿà1
16   ŸàŸÑŸÄŸÄŸà ŸÑŸÄŸÄŸà ÿπŸÜŸÄŸÄÿØŸÜÿß
Serial 5-bit Quantizer  ŸáŸÜÿ≠ÿ™ÿßÿß 5 comparator  ÿπŸÜÿØ1
2  Ÿà1
4  Ÿà1
8  Ÿà1
16  Ÿà1
32  ŸàŸÑŸÄŸÄŸà ÿπŸÜŸÄŸÄÿØŸÜÿß Serial 2-bit 
Quantizer   ŸáŸÜÿ≠ÿ™ÿßÿß 2 comparator   ÿπŸÜÿØ1
2  Ÿà1
4 ŸÅŸÇÿ∑ 
Number of comparators = number of bits  
ùíÉùüè 
MSB  ùíÉùüê ùíÉùüë 
LSB 
 
 
Illustrate the operation of the system of 3 -bit serial quantizer for the following 
two input sample values: 0.2 and 0.8  
Solution  
 
 
 
 
For 0.2 V  
0.2<1
2  
The first comparison with 1
2 would yield a NO answer. Therefor ùëè1=0 
0.2<1
4  
The second comparison with 1
4 would yield a NO answer. Therefor ùëè2=0 
0.2>1
8  
The third comparison with 1
8 would yield a YES answer. Therefor ùëè3=1 
‚à¥ùüé.ùüê‚Üíùüéùüéùüè  
 
For 0. 8 V 
0.8>1
2  
The first comparison with 1
2 would yield a YES answer. Therefor ùëè1=1 
0.8‚àí1
2>1
4‚Üí0.3>1
4  
The second comparison with 1
4 would yield a YES answer. Therefor ùëè2=1 
0.3‚àí1
4<1
8‚Üí0.05<1
8  
The third comparison with 1
8 would yield a NO answer. Therefor ùëè3=0 
‚à¥ùüé.ùüñ‚Üíùüèùüèùüé  Example  2 
 
 
 
‚ñ™ The parallel quantizer is the fastest operation since it develops all bits of the 
code words simultaneously.  
‚ñ™ It is also the most complex, requiring a number of comparators that is only one 
less than  the number of levels of quantization.  
‚ñ™ The block labeled "coder" observes the output of seven comparators.  
 
 ŸáŸà ÿßÿ≥ÿ±ÿπŸÖŸÜ ÿßŸÑŸÄ serial  ÿ£ŸÑŸÜŸá ÿ®ŸäÿÆÿ±ÿß ŸÉŸÑÿßŸÑŸÄ bits ŸÅŸä ŸàŸÇ  Ÿàÿßÿ≠ŸÄŸÄÿØÿå ŸàŸÑŸÉŸÜŸÄŸÄ Ÿá ÿßÿπŸÇŸÄŸÄÿØ more complex  ÿßŸÑŸÜŸÄŸÄŸá
ÿ®Ÿäÿ≠ÿ™ÿßÿß ÿπÿØÿØ comparators  ÿßŸÇŸÑ ŸÖŸÜ levels of quantization   ÿ®ŸÖŸÇÿØÿßÿ±1 
3-bit parallel quantizer  will use 23‚àí1=7 comparators  
 
ÿßŸÑŸÄ  input    ÿ®ŸäÿØÿÆŸÑ ÿπŸÑŸâ sampler    ŸÑÿπŸÖŸÑ sampling  Ÿàÿ®ÿπÿØ ŸÉÿØŸá  ŸÉŸÑ ŸÇŸäŸÖÿ© ÿ™ÿØÿÆŸÑ ÿπŸÑŸâ  comparator   ÿπÿ¥ŸÄŸÄÿßŸÜ
ÿ™ÿ¥ŸàŸÅ ÿßŸÑŸÇŸäŸÖŸÄŸÄÿ© ÿØŸä ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ ÿßŸÑŸÇŸäŸÖŸÄŸÄÿ© ŸÅŸÄŸÄŸä ÿßŸÑŸÄŸÄ ŸÄ comparator  ŸàÿßŸÑ ÿßŸÑ, ŸàŸÑŸÄŸÄŸà ÿßŸÉÿ®ŸÄŸÄÿ± ŸáŸäÿÆŸÄŸÄÿ±ÿß1  ŸàŸÑŸÄŸÄŸà ÿßÿµŸÄŸÄÿ∫ÿ±
ŸáŸäÿÆÿ±ÿß  0 ŸàŸÅŸä ÿßÿßŸÑÿÆÿ± ÿßÿßŸÑÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿ≠ÿßŸäÿØ ÿØŸä ÿ™ÿØÿÆŸÑ ÿπŸÑŸâ  coder Ÿàÿ™ÿ∑ŸÑÿπ  ÿßŸÑŸÄ code ÿßŸÑŸÜŸáÿßÿ¶Ÿä 
 
‚ñ™ If all seven outputs are 1,the coder output is 111 ,since the sample value had to be 
greater than 7/8  
 ŸÑŸà ŸÉÿßŸÜ ÿÆÿ±ÿß ŸÉŸÑÿßŸÑŸÄ comparators  Ÿäÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ ÿØÿß ŸÖÿπŸÜÿßŸá ÿßŸÜÿßŸÑŸÄ sample value  ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ7/8  Ÿàÿ™ŸÇŸÄŸÄÿπ 
ŸÅŸä ÿßŸÑŸÄ range  ŸÖŸÜ7/8 ÿßŸÑŸâ 1 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ output code  Ÿäÿ≥ÿßŸàŸä111 
 
‚ñ™ If comparator outputs 1 through 6 are 1 ,and output 7 are 0, the coder output is 110 
, since the sample had to be between 6/8 and 7/8  
 ŸÑŸÄŸÄŸà ŸÉŸÄŸÄÿßŸÜ ÿÆŸÄŸÄÿ±ÿßÿßŸÑŸÄŸÄ ŸÄ comparators  Ÿäÿ≥ŸÄŸÄÿßŸàŸä Ÿàÿßÿ≠ŸÄŸÄÿØ ŸÖŸÄŸÄŸÜ1  ÿßŸÑŸÄŸÄŸâ6  ŸàŸäÿ≥ŸÄŸÄÿßŸàŸä0  ÿπŸÜŸÄŸÄÿØ7  ÿØÿß ŸÖÿπŸÜŸÄŸÄÿßŸá ÿßŸÜ
ÿßŸÑŸÄ sample value  ÿ™ŸÇÿπ ŸÅŸäÿßŸÑŸÄ range ŸÖŸÜ 6/8 ÿßŸÑŸâ 7/8 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ output code  Ÿäÿ≥ÿßŸàŸä110 
Parallel  Quantizer  
3-bit parallel quantizer  
 
 
‚ñ™ The quantization noise is defined as:  
ùëí(ùëõùëáùë†)=ùëÜ(ùëõùëáùë†)‚àíùëÜùëû(ùëõùëáùë†) 
 
‚ñ™ where ùëÜ(ùëõùëáùë†) is the original sample value and ùëÜùëû(ùëõùëáùë†) is the quantized sample value.  
ŸáŸà ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÅÿπŸÑŸäÿ© ŸÑŸÑŸÄ sample ŸàÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÑŸä ŸÇÿ±ÿ®  ŸÑŸäŸáÿß.  
 
‚ñ™ The amplitude range of the signal is divided into L uniformly spaced intervals, each of 
width ‚àÜùë† 
‚ñ™ The maximum quantization error is ¬±‚àÜùë†
2 
‚ñ™ The quantization error lies in the range (‚àí‚àÜùë†
2 ,+‚àÜùë†
2)  
 ÿßŸÇÿµŸâ ŸÇŸäŸÖÿ© ŸÑŸÑÿÆÿ∑ŸÄŸÄŸÖ ŸÖŸÖŸÉŸÄŸÄŸÜ ÿ™ÿ≠ÿµŸÄŸÄŸÑ ÿßŸÜÿßŸÑŸÄŸÄ ŸÄ quantization error  ÿ™ÿ≥ŸÄŸÄÿßŸàŸä ¬±‚àÜùíî
ùüê  ÿßŸÑŸÜ ŸÅŸÄŸÄŸä ÿßŸÑÿ≠ÿßŸÑŸÄŸÄÿ© ÿØŸä
ÿßŸÑŸÇŸäŸÖÿ© Ÿáÿ™ŸÉŸàŸÜ ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä ÿßŸÑŸÜÿß ÿ®ŸäŸÜ ŸÖÿ≥ÿ™ŸàŸäŸäŸÜ  ŸàÿØÿß ÿßŸÇÿµŸâ ŸÅÿ±ŸÇ ŸÖŸÖŸÉŸÜ Ÿäÿ≠ÿµŸÑ ŸÖÿß ÿ®ŸäŸÜ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÅÿπŸÑŸäŸÄŸÄÿ© 
ŸÑŸÑŸÄ sample ŸàÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÑŸä ŸÇÿ±ÿ®  ŸÑŸäŸáÿß. 
‚àí‚àÜùíî
ùüê‚â§ ùíÜùíìùíìùíêùíì ‚â§‚àÜùíî
ùüê  
 
Assuming that the error is equally likely to lie anywhere in this range.  
  ŸÜŸÅÿ™ÿ±ÿ∂ ÿßŸÜ ÿßÿ≠ÿ™ŸÖÿßŸÑ ŸàŸÇŸàÿπ ÿßŸÑÿÆÿ∑ŸÖ ŸÖÿ™ÿ≥ÿßŸàŸä ŸÅŸäÿ£Ÿä  ŸÖŸÉÿßŸÜ ŸÅŸÄŸÄŸä  ÿßŸÑŸÄŸÄ ŸÄ  range  (‚àí‚àÜùíî
ùüê ,+‚àÜùíî
ùüê)    ŸàÿØÿß ŸÖÿπŸÜŸÄŸÄÿßŸá
ÿßŸÜ probability density functio n is uniform   ŸÇŸäŸÖÿ™Ÿáÿß ÿ®ŸÄ 1
‚àÜùíî  ÿπÿ¥ÿßŸÜ ÿ™ŸÉŸàŸÜ ÿßŸÑŸÖÿ≥ÿßÿ≠ÿ© ÿ™ÿ≠  ÿßŸÑŸÖŸÜÿ≠ŸÜŸÄŸÄŸâ
ÿ™ÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ 
 
 
 
 
 
 
 
 Quantization Noise   
ùíë(ùíÜ) 
ùíÜ 
‚àí‚àÜùíî
ùüê ‚àÜùíî
ùüê ùüè
‚àÜùíî 
 
 Ÿàÿ®ÿßŸÑÿ™ŸÄŸÄÿßŸÑŸä ŸäŸÖŸÉŸÄŸÄŸÜ ÿßŸÜ ŸÜÿ≠ÿµŸÄŸÄŸÑ ÿπŸÑŸÄŸÄŸâ ÿπÿßŸÑŸÇŸÄŸÄÿ© ŸÖŸáŸÖŸÄŸÄÿ© ŸàŸáŸÄŸÄŸä Mean square error (mse)   ŸàŸáŸÄŸÄŸä 
ÿßŸÑŸÄ expected value  ŸÑŸÑŸÄ error  ÿ™ÿ±ÿ®Ÿäÿπ ÿßŸàŸÖÿ™Ÿàÿ≥ÿ∑ ÿßŸÑŸÄ error  ÿ™ÿ±ÿ®Ÿäÿπ ŸàŸáŸà ÿØÿß ÿßŸÑÿ®ÿßŸàÿ± ÿ®ÿ™ÿßÿπŸÄŸÄÿ©ÿßŸÑŸÄŸÄ ŸÄ error  ÿßŸà
ÿßŸÑŸÄ  noise  ,Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä  ŸÑŸà ÿ∑ÿ®ŸÇŸÜÿß ÿßŸÑŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ≥ÿßÿ®ŸÇ ŸÖŸÖŸÉŸÜ ŸÜÿ≠ÿµŸÑ ÿπŸÑŸÄŸÄŸâ ŸÇŸäŸÖŸÄŸÄÿ© ÿ®ŸÄŸÄÿßŸàÿ±  ÿßŸÑŸÄŸÄ ŸÄ  noise   ŸÖŸÄŸÄŸÜ ÿÆŸÄŸÄÿßŸÑŸÑ
ŸÇÿßŸÜŸàŸÜ  ÿßŸÑŸÄ expected value  . 
 
 ŸÅŸäÿßŸÑŸÖÿ´ŸÄŸÄÿßŸÑ ÿßŸÑÿ≥ŸÄŸÄÿßÿ®ŸÇ  ÿßŸÑŸÄŸÄ ŸÄ random variable  ŸáŸÄŸÄŸàùëí2  Ÿà probability density function ŸáŸÄŸÄŸä  ùíë(ùíÜ) 
 ŸàŸÑŸäŸáÿß ŸÇŸäŸÖÿ© ÿ´ÿßÿ®ÿ™Ÿá ŸàŸáŸäùüè
‚àÜùíî ÿ£ŸÑŸÜŸáÿß uniform distribution function  Ÿàÿ≠ÿØŸàÿØ ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿ™ŸÇÿπ ŸÅŸäÿßŸÑŸÄŸÄ ŸÄ range 
(‚àí‚àÜùíî
ùüê ,+‚àÜùíî
ùüê)  
ùê∏{ùëí2}=‚à´ ùëí2 ùëù(ùëí) ùëëùëí‚àÜùë†
2
‚àí‚àÜùë†
2     , ùëù(ùëí)=1
‚àÜùë† 
=1
‚àÜùë†‚à´ ùëí2 ùëëùëí‚àÜùë†
2
‚àí‚àÜùë†
2=1
‚àÜùë†    ùëí3
3|
‚àí‚àÜùë†
2‚àÜùë†
2
=1
3 ‚àÜùë† [(‚àÜùë†
2)3
‚àí(‚àí‚àÜùë†
2)3
]  
=1
3 ‚àÜùë†√ó2(‚àÜùë†
2)3
=2
3 ‚àÜùë†√ó‚àÜùë†3
8=‚àÜùë†2
12 
   
‚à¥ùëí2ÃÖÃÖÃÖ=ùëöùë†ùëí =‚àÜùë†2
12  
 
   ÿ≤Ÿä ŸÖÿß ÿπÿ±ŸÅŸÜÿß ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿßÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿßŸÜ ÿßŸÑÿ®ÿßŸàÿ± ŸáŸà ŸÖÿ™Ÿàÿ≥ÿ∑ ÿ™ŸÉÿßŸÖŸÑ ÿßŸÑÿØÿßŸÑÿ© ÿ™ÿ±ÿ®Ÿäÿπ ÿ≠Ÿäÿ´ ÿ™ŸÉÿßŸÖŸÑ ÿßŸÑÿØÿßŸÑŸÄŸÄÿ©  
ÿ™ÿ±ÿ®Ÿäÿπ ŸáŸà ÿßŸÑŸÄ Energy Ÿàÿπÿ¥ÿßŸÜ ÿßÿ¨Ÿäÿ® ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ŸáŸÇÿ≥ŸÖ ÿπŸÑŸâ  ÿßŸÑŸÄ interval .ÿßŸÑŸÑŸä ŸÉÿßŸÖŸÑ  ŸÅŸäŸáÿß 
 
ŸÑŸà ÿ∑ÿ®ŸÇŸÜÿß ÿßŸÑŸÖŸÅŸáŸàŸÖ ÿØÿß ŸáŸÜŸàÿµŸÑ ŸÑŸÜŸÅÿ≥ ÿπÿßŸÑŸÇÿ© ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿ≠Ÿäÿ´ ÿßŸÜŸÜÿß ŸáŸÜŸÉÿßŸÖŸÑ ÿßŸÑŸÄŸÄ ŸÄ  error ÿ™ÿ±ÿ®ŸäŸÄŸÄÿπ( ùëí2)  ŸÅŸÄŸÄŸäÿßŸÑŸÄŸÄ ŸÄ 
range (‚àí‚àÜùíî
ùüê ,+‚àÜùíî
ùüê)   ŸàŸÜŸÇÿ≥ŸÖ ÿπŸÑŸâÿßŸÑŸÄ interval  ÿßŸÑŸÑŸä ŸÉÿßŸÖŸÑ  ŸÅŸäŸáÿß1
‚àÜùë† 
 
 
 
 
 
 
 
ùë∫ùëµùëπ =ùíëùíêùíòùíÜùíì  ùíêùíá ùíîùíäùíàùíèùíÇùíç
ùíëùíêùíòùíÜùíì  ùíêùíá ùíÜùíìùíìùíêùíì=ùë∫ùüêÃÖÃÖÃÖ
ùíÜùüêÃÖÃÖÃÖ=ùüèùüê ùë∫ùüêÃÖÃÖÃÖ
‚àÜùíîùüê 
 
SNR   ŸáŸà ÿßŸÑŸÜÿ≥ÿ®ÿ© ÿ®ŸäŸÜ ÿ®ÿßŸàÿ±ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸâ ÿ®ÿßŸàÿ± ÿßŸÑŸÄ noise 
  ÿ®ÿßŸàÿ±ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  ùëÜ2ÃÖÃÖÃÖ=ùê¥2
2     ŸÑŸà ŸÉÿßŸÜÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ©  sinusoidal  ÿ≠ŸäŸÄŸÄÿ´  ùê¥    ŸáŸÄŸÄŸà amplitude  ÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ©  ,Ÿà  ùëí2ÃÖÃÖÃÖ 
  ŸáŸä ÿ®ÿßŸàÿ±ÿßŸÑŸÄ noise ( Mean square error)  
 
 ÿßŸÑŸÖÿ≥ÿßÿ¶ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿØÿß ÿπŸÑŸâ uniform quantization ŸàŸÖÿ∑ŸÑŸàÿ® ŸÅŸäŸá ÿ≠ÿ≥ÿßÿ®  Signal to Noise Ratio 
(SNR)  ŸàÿßŸÑŸÖŸÇÿµŸàÿØ ÿ®ÿßŸÑŸÄ Noise ŸáŸÜÿß ŸáŸä Quantization Noise 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Signal to noise ratio (SNR)  
 
Consider an audio signal comprised of the sinusoidal term  
ùíî(ùíï)=ùüë ùêúùê®ùê¨ (ùüìùüéùüéùíï ) 
(a) Find the signal to quantization noise ratio if this is quantized using 10 -bit PCM  
(b) How many bits of quantization are needed to achieve a signal to quantization 
noise ratio of at least 40 dB?  
Solution  
(a) SNR  
the total swing of the signal is 6 V so the size of each interval equal  
‚àÜùíî=ùê∑ùëÖ
2ùëõ=6
210=5.86√ó10‚àí3 
The signal power is  
ùëÜ2ÃÖÃÖÃÖ=ùê¥2
2=32
2=4.5 
The signal to quantization noise ratio is then given by  
SNR =12 ùëÜ2ÃÖÃÖÃÖ
‚àÜùë†2=12 √ó4.5
(5.86√ó10‚àí3)2=1.57√ó106 
If we wish to express this in decibels, we take its log and multiply by 10. Therefore,  
SNR =10log(1.57√ó106)=62 ùëëùêµ 
 
(b)  
40 ùëëùêµ=10log(SNR )‚ÜíSNR =104 
SNR ‚â•104 
12 √ó4.5
‚àÜùë†2‚â•104 
‚àÜùë†‚â§7.35√ó10‚àí2 
6
2ùëõ‚â§7.35√ó10‚àí2 
2ùëõ‚â•81.6 
ùëõ‚â•6.35 
‚à¥ùëõùëöùëñùëõ =7 ùëèùëñùë°ùë† Example  3 
 
 
It's desired to set up control station for monitoring ECGs of 10 patients. The data 
from the room of the 10 patients are sampled, quantized and binary coded and time 
division multiplexed. The multiplexed data are now transmitted over wires to the 
monitoring station. The ECGs signal bandwidth is 100Hz . The maximum acceptance 
error in sample amplitude is 0.25% oof the peak signal amplitude. The sampling 
must be at least twice the Nyquist rate.  
Determine the data rate (bit/sec) transmitted over the cable.  
Solution  
  ŸÖÿ∑ŸÑŸàÿ® ÿπŸÖŸÑ station    ŸÑŸÖÿ±ÿßŸÇÿ®ÿ©ÿ•ÿ¥ÿßÿ±ÿß   ÿßŸÑŸÇŸÑÿ®  ECGs  ŸÑŸÄ  10    ŸÖÿ±ÿ∂Ÿâ ÿ≠Ÿäÿ´ Ÿäÿ™ŸÖ ÿßÿÆÿ∞ ÿπŸäŸÜÿß  ŸÖŸÜÿ•ÿ¥ÿßÿ±ÿ©  ŸÉŸÄŸÄŸÑ 
ŸÖÿ±Ÿäÿ∂ ŸàŸÜÿ≠ŸàŸÑŸáŸÄŸÄÿß ÿßŸÑŸÄŸÄŸâ ÿßŸÑŸÖŸÉŸÄŸÄÿßŸÅÿ¶ ÿßŸÑŸÄŸÄ ŸÄ  binary    ŸàŸÜÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖ TDM  ÿ≠ŸäŸÄŸÄÿ´  ŸäŸÄŸÄÿ™ŸÖ ÿßÿ±ÿ≥ŸÄŸÄÿßŸÑŸáŸÖ ŸÅŸÄŸÄŸä  frame  ŸÖŸÉŸÄŸÄŸàŸÜ ŸÖŸÄŸÄŸÜ 
samples 10 ŸàŸäÿ™ŸÖ ÿßÿ±ÿ≥ÿßŸÑ ÿßŸÑÿØÿßÿ™ÿß ÿπÿ®ÿ± ÿßÿßŸÑÿ≥ÿßŸÑŸÉ ÿßŸÑŸâ ÿßŸÑŸÖŸÉÿßŸÜ ÿßŸÑÿ∞Ÿä ÿ≥Ÿäÿ™ŸÖ ŸÅŸäŸá ŸÖÿ±ÿßŸÇÿ®ÿ© ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
ÿßÿπŸÑŸâ ÿ™ÿ±ÿØÿØ ŸÅŸä ÿ•ÿ¥ÿßÿ±ÿß  ÿßŸÑŸÄ ECGs  Ÿäÿ≥ÿßŸàŸä 100 Hz ,ŸàÿßÿπŸÑŸâ error  ŸÖÿ≥ŸÖŸàÿ≠ ÿ®ŸäŸá ŸÅŸäÿßÿ•ŸÑÿ¥ŸÄŸÄÿßÿ±ÿ© ŸáŸÄŸÄŸà 0.25% 
 ŸÖŸÜ ŸÇŸäŸÖÿ© ÿßÿπŸÑŸâ amplitude  ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©. 
Nyquist rate  =200  ùêªùëß=2ùëìùëö 
Sampling rate =2 Nyquist rate  =4 ùëìùëö=4√ó100 =400  ùêªùëß 
Total number of samples for 10 signals =400 √ó10=4000  samples /sec 
 
 ÿßŸÇÿµŸâ ÿÆÿ∑ŸÖ ŸÖÿ≥ŸÖŸàÿ≠ ÿ®ŸäŸá ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸàŸÜÿ≥ÿßŸàŸäŸá ÿ® ŸÄ  ‚àÜùë†
2   Ÿáÿ≠ÿµŸÑ ÿπŸÑŸâ ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸàŸäÿßL   ŸàŸÖŸÜŸáÿß ŸÜÿ≠ÿµŸÑ
ÿπŸÑŸâ ÿπÿØÿØ ÿßŸÑŸÄ  bits ŸÑŸÉŸÑ sample 
Quantized error ‚â§0.25
100ùëöùëù=ùëöùëù
400 
Maximum quantization error =‚àÜùë†
2=  2ùëöùëù
2ùêø=ùëöùëù
ùêø      ,    ‚àÜùë†=2ùëöùëù
ùêø 
ùëöùëù
ùêø=ùëöùëù
400 ‚Üíùêø=400 
ùëõ=log 2400 ‚âÖ9 ùëèùëñùë° 
 
 ŸÅŸäŸá  4000  samples  ŸàŸÉŸÑ sample  ŸÖÿπŸÖŸàŸÑŸáÿß coding ŸÅŸä 9 bit Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ bit rate  :Ÿäÿ≥ÿßŸàŸä 
‚à¥bit rate =9 √ó4000 =36 KHz  (Kbit /sec)  
 
 
 Example  4 
 
 
 
‚ñ™ Delta modulation is a simple technique for reducing the dynamic range of the 
numbers to be coded.  
 
‚ñ™  Instead of sending each sample value, we send the difference between a sample 
and previous sample.  
 
‚ñ™ Delta modulation quantizes this difference using only one bit of quantization. Thus , a 
"1" is sent if the difference is positive, and a "0" is sent if the difference is negative.  
 
‚ñ™ We shall refer to these two possibilities as either +‚àÜ or ‚àí‚àÜ. At every sample point, 
the quantized waveform can increase or decrease by ‚àÜ. 
 
 ŸáŸä ÿ∑ÿ±ŸäŸÇÿ© ÿ®ŸÇŸÑŸÑ ÿ®ŸäŸáÿßÿßŸÑŸÄ  dynamic rangeŸÑÿ£ŸÑÿ±ŸÇÿßŸÖ ÿßŸÑŸÑŸä ÿπÿßŸäÿ≤ ÿßÿπŸÖŸÑŸáÿß coding  ÿßŸÑŸÜ ÿ®ÿØŸÑ ŸÖŸÄŸÄÿß ÿßÿ®ÿπŸÄŸÄ  ÿßŸÑŸÖŸÉŸÄŸÄÿßŸÅÿ¶
ÿßŸÑŸÄ binary ŸÑŸÑŸÄ sample value ÿ®ÿ®ÿπ  ŸÉŸàÿØ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ  ŸÇŸäŸÖÿ©  ÿßŸÑŸÄ sample  ŸàÿßŸÑŸÄ previous sample. 
 Ÿäÿ™ŸÖ ÿßŸÑÿ™ÿπÿ®Ÿäÿ± ÿπŸÜ ÿßŸÑŸÅÿ±ŸÇ ÿØÿß ÿ®ŸÄ 1 bit   ŸÅŸÇÿ∑, ŸÑŸà ÿßŸÑŸÅÿ±ŸÇ positive    ÿ®ÿ®ÿπ1  Ÿà ŸÑŸà ÿßŸÑŸÅÿ±ŸÇ  negative     ÿ®ÿ®ÿπ0. 
 
 ŸÑŸà ÿßŸÑŸÅÿ±ŸÇ positive   ŸáŸäÿ≤ŸäÿØ ÿ®ŸÖŸÇÿØÿßÿ±‚àÜ  ŸàŸÑŸà   negative   ŸáŸäŸÇŸÑ ÿ®ŸÖŸÇÿØÿßÿ±‚àÜ ŸàÿØÿß ŸáŸäŸàÿ∂ÿ≠ ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿßŸÑÿ±ÿ≥ŸÖ. 
 
 
ŸÅŸä ÿßŸÑÿ®ÿØÿßŸäÿ© ŸáŸÜÿßÿÆÿØ ŸÇŸäŸÖÿ© ÿßŸÑŸÄ  sample  (ŸáŸÜÿ±ŸÖÿ≤ ŸÑŸäŸá ÿ®ÿßŸÑÿ±ŸÖÿ≤a  ) ŸÖÿπ ÿÆŸÄŸÄÿ±ÿßÿßŸÑŸÄŸÄ ŸÄ staircase  (ŸáŸÜÿ±ŸÖŸÄŸÄÿ≤ ŸÑŸäŸÄŸÄŸá ÿ®ŸÄŸÄÿßŸÑÿ±ŸÖÿ≤b )
  ŸàŸÜÿØÿÆŸÑŸáŸÖ ÿπŸÑŸâ comparator    . ŸÑŸÄŸÄŸà ùíÇ>ùíÉ  ÿßÿ∞ÿß    ÿßŸÑŸÄŸÄ ŸÄ  staircase  ŸáŸäÿ≤ŸäŸÄŸÄÿØ ÿÆÿ∑ŸÄŸÄŸàÿ© ŸÑŸÅŸÄŸÄŸàŸÇ  (ŸáŸäÿ≤ŸäŸÄŸÄÿØ ÿ®ŸÖŸÇŸÄŸÄÿØÿßÿ±  ‚àÜ  ,)   ŸÑŸÄŸÄŸà
ùíÇ<ùíÉ ÿßÿ∞ÿß   ÿßŸÑŸÄŸÄ ŸÄ staircase  ŸáŸäŸÇŸÄŸÄŸÑ ÿÆÿ∑ŸÄŸÄŸàÿ© ŸÑÿ™ÿ≠ŸÄŸÄ (ŸáŸäŸÇŸÄŸÄŸÑ ÿ®ŸÖŸÇŸÄŸÄÿØÿßÿ± ‚àÜ  ), ŸàŸáŸÉŸÄŸÄÿ∞ÿß ŸÖŸÄŸÄÿπ ŸÉŸÄŸÄŸÑ sample ŸàŸÅŸÄŸÄŸä ÿßŸÑŸÜŸáÿßŸäŸÄŸÄÿ© 
ŸáŸÜ ÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ¥ŸÉŸÑ ŸÖŸÇÿßÿ±ÿ® ŸÑÿ¥ŸÉŸÑ  ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿ£ŸÑÿµŸÑŸäÿ©. 
ŸàÿØÿß ÿ≥ÿ®ÿ® ÿ™ÿ≥ŸÖŸäÿ™Ÿá  ÿ® ŸÄ  Delta Modulation  ÿ£ŸÑŸÜ  ÿßŸÑÿÆÿ±ÿß ÿ®ŸäŸÇŸÑ ŸàŸäÿ≤ŸäÿØ ÿ®ŸÖŸÇÿØÿßÿ±  ‚àÜ Delta. 
 
 
Delta Modulation  
 
Since the quantized waveform can only either increase or decrease by ùûì at each  
sample point, we shall attempt to fit a staircase approximation to the analog  waveform.  
 
If the staircase is below the analog sample value, the decision is to increment 
positively (an up step). If the staircase is above, we increment negatively (down step).  
 
 
 
 ÿØŸàŸÑÿßŸÑŸÄ bits  ÿßŸÑŸÑŸäÿ®ÿ™ÿπÿ®ÿ± ÿπŸÜ ÿßŸÑŸÖŸÉÿßŸÅÿ¶ ÿßŸÑ ŸÄ binary ŸÑÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿ≠Ÿäÿ´ ÿßŸÑÿ≤ŸäŸÄŸÄÿßÿØÿ© ÿ®ŸÖŸÇŸÄŸÄÿØÿßÿ± ‚àÜ  ÿ®Ÿäÿ∑ŸÑŸÄŸÄÿπ binary 1 
ŸàÿßŸÑŸÜŸÇÿµÿßŸÜ ÿ®ŸÖŸÇÿØÿßÿ±   ‚àÜ   ÿ®Ÿäÿ∑ŸÑÿπ binary 0.  
 
The key to effective use of delta modulation is the intelligent choice of the two 
parameters, step size and sampling rate.  Increasing the sampling frequency means that 
the delta -modulated waveform requires larger bandwidth.  Increasing the step size 
increases the quantization error.  
 
 ŸÖŸÜ ÿßŸÑÿ≠ÿßÿ¨ÿß  ÿßŸÑŸÖŸáŸÖÿ© ÿßŸÑŸÑŸä ÿßŸÑÿ≤ŸÖ ÿßÿ±ÿßÿπŸäŸáÿß ŸÅŸä ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿØŸä ÿπÿ¥ÿßŸÜÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÄ staircase ÿßŸÑÿÆÿßÿ±ÿ¨ÿ© ÿ™ŸÉŸàŸÜ 
ŸÇÿ±Ÿäÿ®Ÿá ŸÖŸÜ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßÿßŸÑÿµŸÑŸäÿ© ŸàŸáŸä ÿßŸÑŸÄ sampling rate  ÿ≠Ÿäÿ´ ÿßŸÜ ÿ≤ŸäÿßÿØÿ©ÿßŸÑŸÄ sampling rate  ŸáŸäÿ≤ŸàÿØ ÿßŸÑŸÄŸÄŸÄ
Bandwidth  ŸàÿßŸÑŸÄ step size   ŸÑŸà ÿ≤ÿßÿØ ÿ®ÿ¥ŸÉŸÑ ŸÉÿ®Ÿäÿ± ÿØÿß ŸáŸäÿ≤ŸàÿØ quantization error. 
 
 
 
 
 
 
  ŸÇŸäŸÖÿ©ÿßŸÑŸÄ  sample  ÿßŸÇŸÑ ŸÖŸÜÿßŸÑŸÄ staircase  
  ÿßÿ∞ÿß ÿßŸÑŸÄ staircase   ŸáŸäŸÇŸÑ ÿ®ŸÖŸÇÿØÿßÿ±‚àÜ 
  ŸÇŸäŸÖÿ©ÿßŸÑŸÄ  sample  ÿßÿπŸÑŸâ ŸÖŸÜÿßŸÑŸÄ   
staircase   ÿßÿ∞ÿß ÿßŸÑŸÄ   
staircase   ŸáŸäÿ≤ŸäÿØ ÿ®ŸÖŸÇÿØÿßÿ±‚àÜ 
  ŸÇŸäŸÖÿ©ÿßŸÑŸÄ  staircase   
 
 ŸÇŸäŸÖÿ©ÿßŸÑŸÄ  sample 
 
 
 
If a bit error occurs in delta modulation, the D/A converter in the receiver will go  up instead 
of down (or vise versa) and all later values will contain an error.  
 ŸÖŸÜ ÿπŸäŸàÿ® Delta modulation  ÿØŸä ÿßŸÜ ŸÑŸà ÿ≠ÿµŸÑ ÿÆÿ∑ŸÖ ŸÅŸäbit Ÿàÿßÿ≠ÿØŸá ŸÉŸÑ ÿßŸÑŸÑŸä ÿ®ÿπÿØŸá ŸáŸäŸÉŸàŸÜ ÿÆÿ∑ŸÖ.  
 
If the steps are too small, we can experience a slope overload condition, where  the 
staircase cannot track rapid changes in the analog signal.  
 
 ŸÑŸàÿßŸÑŸÄ step size  ÿµÿ∫Ÿäÿ±Ÿá ŸàÿßŸÑÿ™ÿ∫Ÿäÿ± ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÉÿßŸÜ ŸÉÿ®Ÿäÿ± ÿßŸÑŸÄ staircase  ŸÖÿ¥
Ÿáÿ™ŸÇÿØÿ± ÿ™ŸàÿµŸÑ ŸÑÿ¥ŸÉŸÑ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÄ analog  ÿ≤Ÿä ŸÖÿß Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿµŸàÿ±ÿ©
ŸàŸáŸäÿ≠ÿµŸÑ ÿÆÿ∑ŸÖ ÿßÿ≥ŸÖŸá slope overload 
 
 
 
 
If the steps are too large, considerable overshoot will occur  during periods when the signal 
is not changing rapidly. We have significant  quantization noise, known as granular noise.  
 
 ŸÑŸàÿßŸÑŸÄ step size  ŸÉÿ®Ÿäÿ± ŸàÿßŸÑÿ™ÿ∫Ÿäÿ± ŸÅŸäÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸÉÿßŸÜ ÿµÿ∫Ÿäÿ± ÿßŸÑŸÄ staircase 
 ŸáŸäŸÅÿ∂ŸÑ Ÿäÿ≤ŸäÿØ ŸàŸäŸÇŸÑ ÿ®ŸÄ‚àÜ  ŸàŸÖÿ¥ Ÿáÿ™ŸÇÿØÿ± ÿ™ŸàÿµŸÑ ŸÑÿ¥ŸÉŸÑÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑŸÄ analog  
 ÿ≤Ÿä ŸÖÿß Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿµŸàÿ±ÿ© ŸàŸáŸäÿ≠ÿµŸÑ ÿÆÿ∑ŸÖ ÿßÿ≥ŸÖŸá granular noise 
 
 
 
 
  ÿßŸÑŸÖÿπÿØŸÑ ÿßŸÑŸÑŸä ÿ®ÿ™ÿ™ÿ∫Ÿäÿ± ÿ®ŸäŸáÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ŸáŸà ÿßŸÑŸÑŸä ÿ®Ÿäÿ≠ŸÉŸÖŸÜŸä ŸÅŸä ÿßÿÆÿ™Ÿäÿßÿ±  step size .ŸÖŸÜÿßÿ≥ÿ® 
 
Disadvantages  

 
 
  
 
 
 
 
  
 
¬©
 
Basem Hesham
Frequency Shift Keying (FSK)
 & Phase Shift Keying (PSK)
Lecture
 
11
 
 
 ŸÅŸäÿ≠ÿßŸÑÿ© FSK   Ÿäÿ™ŸÖ ÿ™ÿ∫ŸäŸäÿ± ÿ™ÿ±ÿØÿØ ÿßŸÑŸÄ carrier  ÿ™ÿ®ÿπÿß ŸÑŸÇŸäŸÖÿ© ÿßŸÑŸÄ information signal  ŸÖÿπ ÿ´ÿ®ÿßÿ™ ŸÇŸäŸÖÿ© ÿßŸÑŸÄ amplitude   ,Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä
ÿπŸÜÿØŸä ÿ™ÿ±ÿØÿØŸäŸÜ ŸÑÿ•ŸÑÿ±ÿ≥ÿßŸÑ, ÿßÿ£ŸÑŸàŸÑ  ùëì1  ÿπŸÜÿØŸÖÿß ŸÜÿ±ÿ≥ŸÑ binary 1    , ŸàÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑÿ´ÿßŸÜŸäùëì2  ÿπŸÜÿØŸÖÿß ŸÜÿ±ÿ≥ŸÑ binary 0   
 ÿπÿ¥ÿßŸÜ ŸÜÿπŸÖŸÑ ÿØŸäÿ≤ÿßŸäŸÜ ÿßŸÑŸÄ matched filter  ŸÑŸÑŸÄFSK    ŸáŸÜÿ≠ÿ™ÿßÿ¨ ŸÜÿ¨Ÿäÿ® ŸÖÿπÿßÿØŸÑÿ™ŸäŸÜ ùë†1(ùë°)    Ÿà ùë†2(ùë°)   ÿ≠Ÿäÿ´ ÿßŸÜ ùë†1(ùë°)   ŸáŸä ÿßŸÑŸÄ
modulated signal  ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜ binary 1   Ÿà ùë†2(ùë°)  ŸáŸä ÿßŸÑŸÄ modulated signal  ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜ binary 0 
 
‚ñ™ Digital signal     Sb(t)=¬±v 
ŸÜÿπÿ®ÿ± ÿπŸÜ binary 1  ÿ®ŸÄ  +v  ŸàÿπŸÜ binary 0  ÿ®ŸÄ  ‚àív 
‚ñ™ Carrier signal    ùëìùëê(ùë°)=Acos2œÄfct 
 
‚ñ™ FSK signal      SFSK=Acos(2œÄfct+2œÄ Kf ‚à´Sb(t)ùëëùë° )   
    
 ÿπÿ¥ÿßŸÜ ŸÜŸÅŸáŸÖ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÑÿ≤ŸÖ ŸÜÿπÿ±ŸÅ ÿπÿßŸÑŸÇÿ© ŸÖŸáŸÖŸá ÿ®ŸäŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ ŸàÿßŸÑŸÄ phase  ŸÉŸÜÿß ÿØÿ±ÿ≥ŸÜÿßŸáÿß ŸÇÿ®ŸÑ ŸÉÿØÿß ŸàŸáŸä ÿßŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ
ÿßŸÑŸÑÿ≠ÿ∏Ÿä instantaneous frequency ùëìùëñ(ùë°)   Ÿäÿ≥ÿßŸàŸä ÿ™ŸÅÿßÿ∂ŸÑ ÿßŸÑŸÄ phase   : 
ùëìùëñ(ùë°)=1
2ùúã ùúÉÃá(ùë°)=1
2ùúã ùëëùúÉ(ùë°)
ùëëùë°  
  ÿπÿ¥ÿßŸÜÿßŸÑÿ™ÿ±ÿØÿØ Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ  ÿßŸÑŸÄ information signal  Ÿäÿ®ŸÇŸâ ÿßŸÑÿ≤ŸÖ ŸÅŸä ŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÄ phase ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑŸÑŸä ŸáŸÜÿ∂ŸäŸÅŸá ÿπÿ¥ÿßŸÜ ŸÜÿ∫Ÿäÿ±  
ŸÅŸä ÿßŸÑÿ™ÿ±ÿØÿØ ŸäŸÉŸàŸÜ ÿ≠ÿßÿ¨ÿ© ŸÖÿ∂ÿ±Ÿàÿ®ÿ© ŸÅŸä  ùë°   ÿπÿ¥ÿßŸÜ ŸÑŸÖÿß ŸÜŸÅÿßÿ∂ŸÑ ŸàŸÜÿ¨Ÿäÿ® ùëìùëñ(ùë°)    ŸäŸÉŸàŸÜ ÿπÿ®ÿßÿ±ÿ© ÿπŸÜ ÿßŸÑŸÄ carrier  ŸÖŸèÿ∂ÿßŸÅ ÿπŸÑŸäŸáÿ•ÿ¥ÿßÿ±ÿ©   
ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖÿ∂ÿ±Ÿàÿ®ÿ© ŸÅŸä ÿ´ÿßÿ®ÿ™ ÿßŸÑÿ™ŸÜÿßÿ≥ÿ®  Kf  . 
 ÿßÿÆÿØŸÜÿß ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿØŸä ŸÅŸäFM   ŸàŸÉŸÜÿß ÿ®ŸÜÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿπÿØÿØ ÿßŸÑ ŸÜŸáÿßÿ¶Ÿä ŸÖŸÜ ÿßŸÑŸÇŸäŸÖ ŸÅŸäÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÑÿ≠ÿ∏Ÿä ŸÑŸäŸá ŸÇŸäŸÖ  
ŸÉÿ™Ÿäÿ± ÿ¨ÿØÿß ŸàŸÑŸÉŸÜ ŸÅŸä  FSK  ÿπŸÜÿØŸÜÿß ŸÇŸäŸÖÿ™ŸäŸÜ ŸÅŸÇÿ∑ ŸÅŸäÿ•ÿ¥ÿßÿ±ÿ©  ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™   ŸàŸÅŸä ÿßŸÑÿ≠ÿßŸÑÿ© ÿØŸä ŸáŸÖ  ¬±v 
 
SFSK=Acos(2œÄ fct¬±2œÄ Kf v t) 
 
 ÿßŸÑÿ™ÿ±ÿØÿØ ŸàÿßŸÑŸÄ phase  ÿßÿßŸÑÿ™ŸÜŸäŸÜ ÿ®Ÿäÿ™ÿ∫Ÿäÿ±Ÿà ŸÅŸäFSK   ŸàŸÑŸÉŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπÿßŸÑŸÄ information signal  ŸàÿßŸÑŸÄ phase   Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ
ÿ™ŸÉÿßŸÖŸÑ ÿßŸÑŸÄ information signal 
‚ñ™ Maximum  frequency deviation       ‚àÜf=Kf v 
SFSK=Acos(2œÄfct¬±2œÄ ‚àÜf t)=Acos(2œÄ(fc¬±‚àÜf) t) 
‚àÜf  ŸáŸà ÿßŸÇÿµŸâ ÿ≤ŸäÿßÿØÿ© ÿßŸà ŸÜŸÇÿµ ŸÖŸÖŸÉŸÜ ÿ™ÿ≠ÿµŸÑ ÿπŸÜ ÿ™ÿ±ÿØÿØ ÿßŸÑŸÄ carrier  Ÿàÿ®ŸÖÿß ÿßŸÜ ÿπŸÜÿØŸÜÿß ŸÇŸäŸÖÿ™ŸäŸÜ ŸÅŸÇÿ∑ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿ™ÿ±ÿØÿØŸäŸÜ ,ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ£ŸÑŸàŸÑ   
ŸáŸà ŸÇŸäŸÖÿ© ÿßŸÑŸÄ carrier   ŸÖŸèÿ∂ÿßŸÅ ÿπŸÑŸäŸáÿß ‚àÜf  ŸàÿØÿßÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÖÿπÿ®ÿ± ÿπŸÜ ÿßÿ±ÿ≥ÿßŸÑ  binary 1    ,ŸàÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑÿ´ÿßŸÜŸä ŸáŸà ŸÇŸäŸÖÿ© ÿßŸÑŸÄ carrier  ŸÜÿßŸÇÿµ
‚àÜf  ŸàÿØÿß ÿßŸÑÿ™ÿ±ÿØÿØ ÿßŸÑŸÖÿπÿ®ÿ± ÿπŸÜ ÿßÿ±ÿ≥ÿßŸÑ binary 0   
 Frequency Shift Keying (FSK) 
 
ùëìùëñ(ùë°)=1
2ùúã ùëëùúÉ(ùë°)
ùëëùë° =2œÄ(fc¬±‚àÜf)
2ùúã=fc¬±‚àÜf 
 
f1=fc+‚àÜf 
f2=fc‚àí‚àÜf 
 
 ÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÖÿπÿßÿØŸÑÿ™ŸäŸÜ ÿßŸÑŸÑŸä Ÿáÿ≥ÿ™ÿÆÿØŸÖŸáŸÖ ŸÅŸä ÿßŸÑÿ™ÿπÿ®Ÿäÿ± ÿπŸÜ binary 1  Ÿà binary 0 : 
 
s1(t)=Acos(2œÄf1t)        binary  1  
s2(t)=Acos(2œÄf2t)         binary  0  
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
s1(t)=Acos(2œÄf1t) 
s2(t)=Acos(2œÄf2t) 
 
ÔÅ≤ Energy  (E) 
 
ùê∏1=‚à´ùë†12(ùë°) ùëëùë°ùëáùëè
0=ùê¥2
2 ùëáùëè 
 
ùê∏2=‚à´ùë†22(ùë°) ùëëùë°ùëáùëè
0=ùê¥2
2 ùëáùëè 
 
E=E1+E2
2= [A2
2 Tb+A2
2 Tb
2]=A2
2 Tb 
 
ÔÅ≤ Threshold (ùíöùüé) 
 
ùë¶ùëú=ùê∏1‚àíùê∏2
2=[ùê¥2
2 ùëáùëè‚àíùê¥2
2 ùëáùëè
2]=0 
 
Design and Performance for FSK 
‚à´   ùëëùë°ùëáùëè
0 ‚à´   ùëëùë°ùëáùëè
0 
ùë†2(ùë°)=ùê¥ùëêùëúùë†(2ùúãùëì2ùë°) ùëÜùêπùëÜùêæ 
ùë†1(ùë°)=ùê¥ùëêùëúùë†(2ùúãùëì1ùë°) ‚àë  
 
ÔÅ≤ Cross correlation  ( ùùÜ ) 
 
ùúå=1
E‚à´ s1(t) s2(t) dtTb
0=2
A2Tb‚à´ Acos(2œÄf1t) Acos(2œÄf2t) dtTb
0 
=2
A2Tb√óA2
2‚à´ [cos(2œÄ(f1‚àíf2)t)+ cos(2œÄ(f1+f2)t)] dtTb
0 
=1
Tb[‚à´ cos(2œÄ(f1‚àíf2)t) dtTb
0+‚à´ cos(2œÄ(f1+f2)t) dtTb
0] 
f1=fc+‚àÜf 
f2=fc‚àí‚àÜf 
‚à¥f1‚àíf2=2‚àÜf                ,                      f1+f2=2fc  
ùúå=1
Tb[‚à´ cos(4œÄ‚àÜf t) dtTb
0+‚à´ cos(4œÄfct) dtTb
0] 
=1
Tb[  sin(4œÄ‚àÜf t)
4œÄ‚àÜf|
0Tb
+ sin(4œÄfct)
4œÄfc|
0Tb
]=1
Tb[  sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf+ sin(4œÄfcTb)
4œÄfc] 
fc‚â´‚àÜf 
 sin(4œÄfct)Tb
4œÄfc   ÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ™ÿ≥ÿßŸàŸä ÿµŸÅÿ± ÿßŸÑŸÜùëìùëê  ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÖŸÇÿßŸÖ ŸÇŸäŸÖÿ™Ÿáÿß ŸÉÿ®Ÿäÿ±Ÿá ÿ¨ÿØÿßŸÖŸÇÿßÿ±ŸÜÿ© ÿ®ŸÄ  ‚àÜf 
 
‚à¥ ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb 
 
  ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäŸÜ s1(t)  Ÿà  s2(t)   Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ‚àÜf  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿ™ÿ±ÿØÿØÿßÿ™f1    Ÿàf2 ,ŸàÿßŸäÿ∂ÿß  Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ  Tb 
ŸàŸÖŸÇŸÑŸàÿ®Ÿá ŸáŸà ŸÖÿπÿØŸÑ ÿßÿßŸÑÿ±ÿ≥ÿßŸÑ  bit rate Rb  
 
 
 
 
 
 
 
 
ÔÅ≤ Probability of error  
 ŸÑÿ≠ÿ≥ÿßÿ® ŸÜÿ≥ÿ®ÿ© ÿßŸÑÿÆÿ∑ÿ£ŸÅŸäŸá  ÿπŸÜÿØŸä 3   ÿ≠ÿßÿßŸÑÿ™ ÿπŸÑŸâ ÿ≠ÿ≥ÿ® ŸÇŸäŸÖÿ© ÿßŸÑŸÄ   ùúå  : 
 ÿßŸÑÿ≠ÿßŸÑÿ©ÿßÿ£ŸÑŸàŸÑŸâ  ( General Case )  ŸÅŸä ÿßŸÑÿ≠ÿßŸÑÿ© ÿØŸä ÿ®ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿπÿßŸÖ ŸÑŸÑŸÄ  ùúå  ŸÑŸà ŸÑŸÖ Ÿäÿ™ŸÖ ŸÅŸä ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ∞ŸÉÿ± ÿßŸÑÿ≠ÿßÿßŸÑÿ™ÿßÿßŸÑÿÆÿ±Ÿâ.  
ÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ©  ( Maximum performance   ) ŸàÿØŸä ÿ®ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸäŸáÿßùúå  .ÿßŸÑŸÑŸä ÿ™ÿ≠ŸÇŸÇ ÿßÿπŸÑŸâ ŸÉŸÅÿßÿ°ÿ© 
ÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ´ÿßŸÑÿ´ÿ© ( Orthogonal )   ÿ®ŸÜÿπŸàÿ∂ ŸÅŸäŸáÿß ÿπŸÜ ùúå=0. 
ÔÅ≤ General Case  
ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb 
ùëùùëí=0.5 ùëíùëüùëìùëê ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú 
 
ÔÅ≤ For Maximum performance  
Minimum  probability of error  ùëùùëí at minimum  ùúå 
ÿßÿπŸÑŸâ ŸÉŸÅÿßÿ°ÿ© ŸÖŸÖŸÉŸÜ ÿßÿ≠ÿµŸÑ ÿπŸÑŸäŸáÿß ŸáŸä ÿπŸÜÿØ ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ŸÑŸÑŸÄ   ùúå   ŸÅŸáŸÜÿ¨Ÿäÿ® ŸÇŸäŸÖÿ©‚àÜf   ÿßŸÑŸÑŸä ÿ™ÿ≠ŸÇŸÇ ÿßŸÇŸÑùúå    ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿßŸÜŸÜÿß ŸÜŸÅÿßÿ∂ŸÑ ŸÖÿπÿßÿØŸÑÿ©
ÿßŸÑŸÄ ùúå   ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÄ‚àÜf   ŸàŸÜÿ≥ÿßŸàŸä ŸÜÿßÿ™ÿ¨ ÿßŸÑÿ™ŸÅÿßÿ∂ŸÑ ÿ®ÿµŸÅÿ± 
 
For minimum ùúå ‚Üí  ùúïùúå
ùúï‚àÜf=0 
ùúïùúå
ùúï‚àÜf=4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)√ó4œÄTb‚àísin(4œÄ‚àÜf Tb)√ó4œÄTb
(4œÄ‚àÜf Tb)2=0 
 
4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)√ó4œÄTb‚àísin(4œÄ‚àÜf Tb)√ó4œÄTb=0 
4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)=sin(4œÄ‚àÜf Tb) 
 
4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)
cos(4œÄ‚àÜf Tb)=sin(4œÄ‚àÜf Tb)
cos(4œÄ‚àÜf Tb) 
 
4œÄ‚àÜf Tb=tan(4œÄ‚àÜf Tb) 
4œÄ‚àÜf Tb=0.715 √ó2œÄ   rad 
‚àÜf=0.715 √ó2œÄ
4œÄ Tb=0.715
2 Tb   
 ÿ≠ŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© tan(ùë•)=x   ŸÖŸÖŸÉŸÜ ŸÜÿ¨Ÿäÿ®Ÿáÿßÿ®ÿßŸÜŸÜÿß ÿ£ŸàÿßŸÑ ŸÜÿπŸÖŸÑ ÿßÿßŸÑŸÑŸá ÿπŸÑŸâ ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ±ÿßÿØŸäÿßŸÜ ŸàŸÜŸÉÿ™ŸÄÿ® ÿßŸÑŸÖÿπÿßÿØŸÑŸÄÿ© ŸàŸÜÿπŸÖŸÄŸÑ  solve shift ‚Üí 
ÿπŸÑŸâ ÿßÿßŸÑŸÑŸá  ŸáŸÜÿßŸÑŸÇŸä ÿ≠ŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© 0 Ÿàÿ∑ÿ®ÿπÿß ÿßŸÑÿ≠ŸÑ ÿØÿß ŸÖÿ±ŸÅŸàÿ∂  ÿßŸÑŸÜ ŸÉÿØÿß ‚àÜf Ÿáÿ™ŸÉŸàŸÜ ÿ®ÿµŸÅÿ± ŸÅÿπÿ¥ÿßŸÜ ŸÜÿ¨Ÿäÿ® ÿßŸÑÿ≠ŸÑ ÿßŸÑÿ™ÿßŸÜŸä ŸáŸÜÿπŸÖŸÑ  solve 
shift ‚Üí  ÿ™ÿßŸÜŸä ÿ®ÿ≥ ÿπŸÜÿØ  solve for x  ŸáŸÜÿÆÿ™ÿßÿ± x ÿ®ŸÄ 1  ŸáŸÜÿßŸÑŸÇŸä ÿßŸÑÿ≠ŸÑ ÿ®ÿµŸÅÿ± ÿ®ÿ±ÿØŸà ŸÅŸáŸÜÿ¨ÿ±ÿ® ÿ™ÿßŸÜŸä ÿπŸÜÿØ x ÿ®ŸÄ 2 ŸáŸÜÿßŸÑŸÇŸä ÿßŸÑÿ≠ŸÑ 4.493 
 ŸàÿßŸÑŸÑŸä ŸáŸà 0.715 √ó2œÄ   
 
 ÿßŸÑÿ±ÿ≥ŸÄŸÄŸÄŸÖ ÿßŸÑÿ®ŸäŸÄŸÄŸÄÿßŸÜŸä ÿßÿßŸÑÿ™ŸÄŸÄŸÄŸä ŸäŸàÿ∂ŸÄŸÄŸÄÿØ ÿ≠ŸÑŸÄŸÄŸÄŸàŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑŸÄŸÄŸÄÿ©
tan(ùë•)=x   ÿπŸÜŸÄŸÄŸÄÿØ ÿ™ŸÇŸÄŸÄŸÄÿßÿ∑ÿπ ÿßŸÑÿÆŸÄŸÄŸÄÿ∑ùë•  ŸÖŸÄŸÄŸÄÿπ ŸÖŸÜÿ≠ŸÜŸÄŸÄŸÄŸâ
ùë°ùëéùëõ(ùë•)  ŸàÿßŸÑÿ≠ŸÄŸÄŸÑÿßÿ£ŸÑŸàŸÑ ÿπŸÜŸÄŸÄÿØ ùë•=0  ŸàÿØÿß ŸÖÿ±ŸÅŸÄŸÄŸàÿ∂
ŸàÿßŸÑÿ≠ŸÑ ÿßŸÑÿ™ÿßŸÜŸä ÿπŸÜŸÄÿØ  ùë•=4.493 =0.715 √ó2œÄ  
ŸàÿØÿß ÿßŸÑÿ≠ŸÄŸÄŸÑ ÿßŸÑŸÑŸÄŸÄŸä ŸáŸÜÿßÿÆŸÄŸÄÿØŸá ÿßŸÑŸÜ ÿØÿß ÿßŸÑŸÑŸÄŸÄŸä ŸáŸäÿ≠ŸÇŸÄŸÄŸÇ ÿßŸÇŸÄŸÄŸÑ ùúå 
Ÿàÿ®ÿßŸÇŸä ÿßŸÑÿ≠ŸÑŸàŸÑ   ŸÑŸà ÿπŸàÿ∂ŸÜÿß ÿ®ŸäŸáÿß  ŸáŸÜÿßŸÑŸÇŸä ÿßŸÜŸáÿß ÿ®ÿ™ÿ≠ŸÇŸÇ   ŸÇŸäŸÖ 
ÿßŸÉÿ®ÿ± ŸÑŸÑŸÄ  ùúå 
1
 Tb=Rb 
‚à¥‚àÜf=0.715 √óRb
2   
Substitution in ùúå 
ùúå=sin(4œÄ√ó0.715 √óRb
2√óTb)
4œÄ√ó0.715 √óRb
2√ó Tb=sin(2œÄ√ó0.715 )
2œÄ√ó0.715=‚àí0.22 
 Ÿàÿßÿ≠ŸÜÿß ÿ®ŸÜÿπŸàÿ∂ ŸÜÿ∂ÿ±ÿ® ÿßŸÑŸÑŸä ÿØÿßÿÆŸÑ ÿßŸÑŸÄsin   ŸÅŸä180
œÄ  ŸÑŸà ÿ¥ÿ∫ÿßŸÑŸäŸÜ ÿ®ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÄ degree    ÿπŸÑŸâ ÿßÿßŸÑŸÑŸá 
 
Minimum probability of error at  ùúå=‚àí0.22 
ùëùùëí=0.5 ùëíùëüùëìùëê ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê ‚àö1.22 ùê∏
2 ùëÅùëú 
 
 ÿπÿ±ŸÅŸÜÿß ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿßŸÜ ŸÇŸäŸÖÿ©ùúå  ÿ™ÿ™ÿ±ÿßŸàÿ≠ ŸÖŸÜ1  ÿßŸÑŸâ-1  ‚Üê   ‚àí1<ùúå<1   
 Ÿàÿπÿ±ŸÅŸÜÿß ÿßŸÜ ÿßÿπŸÑŸâ ŸÉŸÅÿßÿ°ÿ© ŸÜÿ≠ÿµŸÑ ÿπŸÑŸäŸáÿß ŸäÿπŸÜŸä ÿßŸÇŸÑ ŸÜÿ≥ÿ®ÿ© ÿÆÿ∑ÿ£ ŸáŸä ÿπŸÜÿØ ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ŸÑŸÑŸÄùúå  ÿπÿ¥ÿßŸÜ ŸÅŸä ŸÖÿπÿßÿØŸÑÿ©ùëùùëí  ÿØÿß ŸáŸäÿ≤ŸàÿØ ÿßŸÑŸÇŸäŸÖŸÄÿ©
ÿØÿßÿÆŸÑ ùëíùëüùëìùëê  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÜÿ≥ÿ®ÿ© ÿßŸÑÿÆÿ∑ÿ£ Ÿáÿ™ŸÇŸÑ ŸàŸÅŸä ÿ≠ÿßŸÑÿ©FSK  ÿßŸÇŸÑ ŸÇŸäŸÖÿ©ùúå  ŸÖŸÖŸÉŸÜ ŸÜŸàÿµŸÑŸáÿß ŸáŸä ‚àí0.22 
 
 ŸÑŸÉŸÜ ŸÑŸà ŸáŸÜÿ™ŸÉŸÑŸÖ ÿπŸÜ minimum ùúå   ŸÖŸÜ ŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ™ÿ¥ŸÄÿßÿ®Ÿá ÿ®ŸÄŸäŸÜÿßÿ•ŸÑÿ¥ŸÄÿßÿ±ÿßÿ™ , ùúå=1  ŸÖÿπŸÜÿßŸáŸÄÿß ÿßŸÜ ÿßÿßŸÑÿ¥ŸÄÿßÿ±ÿ™ŸäŸÜ ŸÖÿ™ÿ¥ŸÄÿßÿ®ŸáŸäŸÜ ÿ™ŸÖÿßŸÖŸÄÿß Ÿà
ùúå=‚àí1  ŸÖÿπŸÜÿßŸáÿß ÿßŸÜ ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäŸÜ ŸÖÿ™ÿ¥ÿßÿ®ŸáŸäŸÜ ÿ®ÿ±ÿØŸà ŸàŸÑŸÉŸÜ ŸÖÿπŸÉŸàÿ≥ŸäŸÜ ŸäÿπŸÜŸä ÿ®ŸäŸÜŸáŸÖ phase shift 180  ŸÅŸÑŸÄŸà ÿπÿ±ŸÅŸÜŸÄÿßÿ•ÿ¥ŸÄÿßÿ±ÿ© ŸÖŸÄŸÜŸáŸÖ 
ŸáŸÜÿπÿ±ŸÅ ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ© ÿßŸÑÿ™ÿßŸÜŸäŸá Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä minimum ùúå   ŸàÿßŸÑŸÑŸä ŸÜŸÇÿµÿØ ÿ®ŸäŸá ŸáŸÜÿß ÿßŸÇŸÑ ÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäŸÜŸÖÿ¥ ÿπŸÜŸÄÿØ ùúå=‚àí1  ŸàŸÑŸÉŸÄŸÜ
ŸÑŸÖÿß ŸäŸÉŸàŸÜŸàÿß orthogonal  ŸäÿπŸÜŸä ùúå=0  
 
 
ÔÅ≤ Orthogonal Tone Spacing  
   
orthogonal   ŸäÿπŸÜŸä ÿ™ÿπÿßŸÖÿØ ŸàÿßŸÑÿ™ÿπÿßŸÖÿØ ŸäÿπŸÜŸä ÿßÿÆÿ™ÿßŸÑŸÅ ŸÖÿπŸÜÿßŸáÿß ÿßŸÜ ŸÑŸà ÿπŸÜÿØŸä ÿßÿ¥ÿßÿ±ÿ™ŸäŸÜ ŸàŸÇŸàŸÑŸÜÿß ÿßŸÜŸáŸÖ orthogonal  Ÿäÿ®ŸÇŸâ ŸáŸÖ ŸÖÿÆÿ™ŸÑŸÅŸÄŸäŸÜ
ÿ™ŸÖÿßŸÖ ÿπŸÜ ÿ®ÿπÿ∂ Ÿàÿ®ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿ±Ÿäÿßÿ∂Ÿä ŸäÿπŸÜŸä  ÿ™ŸÉÿßŸÖŸÑ  ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ®ŸáŸÖ ÿ®ÿµŸÅÿ± Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÄ cross  correlation  ÿ®ŸäŸÜŸáŸÖ Ÿäÿ≥ÿßŸàŸä ÿµŸÅÿ±
 ùúå=0 
 

 
Tone Spacing   ŸäÿπŸÜŸä ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿ™ÿ±ÿØÿØÿ•ÿ¥ÿßÿ±ÿ©  ùë†1(ùë°)   Ÿàÿ™ÿ±ÿØÿØ ÿßÿ¥ÿßÿ±ÿ© ùë†2(ùë°)     ‚Üê ùëì1‚àíùëì2
2=‚àÜf      Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÖŸÇÿµŸàÿØ ÿ®ÿßŸÑŸÄ
spacing  ŸáŸÜÿß ŸáŸä  ‚àÜf 
 
  ÿßÿ∞ÿß ÿßŸÑŸÖŸÇÿµŸàÿØ ÿ®ŸÄ  orthogonal tone spacing ŸáŸä ŸÇŸäŸÖÿ©  ‚àÜf  ÿßŸÑŸÑŸä ÿ™ÿ≠ŸÇŸÇ ÿßŸÑŸÄ orthogonality   ÿ®ŸäŸÜ ÿßÿßŸÑÿ¥ÿßÿ±ÿ™ŸäŸÜ ŸäÿπŸÜŸä ùúå=0  
 
For the two tones with frequencies ùëì1 and ùëì2 to be orthogonal (i.e. ùúå=0 ) a condition must be satisfied  
‚àÜf=ùëõ Rb
2 
ùëì1‚àíùëì2=2‚àÜf=ùëõRb 
 
 ÿßÿ∞ÿß  ŸÅŸä ÿ≠ÿßŸÑÿ© FSK   ÿπÿ¥ÿßŸÜ ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑŸÄ orthogonality  ÿßŸÑÿ≤ŸÖ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ ÿßŸÑÿ™ÿ±ÿØÿØŸäŸÜ  ùëì1   Ÿàùëì2  Ÿäÿ≥ÿßŸàŸä ÿßŸÑŸÄ bit rate   ÿßŸà
ŸÖÿ∂ÿßÿπŸÅÿßÿ™Ÿá  
 
ÿ®ÿßŸÑÿ™ÿπŸàŸäÿ∂ ÿ®ÿπÿßŸÑŸÇÿ©  ‚àÜf   ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ŸáŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ùúå=0   ÿ≠Ÿäÿ´ùëõ ÿ£Ÿä ÿ±ŸÇŸÖ ÿµÿ≠ŸäÿØ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ŸÑŸÄ  ‚àÜf  ÿ™ÿ≠ŸÇŸÇÿßŸÑŸÄ  
orthogonality   ŸÑŸÖÿß ÿ™ŸÉŸàŸÜ ùëõ=1 
The minimum value of ‚àÜf that satisfies orthogonality is for ùëõ=1  
‚àÜf=Rb
2 
Substitution in ùúå 
ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb=sin(4œÄ√óRb
2 √óTb)
4œÄ√óRb
2 √ó Tb=sin(2œÄ)
2œÄ=0 
 
Larger ‚àÜf  means wider separation between signaling frequencies and consequence larger transmission 
bandwidth. To minimize bandwidth, ‚àÜf  should be as small as possible. The minimum value of ‚àÜf that 
can be used for orthogonal signaling is Rb
2 
 
  ÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿßŸÑŸÑŸä ŸÅŸäŸáÿß‚àÜf  ÿ™ÿ≠ŸÇŸÇ ÿßŸÑŸÄ orthogonality   ŸàÿßŸäÿ∂ÿß ÿ™ŸÉŸàŸÜ ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ŸÖŸÖŸÉŸÜŸá ŸÑŸÖÿß ŸÜÿπŸàÿ∂ ÿ®ŸÄ ùëõ=1     ÿ®ŸÜŸÇŸàŸÑ ÿπŸÑŸäŸáÿß
minimum shift keying (MSK)    ŸàŸáŸä ÿ≠ÿßŸÑÿ© ÿÆÿßÿµÿ© ŸÖŸÜFSK   ŸÅŸäŸáÿß ŸÇŸäŸÖÿ© ÿßŸÑŸÄùúå   , ÿ™ÿ≥ÿßŸàŸä ÿµŸÅÿ±Ÿà ÿ®ÿ≠ŸÇŸÇ ŸÖÿπÿßŸá ÿßŸÇŸÑ  ‚àÜf  
 Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸäÿßŸÇŸÑ bandwidth 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê∏
2 ùëÅùëú 
‚à¥ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê¥2 ùëáùëè
4 ùëÅùëú 
 
 
 
 
 
F
ind the probability of error for the FSK system
,
 
where
 
ùíî
ùüè
(
ùíï
)
=
ùüè
.
ùüíùüèùüí
ùíÑùíêùíî
(
ùüèùüéùüèùüéùíï
)
 
ùíî
ùüê
(
ùíï
)
=
ùüè
.
ùüíùüèùüí
ùíÑùíêùíî
(
ùüèùüéùüéùüéùíï
)
 
Noise power spectral density  
ùëµ
ùüé
ùüê
=
ùüé
.
ùüéùüè
  
is added, and 
a
 
matched filter detector is used. Assume 
that the bit period is 1 sec. 
 
Solution
 
 
 
 
 
 
 
 
 
 
 ŸÖÿßÿØÿßŸÖ ŸÖŸÇÿßŸÑÿ¥ ŸÜÿ¥ÿ™ÿ∫ŸÑ
maximum performance
 
 ÿßŸà
orthogonal tone spacing
 
   Ÿäÿ®ŸÇŸâ ŸáŸÜÿ¥ÿ™ÿ∫ŸÑ ÿπŸÑŸâ ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿπÿßŸÖ ŸÑŸÑŸÄ
ùúå
 
ùê∏
1
=
ùê∏
2
=
ùê¥
2
2
 
ùëá
ùëè
=
1
.
414
2
2
 
√ó
1
=
1
 
 
Watt
.
sec
 
ùê∏
=
1
 
 
 
Watt
.
sec
 
ùë¶
ùëú
=
0
 
 
 
Watt
.
sec
 
f
1
‚àí
f
2
=
2
‚àÜ
f
 
‚à¥
‚àÜ
f
=
f
1
‚àí
f
2
2
=
1010
2
ùúã
‚àí
1000
2
ùúã
2
=
2
.
5
ùúã
 
Hz
 
ùúå
=
sin
(
4œÄ
‚àÜ
f
 
T
b
)
4œÄ
‚àÜ
f
 
T
b
=
sin
(
4œÄ
√ó
2
.
5
ùúã
√ó
1
)
4œÄ
√ó
2
.
5
ùúã
√ó
 
1
=
sin
(
10
√ó
180
ùúã
)
10
=
‚àí
0
.
054
 
 
 ŸÖŸÜŸÜÿ≥ÿßÿ¥ ÿßŸÜ ÿßŸÑŸÑŸä ÿØÿßÿÆŸÑ ÿßŸÑŸÄ
sin
 
  ÿ®ÿßŸÑÿ±ÿßÿØŸäÿßŸÜ ŸÅÿ∂ÿ±ÿ®ŸÜÿß ŸÅŸä
180
ùúã
 
 ÿπÿ¥ÿßŸÜ ŸÜÿ≠ŸàŸÑŸáÿß ŸÑŸÄ
degree
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
1
 
(
1
+
0
.
054
)
2
 
√ó
0
.
02
=
0
.
5
 
ùëíùëüùëìùëê
 
(
5
.
1
)
 
 
5
.
1
 
 ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØŸá ŸÅŸä ÿßŸÑÿ¨ÿØŸàŸÑ ŸÅŸáŸÜÿ≥Ÿäÿ® ÿßŸÑŸÜÿßÿ™ÿ¨ ÿ®ÿØÿßŸÑŸÑÿ© ÿßŸÑŸÄ
erfc
 Example
 
1
 
‚à´
 
ùëëùë°
1
0
 
‚à´
 
ùëëùë°
1
0
 
ùë†
(
ùë°
)
 
ùë¶
<
0
 
 
 
 
 
 
 
 
 
0
 
ùë¶
>
0
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
=
0
 
 
 
ùëá
ùëè
=
1
 
ùë†ùëíùëê
 
ùëÜ
ùêπùëÜùêæ
 
ùë†
1
(
ùë°
)
=
1
.
414
ùëêùëúùë†
(
1010
ùë°
)
 
ùë†
2
(
ùë°
)
=
1
.
414
ùëêùëúùë†
(
10
0
0
ùë°
)
 
‚àë
 
 
 ŸÅŸäÿ≠ÿßŸÑÿ© PSK  Ÿäÿ™ŸÖ ÿ™ÿ∫ŸäŸäÿ± phase ÿßÿ¥ÿßÿ±ÿ© ÿßŸÑŸÄ carrier  ÿ™ÿ®ÿπŸÄÿß ŸÑŸÇŸäŸÖŸÄÿ© ÿßŸÑŸÄŸÄ information signal  ŸÖŸÄÿπ ÿ´ÿ®ŸÄÿßÿ™ ŸÇŸäŸÖŸÄÿ© ÿßŸÑŸÄŸÄ amplitude 
 ,Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿπŸÜÿØŸä ÿßÿ™ŸÜŸäŸÜ phase  ,ŸÑÿ•ŸÑÿ±ÿ≥ÿßŸÑÿßÿ£ŸÑŸàŸÑ ùúÉ1  ÿπŸÜÿØŸÖÿß ŸÜÿ±ÿ≥ŸÑ binary 1  , Ÿà ÿßŸÑÿ´ÿßŸÜŸäùúÉ2  ÿπŸÜÿØŸÖÿß ŸÜÿ±ÿ≥ŸÑ binary 0  
 ÿπÿ¥ÿßŸÜ ŸÜÿπŸÖŸÑ ÿØŸäÿ≤ÿßŸäŸÜÿßŸÑŸÄ matched filter  ŸÑŸÑŸÄŸÄPSK  ŸáŸÜÿ≠ÿ™ŸÄÿßÿ¨ ŸÜÿ¨ŸäŸÄÿ® ŸÖÿπŸÄÿßÿØŸÑÿ™ŸäŸÜ ùë†1(ùë°)  Ÿà ùë†2(ùë°)  ÿ≠ŸäŸÄÿ´ ÿßŸÜ ùë†1(ùë°)  ŸáŸÄŸä ÿßŸÑŸÄŸÄ
modulated signal   ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜ binary 1  Ÿà ùë†2(ùë°)  ŸáŸä ÿßŸÑŸÄ modulated signal  ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜ binary 0 
 
‚ñ™ Digital signal     Sb(t)=¬±v 
ŸÜÿπÿ®ÿ± ÿπŸÜ binary 1  ÿ®ŸÄ  +v   ŸàŸÜÿπÿ®ÿ± ÿπŸÜ binary 0  ÿ®ŸÄ  ‚àív 
‚ñ™ Carrier signal    ùëìùëê(ùë°)=Acos2œÄfct 
 
‚ñ™ PSK signal      SPSK =Acos(2œÄfct+Kp Sb(t) )   
    
 ÿπÿ¥ÿßŸÜ ŸÜÿÆŸÑŸä ÿ¨ÿ≤ÿ° ÿßŸÑŸÄ phase  Ÿäÿ™ÿ∫Ÿäÿ± ŸÖÿπ ÿßŸÑŸÄ information signal  ŸáŸÜÿ∂ŸäŸÅ ÿπŸÑŸâ ÿßŸÑŸÄ phase ÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™  Sb(t) 
  ŸÖÿ∂ÿ±Ÿàÿ®ÿ© ŸÅŸä ÿ´ÿßÿ®ÿ™ ÿßŸÑÿ™ŸÜÿßÿ≥ÿ®Kp    
 
  ŸÜÿπŸàÿ∂ ÿπŸÜ Sb(t)    ÿ®ŸÄ¬± v  ÿßŸÑŸÑŸä ÿ™ŸÖÿ´ŸÑ1 binary  Ÿà0 binary  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑÿπÿßŸÑŸÇÿ© ÿßÿßŸÑÿ™ŸäŸá:  
SPSK =Acos(2œÄfct¬±Kp v) 
 ÿßŸÑÿ™ÿ±ÿØÿØ ŸàÿßŸÑŸÄ phase  ÿßÿßŸÑÿ™ŸÜŸäŸÜ ÿ®Ÿäÿ™ÿ∫Ÿäÿ±Ÿà ŸÖÿπ ÿßŸÑÿ≤ŸÖŸÜ ŸÅŸäPSK   ŸàŸÑŸÉŸÜ ÿßŸÑÿ™ÿ±ÿØÿØ Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ ÿ™ŸÅÿßÿ∂ŸÑÿßŸÑŸÄ information signal   ŸàÿßŸÑŸÄ
phase  Ÿäÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπÿßŸÑŸÄ information signal 
ùëìùëñ(ùë°)=1
2ùúã ùëëùúÉ(ùë°)
ùëëùë° =fc+Kp
2ùúã Sb(t) 
 
‚ñ™ Max phase deviation       ùõΩ=‚àÜŒ∏=Kp v 
SPSK =Acos(2œÄfct¬±ùõΩ) 
‚ñ™ Assume   +ùõΩ=ùúÉ1    ,   ‚àíùõΩ=ùúÉ2   
 
 
ùë†1(ùë°)=Acos(2œÄfct+Œ∏1)        binary   1 
ùë†2(ùë°)=Acos(2œÄfct+Œ∏2)         binary   0 
 
 
 
 Phase Shift Keying (PSK)  
 
 
 
 
 
 
 
 
 
 
 
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
+
ùúÉ
1
)
 
 
ùë†
2
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
+
ùúÉ
2
)
 
 
ÔÅ≤
 
Energy
 
(E)
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
ùê¥
2
2
 
ùëá
ùëè
 
 
ùê∏
2
=
‚à´
ùë†
2
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
ùê¥
2
2
 
ùëá
ùëè
 
 
ùê∏
=
ùê∏
1
+
ùê∏
2
2
=
 
[
ùê¥
2
2
 
ùëá
ùëè
+
ùê¥
2
2
 
ùëá
ùëè
2
]
=
ùê¥
2
2
 
ùëá
ùëè
 
 
ÔÅ≤
 
Threshold 
(
ùíö
ùüé
)
 
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
0
2
=
[
ùê¥
2
2
 
ùëá
ùëè
‚àí
ùê¥
2
2
 
ùëá
ùëè
2
]
=
0
 
 
 
 
Design and Performance for PSK
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
ùë†
2
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
+
ùúÉ
2
)
 
 
S
PSK
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
+
ùúÉ
1
)
 
 
‚àë
 
ÔÅ≤ Cross correlation  
 
ùúå=1
ùê∏‚à´ùë†1(ùë°) ùë†2(ùë°) ùëëùë°ùëáùëè
0=2
ùê¥2ùëáùëè‚à´ùê¥ùëêùëúùë†(2ùúãùëìùëêùë°+ùúÉ1) ùê¥ùëêùëúùë†(2ùúãùëìùëêùë°+ùúÉ2) ùëëùë°ùëáùëè
0 
=2
ùê¥2ùëáùëè√óùê¥2
2‚à´[ùëêùëúùë†(4ùúãùëìùëêùë°+ùúÉ1+ùúÉ2)+ ùëêùëúùë†(ùúÉ1‚àíùúÉ2)] ùëëùë°ùëáùëè
0 
=1
ùëáùëè[‚à´ùëêùëúùë†(4ùúãùëìùëêùë°+ùúÉ1+ùúÉ2) ùëëùë°ùëáùëè
0+‚à´ùëêùëúùë†(ùúÉ1‚àíùúÉ2) ùëëùë°ùëáùëè
0] 
=1
ùëáùëè[  ùë†ùëñùëõ(4ùúãùëìùëêùë°+ùúÉ1+ùúÉ0)
4ùúãùëìùëê|
0ùëáùëè
+ùëêùëúùë†(ùúÉ1‚àíùúÉ2) ùë° |0ùëáùëè]‚âÖ1
ùëáùëè[  0+ùëêùëúùë†(ùúÉ1‚àíùúÉ2) ùëáùëè] 
 
 ùë†ùëñùëõ(4ùúãùëìùëêùë°+ùúÉ1+ùúÉ0)
4ùúãùëìùëê   ÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ™ÿ≥ÿßŸàŸä ÿµŸÅÿ± ÿßŸÑŸÜùëìùëê   ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÖŸÇÿßŸÖ ŸÇŸäŸÖÿ™Ÿáÿß ŸÉÿ®Ÿäÿ±Ÿá ÿ¨ÿØÿß 
 
‚à¥ ùúå=ùëêùëúùë†(ùúÉ1‚àíùúÉ2)  
 
 Ÿàÿßÿ∂ÿØ ŸÖŸÜ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÜ ÿßŸÑŸÄ cross correlation   ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâùúÉ1   ŸàùúÉ2   ŸàÿØÿß ŸÖŸÜÿ∑ŸÇŸä ÿßŸÑŸÜ ŸÇŸäŸÖŸáŸÖ ŸÑŸÖÿß ÿ™ÿ™ÿ∫Ÿäÿ± ÿØÿß ÿ®Ÿäÿ∫Ÿäÿ± ÿ¥ŸÉŸÑ
ÿ•ÿ¥ÿßÿ±ÿ©  ùë†1(ùë°)   Ÿà  ùë†2(ùë°)  .Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜŸáŸÖ ŸáŸäÿ™ÿ∫Ÿäÿ± 
 
ÔÅ≤ Maximum performance  
maximum  performance at minimum ùúå               
 
 ÿßÿπŸÑŸâ ŸÉŸÅÿßÿ°ÿ© ÿπŸÜÿØŸÖÿß ÿ™ŸÉŸàŸÜ ŸÇŸäŸÖÿ©   ùúå  ÿßŸÇŸÑ ŸÇŸäŸÖÿ© ÿßŸÑŸÜ ÿØÿßŸáŸäŸÇŸÑŸÑ ŸÇŸäŸÖÿ© ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ£  ùëùùëí  , ŸàÿßŸÇŸÑ ŸÇŸäŸÖÿ© ÿπŸÜÿØ  ùúå=‚àí1    ŸàÿØŸä ŸÖŸÖŸÉŸÜ
ŸÜÿ≠ÿµŸÑ ÿπŸÑŸäŸáÿß ŸÑŸÖÿß ÿßŸÑŸÄ phase shift    ÿ®ŸäŸÜùúÉ1   ŸàùúÉ2  ŸäŸÉŸàŸÜ ÿ®ŸÄ180 
 
ùëêùëúùë†(ùúÉ1‚àíùúÉ2)=‚àí1  
ùúÉ1‚àíùúÉ2=ùúã 
ùúÉ1=ùúã
2      ,     ùúÉ2=‚àíùúã
2 
 
 
ÔÅ≤ Probability of error (bit error rate)  
For maximum performance at ùúå=‚àí1 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1+1)
2 ùëÅùëú 
‚à¥ ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ 
 ùëÅùëú 
 
For the same performance, the pulse energy in ASK must be twice that in PSK ,thus in coherent 
detection, PSK is always performable to ASK  
  ŸÅŸäASK   ŸÅŸäÿ≠ÿßŸÑÿ© OOK  ŸÉÿßŸÜÿ™( ùúå=0)  Ÿàÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿ∑ÿ£ Ÿäÿ≥ÿßŸàŸä  (0.5 ùëíùëüùëìùëê  ‚àöùê∏ 
2 ùëÅùëú)   Ÿàÿ®ÿßŸÑÿ™ŸÄÿßŸÑŸä ÿßÿ≠ÿ™ŸÖŸÄÿßŸÑ ÿßŸÑÿÆÿ∑ŸÄÿ£ ŸÅŸÄŸä
PSK    ÿßŸÇŸÑŸàÿπÿ¥ÿßŸÜ ŸÜÿ≥ÿ™ÿÆÿØŸÖ   ASK    ÿ®ŸÜŸÅÿ≥ ÿßŸÑŸÉŸÅÿßÿ°ÿ© ÿßŸÑŸÄ Energy    ÿ®ÿ™ÿßÿπÿ©ÿßÿ•ŸÑÿ¥ÿßÿ±ÿ©  Ÿáÿ™ŸÉŸàŸÜ ÿßŸÑÿ∂ŸÄÿπŸÅ  ,ŸàÿØÿß ŸÖŸÜÿ∑ŸÇŸÄŸä ÿßŸÑŸÜ   ASK   ÿßŸÉÿ´ŸÄÿ±
ÿ™ÿ£ÿ´ÿ±ÿß ÿ®ÿßŸÑŸÄ noise  ÿßŸÑŸÜ ÿßŸÑŸÄ amplitude  ÿ®Ÿäÿ™ÿ£ÿ´ÿ± ÿ®ÿßŸÑŸÄ noise  ÿßŸÉÿ™ÿ± ŸÖŸÜ ÿßŸÑŸÄ  phase Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸäŸÉŸÅÿßÿ°ÿ© System PSK  ÿßÿπŸÑŸâ ŸÖŸÜ ŸÉŸÅÿßÿ°ÿ©
System ASK 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
A coherent matched filter detector is used to detect the following two signals 
 
ùê¨
ùüè
(
ùê≠
)
=
ùüé
.
ùüéùüì
ùêúùê®ùê¨
(
ùüêùõë
ùêü
ùêú
ùê≠
+
ùüóùüé
)
 
ùê¨
ùüê
(
ùê≠
)
=
ùüé
.
ùüéùüì
ùêúùê®ùê¨
(
ùüêùõë
ùêü
ùêú
ùê≠
‚àí
ùüóùüé
)
 
bit period 
 
ùëª
ùíÉ
=
ùüé
.
ùüì
 
ùíîùíÜùíÑ
 
and
 
power spectral noise
 
ùëµ
ùüé
=
ùüèùüé
‚àí
ùüí
 
Find the 
probability of bit error
 
Solution
 
 
 
 
 
 
 
 
 
 
 ŸÅŸä
ÿßŸÑŸÖÿ≥ÿßŸÑÿ©
 
ŸÑŸÖ Ÿäÿ™ŸÖ ÿ∞ŸÉÿ± ŸÜŸàÿπ ÿßŸÑŸÄ 
modulation
 
 ŸàŸÑŸÉŸÜ ŸÖŸÜ ÿßŸÑŸÖÿπÿßÿØÿßŸÑÿ™ ŸáŸÜÿπÿ±ŸÅ ÿßŸÜŸá
PSK
  
 ÿßŸÑŸÜ ÿßŸÑÿ™ÿ∫Ÿäÿ± ÿ®Ÿäÿ≠ÿµŸÑ ŸÅŸä ÿßŸÑŸÄ
phase
 
ùê∏
1
=
ùê∏
2
=
ùê¥
2
2
 
ùëá
ùëè
=
0
.
05
2
2
 
√ó
0
.
5
=
6
.
25
√ó
10
‚àí
4
 
 
Watt
.
sec
 
ùê∏
=
ùê¥
2
2
 
ùëá
ùëè
=
6
.
25
√ó
10
‚àí
4
 
 
Watt
.
sec
 
ùë¶
ùëú
=
0
 
ùúå
=
ùëêùëúùë†
(
ùúÉ
1
‚àí
ùúÉ
2
)
 
=
ùëêùëúùë†
(
90
+
90
)
=
‚àí
1
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
6
.
25
√ó
10
‚àí
4
 
10
‚àí
4
=
0
.
5
 
ùëíùëüùëìùëê
 
(
2
.
5
)
=
0
.
5
√ó
0
.
00
041
=
2
.
05
√ó
10
‚àí
4
 
 
 
 
 
 
 
 Example
 
2
 
‚à´
 
ùëëùë°
0
.
5
0
 
‚à´
 
ùëëùë°
0
.
5
0
 
ùë†
(
ùë°
)
 
ùë¶
<
0
 
 
 
 
 
 
 
 
 
0
 
ùë¶
>
0
 
 
 
 
 
 
 
 
 
1
 
ùë¶
ùëú
=
0
 
 
 
ùëá
ùëè
=
0
.
5
 
ùë†ùëíùëê
 
ùë†
2
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
‚àí
90
)
 
S
PSK
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
ùëê
ùë°
+
90
)
 
‚àë
 
 
 
Matched 
filter can be
 
used as coherent detector for BFSK demodulation:
 
a.
 
Design a matched filter to demodulate BFSK signal.
 
b.
 
Deduce the probability of bit error rate
.
 
Solution
 
 
 
 
 
 
 
 
 
 
 
 
s
1
(
t
)
=
A
cos
(
2œÄ
f
1
t
)
 
s
2
(
t
)
=
A
cos
(
2œÄ
f
2
t
)
 
 
ÔÅ≤
 
Energy
 
(E)
 
ùê∏
1
=
‚à´
ùë†
1
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
ùê¥
2
2
 
ùëá
ùëè
 
ùê∏
2
=
‚à´
ùë†
2
2
(
ùë°
)
 
ùëëùë°
ùëá
ùëè
0
=
ùê¥
2
2
 
ùëá
ùëè
 
E
=
E
1
+
E
2
2
=
 
[
A
2
2
 
T
b
+
A
2
2
 
T
b
2
]
=
A
2
2
 
T
b
 
 
ÔÅ≤
 
Threshold 
(
ùíö
ùüé
)
 
ùë¶
ùëú
=
ùê∏
1
‚àí
ùê∏
2
2
=
[
ùê¥
2
2
 
ùëá
ùëè
‚àí
ùê¥
2
2
 
ùëá
ùëè
2
]
=
0
 
 
 
  
[Extra] 
Related Exams Questions
 
Question 
1
 
(Final 20
20
)
 
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
‚à´
 
 
ùëëùë°
ùëá
ùëè
0
 
ùë†
1
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
1
ùë°
)
 
ùë†
2
(
ùë°
)
=
ùê¥
ùëêùëúùë†
(
2
ùúã
ùëì
2
ùë°
)
 
ùëÜ
ùêπùëÜùêæ
 
‚àë
 
ÔÅ≤ Cross correlation ( ùùÜ ) 
 
ùúå=1
E‚à´ s1(t) s2(t) dtTb
0=2
A2Tb‚à´ Acos(2œÄf1t) Acos(2œÄf2t) dtTb
0 
=2
A2Tb√óA2
2‚à´ [cos(2œÄ(f1‚àíf2)t)+ cos(2œÄ(f1+f2)t)] dtTb
0 
=1
Tb[‚à´ cos(2œÄ(f1‚àíf2)t) dtTb
0+‚à´ cos(2œÄ(f1+f2)t) dtTb
0] 
f1=fc+‚àÜf 
f2=fc‚àí‚àÜf 
‚à¥f1‚àíf2=2‚àÜf                ,                      f1+f2=2fc  
ùúå=1
Tb[‚à´ cos(4œÄ‚àÜf t) dtTb
0+‚à´ cos(4œÄfct) dtTb
0] 
=1
Tb[  sin(4œÄ‚àÜf t)
4œÄ‚àÜf|
0Tb
+ sin(4œÄfct)
4œÄfc|
0Tb
]=1
Tb[  sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf+ sin(4œÄfcTb)
4œÄfc] 
fc‚â´‚àÜf 
‚à¥ ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb 
 
ÔÅ≤ Bit error rate  (ùíëùíÜ)   
 
ÔÅ≤ General Case  
ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb 
ùëùùëí=0.5 ùëíùëüùëìùëê ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú 
 
ÔÅ≤ For Maximum performance  
Minimum probability of error ùëùùëí at minimum  ùúå 
For minimum ùúå ‚Üí  ùúïùúå
ùúï‚àÜf=0 
 
 
ùúïùúå
ùúï‚àÜf=4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)√ó4œÄTb‚àísin(4œÄ‚àÜf Tb)√ó4œÄTb
(4œÄ‚àÜf Tb)2=0 
4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)√ó4œÄTb‚àísin(4œÄ‚àÜf Tb)√ó4œÄTb=0 
4œÄ‚àÜf Tb√ócos(4œÄ‚àÜf Tb)=sin(4œÄ‚àÜf Tb) 
 
4œÄ‚àÜf Tb=tan(4œÄ‚àÜf Tb) 
4œÄ‚àÜf Tb=0.715 √ó2œÄ   rad 
 
‚àÜf=0.715 √ó2œÄ
4œÄ Tb=0.715
2 Tb   
1
 Tb=Rb 
‚à¥‚àÜf=0.715 √óRb
2   
Substitution in ùúå 
ùúå=sin(4œÄ√ó0.715 √óRb
2√óTb)
4œÄ√ó0.715 √óRb
2√ó Tb=sin(2œÄ√ó0.715 )
2œÄ√ó0.715=‚àí0.22 
 
Minimum probability of error at  ùúå=‚àí0.22 
ùëùùëí=0.5 ùëíùëüùëìùëê ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê ‚àö1.22 ùê∏
2 ùëÅùëú 
 
ÔÅ≤ Orthogonal Tone Spacing  
‚àÜf=Rb
2 
ùëì1‚àíùëì2=2‚àÜf=Rb 
Substitution in ùúå 
ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb=sin(4œÄ√óRb
2 √óTb)
4œÄ√óRb
2 √ó Tb=sin(2œÄ)
2œÄ=0 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê∏
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê¥2 ùëáùëè
4 ùëÅùëú 
 
 
 
A binary matched filter detector is used to demodulate BFSK received signal  
i. Find the value of the correlation coefficient ùùÜ when orthogonal tone spacing is used.  
ii. For the above case, calculate the probability of bit error.  
Solution  
 
For the two tones with frequencies ùëì1 and ùëì2 to be orthogonal (i.e. ùúå=0 ) a condition must be 
satisfied. The minimum value of ‚àÜf that satisfies orthogonality is for ùëõ=1  
‚àÜf=ùëõ Rb
2 
‚àÜf=Rb
2 
ùëì1‚àíùëì2=2‚àÜf=Rb 
 
Substitution in ùúå 
ùúå=sin(4œÄ‚àÜf Tb)
4œÄ‚àÜf Tb=sin(4œÄ√óRb
2 √óTb)
4œÄ√óRb
2 √ó Tb=sin(2œÄ)
2œÄ=0 
 
 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê∏ (1‚àíùúå)
2 ùëÅùëú=0.5 ùëíùëüùëìùëê  ‚àöùê∏
2 ùëÅùëú 
ùê∏=ùê¥2
2 ùëáùëè 
ùëùùëí=0.5 ùëíùëüùëìùëê  ‚àöùê¥2 ùëáùëè
4 ùëÅùëú 
 
 
 
 
 
 
 
 
 
 
 
 Question 2 (Final 201 8) 
 
 
 
In binary phase shift keying (BPSK), the two signals used to transmit a 0 and a 1 are:
 
ùíî
ùüé
(
ùíï
)
=
ùë®
ùíÑùíêùíî
(
ùüêùùÖ
ùíá
ùíÑ
ùíï
+
ùúΩ
ùüé
)
 
ùíî
ùüè
(
ùíï
)
=
ùë®
ùíÑùíêùíî
(
ùüêùùÖ
ùíá
ùíÑ
ùíï
+
ùúΩ
ùüè
)
 
What is the best choice of phase angles that achieves a minimum bit error rate? Why?
 
Solution
 
 
The best choice achieved by
 
ùúÉ
1
‚àí
ùúÉ
0
=
180
 
Because 
the maximum performance occurred 
at minimum
 
ùúå
 
ùúå
=
cos
(
Œ∏
1
‚àí
Œ∏
0
)
 
=
cos
(
180
)
 
=
‚àí
1
 
Then 
the bit error rate will be minimum
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê∏
 
(
1
+
1
)
2
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê∏
 
 
 
 
 
 
 
Find the probability of error of the coherent detector when used for FSK, the bit period is 2
 
msec and the signals have amplitude of 
0.4 volt. The two frequencies are 1KHZ and 2 KHZ .
 
The additive noise has power of 
ùüè
ùüé
‚àí
ùüèùüê
 
W/HZ
 
Solution
 
ùê¥
=
0
.
4
 
volt
 
 
 
 
,
 
 
 
 
 
 
ùëá
ùëè
=
2
 
msec
 
 
 
,
 
 
 
 
 
 
 
ùëì
1
=
2
 
KHz
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
ùëì
2
=
1
 
KHz
 
 
 
 
 
 
 
ùê∏
=
ùê¥
2
2
ùëá
ùëè
=
0
.
4
2
2
√ó
2
√ó
10
‚àí
3
=
1
.
6
√ó
10
‚àí
4
 
 
Watt
.
sec
 
ùëÖ
ùëè
=
1
2
 
msec
=
0
.
5
 
KHz
 
‚àÜ
ùëì
=
ùëì
1
‚àí
ùëì
2
2
=
2
‚àí
1
2
=
0
.
5
 
KHz
 
 
 
 
 
 
ùúå
=
sin
(
4œÄ
‚àÜ
f
 
T
b
)
4œÄ
‚àÜ
f
 
T
b
=
sin
(
4œÄ
√ó
0
.
5
√ó
10
3
 
√ó
2
√ó
10
‚àí
3
)
4œÄ
√ó
0
.
5
√ó
10
3
 
√ó
2
√ó
10
‚àí
3
=
sin
(
4œÄ
)
4œÄ
=
0
 
 
so
 
i
t
‚Ä≤
s
 
orthogonal
 
‚àÜ
ùëì
 
 ÿ≠ŸÇŸÇÿ™ ÿßŸÑŸÄ
orthogonality
 
 Ÿà
ŸÖŸÖŸÉŸÜ ŸÜÿπÿ±ŸÅ
 
ÿßŸÜ  
ùúå
=
0
 
 ŸÖŸÜ ÿ∫Ÿäÿ± ÿßŸÑÿ™ÿπŸàŸäÿ∂ ŸÅŸä ÿßŸÑŸÇÿßŸÜŸàŸÜ
ŸÖŸÜ ÿÆÿßŸÑŸÑ ÿßŸÑÿπÿßŸÑŸÇÿ©    
‚àÜ
f
=
ùëõ
 
R
b
2
   
  ŸàŸÅŸä ÿßŸÑÿ≠ÿßŸÑÿ© ÿØŸä
ùëõ
=
2
 
ùëù
ùëí
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
ùê∏
 
(
1
‚àí
ùúå
)
2
 
ùëÅ
ùëú
=
0
.
5
 
ùëíùëüùëìùëê
 
‚àö
1
.
6
√ó
10
‚àí
4
2
 
√ó
1
0
‚àí
12
=
0
.
5
 
ùëíùëüùëìùëê
 
(
8944
.
27
)
 
Question 
3
 
(Final 2016)
 
 
Question 
4
 
(Final 201
6
)
 
ùëÅ
ùëú
 
 
 
 
The 
Minimum Shift Keying (MSK) is a special case of Frequency Shift Keying (FSK),
 
explain.
 
Solution
 
 
Minimum shift keying (MSK) is a special type of frequency shift keying (FSK) when the two FSK 
frequencies are orthogonal
 
and 
the 
frequency separation 
(
‚àÜ
f
)
 
is minimum
 
 
‚àÜ
f
=
ùëõ
 
R
b
2
 
when the two frequencies are orthogonal 
,the minimum value of 
‚àÜ
f
 
that satisfies orthogonality is for 
ùëõ
=
1
 
 
‚àÜ
f
=
R
b
2
 
The minimum frequency separation in MSK is chosen to be half the bit rate
 
 
 
 
 
 
 
 
 
 
ÿ®ŸÉÿØÿß
 
ŸÜŸÉŸàŸÜ ŸàÿµŸÑŸÜÿß ŸÑŸÜŸáÿßŸäÿ© ÿßŸÑŸÉŸàÿ±ÿ≥  
Ÿäÿßÿ±ÿ® ŸäŸÉŸàŸÜ ÿ∂ÿßŸÅ ŸÑŸäŸÉŸà ÿ≠ÿßÿ¨ÿ© ÿ¨ÿØŸäÿØŸá ŸàÿßŸÉŸàŸÜ ŸÅÿπÿßŸÑ ŸÇÿØÿ±ÿ™ ÿßŸÇÿØŸÖ ÿ≠ÿßÿ¨ÿ© ÿ™ŸÅŸäÿØ ÿßŸÑŸÜÿßÿ≥. 
 
Ÿàÿßÿ±ÿ¨Ÿà ŸÖŸÜ ŸáŸÑŸÑÿß ÿßŸÜ ŸäŸÉŸàŸÜ Ÿáÿ∞ÿß ÿßŸÑÿπŸÖŸÑ ŸÅŸä ŸÖŸäÿ≤ÿßŸÜ ÿ≠ÿ≥ŸÜÿßÿ™Ÿä ŸàÿßŸÑ ÿ™ŸÜÿ≥ŸàŸÜŸä ŸÖŸÜ ÿµÿßŸÑÿ≠ ÿØÿπÿßÿ¶ŸÉŸÖ.  
 
 
 
 
 
 
 
 
 
 
For any comments or feedback:
 
https://forms.gle/ouviBGLr4nae9pLC6
 
 
 
End of the course
 
Question 
5
 
(Final 201
6
)
 

 
 
 
 
 
 
   
 
¬©
 
Basem HeshamDelta PCM & Differential PCM & 
Convolution & AWGN
Lecture
 
7
 
 
‚ñ™ We approximate continuous waveform to a staircase signal at each sampling point we 
developed an error term (the difference between the signal and the staircase).  
‚ñ™ In case of delta modulation, the coding is done in units of 1 -bit. 
‚ñ™ In case of delta PCM modulation we code the difference into more than one bit.  
Delta PCM  ŸÜŸÅÿ≥ ŸÖÿ®ÿØÿ£ Delta modulation ŸàŸáŸä ÿßŸÜ ÿ®ÿØŸÑ ŸÖÿßÿßŸÜ ŸÜÿ® ÿßÿßŸÖ ÿßÿßÿß   ÿßŸÑÿßÿß ÿß samples  ŸáŸÜÿ® ÿßÿßŸÖ ÿßŸÑŸÅÿßÿßŸäŸÜ ÿ®ÿßÿß ŸÜ
ÿß ŸÖÿ© ÿßŸÑÿß sample Ÿàÿß ŸÖÿ© ÿßŸÑÿß staircase  Ÿàÿ®ŸÜ ÿ®Ÿä ÿπŸÜŸá ÿ®ÿß bit  Ÿàÿßÿ≠ÿØŸá ÿ®ÿ≥ ŸàŸÑŸÉŸÜ ŸÅŸä ÿ≠ŸÜŸÑÿ© Delta PCM  ÿ®ŸÜ ÿ®Ÿä ÿπŸÜŸá
ÿ®ŸÜŸÉÿ™Ÿä ŸÖŸÜ  bit 
Delta modulation ÿ®ŸÜÿ≥ÿßÿßÿ™ŸÖÿØŸÖŸá ŸÑŸÖÿßÿßŸÜ  ŸÉÿßÿßŸàŸÜ ÿßŸÑÿßÿß ÿß sampling rate ÿπÿßÿßŸÜŸÑŸä Ÿàÿ®ŸÜŸÑÿ™ÿßÿßŸÜŸÑŸä ÿßŸÑŸÅÿßÿßŸäŸÜ ÿ®ÿßÿß ŸÜ ÿßŸÑÿßÿß ÿß samples 
 Ÿàÿ® ÿ∂ŸáŸÜ ÿµÿ∫ Ÿä Ÿàÿ® ŸÜŸÑÿ™ŸÜŸÑŸä bit  Ÿàÿßÿ≠ÿØŸá ŸÉŸÜŸÅ Ÿá ÿπÿ¥ŸÜŸÜ ÿßÿπÿ®Ÿä ÿπŸÜ ÿßŸÑŸÅŸäŸÜ,ŸàŸÑŸÉÿßÿßŸÜ ŸÑÿßÿßŸà ŸÉÿßÿßŸÜŸÜ ÿß ŸÑÿßÿß ÿß sampling rate  ÿßŸÑ ÿßÿßŸÑ
Ÿàÿ®ŸÜŸÑÿ™ŸÜŸÑŸä ÿßŸÑŸÅŸäŸÜ ÿ® ŸÜ ÿßŸÑÿß  samples  ŸÉÿ® Ÿä  Ÿàÿ®ŸÜŸÑÿ™ÿßÿßŸÜŸÑŸä  bit  Ÿàÿßÿ≠ÿßÿßÿØŸá ŸÖÿßÿß  Ÿá  ŸÉÿßÿßŸàŸÜ ŸÉÿßÿßŸÜŸÅŸä  ŸÅŸáŸÜÿ≠ÿ™ÿßÿßŸÜ   ÿßŸÉÿ™ÿßÿßŸä ŸÖÿßÿßŸÜ  bit   ŸÅÿßÿßŸä
ÿßŸÑÿ≠ŸÜŸÑÿ© ÿØŸä ŸáŸÜÿ≥ÿ™ŸÖÿØ   ÿ∑Ÿä  ŸÇÿ©  Delta PCM . 
 
 
 
 
 
‚ñ™ Is another technique for sending information about changes in the  samples rather than 
about the sample values themselves.  
‚ñ™ The modulator sends the difference between a sample and its  predicted value.  
 
  ùëÜÃÇ(ùëõùëáùë†) ŸáŸä ÿßŸÑÿß  predicted value.ŸÅŸä ÿßŸÑÿß Transmitter   ŸÅ Ÿá ÿØÿß Ÿäÿ© ÿßÿ≥ŸÖŸáŸÜ predictor  Ÿàÿ®ÿ™ÿ≠ÿ≥ÿ® ÿßŸÑŸÇ ŸÖÿ© ÿßŸÑŸÖÿ™Ÿàÿß ÿ©
ÿ®ŸÜŸÜÿ° ÿπŸÑŸâ ÿßŸÑÿß  previous sample   ÿßŸÑŸÜ ŸÅŸä ŸÖ ÿ∏ÿßÿ•ŸÑÿ¥ŸÜŸäÿßŸÖ ŸÅ Ÿá correlation ŸÖŸÜ ÿ® ŸÜ ÿß   ÿßŸÑÿß sample   ÿßŸÑŸÇŸä ÿ®ÿ© ŸÖŸÜ
ÿ® ÿ∂ŸáŸÜ Ÿàÿß ŸÖÿ© ÿßŸÑÿß  sample  .ÿßŸÑÿ¨ÿØ ÿØÿ© ÿ™ÿ≥ŸÜŸàŸä ÿßŸÑŸÇ ŸÖÿ© ÿßŸÑŸÑŸä ÿßÿ®ŸÑŸáŸÜ ÿ®ÿ≥ ŸÖÿ∂ŸäŸàÿ®ÿ© ŸÅŸä ÿß ŸÖÿ© ŸÖ  ŸÜŸá 
ùëÜÃÇ(ùëõùëáùë†)=ùê¥[ùëÜ(ùëõ‚àí1)ùëáùë†] 
 
  ÿπÿ¥ŸÜŸÜ ÿßÿπŸÖŸÑ ÿØ ÿ≤ÿß ŸÜ ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠ ÿ≠ ÿßŸÑÿ≤  ÿßÿ¨ ÿ® ÿß ŸÖÿßÿßÿ© ÿßŸÑÿ®ŸÜÿ®ÿßÿßŸÖA    ÿßŸÑŸÑÿßÿßŸä ÿ™ÿ≠ŸÇÿßÿßŸÑ ÿßÿßÿßÿßŸÑ error  ÿßŸÑŸÜ  ÿßŸÑÿßÿß ÿß predictor  ÿπÿ¥ÿßÿßŸÜŸÜ
ÿ™ŸÉŸàŸÜ ŸÉŸÅŸÜÿ¶ÿ™Ÿá  ÿπŸÜŸÑ ÿ© ÿßŸÑÿ≤  ÿßŸÑŸÅŸäŸÜ ÿ® ŸÜ ÿßŸÑŸÇ ŸÖÿ© ÿßÿßŸÑÿµŸÑ ÿ© ŸàÿßŸÑŸÇ ŸÖÿ© ÿßŸÑŸÖÿ™Ÿàÿß ÿ© ÿßÿßŸÑ ŸÖŸÜ  ŸÉŸàŸÜ. 
 
ŸÅŸÉŸäÿ© ÿßÿßŸÑÿ®ÿ®ŸÜŸÖ ÿßŸÜŸÜŸÜ ŸáŸÜÿ≠ÿ≥ÿ® ÿßŸÑÿß  ùëí(ùëõùëáùë†) error   ŸàŸáŸà  ÿ≥ŸÜŸàŸä ÿßŸÑŸÅŸäŸÜ ÿ® ŸÜ ÿßŸÑŸÇ ŸÖÿ©ÿßŸÑÿ≠ŸÇ ŸÇ ÿ© ŸÑŸÑÿß sample  ùëÜ(ùëõùëáùëÜ)  
Ÿàÿß ŸÖÿ© ÿßŸÑÿß ùëÜÃÇ(ùëõùëáùë†)  predicted value   Ÿàÿ® ÿØŸáŸÜ ŸÜÿ¨ ÿ®   ùëöùë†ùëí Ÿàÿ≤Ÿä ŸÖŸÜ ÿπŸäŸÅŸÜŸÜ ÿßŸÑŸÖÿ≠ŸÜÿ∂Ÿäÿ© ÿßŸÑŸÑŸä ŸÅŸÜÿ™ŸÖ ÿßŸÜŸáŸÜ ÿßŸÑÿß  
expected value  ŸÑŸÑÿß  error ÿ™Ÿäÿ® ÿπ Ÿàÿ® ÿØ ŸÅŸÉ ÿßŸÑŸÖ ŸÜÿØŸÑÿ© ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ  ùëöùë†ùëí ÿ®ŸÜÿ≥ÿ™ŸÖÿØÿß  ÿßŸÑÿß  correlation 
ÿπÿ¥ŸÜŸÜ ÿßÿ¨ ÿ® ÿßÿßŸÑ ÿß ŸÖÿ© A ÿßŸÑŸÑŸä ÿ™ÿ≠ŸÇŸÑ ÿßÿßŸÑ  ùëöùë†ùëí ŸáŸÅŸÜÿ∂ŸÑ ÿßŸÑÿß  ùëöùë†ùëí  ÿ®ŸÜŸÑŸÜÿ≥ÿ®ÿ©ŸÑÿß A  Ÿàÿßÿ≥ŸÜŸàŸäÿßŸÑÿ™ŸÅŸÜÿ∂ŸÑ ÿ® ŸÜŸÑÿµŸÅŸä. 
 Delta PCM  
Differential  PCM  
 
ùëÜÃÇ(ùëõùëáùë†)=ùê¥ùëÜ([ùëõ‚àí1]ùëáùë†) 
ùëí(ùëõùëáùë†)=ùëÜ(ùëõùëáùëÜ)‚àíùëÜÃÇ(ùëõùëáùë†)=ùëÜ(ùëõùëáùëÜ)‚àíùê¥ùëÜ([ùëõ‚àí1]ùëáùë†) 
ùëöùë†ùëí =ùê∏{ùëí2(ùëõùëáùë†)}=ùê∏{[ùëÜ(ùëõùëáùëÜ)‚àíùëÜÃÇ(ùëõùëáùë†)]2} 
=ùê∏{[ùëÜ(ùëõùëáùëÜ)‚àíùê¥ùëÜ([ùëõ‚àí1]ùëáùë†)]2} 
 
ùëöùë†ùëí =ùê∏{ùëÜ2(ùëõùëáùëÜ)‚àí2ùê¥ùëÜ(ùëõùëáùë†)ùëÜ([ùëõ‚àí1]ùëáùë†)+ùê¥2ùëÜ2([ùëõ‚àí1]ùëáùë†)} 
=ùê∏{ùëÜ2(ùëõùëáùëÜ)}‚àí2ùê¥ ùê∏{ùëÜ(ùëõùëáùë†)ùëÜ([ùëõ‚àí1]ùëáùë†)}+ùê¥2 ùê∏{ùëÜ2([ùëõ‚àí1]ùëáùë†)} 
=ùëÖ(0)‚àí2ùê¥ùëÖ(ùëáùë†)+ùê¥2ùëÖ(0) 
=ùëÖ(0)[1+ùê¥2]‚àí2ùê¥ùëÖ(ùëáùë†) 
 
For min error  
ùëë(ùëöùë†ùëí )
ùëëùê¥=0 
‚à¥ ùëë(ùëöùë†ùëí )
ùëëùê¥=2ùê¥ùëÖ(0)‚àí2ùëÖ(ùëáùë†)=0 
‚à¥ ùê¥=ùëÖ(ùëáùë†)
ùëÖ(0) 
 
ÿßŸÑÿß  predictor  ŸÖŸÖŸÉŸÜ   ÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÉÿ™Ÿä ŸÖŸÜ previous sample  .ŸÑŸà ŸÖÿ®ÿßŸÑ ŸÉŸÜŸÜ ÿ®  ÿ™ŸÖÿØ ÿπŸÑŸâ ÿß ŸÖÿ™ ŸÜ previous 
sample 2 ŸÖ ŸÜÿØŸÑÿ™Ÿá Ÿáÿ™ŸÉŸàŸÜ ÿ®ŸÜŸÑÿ¥ŸÉŸÑ ÿØÿß ŸàŸÅŸä ÿßŸÑÿ≠ŸÑ ŸáŸÜÿ≠ÿ™ŸÜ  ŸÜŸÅŸÜÿ∂ŸÑ ŸÖŸäÿ™ ŸÜ  ŸÖŸäŸá ÿ®ŸÜŸÑŸÜÿ≥ÿ®ÿ© ŸÑÿß A  ŸàŸÖŸäŸá ÿ®ŸÜŸÑŸÜÿ≥ÿ®ÿ©ŸÑÿß B 
ùëÜÃÇ(ùëõùëáùë†)=ùê¥ùëÜ([ùëõ‚àí1]ùëáùë†)+ùêµùëÜ([ùëõ‚àí2]ùëáùë†)  
 
 
 
 
 
 
 
 
 
‚ñ™ Correlation is a measurement of the similarity between two  signals/sequences.  
‚ñ™ Convolution is a measurement of the effect of one signal on the  other signal.  
‚ñ™ Convolution is the common operation a linear and time  invariant system can perform on a 
given input signal.  
‚ñ™ In convolution, there is some input -output relationship, so this  acts like a filtering operation.  
 
Convolution  
 
 
 
 
ùë¶(ùëõ)= ùë•(ùëõ)‚àó‚Ñé(ùëõ)=‚àë ùë•(ùëõ‚àíùëò) ‚Ñé(ùëò)‚àû
ùëò = 0 
 
 
 
 
= ùë•(ùëõ) ‚Ñé(0)+ùë•(ùëõ‚àí1) ‚Ñé(1)+ùë•(ùëõ‚àí2) ‚Ñé(2)+ùë•(ùëõ‚àí3) ‚Ñé(3) 
 
 ùë¶(ùëõ) ÿ•ÿ¥ŸÜŸäÿ©  ÿßŸÑŸÖŸä  output signal   Ÿà ùë•(ùëõ) ÿ•ÿ¥ŸÜŸäÿ© ÿßŸÑÿØŸÖŸÑ input signal 
‚Ñé(ùëõ)     ÿßÿ≥ÿ™ÿ¨ŸÜÿ®ÿ© ÿßŸÑŸÜÿ∏ŸÜ response 
 
convolution  ŸáŸä ÿπŸÖŸÑ ÿ© Ÿä ŸÜÿ∂ ÿ© ÿ®ŸÜÿ¨ ÿ® ŸÅ ŸáŸÜ ÿ™ÿ£ÿ® Ÿä ÿ•ÿ¥ŸÜŸäÿ© ÿπŸÑŸâ ÿ•ÿ¥ŸÜŸäÿ© ÿ™ŸÜŸÜ ÿ© Ÿàÿ™ÿ≥ÿ™ŸÖÿØ  ŸÅŸä ÿ• ÿ¨ÿßÿßŸÜÿØ ÿßŸÑ ÿßŸÑÿßÿßÿßÿ© 
ÿ® ŸÜ ŸÖŸä   ÿ£Ÿä ŸÜÿ∏ŸÜ   Linear Time Invariant (LTI)  ŸàÿØŸÖŸÑŸá. 
 
ŸÖŸÜ  ÿßÿ£ŸÑŸÖÿ®ŸÑÿ©  ÿπŸÑŸâ  convolution    ÿßŸÜŸÑÿßÿßŸà ÿπŸÜÿßÿßÿØŸÜŸÜ  ÿ•ÿ¥ÿßÿßŸÜŸäÿ©  ùë•(ùëõ)    ŸàŸáŸÜÿ® ÿßÿßŸÖÿßÿ•ŸÑÿ¥ÿßÿßŸÜŸäÿ©  ÿØŸä ŸÅÿßÿßŸä ÿßŸÑŸáÿßÿßŸàÿß ŸàÿπÿßÿßŸÜ ÿ≤ ŸÜ 
ŸÜ ŸäŸÅ ÿßÿ≤Ÿä ÿßŸÑŸàÿ≥ÿ∑ ÿßŸÑŸÑŸä ŸáŸÜÿ® ŸÖ ŸÅ  Ÿá  ÿßÿ•ŸÑÿ¥ŸÜŸäÿ©  Ÿá ÿ£ÿ®Ÿä ÿπŸÑ Ÿáÿßÿß ŸÜ  ,ŸàÿØÿß ŸáŸÜ ŸäŸÅÿßÿßŸá  ŸÖÿßÿßÿßŸÑŸÑ ÿßÿ≥ÿßÿßÿ™ÿ¨ŸÜÿ®ÿ© ÿßŸÑŸÜÿ∏ÿßÿßŸÜ   ‚Ñé(ùëõ) 
 ŸàÿßŸÑŸÑŸä ÿ® Ÿàÿ∂ÿ≠ ÿ™ÿ£ÿ® Ÿä ÿßŸÑŸÖÿ≥ŸÜŸÅÿ© ŸàÿßŸÑÿ∏ŸäŸàŸÅ ÿßŸÑÿ® ÿ¶ ÿ© ÿπŸÑŸâÿßÿ•ŸÑÿ¥ŸÜŸäÿ© Ÿà  ŸÑŸà ÿ∑ÿ®ŸÇŸÜŸÜ ÿßŸÑÿß  convolution  ŸáŸÜ ŸäŸÅ ÿ™ÿßÿßÿ£ÿ® Ÿä
ÿßŸÑŸÜÿ∏ŸÜ  ‚Ñé(ùëõ)  ÿπŸÑŸâÿßÿ•ŸÑÿ¥ŸÜŸäÿ© ùë•(ùëõ)   Ÿàÿ®ŸÜŸÑÿ™ŸÜŸÑŸä ŸÜ ŸäŸÅ ÿßŸÑŸÖŸä ùë¶(ùëõ) 
 
  ŸÑŸà ÿßŸÑŸÜÿ∏ŸÜ  ÿπŸÜÿØŸÜŸÜ ŸÉŸÜŸÜ ŸÅŸÑÿ™Ÿä ŸÖÿ®ÿßŸÑ Ÿà‚Ñé (ùëõ)    ŸáŸä ÿßÿ≥ÿ™ÿ¨ŸÜÿ®ÿ© ÿßŸÑŸÜÿ∏ŸÜ  ŸàÿßŸÑŸÑŸäÿ®ÿ™Ÿàÿ∂ÿ≠ ÿ™ÿ£ÿ® Ÿä ÿßŸÑŸÅŸÑÿ™Ÿä ÿØÿß   ÿπŸÑŸâ  ÿßÿ•ŸÑÿ¥ÿßÿßŸÜŸäÿ©  
ŸàŸÑŸÜŸÅÿ™Ÿäÿ∂ ÿßŸÜŸá LPF  Ÿàÿ®ŸÜŸÑÿ™ŸÜŸÑŸä ŸÑŸà ÿ∑ÿ®ŸÇŸÜŸÜÿßŸÑÿß  convolution   ŸáŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ™ŸÜÿ® Ÿä ÿßŸÑŸÅŸÑÿ™Ÿä ÿπŸÑŸâ ÿßŸÑÿØŸÖŸÑ ŸàŸáŸÜÿßŸÑÿßÿßÿßŸä
ÿßŸÜ ÿßŸÑÿ™ŸäÿØÿØÿßŸÖ ÿßŸÑ ŸÜŸÑ ÿ© ÿßŸÑŸÅŸÑÿ™Ÿä ÿ¥ŸÜŸÑŸáŸÜ. Difference Between Correlation and 
Convolution  
ùíâ(ùíè) 
 ùíô(ùíè) 
 ùíö(ùíè) 
 
ùë•(ùëõ) 
 ùë¶(ùëõ) 
 
ùíâ(ùüé) ùíâ(ùüè) ùíâ(ùüê) ùíâ(ùüë) 
 
 
Given : ùíô[ùíè]=[ùüè ,ùüè ,ùüé ,ùüé ,ùüè]   ,   ùíâ[ùíè]=[ùüè ,ùüé ,ùüè]    
Calculate  ùíô[ùíè] convoluted with ùíâ[ùíè] 
Solution  
ùë¶(ùëõ)=‚àë ùë•(ùëõ‚àíùëò) ‚Ñé(ùëò)‚àû
ùëò = 0 
 =  ùë•(ùëõ) ‚Ñé(0)+ùë•(ùëõ‚àí1) ‚Ñé(1)+ùë•(ùëõ‚àí2) ‚Ñé(2) 
 
 
              1    1    0    0    1  
                    0    0    0    0    0 
                          1    1    0    0    1 
 
ùíö(ùíè)=   1    1    1    1    1    0    1    
 
ùíö(ùíè)=[ùüè    ùüè    ùüè    ùüè    ùüè    ùüé    ùüè] 
 
 
 
Given : ùíô[ùíè]=[ùüí ,ùüè ,ùüê ,ùüì]   ,   ùíâ[ùíè]=[ùüè  ,ùüê  ,‚àíùüè]    
Calculate  ùíô[ùíè] convoluted with ùíâ[ùíè] 
Solution  
ùë¶(ùëõ)=‚àë ùë•(ùëõ‚àíùëò) ‚Ñé(ùëò)‚àû
ùëò = 0 
=ùë•(ùëõ) ‚Ñé(0)+ùë•(ùëõ‚àí1) ‚Ñé(1)+ùë•(ùëõ‚àí2) ‚Ñé(2) 
 
 
              4     1     2     5     
                     8     2     4     10     
                           -4    -1     -2     -5 
 
ùíö(ùíè)=   4     9     0     8     8      -5      
 
ùíö(ùíè)=[ùüí     ùüó     ùüé     ùüñ       ùüñ      ‚àíùüì     ] 
 Example 1  
Example 2  
 
 
Given : 
ùíô
[
ùíè
]
=
[
ùüè
 
,
ùüê
 
,
ùüë
]
 
  
,   
ùíâ
[
ùíè
]
=
[
ùüí
 
 
,
ùüì
 
]
 
  
 
Calculate  
ùíô
[
ùíè
]
 
convoluted with 
ùíâ
[
ùíè
]
 
Solution
 
ùë¶
(
ùëõ
)
=
‚àë
ùë•
(
ùëõ
‚àí
ùëò
)
 
‚Ñé
(
ùëò
)
=
ùë•
(
ùëõ
)
 
‚Ñé
(
0
)
+
ùë•
(
ùëõ
‚àí
1
)
 
‚Ñé
(
1
)
‚àû
ùëò
 
=
 
0
 
 
              
4      8      12         
 
                      
5      10      15         
 
                          
 
ùíö
(
ùíè
)
=
 
 
4      13     22     15     
 
 
ùíö
(
ùíè
)
=
[
ùüí
 
 
 
 
 
 
ùüèùüë
 
 
 
 
 
ùüêùüê
 
 
 
 
 
ùüèùüì
]
 
 
 
Cross Correlation
 
  ÿ®ÿ∂Ÿäÿ® ŸÉŸÑ ÿß ŸÖÿ©
ŸÑ
ÿ•ŸÑÿ¥ŸÜŸäÿ©
  
ùë•
[
ùëõ
]
  
  ŸÅŸä ÿßŸÑŸÇ   ÿßŸÑŸÖŸÜŸÜÿ∏Ÿäÿ© ŸÑ ŸáŸÜ ŸÅŸä
ÿßÿ•ŸÑÿ¥ŸÜŸäÿ©
  
‚Ñé
[
ùëõ
]
  
  ŸàŸáŸÜŸÜ ÿ®ÿ∂Ÿäÿ®
ÿßÿ•ŸÑÿ¥ŸÜŸäÿ©
  
ŸÉŸÜŸÖŸÑŸá ŸÖ  
ÿß ŸÖÿ© Ÿàÿßÿ≠ÿØŸá ÿ®ÿ≥  
ÿ£ŸÑŸÜŸÜŸÜ
  
ÿ®ŸÜÿ¨ ÿ® ÿßŸÑÿ™ÿ¥ŸÜÿ®Ÿá ÿ® ŸÜ ÿßÿ¥ÿßÿßŸÜŸäÿ™ ŸÜ
  
ŸÅÿ®ŸÜŸÇÿßÿßŸÜŸäŸÜ ŸÉÿßÿßŸÑ ŸÜŸÇÿ∑ÿßÿßÿ© ŸàÿßŸÑŸÖŸÜÿßÿßŸÜÿ∏Ÿä ŸÑ ŸáÿßÿßŸÜ
  
Ÿàÿ®ŸÜÿ≠ÿ≥ÿßÿßÿ®  
ÿ®ÿßÿßŸäÿØŸà  
ÿπŸÜÿßÿßÿØ 
ÿ≠ŸÜÿßŸÑŸÖ ÿ≠ÿµŸÑ ŸÅ ŸáŸÜ 
shift
  
ŸÑÿ•ŸÑÿ¥ŸÜŸäÿ©
 
Ÿàÿ®ŸÜ ŸäŸÅ ŸÖŸÜŸáŸÜ ŸÖÿØŸâ 
ÿßŸÑÿ™ÿ¥ŸÜÿ®Ÿá
 
ÿ® ŸÜ  
ÿßÿ•ŸÑÿ¥ŸÜŸäÿßŸÖ.
 
 
 
 
Given :
 
ùíâ
[
ùíè
]
=
[
ùüè
,
ùüê
,
ùüë
]
 
  
,   
ùíô
[
ùíè
]
=
[
ùüí
 
,
ùüì
]
 
  
 
Calculate  
ùíô
[
ùíè
]
 
correlated with 
ùíâ
[
ùíè
]
 
Solution
 
 
ùíâ
[
ùíè
]
       
1       2       3
 
ùíô
[
ùíè
]
       
4       5       0
 
 
               
4      10      0   
=  14
 
 
 
 
 
Example 
3
 
 
Example 
4
 
 
 
ùíâ[ùíè]              1       2       3  
ùíô[ùíè‚àíùüè]       0       4       5 
 
                       0       8      15       =  23  
 
 
ùíâ[ùíè]              1       2       3        0     
ùíô[ùíè‚àíùüê]       0       0       4       5  
 
                       0       0       12      0      =  12  
 
 
ùíâ[ùíè]              1       2       3       0       0   
ùíô[ùíè‚àíùüë]       0       0       0      4       5  
 
                       0       0       0      0        0        =  0  
 
 
 
ùíâ[ùíè]               0      1        2       3  
ùíô[ùíè+ùüè]       4       5       0       0  
 
                       0       5       0        0       =  5  
 
 
ùíâ[ùíè]              0        0       1       2       3  
ùíô[ùíè+ùüê]       4       5       0       0        0  
 
                       0       0       0        0            =  0  
 
 
ùíÑùíêùíìùíì (ùíâ,ùíô)=[ùüé     ùüì     ùüèùüí    ùüêùüë    ùüèùüê   ùüé] 
 
 
 ùíâ[ùíè] ùíô[ùíè] ùíâ[ùíè] ùíô[ùíè+ùüè] ùíâ[ùíè] ùíô[ùíè+ùüê] ùíâ[ùíè] ùíô[ùíè‚àíùüè] ùíâ[ùíè] ùíô[ùíè‚àíùüê] ùíâ[ùíè] ùíô[ùíè‚àíùüë] 
 
 
 
‚ñ™
 
The performance of a digital system is 
quantified by the probability of bit detection 
errors in the presence of thermal noise. Its main source is addition of random signals a 
rising from the vibration of atoms in the receiver electronics.
 
 
  ŸÉŸÅŸÜÿ°ÿ©
ÿß
ŸÑ
ÿß
  
System
  
ŸÜ
ŸÖ
ŸÑ
 
ÿ≥
ŸÜ
ŸÇ
Ÿè
ÿ™
  
ŸÜŸÇÿØŸä ŸÜŸÉÿ™ÿ¥ŸÅ ÿßÿßŸÑŸÖÿ∑ŸÜÿ°
  
ŸÅŸä ÿß
ŸÑ
ÿß
  
Receiver
 
 ŸàŸáŸà
 
ŸÜ
ÿß
ÿß
Ÿá
 
ŸÑ
ÿπ
 
ŸÅ
ŸÜ
Ÿè
ÿ∂
ŸÖ
noise
 
 
 
ÿ©
ÿØ
ŸÜ
ÿß
ÿß
ÿπ
Ÿà
ÿ®ÿ™ŸÉŸàŸÜ 
thermal noise
 
 ,ŸàÿßŸÑŸÖÿµÿØŸä
ÿßÿ£ŸÑÿ≥ŸÜÿ≥Ÿä
 
ŸÑ
 ŸáŸÜ ÿßŸáÿ™ÿ≤ÿßÿ≤ ÿßŸÑÿ∞ŸäÿßŸÖ ŸÅŸä ÿß
ŸÑÿßÿß 
ÿß
 
Receiver
 
 ŸàÿßŸÑŸÑÿßÿßŸä ÿ®ÿ™ÿ≥ÿßÿßÿ®ÿ® ÿ™ŸàŸÑ ÿßÿßÿØ
ÿ•ÿ¥ŸÜŸäÿßŸÖ
 
ÿπÿ¥Ÿàÿßÿ¶ ÿ©.
 
 
Additive
 
ùíì
(
ùíï
)
=
ùíî
(
ùíï
)
+
ùùé
(
ùíï
)
 
The noise is added to the signal and is 
statistically independent of the signal 
 
:
Additive
 
ÿß
 
ŸÜ
ÿß
 
Ÿâ
ŸÑ
ÿß
 
Ÿä
 
ÿ¥
Ÿè
ÿ™
ŸÑ
ÿß
 
noise
 
 ÿ™  ÿßÿ∂ŸÜŸÅÿ™ŸáŸÜ ÿßŸÑŸâ ÿß
ŸÑ
ÿß
 
transmitted signal
 
 ÿ≠ ÿ´ ÿßŸÜ
ùëü
(
ùë°
)
 
ÿπÿ®ŸÜŸäÿ© ÿπŸÜ ÿß
ŸÑÿßÿß 
ÿß
 
transmitted signal
 
ùë†
(
ùë°
)
 
ÿß
 
ŸÜ
Ÿá
 
ŸÑ
ÿπ
 
ŸÅ
ŸÜ
ÿ∂
Ÿè
ŸÖ
ŸÑ
ÿß
 
noise
 
ùúî
(
ùë°
)
 
statistically independent
 
ŸÖ ŸÜŸÜŸáŸÜ ÿßŸÜ
 
ÿßÿßŸÑÿ¥ÿßÿßŸÜŸäÿ™ ŸÜ ŸÖÿ®  ÿ™ŸÖÿßÿßÿØŸà  ÿπŸÑÿßÿßŸâ 
ÿ® ÿ∂   ŸÜŸä ÿß
ŸÑ
ÿß
 
cross correlation
 
ÿ® ŸÜŸá   ÿ≥ŸÜŸàŸä ÿµŸÅŸä
.
 
 
 
White 
 
Just like the white colour which is  composed of all frequencies in the visible spectrum
. 
White noise has uniform power across the whole frequency
 
bands.
 
White
 : ÿ™ ŸÜŸä ÿ£ŸÜ
ÿß
ŸÑ
ÿß
  
noise
  
  ÿ™ŸÉŸàŸÜ ŸÖÿ™ÿ≥ŸÜŸà ÿ©
ŸÅŸä ÿß ŸÖÿ© ÿßŸÑÿ®ŸÜŸàŸä
  
ŸÅŸä ÿ¨ŸÖ ÿπ ÿßŸÑÿ™ŸäÿØÿØÿßŸÖ
.
  
ŸÑŸà Ÿäÿ≥ŸÖŸÜŸÜ 
ÿß
ŸÑ
ÿß
  
(PSD)
  
power 
spectral density
 
  ŸáŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä
.
 
 
 
 
 
 
 
 
(PSD)
 
power spectral density
  
 ÿπÿßŸÑÿßÿßÿßÿ© ÿ®ÿßÿß ŸÜ ÿßŸÑÿ™ÿßÿßŸäÿØÿØ ÿπŸÑÿßÿßŸâ ÿßŸÑŸÖÿ≠ÿßÿßŸàŸä ÿßÿßŸÑŸÅŸÇÿßÿßŸä Ÿà ÿßŸÑÿ®ÿßÿßŸÜŸàŸä ÿπŸÑÿßÿßŸâ ÿßŸÑŸÖÿ≠ÿßÿßŸàŸä
ÿßŸÑŸäÿ£ÿ≥Ÿä ŸàŸàÿ≠ÿØÿ™ŸáŸÜ  
watt/Hz
 
ŸàŸÑŸÖŸÜ ŸÜŸÉŸÜŸÖŸÑ ÿßŸÑŸÖÿ≥ŸÜÿ≠ÿ© ÿ™ÿ≠ŸÖ ÿßŸÑŸÖŸÜÿ≠ŸÜŸâ ŸáŸÜÿ¨ ÿ® ÿß ŸÖÿ© ÿßŸÑÿ®ŸÜŸàŸä
  
ÿ®ŸÜ
ŸÑ
ÿß
 
watt
.
 
Additive White Gaussian Noise 
(AWGN)
 
ùíá
 
PSD
 
Constant
 
 
Gaussian
 
The probability distribution of the noise samples is Gaussian with zero mean.
 
The values close to zero 
have a higher chance to occurrence.
 
 
Gaussian
 
 :
  ŸÜŸä ÿ£ŸÜ
 
ÿßŸÑ
ÿß
 
 
probability distribution
 
function
ŸÑŸÑ
ÿß
 
noise
 
  
ŸáŸä
 
Gaussian
 
  Ÿàÿß ŸÖÿ©
ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ÿßŸÑÿ≠ÿ≥ŸÜÿ®Ÿä ŸÑ Ÿá ÿ™ÿ≥ŸÜŸàŸä ÿµŸÅŸä   ŸÜŸä ÿßŸÑŸÇ   ÿßŸÑŸÇŸä ÿ®ÿ© ŸÖŸÜ ÿßŸÑÿµŸÅŸä ŸÑ ŸáŸÜ 
ÿßÿ≠ÿ™ŸÖŸÜŸÑ
 
ÿßŸÉÿ®Ÿä ŸÅŸä ÿßŸÑÿ≠ÿØŸàÿ´
.  
 
 
The time domain average of large number of noise samples is equal to zero.
 
 ÿØÿß ŸÖ ŸÜŸÜŸá ÿßŸÜŸÜŸÜ
ŸÑŸà 
  
ÿ≠ÿ≥ÿ®ŸÜŸÜ ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ 
 
ŸÑ
 ÿØÿØ ŸÉÿ® Ÿä ŸÖŸÜ ÿπ ŸÜŸÜŸÖ 
ÿß
ŸÑ
ÿß
  
noise
 
 ÿå
ÿß ŸÖÿ© 
ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ 
Ÿáÿ™ÿ≥ŸÜŸàŸä
 
ÿµŸÅŸä
 
Ÿà 
 
ÿØÿß ŸÖ ŸÜŸÜŸá  
ÿßŸÜ
 
 
mean = 0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
The following figure shows the transition probability diagram of binary symmetric 
channel.  
 
The probabilities of sending binary 0 and 1 are : ùë∑(ùë®ùüé)=ùë∑ùüé and ùë∑(ùë®ùüè)=ùë∑ùüè 
1. Calculate the probability ùë∑(ùë©ùüé) of receiving symbol zero  and   ùë∑(ùë©ùüè) of 
receiving symbol one.  
2.  ùë∑(ùë®ùüè|ùë©ùüè)   and  ùë∑(ùë®ùüé|ùë©ùüé)  
Solution  
 
ùëÉ(ùêµ1|ùê¥0)=ùëÉ(ùêµ0|ùê¥1)= ùëù  ,  and    ùëÉ(ùêµ0|ùê¥0)=ùëÉ(ùêµ1|ùê¥1)= 1‚àíùëù   
 
1. ùë∑(ùë©ùüé)   and   ùë∑(ùë©ùüè) 
 
ùëÉ(ùêµ0)=ùëÉ(ùêµ0|ùê¥0) ùëÉ(ùê¥0)+ùëÉ(ùêµ0|ùê¥1) ùëÉ(ùê¥1)  
=(1‚àíùëÉ)ùëÉ0+ùëÉ ùëÉ1 
 
ùëÉ(ùêµ1)=ùëÉ(ùêµ1|ùê¥0) ùëÉ(ùê¥0)+ùëÉ(ùêµ1|ùê¥1) ùëÉ(ùê¥1)  
=ùëÉ ùëÉ0+(1‚àíùëÉ) ùëÉ1 
 
2. ùë∑(ùë®ùüé|ùë©ùüé)   and  ùë∑(ùë®ùüè|ùë©ùüè)  
ùëÉ(ùê¥0|ùêµ0) : if a 0 was received what is the probability that a 0 was sent  
 ùëÉ(ùê¥0|ùêµ0)  ŸÖ ŸÜŸÜŸáŸÜ ŸÑŸà ŸàÿµŸÑ0 ŸÅŸä ÿßŸÑÿß  Receiver    ÿ£Ÿä ÿßÿ≠ÿ™ŸÖŸÜŸÑ ÿßŸÜ ÿßŸÑŸÑŸä ÿ™  ÿßŸäÿ≥ŸÜŸÑŸá0 
 
ùëÉ(ùê¥1|ùêµ1) : if a 1 was received what is the probability that a 1 was sent  
 ùëÉ(ùê¥1|ùêµ1)   ŸÖ ŸÜŸÜŸáŸÜ ŸÑŸà ŸàÿµŸÑ1   ŸÅŸäÿßŸÑÿß  Receiver   ÿ£Ÿä ÿßÿ≠ÿ™ŸÖŸÜŸÑ ÿßŸÜ ÿßŸÑŸÑŸä ÿ™  ÿßŸäÿ≥ŸÜŸÑŸá1 
 
 
Example  5 
 
 ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿßŸÑŸÑŸä ŸÅŸÜŸÖ ÿ®ŸÜÿ≥ŸÖ Ÿá Posterior probability  ÿ®ÿ®ÿØÿ£ ÿ®ŸÜŸÑŸÑŸä ÿßÿ≥ÿ™ŸÇÿ®ŸÑÿ™Ÿá Ÿàÿßÿ≠ÿ≥ÿ® ÿßŸÑŸÑŸä ÿ™  ÿßŸäÿ≥ŸÜŸÑŸá ŸàÿßŸÑŸÑŸä 
ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿßÿ£ŸÑŸàŸÑ  ÿ®ŸÜÿ≥ŸÖ Ÿá Prior probability     ŸàÿØÿß ÿßŸÑÿ™Ÿä ÿ® ÿßŸÑ ŸÜÿØŸä ÿßŸÑŸÑŸä ÿ®ŸÜÿ¥ŸàŸÅŸáŸÜÿ® ŸÖ  
ŸàŸáŸÜÿ≥ÿ™ŸÇÿ®ŸÑ ŸÉŸÜ .  
 
Posterior probability  ÿ®ŸÜÿ≠ŸÑŸá ÿ®ŸÇŸÜŸÜŸàŸÜ ÿßÿ≥ŸÖŸá Bay's Rule 
 
Bay's Rule  
ùëÉ(ùê¥|ùêµ)=ùëÉ(ùêµ|ùê¥)  ùëÉ(ùê¥) 
ùëÉ(ùêµ) 
 
ùëÉ(ùê¥0|ùêµ0)=ùëÉ(ùêµ0|ùê¥0)  ùëÉ(ùê¥0) 
ùëÉ(ùêµ0)=(1‚àíùëÉ)ùëÉ0
(1‚àíùëÉ)ùëÉ0+ùëÉ ùëÉ1 
ùëÉ(ùê¥1|ùêµ1)=ùëÉ(ùêµ1|ùê¥1)  ùëÉ(ùê¥1) 
ùëÉ(ùêµ1)=(1‚àíùëÉ)ùëÉ1
ùëÉ ùëÉ0+(1‚àíùëÉ) ùëÉ1 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
¬©
 
Basem HeshamInformation Theory &  Entropy Coding 
& Channel Capacity
Lecture
 
8
 
 
The concept of information content is related to predictability. That is , the more likely a 
particular message, the less information is given by transmitting the message.  
 
ùêºùë•=log 2(1
ùëÉùë•) 
 
ùêºùë•  : information con tent of the message ùë•   
ùëÉùë•  : probability of occurrence of the message  
 
  ùêºùë•  ŸáŸàÿπÿØÿØ ÿßŸÑŸÄ bits ÿßŸÑŸÖÿπÿ®ÿ±ÿ© ÿπŸÜ ÿßŸÑŸÄ ùë• message 
ùëÉùë•  ŸáŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿ´ ÿßŸÑŸÄ ùë• message 
 
ÿßŸÑŸÄ message ÿ∞ÿßÿ™ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑÿπÿßŸÑŸäÿ© ÿ™ÿ≠ŸÖŸÑ ŸÇŸäŸÖ ŸÇŸÑŸäŸÑŸá ŸÖŸÜ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ (ÿπÿØÿØ ÿßŸÇŸÑ ŸÖŸÜ ÿßŸÑŸÄ bits   ) ŸàÿßŸÑÿπŸÉÿ≥ ÿµÿ≠Ÿäÿ≠ 
 
ŸÖŸÜ ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑŸÖŸáŸÖÿ© ÿßŸÑŸÑŸä ÿßŸÑÿ≤ŸÖ ŸÜÿ™ÿπŸÑŸÖŸáÿß ÿ±ÿ®ÿ∑ ŸÖÿ≠ÿ™ŸÄŸÄŸàÿß ÿßŸÑŸÄŸÄ ŸÄ message ÿ®ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ŸÄŸÄÿØŸàŸàŸáÿß Ÿàÿ≤ŸÄŸÄŸä  ŸÄŸÄÿ≤  ÿßŸÑŸÄŸÄ ŸÄ source 
coding    ŸÉŸÜÿßÿ® ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿπÿØÿØ  bits    Ÿàÿßÿ®ÿ™ ŸÑŸÉŸÑ message   ŸàŸÑŸÉŸÄŸÄŸÜ ÿπŸÖŸÑŸäŸÄŸÄÿß ŸÖÿ®ŸÜÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖ  ŸÜŸÅŸÄŸÄÿ≥ ÿßŸÑÿπŸÄŸÄÿØÿØ ŸÑŸÑÿ™ÿπÿ®ŸäŸÄŸÄÿ± ÿπŸÄŸÄŸÜ ŸÉŸÄŸÄŸÑ 
message  ŸàŸÖŸÄŸÄŸÜ ÿßŸÑÿ∑ŸÄŸÄÿ±ŸÑ ÿßŸÑŸÑŸÄŸÄŸä ŸáŸÜÿ™ÿπŸÑŸÖŸáŸÄŸÄÿß ÿßŸÜ  ÿπŸÄŸÄÿØÿØ ÿßŸÑŸÄŸÄ ŸÄ bits ÿßŸÑŸÑŸÄŸÄŸä ÿ®ŸÜÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖŸá Ÿäÿπÿ™ŸÖŸÄŸÄÿØ ÿπŸÑŸÄŸÄ  ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ŸÄŸÄÿØŸàÿ´ ÿßŸÑŸÄŸÄ ŸÄ 
message  .ŸÑŸà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàÿ´ ÿßŸÑŸÄ message  ŸÉÿ®Ÿäÿ± ŸáŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿπŸÄŸÄÿØÿØbits  ÿßŸÇŸÄŸÄŸÑ ŸÑŸÑÿ™ÿπÿ®ŸäŸÄŸÄÿ± ÿπŸÜŸÄŸÄŸá ŸàŸÑŸÄŸÄŸà ÿßÿ≠ÿ™ŸÖŸÄŸÄÿßŸÑ ÿ≠ÿØŸàŸàŸÄŸÄŸá
ÿ∂ŸÄŸÄÿπŸäŸá ŸáŸÜÿ≥ŸÄŸÄÿ™ÿÆÿØŸÖ ÿπŸÄŸÄÿØÿØ ÿßŸÉÿ®ŸÄŸÄÿ± ŸÖŸÄŸÄŸÜ ÿßŸÑŸÄŸÄ ŸÄ bits  Ÿàÿ®ŸÉŸÄŸÄÿØÿß ŸáŸÜŸÄŸÄŸàÿ≤ÿ± ÿ≤ŸÄŸÄŸä ÿß ŸÑŸÄŸÄ ŸÄ Bandwidth  ÿßŸÑŸÜŸÑŸÄŸÄŸà ÿ®ŸÜÿ®ÿπŸÄŸÄÿ™ ÿπŸÄŸÄÿØÿØ ŸÖŸÄŸÄŸÜ ÿß ŸÑŸÄŸÄ ŸÄ 
messages  ÿ≤Ÿä ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ŸáŸÜÿ≥ÿ™ÿÆÿØŸÖ  ÿπÿØÿØ  bits ÿßŸÇŸÑ. 
 
ÿ™ÿπÿ±ŸäŸá ÿßÿØŸÑ ŸÑŸÑŸÄ ùêºùë•  : 
Information  content  ùêºùë• can be interpreted as the minimum number of binary digits required to 
encode the message.  
 ùêºùë•  ŸáŸàÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜ  ŸÖŸÜ ÿßŸÑŸÄ bits   ŸÖÿ∑ŸÑŸàÿ® ŸÑÿπŸÖŸÑ coding ŸÑŸÑŸÄ message 
 
 
 
 
 
 
 
 Information Theory  
bits  
 
 
Is defined as the average information per message.  
 
ùêª=‚àëùëÉùë•ùëñ  ùêºùë•ùëñùëõ
ùëñ=1=‚àëùëÉùë•ùëñ log 2(1
ùëÉùë•ùëñ)ùëõ
ùëñ=1 
 
Entropy  ŸáŸà ŸÖÿ™Ÿàÿ≥ÿ∑ ŸÖÿß ÿ™ÿ≠ŸÖŸÑŸá ŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑŸÄ  messages   ŸÖŸÜ ŸÖÿπŸÑŸàŸÖÿßÿ™ Ÿàÿπÿ¥ÿßŸÜ ŸÜÿ≠ÿ≥ÿ®Ÿá ÿ®ŸÜÿ∂ÿ±ÿ® ŸÖÿ≠ÿ™Ÿàÿß
ŸÉŸÑ  message ùêºùë•   ÿ≤Ÿä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàŸàŸáùëÉùë• .ŸàŸÜ ŸÖÿπ ÿßŸÑŸÜŸàÿßÿ™ÿ¨ 
Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÜ ÿßŸÑŸÄ Entropy ŸÇŸäŸÖÿ™Ÿáÿß ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑ  ÿßŸÑŸÄ probability. 
  ùëõŸáŸä ÿπÿØÿØ ÿßŸÑŸÄ messages 
 
ÿ™ÿπÿ±ŸäŸá ÿßÿØŸÑ ŸÑŸÑŸÄ Entropy : 
Entropy is equal to the minimum number of digits per message required on the average 
for encoding  
ùêª ŸáŸà ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÜ ÿßŸÑŸÄ bits   ŸÖÿ∑ŸÑŸàÿ® ÿ≤Ÿä ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ŸÑÿπŸÖŸÑ coding ŸÑŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑŸÄ message 
 
 
 
 
A communication system consists of six  possible messages  with probabilities  ùüè
ùüí , ùüè
ùüí , 
ùüè
ùüñ , ùüè
ùüñ , ùüè
ùüñ , ùüè
ùüñ  .Find the Entropy  
 
Solution  
ùêª=‚àëùëÉùë•ùëñ  ùêºùë•ùëñùëõ
ùëñ=1=‚àëùëÉùë•ùëñ log 2(1
ùëÉùë•ùëñ)ùëõ
ùëñ=1 
=1
4√ó log 2(4)+1
4√ó log 2(4)+1
8√ó log 2(8)+1
8√ó log 2(8)+1
8√ó log 2(8)
+1
8√ó log 2(8)=2.5 bits/message  
 ŸäÿπŸÜŸä ŸÖÿ≠ÿ™ÿßÿ¨ ÿ≤Ÿä ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ 2.5 bits   ŸÑŸÉŸÑ message  ÿπÿ¥ÿßŸÜ ÿßŸÇÿØÿ± ÿßÿπŸÖŸÑ coding   ÿ®ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜ ŸÖŸÜÿßŸÑŸÄ bits 
 
 Example 1  Entropy  
bits / message  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ùêª
ùëèùëñùëõùëéùëüùë¶
=
‚àë
ùëÉ
ùë•ùëñ
 
 
ùêº
ùë•ùëñ
2
ùëñ
=
1
=
ùëÉ
ùë•
1
 
log
2
(
1
ùëÉ
ùë•
1
)
+
ùëÉ
ùë•
2
 
log
2
(
1
ùëÉ
ùë•
2
)
 
=
ùëÉ
ùë•
1
 
log
2
(
1
ùëÉ
ùë•
1
)
+
(
1
‚àí
ùëÉ
ùë•
1
)
log
2
(
1
1
‚àí
ùëÉ
ùë•
1
)
 
 
 ÿßÿ≥ŸÖŸáÿß
ùêª
ùëèùëñùëõùëéùëüùë¶
 
ÿ£ŸÑŸÜŸÜÿß
  
ÿ®ŸÜÿ®ÿπÿ™  
message
 
2
 
ÿ≤ŸÇÿ∑
 
ÿ®ÿßŸÑÿ™ÿπŸàŸäÿß ÿ≤Ÿä ÿßŸÑŸÇÿßŸÜŸàŸÜ ÿßŸÑÿ≥ÿßÿ®ŸÇ ÿπŸÜÿØ ŸÇŸäŸÖ ŸÖÿÆÿ™ŸÑŸÅÿ©
 
At 
ùëÉ
ùë•
1
=
0
 
‚üπ
   
ùêª
ùëèùëñùëõùëéùëüùë¶
=
0
+
(
1
)
log
2
(
1
1
)
=
0
 
At 
ùëÉ
ùë•
1
=
1
4
 
‚üπ
   
ùêª
ùëèùëñùëõùëéùëüùë¶
=
1
4
 
log
2
(
4
)
+
(
3
4
)
log
2
(
4
3
)
=
0
.
8
 
At 
ùëÉ
ùë•
1
=
1
2
 
‚üπ
   
ùêª
ùëèùëñùëõùëéùëüùë¶
=
1
2
 
log
2
(
2
)
+
1
2
 
log
2
(
2
)
=
1
 
At 
ùëÉ
ùë•
1
=
3
4
 
‚üπ
   
ùêª
ùëèùëñùëõùëéùëüùë¶
=
(
3
4
)
log
2
(
4
3
)
+
1
4
 
log
2
(
4
)
=
0
.
8
 
At 
ùëÉ
ùë•
1
=
1
 
‚üπ
   
ùêª
ùëèùëñùëõùëéùëüùë¶
=
(
1
)
log
2
(
1
1
)
+
0
=
0
 
 
The result is sketched as a function of 
ùëÉ
ùë•
1
 
 
Note that as either of the two messages becomes more likely , the entropy decreases. 
When either message has probability 1, the entropy goes to zero. This is reasonable, since
,
 
at these points
, the outcome 
is certain. If 
ùëÉ
ùë•
1
=
1
 
we know that message 
ùë•
1
 
will be sent
 
all the time. No information is transmitted by sending the message.
 
Proof 
of
 
Maximum 
Entropy
 
  
ùëÉ
ùë•
2
 
=
 
1
 
‚àí
 
ùëÉ
ùë•
1
:ŸÜÿ≠ÿµŸÑ ÿπŸÑ  ÿßÿßŸÑÿ™Ÿä
 
Entropy
 
,Ÿàÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÇÿßŸÜŸàŸÜ ÿßŸÑŸÄ
 
ùëÉ
ùë•
1
 
ŸáŸä Ÿàÿßÿ≠ÿØ ŸÜÿßŸÇÿµ
 
ùëÉ
ùë•
2
 
Ÿäÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ ÿßÿ∞ÿß ÿßÿ≠ÿ™ŸÖÿßŸÑ
Ÿàÿ®ŸÖŸÄŸÄÿß ÿßŸÜ ŸÖ ŸÖŸÄŸÄŸàÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖŸÄŸÄÿßÿßŸÑÿ™
 
ùë•
2
 
Ÿà
 
ùë•
1
 
ÿ≤ŸÇŸÄŸÄÿ∑ ŸáŸÄŸÄŸÖ
 
2
 
message
 
ÿ®Ÿäÿ®ÿπŸÄŸÄÿ™
 
digital 
 
system
 
ŸÜŸÅÿ™ŸÄŸÄÿ±ÿß ÿßŸÜ ÿπŸÜŸÄŸÄÿØŸÜÿß
Consider a communication scheme made up of two  possible message
 
ùë•
1 
and
 
ùë•
2
 
Maximum entropy occurs in the case of equally possible messages
.
 
ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ÿßŸÜ ÿßÿπŸÑ  ŸÇŸäŸÖÿ© ŸÑ
ŸÑ
ŸÄ
  
entropy
 
  ÿ®ÿ™ÿ≠ÿµŸÑ ŸÑŸÖÿß ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ≠ÿØŸàŸàŸáŸÖ ÿ™ŸÉŸàŸÜ ŸÖÿ™ÿ≥ÿßŸàŸäÿ© ŸàÿØÿß ŸÖŸÜÿ∑ŸÇŸä
ÿßŸÑŸÜ 
ŸÑŸà ÿßÿ≠ÿØ ÿß
ŸÑ
ŸÄ
 
messages
 
  ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàŸàŸáÿß ÿßŸÉŸäÿØ ÿ≤Ÿä ÿßŸÜ
ùëÉ
ùë•
1
=
1
 
  ÿ≤ÿØÿß ŸÖÿπŸÜÿßŸá ÿßŸÜŸÜÿß ÿØÿßŸäŸÖÿß ÿ®ŸÜÿ®ÿπÿ™
ùë•
1
 
message
 
 ÿ≤ŸÇÿ∑ Ÿàÿπÿ±ÿ≤ŸÜÿß
ÿßŸÜ ŸÉŸÑ ŸÖÿß ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿßŸÑÿ±ÿ≥ÿßŸÑ ÿ≤ÿßÿØ ŸÉŸÑ ŸÖÿß 
ÿß
ŸÑ
ŸÄ
 
information
 
 ŸÇŸÑÿ™
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸÖÿß ÿßÿßŸÑÿ≠ÿ™ŸÖÿßŸÑ ŸäŸÉŸàŸÜ ÿßŸÉŸäÿØ ÿß
ŸÑ
ŸÄ
 
Entropy
 
  ŸáŸäÿ≥ÿßŸàŸä ÿµŸÅÿ± Ÿàÿ®ÿßŸÑŸÖŸàŸÑ ÿπŸÜÿØ
ùëÉ
ùë•
1
=
0
 
 ÿØÿß ŸÖÿπŸÜÿßŸá ÿßŸÜ
ùëÉ
ùë•
2
=
1
 
 
 ŸÉŸÑ ŸÖÿß ÿßÿ®ÿπÿØ ÿπŸÜ ŸÜŸÇÿ∑
ÿßŸÑÿ™ÿ£ŸÉŸäÿØ
 
ÿßÿ≠ÿ™ŸÖÿßŸÑŸäÿ© ÿßÿ±ÿ≥ÿßŸÑ ÿßÿ≠ÿØ ÿß
ŸÑ
ŸÄ
 
messages
 
  ÿ®ÿ™ÿπŸÑ  ŸàÿßŸÑÿ™ÿßŸÜŸäŸá ÿ®ÿ™ŸÇŸÑ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿ≤ŸäŸá ŸÖÿπŸÑŸàŸÖÿ©
ÿ®ÿ™ÿ™ŸÜŸÇŸÑ
 
ŸÑÿ≠ÿØ ŸÖÿß ŸÜŸàÿµŸÑ ÿ£ŸÑÿπŸÑ  ŸÇŸäŸÖÿ© ŸÑ
ŸÑ
ŸÄ
 
entropy
 
 ÿπŸÜÿØ
1
2
 
 
 .ŸÑŸÖÿß ÿ™ŸÉŸàŸÜ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ŸáŸÖ ŸÖÿ™ÿ≥ÿßŸàŸäÿ©
 
 
 
 
 
Given M possible message, we wish to convert 
them
 
into M possible code words.
 
The code 
words can be selected to achieve objectives such as efficiency, error
 
detection and
 
correction 
,
security.
 
 
  ÿ≤Ÿä ÿßŸÑ ÿ≤  ÿØÿß ŸáŸÜÿ™ÿπŸÑŸÖ ÿ∑ÿ±ŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑÿπŸÖŸÑ
Coding
  
ŸÑŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿß
ŸÑ
ŸÄ
  
messages
  
ŸàŸÖŸÖŸÉŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸáÿØŸá ŸÖŸÜ ÿß
ŸÑ
ŸÄ
 
Coding
  
  ÿØÿß
ÿ™ÿ≠ŸÇŸäŸÇ  
efficiency
  
ÿπÿßŸÑŸäÿ©
  
ŸäÿπŸÜŸä ÿßÿ®ÿπÿ™ ÿπÿØÿØ  
bits
  
  ÿßŸÇŸÑ
ÿßŸà 
  
ŸÖŸÖŸÉŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸáÿØŸá ÿßŸÜ ÿßŸÑÿØÿßÿ™ÿß ÿ™ŸàÿµŸÑ ÿµÿ≠ 
Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸáÿØŸá ÿπŸÜÿØŸä ÿ≤ŸäÿßÿØÿ©
 
error correction
 
capability
 
ÿ≤Ÿä ÿß
ŸÑ
ŸÄ
  
System
 
  ŸÖŸÜ ÿÆÿßŸÑŸÑ
ÿ•ÿ∂ÿßÿ≤ÿ©
  
Redundant 
bits
  
  ÿπÿ¥ÿßŸÜ ŸÜŸÇÿØÿ± ŸÜŸÉÿ™ÿ¥Ÿá ŸàŸÜÿµÿ≠ÿ≠
ÿßÿ£ŸÑÿÆÿ∑ÿß 
  
,ŸàŸÖŸÖŸÉŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸáÿØŸá ŸáŸà ÿ™ÿ≠ŸÇŸäŸÇ  
ÿß
ŸÑ
ŸÄ
  
Security
  
ÿ≤ŸÄŸÄŸä ÿß
ŸÑŸÄŸÄ 
ŸÄ
 
System
 
ÿ≤Ÿä ÿß
ŸÑ
ŸÄ
Encryption
 
We discuss codes to achieve efficiency under the 
heading entropy coding. Such codes 
attempt to send the information using the minimum number of bits.
 
 
ŸáŸÜÿØÿ±ÿ≥ ÿ≤Ÿä ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿØŸä ÿ∑ÿ±ŸÑ ÿß
ŸÑ
ŸÄ
  
Coding
  
ÿßŸÑŸÑŸÄŸÄŸä ÿ®ÿ≠ŸÇŸÄŸÄŸÇ ŸÖŸÄŸÄŸÜ ÿÆÿßŸÑŸÑŸáŸÄŸÄÿß ÿß
ŸÑŸÄŸÄ 
ŸÄ
  
efficiency
  
 Ÿà
ÿßŸÑŸÖŸÇÿµŸÄŸÄŸàÿØ ÿ®ÿßŸÑŸÉŸÅŸÄŸÄÿß ÿ© 
efficiency
 
ŸáŸà ÿßŸÑŸÜŸÇŸÑ ÿ®ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÜ ÿß
ŸÑ
ŸÄ
 
bits
 
ŸàŸáŸÜÿØÿ±ÿ≥ ÿ∑ÿ±ŸäŸÇ
ÿ™ŸäŸÜ ÿ®ÿ≠ŸÇŸÇ
 
ŸÖŸÄŸÄŸÜ ÿÆÿßŸÑŸÑŸáŸÄŸÄŸÖ ÿØÿß ŸàŸáŸÖŸÄŸÄÿß 
 
ÿß
ŸÑŸÄŸÄ 
ŸÄ
 
Huffman 
coding
 
 Ÿà
Shannon coding
 
  ÿßŸÑŸÑŸä ŸäŸÜÿØÿ± Ÿàÿß ÿ™ÿ≠ÿ™ ŸÖÿ≥ŸÖ
entropy coding
 
 (ŸÉÿßŸÑ ÿßŸÑÿ∑ÿ±ŸäŸÇÿ™ŸäŸÜ ŸÖŸÜ
ÿ£ŸÜŸÄŸÄŸàÿßÿ≠
 
ÿß
ŸÑ
ŸÄ
 
lossless compression
 
)
 
 
We illustrate error detection and correction using block 
coding.
 
 ÿ≤ŸäŸá ÿ∑ÿ±ŸäŸÇÿ© ŸáŸÜÿ™ÿπŸÑŸÖŸáÿß
ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿßŸÑ ÿßŸäŸá 
ÿÆÿßÿµÿ© ÿ® 
ÿßŸÑŸÄŸÄ 
ŸÄ
 
error detection and correction
 
 ÿßÿ≥ŸÄŸÄŸÖŸáÿß
block coding
 
  ŸáŸÜÿ™ÿπŸÑŸÖ ÿ≤ŸäŸáÿß ÿßÿ≤ÿßŸä ŸÜÿπŸÖŸÑ
coding
 
ŸÑ
ŸÄ
 
message
 
  ÿ®ÿ≠Ÿäÿ´ ŸÜŸÇÿØÿ± ŸÜŸÉÿ™ÿ¥Ÿá ŸàŸÜÿµÿ≠ÿ≠
ÿßÿ£ŸÑÿÆÿ∑ÿß 
  
ÿ≤ŸäŸáÿß.
 
 
ŸÇÿ®ŸÑ ŸÖÿß ŸÜÿØÿ±ÿ≥ ÿ∑ÿ±ŸÑ ÿß
ŸÑ
ŸÄ
 
Coding
 
 ÿßŸÑÿ≤ŸÖ ŸÜÿπÿ±Ÿá ÿÆÿµÿßÿ¶ÿµ ÿßÿßŸÑŸÉŸàÿßÿØ ÿßŸÑŸÑŸä
ÿßŸÑÿ≤ŸÖ ÿ™ÿ™ÿ≠ŸÇŸÇ.
 
 
Coding
 
 
 
‚ñ™ Uniquely decodable  
ŸÖÿπŸÜÿßŸáÿß ÿßŸÜ ÿßÿßŸÑŸÉŸàÿßÿØ ÿ™ŸèŸÅÿ≥ÿ± ÿ®ÿ∑ÿ±ŸäŸÇÿ© Ÿàÿßÿ≠ÿØÿ© ÿ≤ŸÇÿ∑ ÿ≤Ÿä ÿßŸÑŸÄ   Receiver   ŸäÿπŸÜŸä ŸÉŸÑ ŸÉŸàÿØ ŸäŸèÿπÿ®ÿ± ÿπŸÜ message  Ÿàÿßÿ≠ÿØŸá ÿ®ŸÄŸÄÿ≥
ŸÖ   ÿ£Ÿä message .ÿ™ÿßŸÜŸäŸá ÿßŸà ÿßÿ™ŸÜŸäŸÜ ŸÖÿπ ÿ®ÿπÿß 
 
ŸÖŸàÿßŸÑ : ŸÑŸà ÿπŸÜÿØŸÜÿß  4 messages :ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
ùëÄ1=1   ,   ùëÄ2=10  ,   ùëÄ3=01  ,   ùëÄ4=101 
If 101 is received ‚áí ùëÄ4 or  ùëÄ2 ùëÄ1 or ùëÄ1ùëÄ3 
 ŸÑŸà ŸàÿµŸÑ101 ŸÑŸÑŸÄ Receiver  Ÿáÿ™ÿ™ŸÅÿ≥ŸÄŸÄÿ± ÿ®ŸÄŸÄÿßŸÉÿ™ÿ± ŸÖŸÄŸÄŸÜ ÿ∑ÿ±ŸäŸÇŸÄŸÄÿ© Ÿàÿ®ÿßŸÑÿ™ŸÄŸÄÿßŸÑŸä ÿßÿßŸÑŸÉŸÄŸÄŸàÿßÿØ ÿØŸä ŸÑŸäÿ≥ŸÄŸÄÿ™ uniquely decodable 
 ŸàŸÖÿ™ŸÜŸÅÿπ  ÿ≤Ÿä ÿßŸÑŸÄ Coding 
 
‚ñ™ Prefix property  
No code word forms the starting sequence (prefix) of any other code word.  
ÿßŸÑÿÆÿßÿµŸäÿ© ÿØŸä ŸÖÿπŸÜÿßŸáÿß ÿßŸÜ ÿßÿßŸÑŸÉŸàÿßÿØ ÿßŸÑ ÿ™ŸÖŸàŸÑ ÿ®ÿØÿßŸäÿ© ŸÑŸÉŸàÿØ ÿßÿÆÿ±, ŸàŸÑŸà ÿßŸÑÿÆÿßÿµŸÄŸÄŸäÿ© ÿØŸä ÿ™ÿ≠ŸÇŸÇŸÄŸÄÿ™ ÿ®ŸÜŸÇŸÄŸÄŸàŸÑ ÿπŸÑŸÄŸÄ  ÿßÿßŸÑŸÉŸÄŸÄŸàÿßÿØ ÿØŸä  
instantaneous  
 
  ŸÖŸàÿßŸÑ : ŸÑŸà ÿπŸÜÿØŸÜÿß 4 messages :ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßÿßŸÑÿ™Ÿä 
ùëÄ1=1   ,   ùëÄ2=01  ,   ùëÄ3=001   ,   ùëÄ4=0001  
  ŸÖŸÅŸäÿ£Ÿä ŸÉŸàÿØ Ÿäÿπÿ™ÿ®ÿ± ÿ®ÿØÿßŸäÿ© ŸÑŸÉŸàÿØ ÿ™ÿßŸÜŸä Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿÆÿßÿµŸäÿ© ÿßŸÑŸÄ prefix  ÿ™ÿ≠ŸÇŸÇÿ™ 
 
The prefix restriction is sufficient but not necessary for uniquely decodable  
ÿÆÿßÿµŸäÿ© ÿßŸÑŸÄ prefix  ŸÉÿßÿ≤ŸäŸá ÿßŸÜŸÜÿß ŸÜÿπÿ±Ÿá ÿßŸÜ ÿßŸÑŸÉŸàÿØ uniquely decodable  ÿßŸÑŸÜÿßŸÑŸÄŸÄ ŸÄ prefix  ŸÑŸÖŸÄŸÄÿß ÿ™ÿ™ ÿ≠ŸÇŸÄŸÄŸÇ Ÿáÿ™ÿ∂ŸÄŸÄŸÖŸÜ ÿßŸÜ 
ÿßÿßŸÑŸÉŸÄŸÄŸàÿßÿØ uniquely decodable ,  ŸàŸÑŸÉŸÄŸÄŸÜŸÑŸÄŸÄŸà ÿßŸÑŸÄŸÄ ŸÄ prefix  ŸÖÿ™ÿ≠ŸÇŸÇŸÄŸÄÿ™  ÿØÿß ŸÖŸÄŸÄ  ŸÖÿπŸÜŸÄŸÄÿßŸá ÿßŸÜŸáŸÄŸÄÿß ŸÑŸäÿ≥ŸÄŸÄÿ™ uniquely  
decodable ÿ≤Ÿä ÿßŸÑŸÖŸàÿßŸÑ ÿßÿßŸÑÿ™Ÿä 
 
ùëÄ1=1   ,   ùëÄ2=10  ,   ùëÄ3=100   ,   ùëÄ4=1000  
ÿßŸÑŸÄ  prefix    ŸáŸÜÿß ÿ∫Ÿäÿ± ŸÖÿ≠ŸÇŸÇŸá ÿßŸÑŸÜùëÄ1    ŸáŸä prefix    ÿßŸà ÿ®ÿØÿßŸäÿ© ŸÑùëÄ2  , ŸàùëÄ2  ŸáŸä prefix  ŸÑùëÄ3 , ŸàùëÄ3  ŸáŸÄŸÄŸä ÿ®ÿØÿßŸäŸÄŸÄÿ©
ùëÄ4  ,ŸàŸÑŸÉŸÜÿ®ÿßŸÑÿ±ÿ∫ŸÖ ÿßŸÜ ÿßŸÑ ŸÄ prefix  ŸáŸÜÿß ÿ∫ŸäŸÄŸÄÿ± ŸÖÿ≠ŸÇŸÇŸÄŸÄŸá ŸÑŸÉŸÄŸÄŸÜÿßÿßŸÑŸÉŸÄŸÄŸàÿßÿØ uniquely decodable   ÿßŸÑŸÜ ŸÉŸÄŸÄŸÑ ŸÉŸÄŸÄŸàÿØ ŸäŸÅŸèÿ≥ŸÄŸÄÿ±
ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿÆÿ™ŸÑŸÅÿ© ÿ≤Ÿä ÿßŸÑŸÄ Receiver 
 
 
 
 
 Properties of codes  
 
 
‚ñ™ It is of interest to find uniquely decodable codes of minimum length  by assigning the 
shorter code word to the most probable messages.  
‚ñ™ Different messages are coded into words of different length.  
‚ñ™ When talking about length of a code, we therefore must refer to the average length of the 
code words. This average is computed by taking the probabilities of each message into 
account.  
‚ñ™ We want to get code words with minimum average length for code word  
 
 ŸÖŸÜ ÿßŸÑŸÖŸáŸÖ ÿ≤Ÿäÿ£Ÿä digital system  ŸáŸä ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑ  ÿßŸÉŸàÿßÿØ ŸÑŸáÿß minimum length  ŸàŸäÿ™ŸÖ ÿπŸÖŸÄŸÄŸÑ coding  ÿ®ÿ≠ŸäŸÄŸÄÿ´
ÿßŸÜ ŸÉŸÑ  message ÿ™ÿßÿÆÿØ ŸÉŸàÿØ ŸÖÿÆÿ™ŸÑŸá ÿ≤Ÿä ÿßŸÑŸÄ Length ÿπŸÑ  ÿ≠ÿ≥ÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑŸäŸá ÿ≠ÿØŸàÿ´ ÿßŸÑŸÄ message 
  ÿ®ŸÖÿß ÿßŸÜŸÜÿß ÿ®ŸÜÿ®ÿπÿ™ ÿ≤Ÿäÿ£Ÿä  system  ŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑŸÄ  messages   ÿ≤ŸáŸÜÿ™ÿπÿßŸÖŸÄŸÄŸÑ ŸÖŸÄŸÄÿπ ŸÖÿ™Ÿàÿ≥ŸÄŸÄÿ∑ ÿßŸÑŸÄŸÄŸÄ  Length   ŸàÿØÿß ÿ®ŸÜÿ≠ÿµŸÄŸÄŸÑ
ÿπŸÑŸäŸá ŸÖŸÜ ÿÆÿßŸÑŸÑ  ÿ∂ÿ±ÿ®  ÿ∑ŸàŸÑ ŸÉŸÑ  message .ÿ≤Ÿä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàŸàŸá ŸàŸÜ ŸÖÿπ ÿßŸÑŸÜŸàÿßÿ™ÿ¨ 
 
ÿßŸÑŸáŸÄŸÄÿØŸá ŸáŸÜŸÄŸÄÿß ŸáŸÄŸÄŸà ÿßŸÑÿ≠ÿµŸÄŸÄŸàŸÑ ÿπŸÑŸÄŸÄ  code words with minimum average length  ŸàÿØÿß ŸÖŸÄŸÄŸÜ ÿÆŸÄŸÄÿßŸÑŸÑ ÿßŸÜ ÿßŸÑŸÄŸÄŸÄ 
messages  ÿßŸÑŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ÿØŸàŸàŸáÿß ŸÉÿ®Ÿäÿ± ÿ™ÿßÿÆÿØ ÿπÿØÿØ ÿßŸÇŸÑ ŸÖŸÜ ÿßŸÑŸÄ  bits  Ÿà ÿßŸÑŸÄ  messages   ÿßŸÑŸÑŸä ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿ≠ŸÄŸÄÿØŸàŸàŸáÿßŸÇŸÑŸäŸÄŸÄ ŸÑ 
ÿ™ÿßÿÆÿØ ÿπÿØÿØ ÿßŸÉÿ®ÿ± ŸÖŸÜ ÿßŸÑŸÄ bits Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿ≤Ÿä ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ÿßŸÑŸÄ average length ŸáŸäŸÉŸàŸÜ ÿßŸÇŸÑ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÉŸÅÿßÿ¶ÿ© Ÿáÿ™ÿ≤ŸäÿØ. 
 
A fundamental theorem exists in noiseless coding theorem. The theorem states that :  
For binary coding alphabets, the average code word length is greater than or equal to the 
entropy  
Entropy H  ŸáŸä ÿßÿ≤ÿ∂ŸÑ ŸÇŸäŸÖÿ© ŸäŸÖŸÉŸÜ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸäŸáŸÄŸÄÿß ÿπŸÜŸÄŸÄÿØ ÿπŸÖŸÄŸÄŸÑ coding  ŸÑŸÑŸÄŸÄŸÄ messages  ŸàŸÇŸäŸÖŸÄŸÄÿ© ÿßŸÑŸÄŸÄŸÄ average 
length ÿßŸÉÿ®ÿ± ŸÖŸÜ ÿßŸà ÿ™ÿ≥ÿßŸàŸä ŸÇŸäŸÖÿ© ÿßŸÑŸÄ  Entropy   Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸáÿØÿ≤ŸÜÿß ŸáŸà ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑ code ŸäŸÉŸàŸÜ ÿ≤ŸäŸÄŸÄŸá ŸÖÿ™Ÿàÿ≥ŸÄŸÄÿ∑ ÿπŸÄŸÄÿØÿØ 
ÿßŸÑŸÄ bits ŸäÿµŸÑ ÿßŸÑ  ÿßŸÑŸÄ Entropy ÿ£Ÿä ÿßŸÇŸÑ ŸÖÿß ŸäŸÉŸàŸÜ Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÜÿ≠ŸÇŸÇ ÿßÿπŸÑ  ŸÉŸÅÿß ÿ©. 
 
ùêøÃÖ=‚àëùëÉùëñ ùêøùëñùëõ
ùëñ=1 
ùêøÃÖ‚â•ùêª 
ùúÇ=ùêª
ùêøÃÖ√ó100 
ùêøÃÖ‚Üí Average length (average bits/message)          ùúÇ‚Üí  Efficiency  
ùêø‚Üí bits /message   
 ùêøŸáŸä ÿπÿØÿØ ÿßŸÑŸÄ bits ÿßŸÑŸÅÿπŸÑŸä ( actual) ÿßŸÑŸÖŸà ŸàÿØÿ© ÿ≤Ÿä ÿßŸÑŸÄ message  ŸàŸÑŸÉŸÜùêºùë• ( ŸáŸä ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜ perfect ) 
 Entropy Coding  Techniques  
 
Maximum efficiency occurs at ùêøÃÖ=ùêª and requires that the probability of every message be 
inverse power of 2  (ùëÉùëñ=1
2ùëõ) 
 
  ÿßÿπŸÑ  ŸÉŸÅÿß  ŸÖŸÖŸÉŸÜ ŸÜÿ≠ÿµŸÑ ÿπŸÑŸäŸáÿß ÿπŸÜÿØ ùêøÃÖ=ùêª  ŸàÿØÿß ŸÖŸÖŸÉŸÜ Ÿäÿ™ÿ≠ŸÇŸÇ ŸÑŸÖÿß ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ÿßŸÑŸÄ message   ÿ™ŸÉŸàŸÜ
inverse power of two   ŸäÿπŸÜŸä ŸÑŸà ŸÇŸäŸÖ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸÉÿßŸÜÿ™‚Ä¶ ,1
16 ,1
8 ,1
4 ,1
2   ÿßÿ∞ÿßŸã ùêøÃÖ=ùêª   Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä Ÿáÿ≠ÿµŸÑ
ÿπŸÑ  ÿßÿπŸÑ  ŸÉŸÅÿß ÿ© 100%. 
 
 
 
 
ùêøÃÖ=ùêª 
‚àëùëÉùëñ ùêøùëñùëõ
ùëñ=1=‚àëùëÉùëñ  ùêºùëñùëõ
ùëñ=1 
‚àëùëÉùëñ ùêøùëñùëõ
ùëñ=1=‚àëùëÉùëñ log 2(1
ùëÉùëñ)ùëõ
ùëñ=1 
‚àë ùêøùëñùëõ
ùëñ=1=‚àëlog 2(1
ùëÉùëñ)ùëõ
ùëñ=1 
ùêø=log 2(1
ùëÉ) 
ùêø ŸáŸä ÿπÿØÿØ ÿßŸÑŸÄ bits ÿßŸÑŸÖŸà ŸàÿØÿ© ÿ≤Ÿä ÿßŸÑŸÄ message ÿ≤ÿßŸÑÿ≤ŸÖ ÿ™ÿ∑ŸÑÿπ ÿπÿØÿØ ÿµÿ≠Ÿäÿ≠ ŸàÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑŸàÿ≠ŸäÿØÿ© ÿßŸÑŸÑŸä ÿ™ÿ≠ŸÇŸÇ ÿØÿß ÿßŸÜ 
ùëÉ ÿ™ŸÉŸàŸÜ inverse power of two   
‚à¥ ùëÉ=1
2ùëõ 
 
 
ŸáŸÜÿØÿ±ÿ≥ ÿ∑ÿ±ŸäŸÇÿ™ŸäŸÜ ŸÖŸÜ ÿßŸÑŸÄ  Entropy Coding    ŸàŸáŸÖÿß Huffman coding    Ÿà Shannon coding    Ÿàÿßÿ≥ŸÖŸáŸÖ Entropy 
Coding ÿ£ŸÑŸÜ ÿ≤Ÿä ÿßŸÑÿ∑ÿ±ŸÑ ÿØŸä ÿ®ŸÜÿ≠ÿßŸàŸÑ ŸÜŸàÿµŸÑ ÿßŸÑŸÄ length ÿ®ÿ™ÿßÿ≠ ÿßŸÑŸÄ message ŸÑŸÑŸÄ  Entropy   ŸÇÿØÿ±ÿßÿ•ŸÑŸÖŸÉÿßŸÜ. 
 
 
 
 Proof of maximum efficiency  
 
 
Provide an organized technique for finding efficient variable length codes for a given set of 
messages  
 ŸáŸäÿ∑ÿ±ŸäŸÇÿ© ŸÖŸÜ ÿ∑ÿ±ŸÑ ÿßŸÑŸÄ Entropy Coding ÿ®ÿπÿ±Ÿá ÿßŸà ÿØ ÿ®ŸäŸáÿß ÿßŸÉŸàÿßÿØ ÿ®ÿ£ÿ∑ŸàÿßŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÑŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑŸÄ  
messages ,ÿ≤Ÿä ÿßŸÑŸÖÿ≥ÿ£ŸÑÿ© ÿ®ŸäŸÉŸàŸÜ ŸÖÿπÿ∑  ŸÖ ŸÖŸàÿπÿ© ŸÖŸÜ ÿßŸÑŸÄ messages   Ÿàÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ≠ÿØŸàŸàŸáŸÖ ŸàŸÖÿ∑ŸÑŸàÿ® ÿßŸà ÿØ ŸÉŸàÿØ
ŸÉŸÑ message  ŸàŸÜÿ™ÿ£ŸÉÿØ ÿßŸÜ ÿßÿßŸÑŸÉŸàÿßÿØ uniquely dec odable   ŸàŸÑŸäŸáÿß minimum length 
 
 ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ 
 
1.  ŸÜÿ™ÿ£ŸÉÿØ ÿßŸÜ ŸÖ ŸÖŸàÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ÿßŸÑŸÖÿπÿ∑ÿßÿ© ÿ™ÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ.  
2. ŸÜÿ±ÿ™ÿ® ÿßŸÑŸÄ messages  .ÿ™ÿ±ÿ™Ÿäÿ®ÿßŸã ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ŸÇŸäŸÖ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ≠ÿØŸàŸàŸáÿß  
3.  ŸÜ ŸÖÿπ ÿßÿÆÿ± ŸÇŸäŸÖÿ™ŸäŸÜ ÿ≤Ÿä ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸàŸÜÿπŸäÿØ ÿ™ÿ±ÿ™Ÿäÿ® Ÿáÿ∞Ÿá ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ÿ®ÿ≠Ÿäÿ´ ŸÜÿ≠ÿßÿ≤ÿ∏ ÿπŸÑ  ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑÿ™ŸÜÿßÿ≤ŸÑŸä.   
4.  ŸÜŸÉÿ±ÿ± ÿßŸÑÿπŸÖŸÑŸäÿ© ÿ±ŸÇŸÖ2   Ÿà3   ÿ≠ÿ™  ŸÜÿ≠ÿµŸÑ ÿπŸÑ  ÿπÿØÿØŸäŸÜ ÿ≤ŸÇÿ∑ 
5.  ÿßŸÑÿπÿØÿØÿßÿ£ŸÑŸàŸÑ  ŸÜŸÅÿ™ÿ±ÿß ŸÑŸá ŸÉŸàÿØ 0 ŸàÿßŸÑÿπÿØÿØ ÿßŸÑÿ™ÿßŸÜŸä 1   
6. ŸÜŸÇŸàŸÖ ÿ®ÿßŸÑÿ± Ÿàÿ≠ ÿ®ÿßÿ£ŸÑÿ≥ŸáŸÖ ŸàÿπŸÑŸäŸáÿß ÿßÿßŸÑŸÉŸàÿßÿØ ŸÑŸà ÿßŸÑÿ≥ŸáŸÖ ŸÖŸèŸÅÿ±ÿØ ÿßŸÑŸÉŸàÿØ Ÿäÿ± ÿπ ÿ≤Ÿä ŸÖÿß ŸáŸà  
7.  ŸÑŸàÿßŸÑÿ≥ŸáŸÖ ŸÖÿ≤ÿØŸàÿ¨  ŸÜÿ± ÿπ ÿßŸÑŸÉŸàÿØ ŸàŸÜÿ∂ŸäŸá ÿπŸÑ  ÿßŸÑ ÿ≤  ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ 0  ŸàÿßŸÑ ÿ≤  ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™1   
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
ùë∫ùüè  ùüè
ùüèùüî   ,  ùë∫ùüê  ùüè
ùüñ   ,  ùë∫ùüë  ùüè
ùüí   ,  ùë∫ùüí  ùüè
ùüèùüî  ,  ùë∫ùüì  ùüè
ùüê 
Construct the Huffman cod es and calculate the efficiency of the code  
Solution  
 
 
Huffman coding  
Example 2  
 
codes  
ùë∫ùüè    1110  
ùë∫ùüê    110  
ùë∫ùüë    10   
ùë∫ùüí    1111   
ùë∫ùüì    0    
 ÿßÿßŸÑŸÉŸàÿßÿØ ÿ™ÿ≠ŸÇŸÇ ÿ≤ŸäŸáÿß ÿÆÿßÿµŸäÿ© uniquely decodable ÿ≠Ÿäÿ´ ÿßŸÜ ŸÉŸÑ ŸÉŸàÿØ ŸÖÿÆÿ™ŸÑŸá ÿπŸÜ ÿßÿßŸÑÿÆÿ± Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸäŸèŸÅÿ≥ÿ± ÿ≤Ÿä ÿßŸÑŸÄ   
Receiver  .ÿ®ÿ∑ÿ±ŸäŸÇÿ© Ÿàÿßÿ≠ÿØŸá, ŸàÿßÿßŸÑŸÉŸàÿßÿØ ŸÑŸäŸáÿß ÿßÿ∑ŸàÿßŸÑ ŸÖÿÆÿ™ŸÑŸÅÿ© Ÿàÿ™ÿπÿ™ŸÖÿØ ÿπŸÑ  ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑÿ≠ÿØŸàÿ´ 
 
Average Length  
ùêøÃÖ=‚àëùëÉùëñ ùêøùëñ=4√ó1
16+3√ó 1
8+2√ó 1
4+4√ó 1
16+1√ó 1
25
ùëñ=1 
=15
8  bits/message   
 
Entropy  
ùêª=‚àëùëÉùëñ log 2(1
ùëÉùëñ)5
ùëñ=1 
=1
16√ó log 2(16)+1
8√ó log 2(8)+1
4√ó log 2(4)+1
16√ó log 2(16)+1
2√ó log 2(2) 
=15
8  bits/message   
 
Efficiency  
ùúÇ=ùêª
ùêøÃÖ√ó100 =15
8
15
8√ó100 =100%  
 ŸÜÿßŸÑÿ≠ÿ∏ ÿßŸÜ ŸÇŸäŸÖ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ŸáŸä inverse power of two  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ùêª=ùêøÃÖ  Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÜÿ≠ÿµŸÑ ÿπŸÑ  ÿßÿπŸÑ  ŸÉŸÅÿß ÿ©
ÿ£ŸÑŸÜŸÜÿß ÿ≠ÿµŸÑŸÜÿß ÿπŸÑ  ÿßŸÉŸàÿßÿØ ÿ®ÿßŸÇŸÑ ÿπÿØÿØ ŸÖŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑ ŸÄ bits 
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
ùë∫ùüè  ùüé.ùüéùüì   ,  ùë∫ùüê   ùüé.ùüèùüì   ,  ùë∫ùüë  ùüé.ùüê   ,  ùë∫ùüí  ùüé.ùüéùüì  ,  ùë∫ùüì  ùüé.ùüèùüì ,  ùë∫ùüî  ùüé.ùüë  ,  ùë∫ùüï  ùüé.ùüè 
Construct the Huffman cod es and calculate the efficiency of the code  
Solution  
 
 
 
 
codes  
ùë∫ùüè    1110  
ùë∫ùüê    010  
ùë∫ùüë    10   
ùë∫ùüí    1111   
ùë∫ùüì    011    
ùë∫ùüî    00   
ùë∫ùüï    110    
 
Example 3  
 
Average Length  
ùêøÃÖ=‚àëùëÉùëñ ùêøùëñ7
ùëñ=1 
=4√ó0.05+3√ó 0.15+2√ó 0.2+4√ó 0.05+3√ó 0.15+2√ó0.3+3√ó0.1 
=2.6  bits/message   
 
Entropy  
ùêª=‚àëùëÉùëñ log 2(1
ùëÉùëñ)7
ùëñ=1 
=0.05√ólog 2(1
0.05)+0.15√ólog 2(1
0.15)+0.2√ólog 2(1
0.2) 
+0.05√ólog 2(1
0.05)+0.15√ólog 2(1
0.15)+0.3√ólog 2(1
0.3)+0.1√ólog 2(1
0.1)  
 
=2.57  bits/message   
 
Efficiency  
ùúÇ=ùêª
ùêøÃÖ√ó100 =2.57
2.6√ó100 =98.85%  
 
 
 
 
 
 
 
 
 
 
 
 
 
Shannon Coding  is similar to the Huffman, a major being that the operations are performed 
in forward rather than backward direction.  
ÿ™ÿßŸÜŸä  ÿ∑ÿ±ŸäŸÇÿ© ŸÖŸÜ ÿ∑ÿ±ŸÑ ÿßŸÑŸÄ  Entropy Coding  Ÿàÿ≤ŸÉÿ±ÿ™Ÿáÿß ŸÖŸÇÿßÿ±ÿ®ÿ© ŸÑŸÅŸÉÿ±ÿ© Huffman  Coding   Ÿàÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ®ÿ™ÿØŸä ŸÜŸÅÿ≥
ÿßŸÑŸÉŸÅÿß ÿ© ŸàŸÑŸÉŸÜ ÿßÿßŸÑÿÆÿ™ÿßŸÑŸá ÿ≤Ÿä ÿßŸÑŸÄ operation  ÿ≠Ÿäÿ´ ÿßŸÜ ÿ≤Ÿä Huffman  Coding  ŸÉŸÜÿß ÿ®ŸÜÿ®ÿØÿ£ ŸÖŸÜ ÿßÿßŸÑÿÆÿ± ŸÑÿ≠ÿØ ŸÖÿßÿ£ŸàÿµŸÑ  
ŸÑŸÑÿ®ÿØÿßŸäÿ© ( backward ) ŸÑŸÉŸÜ ÿ≤Ÿä Shannon Coding  ÿ®Ÿäÿ™ŸÖ ÿ®ÿ¥ŸÉŸÑ forward  ÿ≤Ÿä ŸÖÿß ŸáŸÜÿ¥ŸàŸá 
 
ÿßŸÑÿÆÿ∑Ÿàÿßÿ™  
 
1. .ŸÜÿ™ÿ£ŸÉÿØ ÿßŸÜ ŸÖ ŸÖŸàÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑŸÖÿπÿ∑ÿßÿ© ÿ™ÿ≥ÿßŸàŸä Ÿàÿßÿ≠ÿØ 
2. ŸÜÿ±ÿ™ÿ® ÿßŸÑŸÄ messages  .ÿ™ÿ±ÿ™Ÿäÿ®ÿßŸã ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ŸÇŸäŸÖ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ≠ÿØŸàŸàŸáÿß  
3.   ŸÜŸÇŸàŸÖ ÿ®ÿ™ŸÇÿ≥ŸäŸÖ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿßŸÑ  ŸÜÿµŸÅŸäŸÜ(ŸÇÿØÿ± ÿßÿ•ŸÑŸÖŸÉÿßŸÜ) ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ    
4.  ŸÖÿß ÿ≤ŸàŸÑÿßŸÑÿÆÿ∑ ŸÜÿπÿ∑ŸäŸá ÿßŸÑŸÉŸàÿØ  0  ŸàŸÖÿß ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ŸÜÿπÿ∑ŸäŸá ÿßŸÑŸÉŸàÿØ1 
5.  ŸÜŸÉÿ±ÿ± ÿßŸÑÿÆÿ∑Ÿàÿ™ŸäŸÜ ÿßŸÑÿ≥ÿßÿ®ŸÇÿ™ŸäŸÜ ŸÖÿπ ŸÉŸÑ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ Ÿàÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ≠ÿ™  ŸäŸÉŸàŸÜ ÿ≤ŸàŸÑ ŸÉŸÑ ÿÆÿ∑ ÿßÿ≠ÿ™ŸÖÿßŸÑ Ÿàÿßÿ≠ÿØ 
 
ÿßŸÑŸÅŸÉÿ±ÿ© ÿ®ÿßÿÆÿ™ÿµÿßÿ± ÿßŸÜŸÜÿß  ÿ®ŸÜŸÇÿ≥ŸÖ ÿ®ÿßŸÑÿÆÿ∑ ÿ®ÿ≠Ÿäÿ´ ŸäŸÉŸàŸÜ ŸÖ ŸÖŸàÿ≠ ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑  Ÿäÿ≥ÿßŸàŸä ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑  ÿ®ŸÇÿØÿ± ÿßÿ•ŸÑŸÖŸÉÿßŸÜ.  
 
 
 
A communication system consists of five possible messages. The probability of 
them is as follows :  
ùë∫ùüè  ùüè
ùüèùüî   ,  ùë∫ùüê  ùüè
ùüñ   ,  ùë∫ùüë  ùüè
ùüí   ,  ùë∫ùüí  ùüè
ùüèùüî  ,  ùë∫ùüì  ùüè
ùüê 
Construct the Shannon  codes and calculate the efficiency of the code  
Solution  
 
 
Shannon Coding  
Example 4  
 
ÿ≠ŸÑ ÿ™ŸÅÿµŸäŸÑŸä ÿ®ÿßŸÑÿÆÿ∑Ÿàÿßÿ™
  
:
 
 
 
 
 
1
.
 
ŸáŸÜŸÇÿ≥ŸÖ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™
  
ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ ÿ®ÿ≠ŸäŸÄŸÄÿ´ ŸÜÿ≠ÿµŸÄŸÄŸÑ ÿπŸÑŸÄŸÄ  ŸÇŸäŸÖŸÄŸÄÿ© ŸÜŸÄŸÄÿµ ÿ≤ŸÄŸÄŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ 
ÿßŸÑŸÅÿßÿµŸÑ (ÿ®ÿßŸÑŸÑŸàŸÜ 
ÿßÿ£ŸÑÿ≠ŸÖÿ±
) ŸàŸÖ ŸÖŸÄŸÄŸàÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖŸÄŸÄÿßÿßŸÑÿ™ ÿ™ÿ≠ŸÄŸÄÿ™ ÿßŸÑÿÆŸÄŸÄÿ∑ ŸÇŸäŸÖÿ™ŸáŸÄŸÄÿß ÿ™ÿ≥ŸÄŸÄÿßŸàŸä 
ŸÜÿµ ŸàŸáŸÜÿ≠ÿ∑ ÿßŸÑŸÇŸäŸÖ ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ ÿ® 
ŸÄ
 
0
 
 ŸàŸÉŸÑ ÿßŸÑŸÇŸäŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ®
ŸÄ
 
1
 
 
 
 
 
 
2
.
 
 ÿßŸÑ ÿ≤  ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ŸÑŸäŸá ŸÇŸäŸÖŸÄŸÄÿ© Ÿàÿßÿ≠ŸÄŸÄÿØŸá ÿ®ŸÄŸÄŸÜÿµ ŸÖŸÄŸÄÿß ŸáŸÜŸÇÿ≥ŸÄŸÄŸÖŸá ŸàÿßŸÑ ŸÄŸÄÿ≤  ÿßŸÑŸÑŸÄŸÄŸä ÿ™ÿ≠ŸÄŸÄÿ™
ŸáŸÜŸÇÿ≥ŸÖŸá ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ
 
ÿ®ÿπÿØ 
ùëÜ
3
 
 (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿ£ŸÑÿÆÿ∂ÿ±
)
 
ÿ®ÿ≠Ÿäÿ´ ÿ™ŸÉŸàŸÜ ÿßŸÑŸÇŸäŸÖ ÿ≤ŸÄŸÄŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ 
ÿ®ÿ±ÿ®ÿπ ŸàŸÜÿ≠ÿ∑ ÿπŸÜÿØŸáÿß 
0
 
 ŸàŸÖ ŸÖŸàÿ≠ ÿßŸÑŸÇŸäŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ®ÿ±ÿ®ÿπ ŸàŸÜÿ≠ÿ∑ ÿπŸÜÿØŸáÿß
1
 
 
 
 
 
 
 
3
.
 
  ÿ®ÿπÿØ ŸÉÿØÿß ŸáŸÜÿπŸÖŸÑ ÿÆÿ∑ ÿ≤ÿßÿµŸÑ ÿ®ÿπÿØ
ùëÜ
2
  
  (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿßŸÑÿ≤ÿ±ŸÇ
) ŸÑ ÿ≤ÿ¶ŸäŸÜ ŸÇŸäŸÖÿ© ŸÉŸÄŸÄŸÑ  ŸÄŸÄÿ≤  
ŸÖŸÜŸáŸÖ  
1
8
  
  Ÿàÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ ŸÜÿ≠ÿ∑
0
 
  Ÿàÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ŸÜÿ≠ÿ∑
1
 
 
 
 
 
 
 
4
.
 
 ÿßÿÆÿ± ÿÆÿ∑Ÿàÿ© ŸáŸÜŸÇÿ≥ŸÖ ÿßŸÑ ÿ≤  ÿßŸÑÿ®ÿßŸÇŸä ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ ÿ®ÿπÿØ
ùëÜ
1
 
 (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿßŸÑÿ≥ŸàŸà 
)
 
ÿ®ÿ≠ŸäŸÄŸÄÿ´ 
ŸÜŸÇÿ≥ŸÖŸáŸÖ ŸÑ ÿ≤ÿ¶ŸäŸÜ ŸÉŸÑ  ÿ≤  ŸÇŸäŸÖÿ™Ÿá  
1
16
 
  Ÿàÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ ŸÜÿ≠ÿ∑
0
 
  Ÿàÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ŸÜÿ≠ÿ∑
1
 
 
 
 
 
ÿßŸÑÿÆÿ∑Ÿàÿßÿ™
  
ŸàÿßÿßŸÑ
ŸÑ
ŸàÿßŸÜ
  
ŸÑŸÑÿ™Ÿà 
ÿ∂Ÿäÿ≠ ÿ≤ŸÇÿ∑ ŸÑŸÉŸÜ ÿ≤Ÿä ÿ≠ŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸáŸÜÿ±ÿ≥ŸÖ  
ÿßŸÑÿ±ÿ≥ŸÖÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©
.
 
 

 
codes  
ùë∫ùüè    1110           
ùë∫ùüê    110            
ùë∫ùüë    10             
ùë∫ùüí    1111          
ùë∫ùüì    0    
 
 
Average Length  
ùêøÃÖ=‚àëùëÉùëñ ùêøùëñ=4√ó1
16+3√ó 1
8+2√ó 1
4+4√ó 1
16+1√ó 1
25
ùëñ=1 
=15
8  bits/message   
 
Entropy  
ùêª=‚àëùëÉùëñ log 2(1
ùëÉùëñ)5
ùëñ=1 
=1
16√ó log 2(16)+1
8√ó log 2(8)+1
4√ó log 2(4)+1
16√ó log 2(16)+1
2√ó log 2(2) 
=15
8  bits/message   
 
 
Efficiency  
ùúÇ=ùêª
ùêøÃÖ√ó100 =15
8
15
8√ó100 =100%  
 
 
 
 
A 
communication system consists of five possible messages. The probability of 
them is as follows :
 
ùë∫
ùüè
 
 
ùüé
.
ùüéùüì
   
,  
ùë∫
ùüê
 
 
 
ùüé
.
ùüèùüì
   
,  
ùë∫
ùüë
 
 
ùüé
.
ùüê
   
,  
ùë∫
ùüí
 
 
ùüé
.
ùüéùüì
  
,  
ùë∫
ùüì
 
 
ùüé
.
ùüèùüì
 
,  
ùë∫
ùüî
 
 
ùüé
.
ùüë
  
,  
ùë∫
ùüï
 
 
ùüé
.
ùüè
 
Construct the 
Shannon
 
cod
es and calculate the efficiency of the code 
 
Solution
 
 
 
 
ÿ≠ŸÑ ÿ™ŸÅÿµŸäŸÑŸä ÿ®ÿßŸÑÿÆÿ∑Ÿàÿßÿ™
  
:
 
 
 
1
.
 
 ÿ®ÿπÿØ ÿ™ÿ±ÿ™Ÿäÿ® ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ®ÿ¥ŸÉŸÑ ÿ™ŸÜÿßÿ≤ŸÑŸä
ŸáŸÜŸÇÿ≥ŸÖ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™
 
ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÄŸÄŸÑ
 
ÿ®ÿπŸÄŸÄÿØ 
ùëÜ
3
 
  (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿ£ŸÑÿ≠ŸÖÿ±
)
  
ÿ®ÿ≠Ÿäÿ´  
ŸÖ ŸÖŸàÿ≠ ÿßÿ≠ÿ™ŸÖÿßÿßŸÑÿ™
  
Ÿäÿ≥ŸÄŸÄÿßŸàŸä
  
ŸÜŸÄŸÄÿµ ÿ≤ŸÄŸÄŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ ÿßŸÑŸÅÿßÿµŸÄŸÄŸÑ
 
ŸàŸÖ ŸÖŸàÿ≠ ÿßÿßŸÑÿ≠ÿ™ŸÖÿßÿßŸÑÿ™ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ŸÇŸäŸÖÿ™Ÿáÿß ÿ™ÿ≥ÿßŸàŸä ŸÜÿµ
  
ŸàŸáŸÜÿ≠ÿ∑ ÿßŸÑŸÇŸäŸÖ ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ 
ÿ® 
ŸÄ
 
0
 
 ŸàŸÉŸÑ ÿßŸÑŸÇŸäŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ®
ŸÄ
  
1
 
 
 
 
Example 
5
 
 
 
 
 
 
2
.
 
  ŸÜŸÇÿ≥ŸÖ ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ ŸÖŸÜ ÿ®ÿπÿØ
ùëÜ
6
  
  (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿ£ŸÑÿÆÿ∂ÿ±
)
  
ÿ®ÿ≠Ÿäÿ´ ŸäŸÉŸàŸÜ ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ 
0.3
  
  ŸàÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑
0.2
  
ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ŸÇÿ±Ÿäÿ®ŸäŸÜ ŸÖŸÜ ÿ®ÿπÿß ÿßŸÑŸä ÿ≠ÿØ ŸÖÿß
.
  
Ÿà 
ÿ®ÿπŸÄŸÄÿØ
  
ŸÉŸÄŸÄÿØÿß 
ŸÜŸÇÿ≥ŸÄŸÄŸÖ ÿßŸÑŸÄŸÄŸÜÿµ ÿßŸÑŸÑŸÄŸÄŸä ÿ™ÿ≠ŸÄŸÄÿ™ ÿßŸÑŸÄŸÄ   ŸÄŸÄÿ≤ÿ¶ŸäŸÜ ŸÖÿ™ÿ≥ŸÄŸÄÿßŸàŸäŸäŸÜ 
ÿ®ÿÆŸÄŸÄÿ∑ ÿ≤ÿßÿµŸÄŸÄŸÑ ŸÖŸÄŸÄŸÜ ÿ®ÿπŸÄŸÄÿØ 
ùëÜ
5
 
  (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿ£ŸÑÿÆÿ∂ÿ±
)
  
ÿ®ÿ≠Ÿäÿ´ ŸäŸÉŸàŸÜ ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ ŸÖ ŸÖŸÄŸÄŸàÿπŸáŸÖ 
0.3
 
 ŸàÿßŸÑŸÑŸÄŸÄŸä ÿ™ÿ≠ŸÄŸÄÿ™
ÿßŸÑÿÆÿ∑
 
ŸÖ ŸÖŸàÿπŸáŸÖ
  
0.2
 
 ŸàŸáŸÜÿ≠ÿ∑ ÿßŸÑŸÇŸäŸÖ ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ ÿ®
ŸÄ
 
0
 
 Ÿà
ÿßŸÑŸÇŸäŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ® 
ŸÄ
 
1
 
 
 
 
 
 
3
.
 
 ŸàŸÜŸÇÿ≥ŸÖ ÿ®ÿÆÿ∑ ÿ≤ÿßÿµŸÑ ŸÖŸÜ ÿ®ÿπÿØ
ùëÜ
7
 
 (ÿ®ÿßŸÑŸÑŸàŸÜ
ÿßÿßŸÑÿ≤ÿ±ŸÇ
) 
ÿ®ÿ≠Ÿäÿ´ ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ 
0.
1
 
ŸàÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑
 
ŸÖ ŸÖŸàÿπŸÄŸÄÿ©
 
0.
1
 
 ŸàŸÜŸÇÿ≥ŸÄŸÄŸÖ ÿ®ÿÆŸÄŸÄÿ∑ ÿ≤ÿßÿµŸÄŸÄŸÑ ŸÖŸÄŸÄŸÜ ÿ®ÿπŸÄŸÄÿØ
ùëÜ
2
 
 (ÿ®ŸÄŸÄÿßŸÑŸÑŸàŸÜ
ÿßÿßŸÑÿ≤ÿ±ŸÇ
)  
ÿ®ÿ≠Ÿäÿ´ ÿßŸÑŸÑŸä ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑  
0.
1
5
  
ŸàÿßŸÑŸÑŸä ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑
  
0.
1
5
  
 ŸàŸáŸÜÿ≠ÿ∑ ÿßŸÑŸÇŸÄŸÄŸäŸÖ
ÿ≤ŸàŸÑ ÿßŸÑÿÆÿ∑ ÿ® 
ŸÄ
  
0
 
 Ÿà
ÿßŸÑŸÇŸäŸÖ ÿ™ÿ≠ÿ™ ÿßŸÑÿÆÿ∑ ÿ® 
ŸÄ
 
1
 
 
 
 
 
 
 
 
4
.
 
  ÿßÿÆÿ± ÿÆÿ∑Ÿàÿ© ŸáŸÜŸÇÿ≥ŸÄŸÄŸÖ ÿßŸÑ ŸÄŸÄÿ≤  ÿßŸÑÿ®ŸÄŸÄÿßŸÇŸä ÿ®ÿÆŸÄŸÄÿ∑ ÿ≤ÿßÿµŸÄŸÄŸÑ ÿ®ÿπŸÄŸÄÿØ
ùëÜ
1
  
  (ÿ®ŸÄŸÄÿßŸÑŸÑŸàŸÜ
ÿßÿßŸÑÿ≥ŸàŸà 
)
 
ÿ®ÿ≠Ÿäÿ´ ŸÜŸÇÿ≥ŸÖŸáŸÖ ŸÑ ÿ≤ÿ¶ŸäŸÜ ŸÉŸÑ  ÿ≤  ŸÇŸäŸÖÿ™Ÿá  
0
.
05
  
 Ÿàÿ≤ŸÄŸÄŸàŸÑ ÿßŸÑÿÆŸÄŸÄÿ∑ ŸÜÿ≠ŸÄŸÄÿ∑
0
 
 Ÿàÿ™ÿ≠ŸÄŸÄÿ™
ÿßŸÑÿÆÿ∑ ŸÜÿ≠ÿ∑  
1
 
 
 
 
 
 
 
 
 
  ÿßŸÑÿ≤ŸÖ ÿßÿ≤ÿµŸÑ ÿ®ŸäŸÜ
ÿ£Ÿä
  
ŸÇŸäŸÖÿ™ŸäŸÜ ÿ®Ÿäÿ≥ÿßŸàŸàÿß ÿ®ÿπÿß ÿ®ÿÆŸÄŸÄÿ∑ 
ÿ≤Ÿä ŸÖŸÄŸÄÿß 
ÿ≠ÿ∑ŸäŸÄŸÄÿ™ 
ÿÆŸÄŸÄÿ∑ ÿ®ŸÄŸÄŸäŸÜ 
0.15
 
 Ÿà
0.15
 
 ŸàŸÉŸÄŸÄÿ∞ŸÑÿ® ÿ®ŸÄŸÄŸäŸÜ
0.
05
 
 Ÿà
0.05
 
ÿπÿ¥ÿßŸÜ ÿßÿßŸÑÿ™ŸÜŸäŸÜ ŸÖŸäÿßÿÆÿØŸàÿß ŸÜŸÅÿ≥ ÿßŸÑŸÉŸàÿØ ŸàŸÉŸÖ
ÿßŸÜ ÿßŸÑÿ≤ŸÖ Ÿäÿ®ŸÇ  ÿ™ÿ≠ÿ™ ŸÉŸÑ ÿÆÿ∑ ÿßÿ≠ÿ™ŸÖÿßŸÑ  
 
 
 
code s  
ùë∫ùüè    1110             
ùë∫ùüê    100  
ùë∫ùüë    01                  
ùë∫ùüí    1111   
ùë∫ùüì    101                
ùë∫ùüî    00   
ùë∫ùüï    110    
 
 
Average Length  
ùêøÃÖ=‚àëùëÉùëñ ùêøùëñ7
ùëñ=1 
=4√ó0.05+3√ó 0.15+2√ó 0.2+4√ó 0.05+3√ó 0.15+2√ó0.3+3√ó0.1 
=2.6  bits/message   
Entropy  
ùêª=‚àëùëÉùëñ log 2(1
ùëÉùëñ)7
ùëñ=1 
=0.05√ólog 2(1
0.05)+0.15√ólog 2(1
0.15)+0.2√ólog 2(1
0.2) 
+0.05√ólog 2(1
0.05)+0.15√ólog 2(1
0.15)+0.3√ólog 2(1
0.3)+0.1√ólog 2(1
0.1)  
 
=2.57  bits/message   
 
Efficiency  
ùúÇ=ùêª
ùêøÃÖ√ó100 =2.57
2.6√ó100 =98.85%  
 
 
 
 
Shannon has shown that a given communication channel has a maximum rate of 
information C shown as the channel capacity.  If the information rate R, is less than C, one 
can approach arbitrarily small error probabilities by intelligent choice techniques.  
If the information rate R is greater than the channel capacity C, errors cannot be avoided 
regardless of the coding technique employed.  
 
 ÿßŸÑÿπÿßŸÑŸÖ Shannon  Ÿàÿ∂ÿ≠ ÿßŸÜ ÿ≤ŸäŸárate ŸÖÿπŸäŸÜ ŸÖŸÇÿØÿ±  ÿßÿ™ ÿßŸàÿ≤Ÿá ÿ≤Ÿä ÿßÿ±ÿ≥ÿßŸÑ ÿßŸÑÿØÿßÿ™ÿß ÿ≤Ÿä ÿßŸÑŸÄ channel   ŸàÿßŸÇÿµrate  
  ÿßŸÇÿØÿ± ÿßÿ®ÿπÿ™ ÿ®ŸäŸá ÿßŸÑÿØÿßÿ™ÿß ÿπŸÑÿßŸÑŸÄ channel  ŸáŸàC Channel Capacity Ÿàÿ∑ŸàŸÑ ŸÖÿß ÿßŸÑŸÄ information rate R   ÿßŸÇŸÑ ŸÖŸÜ
C Channel Capacity   ŸáŸÜŸÇÿØÿ± ŸÜÿπŸÖŸÑ coding techniques  ŸäŸÉÿ™ÿ¥Ÿáÿßÿ£ŸÑÿÆÿ∑ÿß  ŸàŸäÿµÿ≠ÿ≠Ÿáÿßÿå ŸàŸÑŸÉŸÜ ŸÑŸà  R > C    ŸÖ
ŸáŸÇÿØÿ± ÿßÿ™ ÿßŸàÿ≤ ÿßÿ£ŸÑÿÆÿ∑ÿß  ŸÖŸáŸÖÿß ÿßÿ≥ÿ™ÿÆÿØŸÖÿ™ ŸÖŸÜ ÿ∑ÿ±ŸÑ ŸÑÿ™ÿµÿ≠Ÿäÿ≠ ŸàÿßŸÉÿ™ÿ¥ÿßŸá ÿßÿ£ŸÑÿÆÿ∑ÿß  ÿßŸÑŸÜ ÿßŸÑŸÄ rate ÿ®ŸÇ  ÿπÿßŸÑŸä  ÿØÿß 
 
We consider the bandlimited channel operation at the presence of additive white Gaussian 
noise. The channel capacity is given by  
 
ùê∂=ùêµlog 2(1+ùëÜ
ùëÅ) 
 
C  is the capacity in bits/sec  
B  is the bandwidth of the channel in Hz  
ùëÜ
ùëÅ  Signal to noise ratio  
 
  ÿßŸÑÿ≥ÿ±ÿπÿ© ÿßŸÑŸÇÿµŸàÿß ŸÑŸÜŸÇŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿπŸÑ  ÿßŸÑŸÇŸÜÿßÿ© Ÿäÿπÿ™ŸÖÿØ ÿπŸÑ channel bandwidth  B   Ÿà ŸÇŸäŸÖÿ© Signal to noise ratio   
  ÿ≠Ÿäÿ´ÿ™ŸÖ ÿßÿ≤ÿ™ÿ±ÿßÿß Ÿà ŸàÿØ   additive white Gaussian noise ÿ≤Ÿä ÿßŸÑŸÄ channel. 
 
If the bandwidth approaches infinity, the capacity also approaches infinity. this is not correct  
Since the noise is assumed to be white, the wider the bandwidth the more noise is admitted 
to the system. Thus, as B increase ,  ùëÜ
ùëÅ decrease  
Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÜ ÿ®ÿ≤ŸäÿßÿØÿ© ÿßŸÑŸÄ Bandwidth  ÿ®Ÿäÿ≤ŸäÿØ ŸÖÿπÿØŸÑ ÿßÿßŸÑÿ±ÿ≥ÿßŸÑŸàÿ£Ÿäÿ∂ÿß ÿ®Ÿäÿ≤ŸäÿØ ÿ™ÿ£ŸàŸäÿ± ÿßŸÑŸÄ noise ÿπŸÑ  ÿßŸÑŸÄ  
system   ŸäÿπŸÜŸäùëÜ
ùëÅ  Ÿáÿ™ŸÇŸÑŸàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸà ÿßÿ≤ÿ™ÿ±ÿ∂ŸÜÿß ÿßŸÜ  ÿßŸÑŸÄ  Bandwidth ÿßŸÑ ŸÜŸáÿßÿ¶Ÿä ÿØÿß ŸÖ  ŸÖÿπŸÜÿßŸá ÿßŸÜŸÜÿß ŸÜŸÇÿØÿ± ŸÜÿ®ÿπÿ™ bit rate  
ÿßŸÑ ŸÜŸáÿßÿ¶Ÿä  ÿßŸÑŸÜ ÿßŸÑŸÄ noise ÿ®ÿ™ÿ£Ÿàÿ± ÿπŸÑ  ÿßŸÑŸÄ rate ÿßŸÑŸÑŸä ÿ®ÿ®ÿπÿ™ ÿ®ŸäŸá ÿßŸÑÿØÿßÿ™ÿß. 
 
 Channel Capacity  
 
 
 
ùê∂
=
ùêµ
log
2
(
1
+
ùëÜ
ùëÅ
)
 
ùê∂
=
ùêµ
log
2
(
1
+
ùëÜ
ùëÅ
o
ùêµ
)
 
Note that B is in hertz and that 
ùëÅ
o
 
is the power spectral density in watts per hertz. We now 
find the value of channel capacity approaches as B goes to infinity:
 
lim
ùêµ
‚Üí
‚àû
ùê∂
=
lim
ùêµ
‚Üí
‚àû
ùêµ
log
2
(
1
+
ùëÜ
ùëÅ
o
ùêµ
)
 
 
 ŸáŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿßŸÑŸÇÿ© ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿ©
ÿßÿ£ŸÑÿ™Ÿäÿ©
 
ÿ≤Ÿä ÿßÿßŸÑŸàÿ®ÿßÿ™ ŸàŸÜÿπÿØŸÑ ÿ¥ŸÉŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿ®ÿ≠Ÿäÿ´ ŸÜŸàÿµŸÑ ŸÑÿ¥ŸÉŸÑ ÿßŸÑÿπÿßŸÑŸÇÿ© ÿØŸä:
 
lim
ùë•
‚Üí
‚àû
ùë•
 
log
2
(
1
+
1
ùë•
)
=
log
2
(
ùëí
)
=
1
.
44
 
 
lim
ùêµ
‚Üí
‚àû
ùê∂
=
lim
ùêµ
‚Üí
‚àû
ùëÜ
ùëÅ
o
 
[
ùëÅ
o
 
ùêµ
ùëÜ
 
log
2
(
1
+
ùëÜ
ùëÅ
o
ùêµ
)
]
 
‚à¥
ùê∂
ùëöùëéùë•
 
=
ùëÜ
ùëÅ
o
 
log
2
(
ùëí
)
=
1
.
44
 
ùëÜ
ùëÅ
o
 
The equation show the maximum possible channel capacity as a function of signal power 
and noise spectral density 
 
 
 
 
Proof of capacity C for infinity Bandwidth
 
 
 
 
If the SNR is 20 dB 
,and the bandwidth available is 4KHz ,which is approach for 
telephone communications, find the maximum rate of information that could be 
transmitted across the channel.
 
Solution
 
20
 
ùëëùêµ
=
10
log
10
(
ùëÜùëÅùëÖ
)
 
‚áí
ùëÜùëÅùëÖ
=
100
 
ùê∂
=
ùêµ
log
2
(
1
+
ùëÜ
ùëÅ
)
=
4
√ó
10
3
 
√ó
log
2
(
1
+
100
)
=
26
.
6
 
Kbit
/
sec
 
 
 
 
 
 
If the requirement to transmit at 50 Kbit/sec and a bandwidth of 1MHz is used, find 
the minimum 
ùë∫
ùëµ
 
required.
 
Solution
 
ùê∂
=
ùêµ
log
2
(
1
+
ùëÜ
ùëÅ
)
 
50
√ó
 
10
3
=
1
√ó
10
6
log
2
(
1
+
ùëÜ
ùëÅ
)
 
0
.
05
=
log
2
(
1
+
ùëÜ
ùëÅ
)
 
2
0
.
05
=
1
+
ùëÜ
ùëÅ
 
ùëÜ
ùëÅ
=
0
.
035
=
‚àí
14
.
5
 
dB
 
Example 
6
 
 
Example 
7
  

SYSTEMS

4th Edition

Simon Hayhin

 

~S¬•STEMS

 

4th ed 4th Edition

Simon Hayhin

  

| COMMUNICATION SYSTEMS

 

 

 

478 EDITION

| COMMUNICATION SYSTEMS

 

 

 

 

 

 

Simon Haykin
McMaster University

W

JOHN WILEY & Sons, INC.
New York # Chichester # Weinheim Brisbane # Singapore

4-m EDITION

@ Toronto

 

 

 

Editor Bill Zobrist

Marketing Manager Katherine Hepburn
Associate Production Director Lucifle Buonocore
Senior Production Editor Monique Calello
Cover Designer Madelyn Lesure
Illustration Coordinator Gene Aiello
Illustration Studio Wellington Studios

Cover Photo NASA/Photo Researchers, Inc.

This book was set in 10/12 Times Roman by UG / GGS Information Services, Inc. and printed and bound
by Hamilton Printing Company. The cover was printed by Phoenix Color Corporation.

This book is printed on acid-free paper.

The paper in this book was manufactured by a mill whose forest management programs include sustained
yield harvesting of its timberlands, Sustained yield harvesting principles ensure that the numbers of trees
cut each year does not exceed the amount of new growth.

Copyright ¬© 2001, John Wiley & Sons, Inc. All rights Reserved.

No part of this publication may be reproduced, stored in a retrieval system or transmitted
in any form or by any means, electronic, mechanical, photocopying, recording, scanning
or otherwise, except as permitted under Sections 107 or 1089 of the 1976 United States
Copyright Act, without either the prior written permission of the Publisher, or
authorization through payment of the appropriate per-copy fee to the Copyright
Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (508) 750-8400, fax

(508) 750-4470. Requests to the Publisher for permission should be addressed to the
Permissions Department, John Wiley & Sons, Inc., 605 Third Avenue, New York, NY
10158-0012, (212) 850-6011, fax (212) 850-6008, E-Mail: PERMREQ@WILEY.COM.

To order books or for customer service call 1-800-CALL-WILEY (225-5945).

Library of Congress Cataloging-in-Publication Data
Haykin, Simon
Communication systems / Simon Haykin.‚Äî4th ed.
p. cm,
ISBN 0-471-17869-1 (cloth : alk. paper)
1. Telecommunication. 2. Signal theory (Telecommunication) I. Title.

TK5101 .H37 2000
621,382‚Äîde21 99-042977

Printed in the United States of America

1098765432

Tn loving memory of Vera

 

 

Electrical engineering education has undergone some radical changes during the past cou-
ple of decades and continues to do so. A modern undergraduate program in electrical
engineering includes the following two introductory courses:

& Signals and Systems, which provides a balanced and integrated treatment of contin-
uous-time and discrete-time forms of signals and systems. The Fourier transform (in
its different forms), Laplace transform, and z-transform are treated in detail. Typi-
cally, the course also includes an elementary treatment of communication systems.

¬Æ Probability and Random Processes, which develops an intuitive grasp of discrete and
continuous random variables and then introduces the notion of a random process
and its characteristics.

Typically, these two introductory courses lead to a senior-level course on communication
systems.

The fourth edition of this book has been written with this background and primary
objective in mind. Simply put, the book provides a modern treatment of communication
systems at a level suitable for a one- or two-semester senior undergraduate course. The
emphasis is on the statistical underpinnings of communication theory with applications.

The material is presented in a logical manner, and it is illustrated with examples,
with the overall aim being that of helping the student develop an intuitive grasp of the
theory under discussion. Except for the Background and Preview chapter, each chapter
ends with numerous problems designed not only to help the students test their understand-
ing of the material covered in the chapter but also to challenge them to extend this material.
Every chapter includes notes and references that provide suggestions for further reading.
Sections or subsections that can be bypassed without loss of continuity are identified with
a footnote. :

A distinctive feature of the book is the inclusion of eight computer experiments using
MATLAB. This set of experiments provides the basis of a ‚ÄúSoftware Laboratory‚Äù, with
each experiment being designed to extend the material covered in the pertinent chapter.
Most important, the experiments exploit the unique capabilities of MATLAB in an instruc-
tive manner. The MATLAB codes for all these experiments are available on the Wiley Web
site: http://www.wiley.com/college/haykin/.

The Background and Preview chapter presents introductory and motivational ma-
terial, paving the way for detailed treatment of the many facets of communication systems
in the subsequent 10 chapters. The material in these chapters is organized as follows:

¬ª Chapter 1 develops a detailed treatment of random, or stochastic, processes, with
particular emphasis on their partial characterization (i.e., second-order statistics). In
effect, the discussion is restricted to wide-sense stationary processes. The correlation

vil

Vi

fais

fate

PREFACE

¬•

properties and power spectra of random processes are described in detail. Gaussian
processes and narrowband noise feature prominently in the study of communication
systems, hence their treatment in the latter part of the chapter. This treatment nat-
urally leads to the consideration of the Rayleigh and Rician distributions that arise
in a communications environment.

Chapter 2 presents an integrated treatment of continuous-wave (CW) modulation
(i.e., analog communications) and their different types, as outlined here:

(i) Amplitude modulation, which itself can assume one of the following forms (de-
pending on how the spectral characteristics of the modulated wave are specified):

Full amplitude modulation

Double sideband-suppressed carrier modulation

Quadrature amplitude modulation

Single sideband modulation

Vestigial sideband modulation

(ii) Angle modulation, which itself can assume one of two interrelated forms:
¬ª Phase modulation
¬Æ Frequency modulation

The time-domain and spectral characteristics of these modulated waves, methods for
their generation and detection, and the effects of channel noise on their performances
are discussed.

¬•¬•Y ¬• ¬• ¬•

Chapter 3 covers pulse modulation and discusses the processes of sampling, quan-
tization, and coding that are fundamental to the digital transmission of analog sig-
nals. This chapter may be viewed as the transition from analog to digital commu-
nications. Specifically, the following types of pulse modulation are discussed:
(i) Analog pulse modulation, where only time is represented in discrete form; it
embodies the following special forms:
¬Æ Pulse amplitude modulation
¬ª Pulse width (duration) modulation
¬Æ Pule position modulation
The characteristics of pulse amplitude modulation are discussed in detail, as it is
basic to all forms of pulse modulation, be they of the analog or digital type.
(ii) Digital pulse modulation, in which both time and signal amplitude are repre-
sented in discrete form; it embodies the following special forms:

& Pulse-code modulation

¬ª Delta modulation

¬ª Differential pulse-code modulation
In delta modulation, the sampling rate is increased far in excess of that used in pulse-
code modulation so as to simplify implementation of the system. In contrast, in
differential pulse-code modulation, the sampling rate is reduced through the use of
a predictor that exploits the correlation properties of the information-bearing signal.
(iii) MPEG/audio coding standard, which includes a psychoacoustic model as a key

element in the design of the encoder.

Chapter 4 covers baseband pulse transmission, which deals with the transmission of
pulse-amplitude modulated signals in their baseband form. Two important issues are
discussed: the effects of channel noise and limited channel bandwidth on the perfor-
mance of a digital communication system. Assuming that the channel noise is additive

PREFACE ix

and white, this effect is minimized by using a matched filter, which is basic to the
design of communication receivers. As for limited channel bandwidth, it manifests
itself in the form of a phenomenon known as intersymbol interference. To combat
the degrading effects of this signal-dependent interference, we may use either a pulse-
shaping filter or correlative encoder/decoder; both of these approaches are discussed.
The chapter includes a discussion of digital subscriber lines for direct communication
between a subscriber and an Internet service provider. This is followed by a deriva-
tion of the optimum linear receiver for combatting the combined effects of channel
noise and intersymbol interference, which, in turn, leads to an introductory treatment
of adaptive equalization.

Chapter S$ discusses signal-space analysis for an additive white Gaussian noise chan-
nel. In particular, the foundations for the geometric representation of signals with
finite energy are established. The correlation receiver is derived, and its equivalence
with the matched filter receiver is demonstrated. The chapter finishes with a discus-
sion of the probability of error and its approximate calculation.

Chapter 6 discusses passband data transmission, where a sinusoidal carrier wave is
employed to facilitate the transmission of the digitally modulated wave over a band-
pass channel, This chapter builds on the geometric interpretation of signals presented
in Chapter 5. In particular, the effect of channel noise on the performance of digital

communication systems is evaluated, using the following modulation techniques:
(i) Phase-shift keying, which is the digital counterpart to phase modulation with
the phase of the carrier wave taking on one of a prescribed set of discrete values.

(ii) Hybrid amplitude/phase modulation schemes including quadrature-amplitude
modulation (QAM), and carrierless amplitude/phase modulation (CAP).

(iii) Frequency-shift keying, which is the digital counterpart of frequency modulation
with the frequency of the carrier wave taking on one of a prescribed set of discrete
values, ;

(iv) Generic multichannel modulation, followed by discrete multitone, the use of
which has been standardized in asymmetric digital subscriber lines.

In a digital communication system, timing is everything, which means that the re-

ceiver must be synchronized to the transmitter. In this context, we speak of the

receiver being coherent or noncoherent. In a coherent receiver, provisions are made
for the recovery of both the carrier phase and symbol timing. In a noncoherent
receiver the carrier.phase is ignored and provision is only‚Äô‚Äòmade for symbol timing.

Such a strategy is dictated by the fact that the carrier phase may be random, making

phase recovery a costly proposition. Synchronization techniques are discussed in the

latter part of the chapter, with particular emphasis on discrete-time signal processing.

Chapter 7 introduces spread-spectrum modulation. Unlike traditional forms of mod-

ulation discussed in earlier chapters, channel bandwidth is purposely sacrificed in

spread-spectrum modulation for the sake of security or protection against interfering
signals, The direct-sequence and frequency-hop forms of spread-spectrum modula-
tion are discussed.

Chapter 8 deals with multiuser radio communications, where a multitude of users

have access to a common radio channel. This type of communication channel is well

represented in satellite and wireless communication systems, both of which are dis-
cussed, The chapter includes a presentation of link budget analysis, emphasizing the
related antenna and propagation concepts, and noise calculations.

Chapter 9 develops the fundamental limits in information theory, which are embod-

ied in Shannon‚Äôs theorems for data compaction, data compression, and data trans-

xX PREFACE

mission. These theorems provide upper bounds on the performance of information
sources and communication channels. Two concepts, basic to formulation of the
theorems, are (1) the entropy of a source (whose definition is analogous to that of
entropy in thermodynamics), and (2) channel capacity.

¬ª Chapter 10 deals with error-contro! coding, which encompasses techniques for the
encoding and decoding of digital data streams for their reliable transmission over
noisy channels. Four types of error-control coding are discussed:

(i) Linear block codes, which are completely described by sets of linearly indepen-
dent code words, each of which consists of message bits and parity-check bits.
The parity-check bits are included for the purpose of error control.

(ii) Cyclic codes, which form a subclass of linear block codes.

(iti) Convolutional codes, which involve operating on the message sequence contin-
uously in a serial manner.

(iv) Turbo codes, which provide a novel method of constructing good codes that
approach Shannon‚Äôs channel capacity in a physically realizable manner.

Methods for the generation of these codes and their decoding are discussed.

The book also includes supplementary material in the form of six appendices as
follows:

¬•

Appendix 1 reviews probability theory.

Appendix 2, on the representation of signals and systems, reviews the Fourier trans-
form and its properties, the various definitions of bandwidth, the Hilbert transform,
and the low-pass equivalents of narrowband signals and systems.

Appendix 3 presents an introductory treatment of the Bessel function and its modified
form. Bessel functions arise in the study of frequency modulation, noncoherent de-
tection of signals in noise, and symbol timing synchronization.

Appendix 4 introduces the confluent hypergeometric function, the need for which
arises in the envelope detection of amplitude-modulated signals in noise.

Appendix 5 provides an introduction to cryptography, which is basic to secure
communications.

¬ª Appendix 6 includes 12 useful tables of various kinds.

¬•

a

a

¬•

As mentioned previously, the primary purpose of this book is to provide a modern
treatment of communication systems suitable for use in a one- or. two-semester under- ~
graduate course at the senior level.‚ÄôThe make-up of the material for the course is naturally
determined by the background of the students and the interests of the teachers involved.
The material covered in the book is both broad and deep enough to satisfy a variety of
backgrounds and interests, thereby providing considerable flexibility in the choice of
course material. As an aid to the teacher of the course, a detailed solutions manual for all
the problems in the book is available from the publisher.

a Acknowledgments

I wish to express my deep gratitude to Dr. Gregory J. Pottie (University of California, Los
Angeles), Dr. Santosh Venkatesh (University of Pennsylvania), Dr. Stephen G. Wilson (Uni-
versity of Virginia), Dr. Gordon Stiiber (Georgia Institute of Technology), Dr. Venugopal
Veeraralli (Cornell University), and Dr. Granville E. Ott (University of Texas at Austin)

PREFACE xi

for critical reviews of an earlier version of the manuscript and for making numerous sug-
gestions that have helped me shape the book into its present form. The treatment of the
effect of noise on envelope detection presented in Chapter 2 is based on course notes made
available to me by Dr. Santosh Venkatesh, for which I am grateful. I am grateful to Dr.
Gordon Stiiber for giving permission to reproduce Figure 6.32.

I am indebted to Dr. Michael Moher (Communications Research Centre, Ottawa)
for reading five chapters of an earlier version of the manuscript and for making many
constructive comments on turbo codes. I am equally indebted to Dr. Brendan Frey (Uni-
versity of Waterloo, Ontario) for his invaluable help in refining the material on turbo
codes, comments on low-density parity-check codes, for providing the software to plot
Fig. 9.18, and giving me the permission to reproduce Figures 10.27 and 10.33. I am grate-
ful to Dr. David Conn (McMaster University, Ontario) for his critical reading of the Back-
ground and Preview Chapter and for making suggestions on how to improve the presen-
tation of the material therein.

I also wish to thank Dr. Jean-Jacque Werner (Lucent Technologies, Holmdel), Dr.
James Mazo (Lucent Technologies, Murray Hill), Dr. Andrew Viterbi (Qualcom, San Di-
ego), Dr. Radford Neal (University of Toronto, Ontario), Dr. Yitzhak (Irwin) Kalet (Tech-
nion, Israel), Dr. Walter Chen (Motorola), Dr. John Cioffi (Stanford University), Dr. Jon
Mark (University of Waterloo, Ontario), and Dr. Robert Dony (University of Guelph,
Ontario); I thank them all for their helpful comments on selected sections in the book.
Corrections and suggestions for improvements to the book made by Dr. Donald Wunsch
II (University of Missouri) are also appreciated.

Iam grateful to my graduate student Mathini Sellathurai (McMaster University) for
performing the computer experiments in the book, and Hugh Pasika (McMaster Univer-
sity) for many useful comments on the Background and Preview Chapter and for doing
the computations on some graphical plots in the book. Proofreading of the page proofs
by Mathini Sellathurai and Alpesh Patel is much appreciated.

Iam particularly grateful to my editor at Wiley, Bill Zobrist, for his strong support
and help throughout the writing of the book. I am indebted to Monique Calello, Senior
Production Editor at Wiley, for her tireless effort in overseeing the production of the book
in its various stages. I thank Katherine Hepburn for advertising and marketing the book.
I thank Karen Tongish for her careful copyediting of the manuscript, Katrina Avery for
her careful proofreading of the page proofs, and-Kristen Maus for composing the index
of the book.

Last but by no means least, as always, I am grateful to my Technical Coordinator,
Lola Brooks, for her tireless effort in typing the manuscript of the book. I also wish to
record my gratitude to Brigitte Maier, Assistant Librarian, and Regina Bendig, Reference
Librarian, at McMaster University, for helping.me on numerous occasions in tracing ref-
erences for the bibliography,

Simon Haykin
Ancaster, Ontario
January, 2000

 

i BACKGROUND AND PREVIEW ; 1

wp EnNnanpawnrp

nn
=

{| CHAPTER I

The Communication Process 1

Primary Communication Resources 3
Sources of Information 3

Communication Networks 10
Communication Channels 15

Modulation Process 19

Analog and Digital Types of Communication 21
Shannon‚Äôs Information Capacity Theorem 23
A Digital Communication Problem 24
Historical Notes 26

Notes and References 29

Random Processes 31

 

 

1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
1.10
1.11

1.13
1.14

Introduction 31

Mathematical Definition of a Random Process 32

Stationary Processes 33

Mean, Correlation, and Covariance Functions 35

Ergodic Processes 41

Transmission of a Random Process Through a Linear Time-Invariant Filter 42
Power Spectral Density 44

Gaussian Process 54

Noise 58

Narrowband Noise 64

Representation of Narrowband Noise in Terms of In-phase and Quadrature
Components 64 ;
Representation of Narrowband Noise in Terms of Envelope and Phase
Components 67

Sine Wave Plus Narrowband Noise 69
Computer Experiments: Flat-Fading Channel 71

xiv CONTENTS

1,15

CHAPTER 2

Summary and Discussion 75
Notes and References 77
Problems 78

Continuous-Wave Modulation

 

2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11
2.12
2.13
2.14
2.15

Introduction 88
Amplitude Modulation 90

_ Linear Modulation Schemes 93

Frequency Translation 103

Frequency-Division Multiplexing 105

Angle Modulation 107

Frequency Modulation 109

Nonlinear Effects in FM Systems 126
Superheterodyne Receiver 128

Noise in CW Modulation Systems 130

Noise in Linear Receivers using Coherent Detection 132
Noise in AM Receivers using Envelope Detection 135
Noise in FM Receivers 142

Computer Experiments: Phase-locked Loop 157
Summary and Discussion 162

Notes and References 165

Problems 166

| CHAPTER 3 Pulse Modulation

3.1
3,2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15

Introduction 183

Sampling Process 184

Pulse-Amplitude Modulation 188

Other Forms of Pulse Modulation 191
Bandwidth‚ÄîNoise Trade-off 193

Quantization Process 193

Pulse-Code Modulation 201

Noise Considerations in PCM Systems 209
Time-Division Multiplexing 211

Digital Multiplexers 214

Virtues, Limitations, and Modifications of PCM | 217
Delta Modulation 218

Linear Prediction 223

Differential Pulse-Code Modulation 227
Adaptive Differential Pulse-Code Modulation 229

88

183

CONTENTS xv

 

 

 

 

3.16 Computer Experiment: Adaptive Delta Modulation 232
3.17 MPEG Audio Coding Standard 234
3.18 Summary and Discussion 236
Notes and References 238
Problems 239
| CHAPTER 4 Baseband Pulse Transmission 247
4.1 Introduction 247
4.2 Matched Filter 248
4.3. Error Rate Due to Noise 253
4.4 Intersymbol Interference 259
4.5 Nyquist‚Äôs Criterion for Distortionless Baseband Binary Transmission 261
4.6 Correlative-Level Coding 267
4.7 Baseband M-ary PAM Transmission 275 .
4.8 . Digital Subscriber Lines 277
4.9 Optimum Linear Receiver 282
4.10 Adaptive Equalization 287

Signal-Space Analysis 309

 

 

Conversion of the Continuous AWGN Channel into a Vector Channel 318

Coherent Detection of Signals in Noise: Maximum Likelihood Decoding 322

Passband Digital Transmission 344

 

 

4.11 Computer Experiments: Eye Patterns 293
4.12 Summary and Discussion 296
Notes and References 297
Problems 300
{ CHapTer 5
5.1. Introduction 309
5.2 Geometric Representation of Signals 311
5.3
5.4 Likelihood Functions 322
5.5
5.6 Correlation Receiver 326
5.7 Probability of Error 328
5.8 Summary and Discussion 337
Notes and References 337
Problems 338
7 CuaPrer 6
6.1 Introduction 344
6.2 Passband Transmission Model 348
6.3 Coherent Phase-Shift Keying 349

xvi CONTENTS

6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11

6.12.

6.13
6.14
6.15
6.16

i CHaPrer 7

Hybrid Amplitude/Phase Modulation Schemes 368

Coherent Frequency-Shift Keying 380

Detection of Signals with Unknown Phase 403

Noncoherent Orthogonal Modulation 407

Noncoherent Binary Frequency-Shift Keying 413

Differential Phase-Shift Keying 414

Comparison of Digital Modulation Schemes Using a Single Carrier 417
Voiceband Modems 420

Multichannel Modulation 431

Discrete Multitone 440

Synchronization 448

Computer Experiments: Carrier Recovery and Symbol Timing 458
Summary and Discussion 464

Notes and References 465

Problems 468

Spread-Spectrum Modulation 479

 

 

7.1
7.2
7.3
74
7.5
7.6
7.7
7.8
7.9

Introduction 479

Pseudo-Noise Sequences 480

A Notion of Spread Spectrum 488

Direct-Sequence Spread Spectrum with Coherent Binary Phase-Shift Keying 490
Signal-Space Dimensionality and Processing Gain 493
Probability of Error 497

Frequency-Hop Spread Spectrum 499

Computer Experiments: Maximal-Length and Gold Codes 505
Summary and Discussion 508

Notes and References 509

Problems 509

Multiuser Radio Communications 512

 

 

 

 

i CHAPTER 8

8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
8.9

Introduction 512

Multiple-Access Techniques 513

Satellite Communications 514

Radio Link Analysis 517

Wireless Communications 529. _

Statistical Characterization of Multipath Channels 535
Binary Signaling over a Rayleigh Fading Channel 542
TDMA and CDMA Wireless Communication Systems 547
Source Coding of Speech for Wireless Communications 550

CONTENTS wii

8.10 Adaptive Antenna Arrays for Wireless Communications $53
8.11 Summary and Discussion 559

Notes and References 560

Problems 562

¬ß CHAPTER 9 Fundamental Limits in Information Theory 567

 

 

 

 

9.1 Introduction 567
9.2 Uncertainty, Information, and Entropy 568
9.3 Source-Coding Theorem 574
9.4 Data Compaction 575
9.5 Discrete Memoryless Channels 581
9.6  MautualInformation 584
9.7 Channel Capacity 587
9.8 Channel-Coding Theorem 589 -- :
9.9 Differential Entropy and Mutual Information for Continuous Ensembles 593
9.10 Information Capacity Theorem 597
9,11 Implications of the Information Capacity Theorem 601
9,12 Information Capacity of Colored Noise Channel 607
9,13 Rate Distortion Theory 611 .
9.14 Data Compression 614
9.15 Summary and Discussion 616
Notes and References 617
Problems 618

| Cuaprer 10 Error-Control Coding 626

10.1. Introduction 626
10.2 Discrete-Memoryless Channels 629
10.3. Linear Block Codes 632
10.4 Cyclic Codes 641
10.5 Convolutional Codes 654
10.6 Maximum Likelihood Decoding of Convolutional Codes, 660
10.7 Trellis-Coded Modulation 668 ‚Äò
10.8 Turbo Codes 674
10.9 Computer Experiment: Turbo Decoding 682
10.10 Low-Density Parity-Check Codes 683
10.11 Irregular Codes 691
10,12 Summary and Discussion 693
Notes and References 694
Problems 696

 

xviii CONTENTS

APPENDIX 1 Probability Theory 703

APPENDIX 2 Representation of Signals and Systems 715

APPENDIX 3 Bessel Functions 735

APPENDIX 4 Confluent Hypergeometric Functions 740

APPENDIX 5 Cryptography 742

APPENDIX 6 Tables 761

GLOSSARY 77%
BIBLIOGRAPHY 777

INDEX 792

 

BACKGROUND
AND PREVIEW

The background and preview material presented herein sets the stage for a statistical
treatment of communication systems in subsequent chapters. In particular, we describe the
following:

¬ª The communication process.

> Primary communication resources, namely, transmitted power and channel bandwidth.
> Sources of information.

¬ª The two primary types of switching: circuit switching and packet switching.

¬ª Communication channels for the transportation of information-bearing signals from the
transmitter to the receiver.

> The modulation process, which is basic to communication systems.
¬ª Analog and digital types of communication systems.
> Shannon's information capacity theorem.

> A digital communications problem.

The chapter concludes with some historical notes, as a source of motivation for the
reader.

i The Conumunication Process

Today, communication enters our daily lives in so many different ways that it is very easy
to overlook the multitude of its facets, The telephones at our hands, the radios and tele-
visions in our living rooms, the computer terminals with access to the Internet in our offices
and homes, and our newspapers are all capable of providing rapid communications from
every corner of the globe. Communication provides the senses for ships on the high seas,
aircraft in flight, and rockets and satellites in space. Communication through a wireless
telephone keeps a car driver in touch with the office or home miles away. Communication
keeps a weather forecaster informed of conditions measured by a multitude of sensors.
Indeed, the list of applications involving the use of communication in one way or another
is almost endless.

2 & BACKGROUND AND PREVIEW

In the most fundamental sense, communication involves implicitly the transmission
of information from one point to another through a succession of processes, as described
here:

1. The generation of a message signal: voice, music, picture, or computer data. -

2. The description of that message signal with a certain measure of precision, by a set
of symbols: electrical, aural, or visual.

3. The encoding of these symbols in a form that is suitable for transmission over a
physical medium of interest.

4. The transmission of the encoded symbols to the desired destination.

5. The decoding and reproduction of the original symbols.

6. The re-creation of the original message signal, with a definable degradation in qual-
ity; the degradation is caused by imperfections in the system.

There are, of course, many other forms of communication that do not directly involve
the human mind in real time. For example, in computer communications involving com-
munication between two or more computers, human decisions may enter only in setting
up the programs or commands for the computer, or in monitoring the results.

Irrespective of the form of communication process being considered, there are three
basic elements to every communication system, namely, iransmitter, channel, and receiver,
as depicted in Figure 1. The transmitter is located at one point in space, the receiver is
located at some other point separate from the transmitter, and the channel is the physical
medium that connects them. The purpose of the transmitter is to convert the message signal
produced by the source of information into a form suitable for transmission over the
channel. However, as the transmitted signal propagates along the channel, it is distorted
due to channel imperfections. Moreover, noise and interfering signals (originating from
other sources) are added to the channel output, with the result that the received signal is
a corrupted version of the transmitted signal. The receiver has the task of operating on
the received signal so as to reconstruct a recognizable form of the original message signal
for a user.

There are two basic modes of communication:

1. Broadcasting, which involves the use of a single powerful transmitter and numerous
receivers that are relatively inexpensive to build. Here information-bearing signals
flow only in one direction. ‚Äî

2. Point-to-point communication, in which the communication process takes place over
a link between a single transmitter and a receiver. In this case, there is usually a
bidirectional flow of information-bearing signals, which requires the use of a trans-
mitter and receiver at each end of the link.

Communication System

t ~ 7~ |

 

 

 

 

 

 

 

 

 

 

 

 

| |
Source of Transmitter Receiver i User of
information Message | Estimate of | information
signal | message |
signal |
]
| i
| Channel -
l Transmitted Received |
signal signal |

 

Figure 1 Elements of a communication system.

Sources of Information 3

The broadcasting mode of communication is exemplified by radio and television, and the
ubiquitous telephone provides the means for one form of point-to-point communication.
Another example of point-to-point communication is the link between an Earth station
and a robot navigating the surface of a distant planet.

All these different communication systems as well as others not mentioned here share
a common feature: The underlying communication process in each and every one of them
is statistical in nature. Indeed, it is for this important reason that much of this book is
devoted to the statistical underpinnings of communication systems. In so doing, we develop
an exposition of the fundamental issues involved in the study of different communication
methodologies and thereby provide a natural forum for their comparative evaluations.

i Primary Communication Resources

 

 

 

In a communication system, two primary resources are employed: transmitted power and
channel bandwidth, The transmitted power is the average power of the transmitted signal.
The channel bandwidth is defined as the band of frequencies allocated for the transmission
of the message signal. A general system design objective is to use these two resources as
efficiently as possible. In most communication channels, one resource may be considered
more important than the other. We may therefore classify communication channels as
power limited or band limited. For example, the telephone circuit is a typical band-limited
channel, whereas a space communication link or satellite channel is typically power
limited.

When the spectrum of a message signal extends down to zero or low frequencies, we
define the bandwidth of the signal as that upper frequency above which the spectral content
of the signal is negligible and therefore unnecessary for transmitting information. For
example, the average voice spectrum extends well beyond 10 kHz, though most of the
average power is concentrated in the range of 100 to 600 Hz, and a band from 300 to
3100 Hz gives good articulation. Accordingly, we find that telephone circuits that respond
well to this latter range of frequencies give quite satisfactory commercial telephone service.

Another-important point that we have to keep in mind is the unavoidable presence
of noise in a communication system. Noise refers to unwanted waves that tend to disturb
the transmission and processing of message signals in a communication system. The
sources of noise may be internal or external to the system.

A quantitative way to account for the effect of noise is to introduce signal-to-noise
ratio (SNR) as a system parameter. For example, we may define the SNR at the receiver
input as the ratio of the average signal power to the average noise power, both being
measured at the same point. The customary practice is to express the SNR in decibels
(dBs), defined as 10 times the logarithm (to base 10) of the power ratio, For example,
signal-to-noise ratios of 10, 100, and 1,000 correspond to 10, 20, and 30 dBs, respectively.

i Sources of Information

The telecommunications environment is dominated by four important sources of infor-
mation: speech, music, pictures, and computer data. A source of information may be
characterized in terms of the signal that carries the information. A signal is defined as a
single-valued function of time that plays the role of the independent variable; at every
instant of time, the function has a unique value. The signal can be one-dimensional, as in
the case of speech, music, or computer data; two-dimensional, as in the case of pictures;

4 & BACKGROUND AND PREVIEW

three-dimensional, as in the case of video data; and four-dimensional, as in the case of
volume data over time. In the sequel, we elaborate on different sources of information.

(i)

Speech is the primary method of human communication. Specifically, the speech
communication process involves the transfer of information from a speaker to a
listener, which takes place in three successive stages:

¬ª Production. An intended message in the speaker‚Äôs mind is represented by a speech
signal that consists of sounds (i.e., pressure waves) generated inside the vocal tract
and whose arrangement is governed by the rules of language.

> Propagation. The sound waves propagate through the air at a speed of 300 m/s,
reaching the listener‚Äôs ears,

¬ª Perception, The incoming sounds are deciphered by the listener into a received
message, thereby completing the chain of events that culminate in the transfer of
information from the speaker to the listener.

The speech-production process may be viewed as a form of filtering, in which a sound
source excites a vocal tract filter. The vocal tract consists of a tube of nonuniform
cross-sectional area, beginning at the glottis (i.e., the opening between the vocal
cords) and ending at the lip. As the sound propagates along the vocal tract, the
spectrum (i.e., frequency content) is shaped by the frequency selectivity of the vocal
tract; this effect is somewhat similar to the resonance phenomenon observed in organ
pipes. The important point to note here is that the power spectrum (i.e., the distri-
bution of long-term average power versus frequency) of speech approaches zero for
zero frequency and reaches a peak in the neighborhood of a few hundred hertz. To
put matters into proper perspective, however, we have to keep in mind that the
hearing mechanism is very sensitive to frequency. Moreover, the type of communi-
cation system being considered has an important bearing on the band of frequencies
considered to be ‚Äúessential‚Äù for the communication process. For example, as men-
tioned previously, a bandwidth of 300 to 3100 Hz is considered adequate for com-
mercial telephonic communication.

The second source of information, music, originates from instruments such as the
piano, violin, and flute. The note made by a musical instrument may last for a short
time interval as in the‚Äôpressing of a key on a piano, or it may be sustained for a long
time interval as in the example of a flute player holding a prolonged note. Typically,
music has two structures: a melodic structure consisting of a time sequence of sounds,
and a harmonic structure consisting of a set of simultaneous sounds. Like a speech
signal, a musical signal is bipolar. However, a musical signal differs from a speech
signal in that its spectrum occupies a much wider band of frequencies that may extend
up to about 15 kHz. Accordingly, musical signals demand a much wider channel
bandwidth than speech signals for their transmission.

The third source of information, pictures, relies on the human visual system for its
perception. The picture can be dynamic, as in television, or static, as in facsimile.
Taking the case of television first, the pictures in motion are converted into electrical
signals to facilitate their transport from the transmitter to the receiver. To do so,
each complete picture is sequentially scanned. The scanning process is carried out in
a TV camera. In a black-and-white TV, the camera contains optics designed to focus
an image on a photocathode consisting of a large number of photosensitive elements.
The charge pattern so generated on the photosensitive surface is scanned by an elec-
tron beam, thereby producing an output current that varies temporally with the way
in which the brightness of the original picture varies spatially from one point to
another. The resulting output current is called a video signal. The type of scanning

Sources of Information 5

used in television is a form of spatial sampling called raster scanning, which converts
a two-dimensional image intensity into a one-dimensional waveform; it is somewhat
analogous to the manner in which we read a printed paper in that the scanning is
performed from left to right on a line-by-line basis. In North American analog tele-
vision, a picture is divided into 525 lines, which constitute a frame. Each frame is
decomposed into. two interlaced fields, each of which consists of 262.5 lines. For
convenience of presentation, we will refer to the two fields as I and II. The scanning
procedure is illustrated in Figure 2. The lines of field I are depicted as solid lines, and
those of field II are depicted as dashed lines. The start and end of each field are also
included in the figure. Field I is scanned first. The scanning spot of the TV camera
moves with constant velocity across each line of the field from left to right, and the
image intensity at the center of the spot is measured; the scanning spot itself is partly
responsible for local spatial averaging of the image. When the end of a particular
line is reached, the scanning spot quickly flies back (in a horizontal direction) to the
start of the next line down in the field. This flyback is called the horizontal retrace.
The scanning process described here is continued until the whole field has been ac-
counted for. When this condition is reached, the scanning spot moves quickly (in a
vertical direction) from the end of field I to the start of field II. This second flyback
is called the vertical retrace. Field II is treated in the same fashion as field I. The time
taken for each field to be scanned is 1/60 s. Correspondingly, the time taken for a
frame or a complete picture to be scanned is 1/30 s. With 525 lines in a frame, the
line-scanning frequency equals 15.75 kHz. Thus, by flashing 30 still pictures per
second on the display tube of the TV receiver, the human eye perceives them to be
moving pictures. This effect is due to a phenomenon known as the persistence of
vision. During the horizontal- and vertical-retrace intervals, the picture tube is made
inoperative by means of blanking pulses that are generated at the transmitter. More-
over, synchronization between the various scanning operations at both transmitter
and receiver is accomplished by means of special pulses that are transmitted during
the blanking periods; thus, the synchronizing pulses do not show on the reproduced
picture. The reproduction quality of a TV picture is limited by. two basic factors:

1. The number of lines available in a raster scan, which limits resolution of the

picture in the vertical direction.
2. The channel bandwidth available for transmitting the video signal, which limits
resolution of the picture in the horizontal direction.

Start of field |

Start of field II

 
  

A

nine

 

  

 

 

 

End of field | End of field II

FiGuRE 2 Interlaced raster scan.

6

BACKGROUND AND PREVIEW

For each direction, resolution is expressed in terms of the maximum number of lines
alternating between black and white that can be resolved in the TV image along the
pertinent direction by a human observer. In the NTSC (National Television System
Committee) system, which is the North American standard, the parameter values
used result in a video bandwidth of 4.2 MHz, which extends down to zero frequency.
This bandwidth is orders of magnitude larger than that of a speech signal. Note also
that whereas a speech signal is bipolar, a video (television) signal is inherently positive
(i.e., unipolar).

In color TV, the perception of color is based on the three types of color recep-
tors (cones) in the human eye: red, green, and blue, whose wavelengths are 570 nm,
535 nm, and 445 nm, respectively. These three colors are referred to as primary
colors because any other color found in nature can be approximated by an additive
mixture of them. This physical reality is indeed the basis for the transmission of color
in commercial TV broadcasting. The three primary colors are represented by the
video signals mtp(t), #t¬¢(t), and mg(t), respectively. To conserve bandwidth and pro-
duce a picture that can be viewed on a conventional black-and-white (monochrome)
television receiver, the transmission of these three primary colors is accomplished by
observing that they can be uniquely represented by any three signals that are inde-
pendent linear combinations of w(t), m¬¢(t), and mp(t). The three signals are as
follows:

¬Æ A luminance signal, m;,(t), which produces a black-and-white version of the color
picture when it is received on a conventional monochrome television receiver.

> A pair of signals, #1;(t) and mo(t), called the chrominance signals, which indicate
the way the color of the picture departs from shades of gray.

The luminance signal *;(f) is assigned the entire 4.2 MHz bandwidth. Owing to
certain properties of human vision, tests show that if the nominal bandwidths of the
chrominance signals #;(¬£) and mg(t) are 1.6 MHz and 0.6 MHz, respectively, sat-
isfactory color reproduction is possible.

Turning next to a facsimile (fax) machine, the purpose of this machine is to
transmit still pictures over a communication channel (most notably, a telephone
channel). Such a machine provides a highly popular facility for the transmission of .
handwritten or printed text from one point to another; transmitting text by facsimile
is treated simply like transmitting a picture. The basic principle employed for signal
generation in a facsimile machine is to scan an original document (picture) and use
an image sensor to convert the light to an electrical signal.

Finally, personal computers (PCs) have become an integral part of our daily lives.
We use them for electronic mail, exchange of software, and sharing of resources. The
text transmitted by a PC is usually encoded using the American Standard Code for
Information Interchange (ASCII), which is the first code developed specifically for
computer communications. Each character in ASCII is represented by seven data bits
constituting a unique binary pattern made up of Os and 1s; bit is acronym for binary
digit. Thus a total of 2‚Äù = 128 different characters can be represented in ASCII. The
characters are various lowercase and uppercase letters, numbers, special control sym-
bols, and punctuation symbols commonly used such as @, $, and %. Some of the
special ‚Äúcontrol‚Äù symbols, such as BS (backspace) and CR (carriage return), are used
to control the printing of characters on a page. Other symbols, such as ENQ (enquiry)
and ETB (end of transmission block), are used for communication purposes. (A com-
plete listing of ASCII characters is given in Table A6.1.) The seven data bits are
ordered starting with the most significant bit b; down to the least significant bit b,,

Sources of Information 7

 

 

 

Idle

High ‚Äî‚Äî
i
(e} bg 1
Low | |
Start Data bits | Parity Stop |
; bit ; bit | bit

FIGURE 3 The bit format for sending asynchronous serial data used in the RS-232
standard.

as illustrated in Figure 3. At the end of the data bits, an extra bit bs is appended for
the purpose of error detection. This error-detection bit is called a parity bit. A se-
quence of eight bits is referred to as a-byte, or an octet. The parity bit is set in such
a way that the total number of 1s in each byte is odd for odd parity and even for
even parity. Suppose, for example, the communicators agree to use even parity; then
the parity bit will be a 0 when the number of 1s in the data bit is even and a 1 when
it is odd. Hence, if a single bit in a byte is received in error and thereby violates the
even parity rule, it can be detected and then corrected through retransmission. Per-
sonal computers are often connected via their RS (recommended standard)-232 ports.
When ASCII data (in fact, all character data) are transmitted through these ports, a
start bit, set to 0, and one or more stop bits, set to 1, as shown in Figure 3, are added
to provide character framing. When the transmission is idle, a long series of 1s is
sent so as to keep the circuit connection alive. In Figure 3, symbols 0 and 1 are
designated as ‚Äúlow‚Äù and ‚Äúhigh,‚Äù respectively. They are also sometimes referred to

‚Äúspace‚Äù and ‚Äúmark,‚Äù‚Äô respectively; the latter terminology comes from the days of
telegraphy. The text prepared on a PC is usually stored and then transmitted over a
communication channel (e.g., a telephone channel} with a single character being sent
at a time. This form of data transmission is called asynchronous transmission, as
opposed to synchronous transmission in which a whole sequence of encoded char-
acters is sent over the channel in one long transmission. Encoded characters produced
by a mixture of asynchronous and synchronous terminals are combined by means
of data multiplexers. The multiplexed stream of data so formed is then applied to a
device called a modem (modulator-demodulator) for the purpose of transmission
over the channel.

In summary, computer-generated data and television signals are both wide-
band signals, in that their power content occupies a wide range of frequencies. An-
other important characteristic of data communication between personal computers
is burstiness, which means that information is usually transmitted from one terminal
to another in bursts with silent periods between bursts. Indeed, data traffic involving
computers in one form or another tends to be of a bursty nature. This is to be
contrasted with traffic in a digital transmission network due to voice or interactive
video, which, relatively speaking, is continuous.

Another way in which we use the computer is to download compressed forms
of text, audio, and video data from a service provider at a remote location. Data
compression provides a practical means for the efficient storage and transmission of
these kinds of data. A data compression system consists of an encoder and a decoder,
where the compression of an incoming data stream and its reconstruction are per-
formed, respectively. Basically, there are two forms of data compression:

1. Lossless compression operates by removing the redundant information contained
in the data of interest. The compression is said to be lossless because it is com-

8

& BACKGROUND AND PREVIEW

pletely reversible in that the original data can be reconstructed exactly. Lossless
compression is also referred to as data compaction.

2. Lossy compression involves the loss of information in a controlled manner; the
compression may therefore not be completely reversible. Lossy compression is,
however, capable of achieving a compression ratio higher than that attainable
with lossless methods.

For digital text, lossless compression is required. In this context, we mention the
Lempel-Ziv algorithm, which is intrinsically adaptive and capable of encoding
groups of source symbols that occur frequently. It achieves a compression of ap-
proximately 55 percent on ordinary English text, which, loosely speaking, corre-
sponds to the compression that would be achieved by encoding pairs of letters. The
Lempel-Ziv algorithm is a form of entropic coding, or source coding, which is dis-
cussed in Chapter 9.

In many other applications, lossy compression is usually the preferred approach
as its use can substantially reduce the data size without significantly altering the
perceptual quality of an image or audio signal. For such applications, this form of
data compression is acceptable, and in high-throughput data-transmission applica-
tions such as the Internet, it is a necessity. But in some other applications such as a
clinical setting, the quality of a medical image (e.g., digital x-ray radiograph) must
not be degraded on reconstruction,

For digital audio and video applications involving storage or transmission to
be viable in today‚Äôs marketplace, we need standard compression algorithms that
enable the interoperability of equipment produced by different manufacturers. In this
context, we mention three prominent standard compression algorithms that cater to
different needs:

¬ª The JPEG image coding standard? is designed to compress full-color or grayscale
images of natural, real-world scenes by exploiting known limitations of the human
visual system; JPEG stands for Joint Photographic Experts Group. At the input to
the encoder, picture elements, or pixels, are grouped into 8 X 8 blocks, which are
applied to a relative of the Fourier transform known as the discrete cosine trans-
form (DCT)*. The DCT decomposes each block of pixels into a set of 64 coeffi-
cients that closely satisfy two related objectives:

1. The coefficients should be as uncorrelated as possible.

2. The energy of the input signal should be packed into the smallest number of
coefficients possible.
The next operation in the encoder is that of quantization, where each of the 64
DCT coefficients is rounded off. In JPEG, quantization is performed in conjunction
with a quantization table supplied by the user as an input to the encoder. Each
element of the table is an integer from 1 to 255 that specifies the step size of the
DCT coefficients, which, in turn, permits the representation of each quantized
DCT coefficient by an 8-bit code word. Basically, the purpose of quantization is
to discard information that is not perceptually discernible. Quantization is a many-
to-one mapping and therefore the principal source of lossiness in the encoder. The
final operation in the encoder is that of Huffman coding, which is a form of
entropic (source) coding also discussed in Chapter 9, Huffman coding achieves
additional data compression in a lossless manner by encoding the quantized DCT
coefficients in accordance with their statistical characteristics. At the decoder, data
reconstruction is performed through a sequence of operations that are the inverse

Vv

Sources of Information 9

of those-in the encoder, namely, Huffman decoding, dequantization in accordance

with the quantization table, and finally the inverse DCT.

The MPEG-1/video coding standard* is designed primarily to compress video sig-

nals at 30 frames per second (fps) into bit streams running at the rate of 1.5

megabits per second (Mb/s); MPEG stands for Motion Photographic Experts

Group. The MPEG-1 video coding standard achieves this design goal by exploiting

four basic forms of redundancy inherently present in video data:

1. Interframe (temporal) redundancy.

2. Interpixel redundancy within a frame.

3. Psychovisual redundancy.

4. Entropic coding redundancy.

It is the exploitation of interframe redundancy that distinguishes MPEG-1 from

JPEG. In principle, neighboring frames in typical video sequences are highly cor-

related. The meaning of this high correlation is that, in an average sense, a video

signal does not change rapidly from one frame to the next, and as a result, the
difference between adjacent frames has a variance (i.¬¢., average power) that is
much smaller than the variance of the video signal itself. Accordingly, the inter-
frame redundancy can be significantly reduced to produce a more efficiently com-
pressed video signal. This reduction is achieved through the use of prediction to
estimate each frame from its neighbors; the resulting prediction error is transmitted
for motion estimation and compensation. The prediction is nonlinear by virtue of
the nature of the problem. As with-JPEG, the interpixel redundancy is reduced
through the combined use of the DCT, quantization, and lossless entropic coding.

The net result is that full-motion.video becomes a 1.5 Mb/s stream of computer

data that can be stored on compact discs or integrated with texts and graphics.

Most important, the full-motion video and associated audio can be delivered over

existing computer and telecommunication networks, which, in turn, makes it pos-

sible to fulfill the need for video-on-demand on the Internet.

The MPEG-1/audio coding standard‚Äô is based on perceptual coding, which is a

waveform-preserving process; that is, the amplitude-time waveform of the decoded

audio signal closely approximates that of the original audio signal. In basic terms,
the encoding process encompasses four distinct operations:

1. Time-frequency mapping, whereby the input audio signal is decomposed into
multiple subbands.

2. Psychoacoustic modeling, which simultaneously operates on the input audio
signal to compute certain thresholds using known rules from the psychoacous-
tic behavior of the human auditory system.

3. Quantization and coding, which, in conjunction with the psychoacoustic
model, works on the output of the time-frequency mapper so as to maintain
the noise resulting from quantization process at an inaudible level.

4. Frame-packing, which is used to format the quantized audio samples into a
decodable bit stream.

The psychoacoustic model builds on a perceptual phenomenon known as auditory

masking. Specifically, the human ear does not perceive quantization noise in a

given frequency band if the average noise power lies below the masking threshold

(ie., the threshold of just noticeable distortion). For a given frequency band of

interest, the masking threshold varies with frequency across that band. The min-

10 & BACKGROUND AND PREVIEW

imum masking threshold is the one that is employed in the psychoacoustic model
on a band-by-band basis. For example, the net result of using the MPEG-1 stan-
dard on the two audio channels of a stereo program is that each digitized audio
signal, coming in at the rate of 768 kilobits per second (kb/s), is compressed to a
rate as low as 16 kb/s. (The incoming data rate of 768 kb/s corresponds to a
sampling rate of 48 kHz, with each sample being represented by a 16-bit code
word.) Thus the MPEG-1/audio coding standard is suitable for the storage of
audio signals in inexpensive media or their transmission over channels with limited
bandwidth, while at the same time maintaining perceptual quality.

i Communication Networks¬Æ

A communication network (or simply network), illustrated in Figure 4, consists of an
interconnection of a number of routers made up of intelligent processors (e.g., microproc-
essors). The primary purpose of these processors is to route data through the network,
hence the name. Each router has one or more hosts attached to it; hosts are devices that
communicate with one another. The network is designed to serve as a shared resource for
moving data exchanged between hosts in an efficient manner and to provide a framework
to support new applications and services.

The telephone network is an example of a communication network in which circuit
switching is used to provide a dedicated communication path, or circuit, between two
hosts. The circuit consists of a connected sequence of links from source to destination. For
example, the links may consist of time slots for which a common channel is available for
access by a multitude of users. The circuit, once in place, remains uninterrupted for the
duration of transmission. Circuit switching is usually controlled by a centralized hierar-
chical control mechanism with knowledge of the network‚Äôs organization, To establish a
circuit-switched connection, an available path through the network is seized and then
dedicated to the exclusive use of the two hosts wishing to communicate. In particular, a
call-request signal must propagate all the way to the destination and be acknowledged
before transmission can begin. Then, the network is effectively transparent to the users.
This means that during the connection time, the bandwidth and resources allocated to the
circuit are essentially ‚Äúowned‚Äù by the two hosts until the circuit is disconnected. The circuit

 

 

   

Boundary
of subnet

 

Hosts

FIGURE 4 Communication network.

~ Communication Networks il

thus represents an efficient use of resources only to the extent that the allocated bandwidth
is properly used. Although the telephone network is used to transmit data, voice constitutes
the bulk of the network‚Äôs traffic. Indeed, circuit switching is well suited to the transmission
of voice signals, since voice gives rise to a stream traffic and voice conversations tend to
be of long duration (about 2 minutes on the average) compared to the time required for
setting up the circuit (about 0.1 to 0.5 seconds). Moreover, in most voice conversations,
there is information flow for a relatively large percentage of the connection time, which
makes circuit switching all the more suitable for voice conversations.

In circuit switching, a communication link is shared between the different sessions
using that link on a fixed allocation basis. In packet switching, on the other hand, the
sharing is done on a demand basis, so it has an advantage over circuit switching in that
when a link has traffic to send, the link may be more fully utilized.

The network principle of packet switching is ‚Äústore and forward.‚Äù Specifically, in a
packet-switched network, any message larger.than a specified size is subdivided prior to
transmission into segments not exceeding the specified size. The segments are commonly
referred to as packets. The original message is reassembled at the destination on a packet-
by-packet basis. The network may be viewed as a distributed pool of network resources
(ie., channel bandwidth, buffers, and switching processors) whose capacity is shared dy-
namically by a community of competing hosts wishing to communicate. In contrast, in a
citcuit-switched network, resources are dedicated to a pair of hosts for the entire period
they are in session. Accordingly, packet switching is far better suited to a computer-
communication environment in which bursts of data are exchanged between hosts on an
occasional basis. The use of packet switching, however, requires that careful control be
exercised on user demands; otherwise, the network may be seriously abused.

The design of a data network (i.e., a network in which the hosts are all made up of
computers and terminals) may proceed in an orderly way by looking at the network in
terms of a layered architecture, regarded as a hierarchy of nested layers. Layer refers to a
process or device inside a computer system, designed to perform a specific function. Nat-
urally, the designers of a layer will be intimately familiar with its internal details and
operation. At the system level, however, a user views the layer merely as a ‚Äúblack box‚Äù
that is described in terms of inputs, outputs, and the functional relation between outputs
and inputs. In a layered architecture, each layer regards the next lower layer as one or
more black boxes with some given functional specification to be used by the given higher
layer. Thus, the highly complex communication problem in data networks is resolved as
a manageable set of well-defined interlocking functions. It is this line of reasoning that has
led to the development of the open systems interconnection (OSI)‚Äô reference model by a
subcommittee of the International Organization for Standardization. The term open refers
to the ability of any two systems conforming to the reference model and its associated
standards to interconnect.

In the OSI reference model, the communications and related-connection functions
are organized as a series of layers, or levels, with well-defined interfaces, and with each
layer built on its predecessor. In particular, each layer performs a related subset of primitive
functions, and it relies on the next lower layer to perform additional primitive functions.
Moreover, each layer offers certain services to the next higher layer and shields the latter
from the implementation details of those services. Between each pair of layers, there is an
interface. It is the interface that defines the services offered by the lower layer to the upper
layer.

The OSI model is composed of seven layers, as illustrated in Figure 5; this figure also
includes a description of the functions of the individual layers of the model. Layer & on
system A, say, communicates with layer k on some other system B in accordance with a

"JosuOD YuY vIDP JOF spuLys eB oup Jo s[ppru aya uy QTC wWAUOIOE oy] {[apow [SQ ¬¢ TUNA

‚ÄúJ@UUBYD BY} SSadde 0} sjUaWaJInbas jeinpedoid pue
*yeUOIOUNY ‚Äú2d JJOaIa ‚Äòjeo|UeYIeW ay) UYIM S|zap JAE] Si}
tyouUeyo jeaysXyd @ JeA0 eJep Jo S}iq Med Jo UOIssWSued]

 

‚Äújauueys au ssou9e
UO}JEWOJU! Jo 4aJSUAd] 21geI[a1 BY} JO} }O4]U0D 101g

‚Äúasnpaooud suijno2 ayy Aq punoy Yul}
Uo!JeI/UNWLWOD B J9A0 BoUBWOLad poos aajuesend 0} pausisep
[O.jU09 MOj} PUP YOAJ@U BL} YBnoIyy sjayoed Jo Buynoy

‚ÄòsiasN Usemjeq pesueyoxe sadessauw
8U} JO fosjUOS (UO!eUNSap-0}-B04NOs ‚Äò‚Äò3'1) puds-0}-PUq

‚ÄúWay] UaaMmjeq angdojeip ayy yo
quaweseuew AjJapio ay} pue ‚Äòssasn Buijeladooo om] UdaMteq
UOJJEHUNWWOD JO} BAN JONAS |01]U09 By} JO UOISIADLY

‚ÄúAyaNDes aptaoid 0] uoldAuaua st
Uo}]eULOjsued] BJep Jo aldwere ue ‚Äòaxe; uolEaIjdde au Aq
pajogjas seaies apiaosd 0} B]ep yndu} 94} Jo UO|yeuoJsued |,

 

‚ÄúS19SN-PUA JO} JUBWUOJIAUS |$Q Bl} 0} S599" Jo UO|SIADLY

uoiaun

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

q Wwaysks @pou Jeuqns yw waysks
ul] Jeoskyd mur yeatsAud
t
[eoiskud jeaistyg | eoiskud JRIISAU
[
Jouju09 _ ma na oe jOuU09
uly eed jooozosd joso}oid sun eed
Z 40ke] \ f Z Jake] |
yIOMJAN) sats T Toe (ome [-‚Äî-‚Äî-‚Äî‚Äî-‚Äî----‚Äî- IOMIaN
Jooojoid joaoyoid
¬© Jake] ‚Ç¨ Jake] |
yodsuedL ‚Äî- ~ yodsuel|
josojoid p Jake]
ugIssas --- -- ‚Äî-- uOGIssaS

 

uolje]Uesald

os0}0id g sake]

 

 

 

uoneaddy

 

on0}01d 9 1842]

 

 

 

 

4 desn-puq

o20}0Id / 42AE]

|

 

 

Uole]UaSa1q

 

|

 

Lolealddy

 

 

 

X 4asn-pug

Jake]

12

Commeunication Networks 13

set of rules and conventions, collectively constituting the layer k protocol, where k = 1,
2,...7. (The term protocol has been borrowed from common usage, describing conven-
tional social behavior between human beings.) The entities that comprise the correspond-
ing layers on different systems are referred.to as peer processes. In other words, commu-
nication is achieved by having the peer processes in two different systems communicate
via a protocol, with the protocol itself being defined by a set of rules of procedure. Physical
communication between peer processes exists only at layer 1. On the other hand, layers 2
through 7 are in virtual communication with their distant peers. However, each of these
six layers can exchange data and control information with its neighboring layers (below
and above} through layer-to-layer interfaces. In Figure 5, physical communication is shown
by solid lines and virtual communication by dashed lines,
Our primary interest in this book is in the physical layer of the OSI model.

@ INTERNET

Any discussion of computer networks naturally leads to the Internet. In the Internet par-
adigm, the underlying network technology is decoupled from the applications at hand by
adopting an abstract definition of network service. In more specific terms, we may say the
following:

¬ª The applications are carried out independently of the technology employed to con-
struct the network.

> By the same token, the network technology is capable of evolving without affecting
the applications.

The Internet architecture, depicted in Figure 6, has three functional blocks: hosts,
subnets, and routers. The hosts constitute nodes of the network, where data originate or
where they are delivered. The routers constitute intermediate nodes that are used to cross
subnet boundaries. Within a subnet, all the hosts belonging to that subnet exchange data
directly; see, for example, subnets 1 and 3 in Figure 6.

Like other computer networks, the Internet has a layered set of protocols. In partic-
ular, the exchange of data between the hosts and routers is accomplished by means of the
Internet protocol (IP), as illustrated in Figure 7. The IP is a universal protocol that resides
in the network layer (i.e., layer 3 of the OSI reference model}. It is simple, defining an
addressing plan with a built-in capability to transport data in the form of packets from
node to node. In crossing a subnetwork boundary, the routers make the decisions as to
how the packets addressed for a specified destination should be routed. This is done on
the basis of routing tables that are developed through the use of custom protocols for

 

 

Hosts Hosts

Ficure 6 An interconnected network of subnets.

14

& BACKGROUND AND PREVIEW

 

 

 

AP AP AP AP
TCP/UDP TCP/UDP TCP/UDP TCP/UDP
IP IP IP iP

 

 

 

 

 

 

 

 

 

 

 

 

AP: Application protocol UDP: User datagram protocol
TCP: Transmission control protocol _‚ÄòIP: Internet protocol

Ficure 7 Illustrating the network architecture of the Internet.

exchanging pertinent information with other routers. The net result of using the layered
set of protocols is the delivery of best effort service. That is, the Internet offers to deliver
each packet of data, but there are no guarantees on the transit time experienced in delivery
or even whether the packets will be delivered to the intended recipient.

& BroaDBAND NETWORKS

With the ever-increasing demand for new services (e.g., video on demand, multimedia
communications) and the availability of key enabling technologies (e.g., optical fibers,
digital switches), the telephone network is evolving into an all-purpose broadband network
known as the broadband integrated services digital network (B-ISDN). The underlying
technology that makes B-ISDN possible is a user-network interface protocol called the
asynchronous transfer mode (ATM). ATM is a high-bandwidth, low-delay, packet-like
technique used for switching and multiplexing; it is independent of the physical means of
transport. The low-delay feature of the technique is needed to support real-time services
such as voice. The high-bandwidth feature is required to handle video on demand. Simply
put, ATM is both a technology that is hidden from the users and a connection-oriented
service that is visible to the users.

As the name implies, ATM is not synchronous (i.e., tied to a master clock). It allows
for the transport of digital information in the form of small, fixed-size packets called cells.
The key feature of ATM to note here is that the connection-oriented service preserves call
sequencing, which means that no reassembly of cells is needed prior to presenting the traffic
stream to the destination host. The deployment of a cell-switching technology in B-ISDN
is a gigantic break with the traditional use of circuit switching in the telephone network.

The primary purpose of ATM is to allocate network resources (i.e., bandwidth, buf-
fers, and processing horsepower) efficiently so as to guarantee the expected quality of
service (QoS) for each connection. QoS is measured in terms of three parameters:

¬Æ Cell loss ratio, defined as the ratio of the number of cells lost in transport across the
network to the total number of cells pumped into the network.

¬ª Cell delay, defined as the time taken for a cell of a particular connection to transit
across the network.

¬ª Cell delay variation, defined as the dispersion or jitter about the mean cell delay.

Quality of service offered in B-ISDN is to be contrasted with best effort service offered by
the Internet.

Communication Channels 15

Taste 1 Hierarchy
of SONET data rates

 

 

Level* Data Rate (Mb/s)
OC-1 51.84
OC-3 155.52
Oc-9 466.56
OC-12 622.08
OC-18 933.12
OC-24 1,244.16
OC-36 1,866.24
OC-+48 2,488.32

 

OC stands for optical carrier level.

After their generation, the ATM cells are structured for transport across the network.
The cells in B-ISDN are placed on an optical transmission system called the synchronous
optical network (SONET);¬Æ optical fibers are discussed in the next section. SONET uses
time-division multiplexing, whereby the entire bandwidth of an optical fiber is devoted to
different incoming data streams on a time-shared basis, hence the need for a synchronous
operation. SONET is controlled by a master clock with an accuracy of about 1 part in
10¬∞. Thus bits of data are sent on a SONET line at extremely precise intervals, controlled
by the master clock. Nevertheless, SONET permits the irregular time arrivals of ATM
cells. :

The basic SONET frame is a block of 810 bytes put out every 125 ys for an overall
data rate of $1.84 Mb/s. Having 8000 frames every second exactly matches the sampling
rate of 8 kHz, which is the standard sampling rate for the digital transmission of voice
signals across the telephone network. The basic data rates of 51.84 Mb/s are synchronously
byte-interleaved to generate a hierarchy of data rates, as summarized in Table 1.

| Communication Channels

 

The transmission of information across a communication network is accomplished in the
physical layer by means of a communication channel. Depending on the mode of trans-
mission used, we may distinguish two basic groups of communication channels: channels
based on guided propagation and those based on free propagation. The first group includes
telephone channels, coaxial cables, and optical fibers. The second group includes wireless
broadcast channels, mobile radio channels, and satellite channels. These six channels are
described in what follows.

(i) As mentioned earlier, a typical telephone network uses circuit switching to establish
an end-to-end communication link on a temporary basis, The primary purpose of
the network is to ensure that the telephone transmission between a speaker at one
end of the link and a listener at the other end is an acceptable substitute for
face-to-face conversation. In this form of communication, the message source is the
sound produced by the speaker‚Äôs voice, and the ultimate destination is the listener‚Äôs
ear. The telephone channel, however, supports only the transmission of electrical
signals. Accordingly, appropriate transducers are used at the transmitting and re-
ceiving ends of the system. Specifically, a microphone is placed near the speaker‚Äôs

16

Insertion loss (dB)

20

e
a

e
Oo

 

% BACKGROUND AND PREVIEW

1

mouth to convert sound waves into an electrical signal, and the electrical signal is
converted back into acoustic form by means of a moving-coil receiver placed near
the listener‚Äôs ear. Present-day designs of these two transducers have been perfected
so as to respond well to frequencies ranging from 20 to 8000 Hz; moreover, a pair
of them can be compactly packaged inside a single telephone set that is easy to speak
into or listen from. The telephone channel is a bandwidth-limited channel. The re-
striction on bandwidth arises from the requirement of sharing the channel among a
multitude of users at any one time. A practical solution to the telephonic commu-
nication problem must therefore minimize the channel bandwidth requirement, sub-
ject to a satisfactory transmission of human voice. To meet this requirement, the
transducers and channel specifications must conform to standards based on subjec-
tive tests that are performed on the intelligibility, or articulation, of telephone signals
by representative. male and female speakers. A speech signal (male or female) is es-
sentially limited to a band from 300 to 3100 Hz in the sense that frequencies outside
this band do not contribute much to articulation efficiency. This frequency band may
therefore be viewed as a rough guideline for the passband of a telephone channel
that provides a satisfactory service, as illustrated in Figure 8 for a typical toll con-
nection. Figure 8a shows the insertion loss of the channel plotted versus frequency;
insertion loss (in dB) is defined as 10 logio(Po/P;,), where P, is the power delivered
to a load from a source via the channel and Py, is the power delivered to the same
load when it is connected directly to the source. Figure 8b shows the corresponding
plot of the envelope (group) delay (in milliseconds) versus frequency; envelope delay
is defined as the negative of the derivative of the phase response with respect to the
angular frequency w = 27f. The plots of Figure 8 clearly illustrate the dispersive
nature of the telephone channel.

The telephone channel is built using twisted pairs for signal transmission. A
twisted pair consists of two solid copper conductors, each of which is encased in a
polyvinylchloride (PVC) sheath. Typically, each pair has a twist rate of 2 to 12 twists
per foot, and a characteristic impedance of 90 to 110 ohms, Twisted pairs are usually
made up into cables, with each cable consisting of many pairs in close proximity to

 

 

Envelope delay (ms)

 

 

 

ell
2 3 4 5
Frequency {kHz} Frequency (kHz)

{a) (h)

Ficune 8 Characteristics of typical telephone connection: (a) Insertion loss. (b) Envelope delay.
(Adapted from Bellamy, 1991.)

(iv)

Communication Channels 17

each other. Twisted pairs are naturally susceptible to electromagnetic interference
(EMI), the effects of which are mitigated through twisting the wires.
A coaxial cable consists of an inner conductor and an outer conductor, separated by
a dielectric insulating material. The inner conductor is made of a copper wire encased
inside the dielectric material. As for the outer conductor, it is made of copper, tinned
copper, or copper-coated steel. Typically, a coaxial cable has a characteristic imped-
ance of 50 or 75 ohms. Compared to a twisted-pair cable, a coaxial cable offers a
greater degree of immunity to EMI. Moreover, because of their much higher band-
width, coaxial cables can support the transmission of digital data at much higher bit
rates than twisted pairs. Rates up to 20 Mb/s are feasible using coaxial cables, with
10 Mb/s being the standard.

Whereas the use of a twisted pair has been confined mainly to point-to-point
service, a coaxial cable can operate as a multiple-access medium by using high-
impedance taps. A common application of coaxial cables is as the transmission me-

dium for local area networks in an office environment.

Another common application of coaxial cables is in cable-television systems,

also known as community-antenna television (CATV) systems. In this application
coaxial cables are used to distribute television, audio, and data signals from the head
end to the subscribers. The bead end is the central originating unit of the CATV
system, where all signals are carried and processed.
An optical fiber is a dielectric wave guide that transports light signals from one place
to another just as a twisted-wire pair or a coaxial cable transports electrical signals.
It consists of a central core within which the propagating electromagnetic field is
confined and which is surrounded by a cladding layer, which is itself surrounded by
a thin protective jacket.¬∞ The core and cladding are both made of pure silica glass,
whereas the jacket is made of plastic. Optical fibers have unique characteristics that
make them highly attractive as a transmission medium. In particular, they offer the
following unique characteristics:

¬ª Enormous potential bandwidth, resulting from the use of optical carrier frequen-
cies around 2 * 10'* Hz; with such a high carrier frequency and a bandwidth
roughly equal to 10 percent of the carrier frequency, the theoretical bandwidth of
a lightwave system is around 2 X 10‚Äò? Hz, which is very large indeed.

¬Æ Low transmission losses, as low as 0.1-dB/km.

¬ª Immunity to electromagnetic interference, which is an inherent characteristic of
an optical fiber viewed as a dielectric waveguide.

& Small size and weight, characterized by a diameter no greater than that of a human

hair.

Ruggedness and flexibility, exemplified by very high tensile strengths and the pos-

sibility of being bent or twisted without damage.

¬•"

Last, but by no means least, optical fibers offer the potential for low-cost line com-
munications since they are fabricated from sand, which, unlike the copper used in
metallic conductors, is not a scarce resource. The unique properties of optical fibers
have fuelled phenomenal advances in lightwave systems technology, which have, in
turn, revolutionized long-distance communications and continue to do so.

Wireless broadcast channels support the transmission of radio and television signals.
The information-bearing signal, representing speech, music, or pictures, is modulated
onto a carrier frequency that identifies the transmitting station; modulation is de-
scribed in the next section. The transmission originates from an antenna that acts as
the transition or matching unit between the source of the modulated signal and

18

& BACKGROUND AND PREVIEW

(vi)

electromagnetic waves in free space. The objective in designing the antenna is to
excite the waves in the required direction or directions, as efficiently as possible.
Typically, the transmitting antenna is mounted on a tower to provide an unob-
structed view of the surrounding area, as far afield as possible. By virtue of the
phenomenon of diffraction, which is a fundamental property of wave motion, radio
waves are bent around the earth‚Äôs surface. Propagation beyond the line of sight is
thereby made possible, albeit with somewhat greater loss than is incurred in free
space,

At the receiving end, an antenna is used to pick up-the radiated waves, estab-
lishing a communication link to the transmitter. Most radio receivers are of the
superheterodyne type. This technique consists of down-converting the received signal
to some convenient frequency band, called the intermediate frequency (IF) band, and
then recovering the original information-bearing signal by means of an appropriate
detector.

A mobile radio channel extends the capability of the public telecommunications net-
work by introducing mobility into the network by virtue of its ability to broadcast.
The term mobile radio is usually meant to encompass terrestrial situations where a
radio transmitter or receiver is capable of being moved, regardless of whether it
actually moves or not. The major propagation effects encountered in the use of a
mobile radio in built-up areas are due to the fact that the antenna of the mobile unit
may lie well below the surrounding buildings. Simply put, there is no ‚Äúline-of-sight‚Äù
path for communication; rather, radio propagation takes place mainly by way of
scattering from the surfaces of the surrounding buildings and by diffraction over and/
or around them. The end result is that energy reaches the receiving antenna via more
than one path. In a mobile radio environment, we thus speak of a multipath phe-
nomenon in that the various incoming radio waves reach their destination from
different directions and with different time delays. Indeed, there may be a multitude
of propagation paths with different electrical lengths, and their contributions to the
received signal could combine in a variety of ways. Consequently, the received signal
strength varies with location in a very complicated fashion, and so a mobile radio
channel may be viewed as a linear time-varying channel that is statistical in nature.

Finally, a satellite channel adds another invaluable dimension to the public telecom-
munications network by providing broad-area coverage in both a continental and
an intercontinental sense. Moreover, access to remote areas not covered by conven-
tional cable or fiber communications is also a distinct feature of satellites. In almost
all satellite communication systems, the satellites are placed in geostationary orbit.
For the orbit to be geostationary, it has to satisfy two requirements. First, the orbit
is geosynchronous, which requires the satellite to be at an altitude of 22,300 miles;
a geosynchronous satellite orbits the Earth in 24 hours (i.e., the satellite is synchro-
nous with the Earth‚Äôs rotation). Second, the satellite is placed in orbit directly above
the equator on an eastward heading (1.c., it has zero inclination). Viewed from Earth,
a satellite in geostationary orbit appears to be stationary in the sky. Consequently,
an Earth station does not have to track the satellite; rather, it merely has to point its
antenna along a fixed direction, pointing toward the satellite. By so doing, the system
design is simplified considerably. Communications satellites in geostationary orbit
offer the following unique system capabilities:

¬ª Broad-area coverage.
¬Æ Reliable transmission links.
¬Æ Wide transmission bandwidths.

Modulation Process 19

In terms of services, satellites can provide fixed point-to-point links extending over
long distances and into remote areas, Communication to mobile platforms (e.g., air-
craft, ships), or broadcast capabilities. Indeed, communications satellites play a key
role in the notion of the whole world being viewed as a ‚Äúglobal village.‚Äù In a typical
satellite communication system, a message signal is transmitted from an Earth station
via an uplink to a satellite, amplified in a transponder (i.e., electronic circuitry) on
board the satellite, and then retransmitted from the satellite via a downlink to another
Earth station. With the satellite positioned in geostationary orbit, it is always visible
to all the Earth stations located inside the satellite antenna‚Äôs coverage zones on the
Earth‚Äôs surface. In effect, the satellite acts as a powerful repeater in the sky. The most
popular frequency band for satellite communications is 6 GHz for the uplink and
4 GHz for the downlink. The use of this frequency band offers the following
attributes:
¬ª Relatively inexpensive microwave equipment.
¬ª Low attenuation due to rainfall; rainfall is a primary atmospheric cause of signal
loss.
¬Æ Insignificant sky background noise; the sky background noise (due to random
noise emissions from galactic, solar, and terrestrial sources) reaches its lowest level
between 1 and 10 GHz.
In the 6/4-GHz band, a typical satellite is assigned a 500 MHz bandwidth that is
divided among 12 transponders on board the satellite. Each transponder, using ap-
proximately 36 MHz of the satellite bandwidth, corresponds to a specific radio chan-
nel. A single transponder can carry at least one color television signal, 1200 voice
circuits, or digital data at a rate of 50 Mb/s.

To summarize, a communication channel is central to the operation of a com-
munication system. Its properties determine both the information-carrying capacity
of the system and the quality of service offered by the system. We may classify com-
munication channels in different ways:

¬ª A channel may be linear or nonlinear; a wireless radio channel is linear, whereas
a satellite channel is usually (but not always) nonlinear.

¬Æ A channel may be time invariant or time varying; an optical fiber is time invariant,
whereas a mobile radio channel is typically time varying.

¬Æ A channel may be bandwidth limited or power limited (i.e., limited in the available
transmitted power); a telephone channel is bandwidth limited, whereas an optical
fiber link and a satellite channel are both power limited.

Now that we have some understanding of sources of information and com-
munication channels, we may return to the block diagram of a communication sys-
tem shown in Figure 1.

| Modulation Process

 

The purpose of a communication system is to deliver a message signal from an information
source in recognizable form to a user destination, with the source and the user being
physically separated from each other. To do this, the transmitter modifies the message
signal into a form suitable for transmission over the channel. This modification is achieved
by means of a process known as modulation, which involves varying some parameter of
a carrier. wave in accordance with the message signal. The receiver re-creates the original

20

& BACKGROUND AND PREVEEW

message signal from a degraded version of the transmitted signal after propagation through
the channel. This re-creation is accomplished by using a process known as demodulation,
which is the reverse of the modulation process used in the transmitter. However, owing
to the unavoidable presence of noise and distortion in the received signal, we find that the
receiver cannot re-create the original message signal exactly. The resulting degradation in
overall system performance is influenced by the type of modulation scheme used. Specifi-
cally, we find that some modulation schemes are less sensitive to the effects of noise and
distortion than others.

We may classify the modulation process into continuous-wave modulation and pulse
modulation. In continuous-wave (CW) modulation, a sinusoidal wave is used as the car-
tier. When the amplitude of the carrier is varied in accordance with the message signal,
we have amplitude modulation (AM), and when the angle of the carrier is varied, we have
angle modulation. The latter form of CW modulation may be further subdivided into
frequency modulation (FM) and phase modulation (PM), in which the instantaneous fre-
quency and phase of the carrier, respectively, are varied in accordance with the message
signal.

In pulse modulation, on the other hand, the carrier consists of a periodic sequence
of rectangular pulses. Pulse modulation can itself be of an analog or digital type. In analog
pulse modulation, the amplitude, duration, or position of a pulse is varied in accordance
with sample values of the message signal. In such a case, we speak of pulse-amplitude
modulation (PAM), pulse-duration modulation (PDM), and pulse-position modulation
(PPM). -

The standard digital form of pulse modulation is known as pulse-code modulation
(PCM) that has no CW counterpart. PCM starts out essentially as PAM, but with an
important modification: The amplitude of each modulated pulse (i.e., sample of the original
message signal) is quantized or rounded off to the nearest value in a prescribed set of
discrete amplitude levels and then coded into a corresponding sequence of binary symbols.
The binary symbols 0 and 1 are themselves represented by pulse signals that are suitably
shaped for transmission over the channel. In any event, as a result of the quantization
process, some information is always lost and the original message signal cannot therefore
be reconstructed exactly. However, provided that the number of quantizing (discrete am-
plitude) levels is large enough, the distortion produced by the quantization process is not
discernible to the human ear in the case of a speech signal or the human eye in the case of
a two-dimensional image. Among all the different modulation schemes, pulse-code mod-
ulation has emerged as the preferred method of modulation for the transmission of analog
message signals for the following reasons:

¬ª Robustness in noisy environments by regenerating the transmitted signal at regular
intervals.

¬ª Flexible operation.

¬Æ Integration of diverse sources of information into a common format.

¬ª Security of information in its transmission from source to destination.

In introducing the idea of modulation, we stressed its importance as a process that
ensures the transmission of a message signal over a prescribed channel. There is another
important benefit, namely, multiplexing, that results from the use of modulation. Multi-
plexing is the process of combining several message signals for their simultaneous trans-
mission over the same channel. Three commonly used methods of multiplexing are as
follows:

¬ª Frequency-division multiplexing (FDM), in which CW modulation is used to trans-
late each message signal to reside in a specific frequency slot inside the passband of

Analog and Digital Types of Communication 21

the channel by assigning it a distinct carrier frequency; at the receiver, a bank of
filters is used to separate the different modulated signals and prepare them individ-
ually for demodulation.

> Time-division multiplexing (TDM), in which pulse modulation is used to position
samples of the different message signals in nonoverlapping time slots.

¬ª Code-division multiplexing (CDM), in which each message ¬Æ signal is identified by a
distinctive code.

In FDM the message signals overlap with each other at the channel input; hence the system
may suffer from crosstalk (i.e., interaction between message signals) if the channel is non-
linear. In TDM the message signals use the full passband of the channel, but on a time-
shared basis. In CDM the message signals are permitted to overlap in both time and
frequency across the channel.

Mention should also be made of wavelength-division multiplexing (WDM), which
is special to optical fibers. In WDM, wavelength is used as a new degree of freedom by
concurrently operating distinct portions of the wavelength spectrum (i.e., distinct colors)
that are accessible within the optical fiber. However, recognizing the reciprocal relation-
ship that exists between the wavelength and frequency of an electromagnetic wave, we
may say that WDM is a form of FDM.

i Analog and Digital Types of Communication

Typically, in the design of a communication system the information source, communica-
tion channel, and information sink (end user) are all specified. The challenge is to design
the transmitter and the receiver with the following guidelines in mind:

 

 

 

¬Æ Encode/modulate the message signal generated by the source of information, transmit
it over the channel, and produce an ‚Äúestimate‚Äù of it at the receiver output that satisfies
the requirements of the end user.

¬Æ¬ª Do all of this at an affordable cost.

We have the option of using a digital or analog communication system.

Consider first the case of a digital communication system represented by the block
diagram of Figure 9, the rationale for which is rooted in information theory. The functional
blocks of the transmitter and the receiver, starting from the far end of the channel, are
paired as follows:

¬ª Source encoder-decoder.
¬ª Channel encoder-decoder.
¬Æ¬ª Modulator-demodulator.

The source encoder removes redundant information from the message signal and is re-
sponsible for the efficient use of the channel. The resulting sequence of symbols is called
the source code word. The data stream is processed next by the channel encoder, which
produces a new sequence of symbols called the channel code word. The channel code word
is longer than the source code word by virtue of the co#trolled redundancy built into its
construction. Finally, the modulator represents each symbol of the channel code word by
a corresponding analog symbol, appropriately selected from a finite set of possible analog
symbols. The sequence of analog symbols produced by the modulator is called a waveform,
which is suitable for transmission over the channel. At the receiver, the channel output
(received signal) is processed in reverse order to that in the transmitter, thereby recon-

22

& BACKGROUND AND PREVIEW

 

   
  

    
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  
  

 

  
  
 

 

 

 

 

 

 

 

 

 

 

Estimate of
Source of Message signal message signa User of
information information
ee ee 1
Source Source 1
encoder ; } decoder
i Koa |
Source | Estimate of
code word Source
code word
Transmitter Channel Channel Receiver
encoder decoder
i
i Estimate of
| Channel channel
' code word code word
Modulator Demodulator
| 1
I
| en J L--____f _____ J
Received
Waveform signal
| Channel

 

  

 

 

 

Ficure 9 Block diagram of digital communication system.

structing a recognizable version of the original message signal. The reconstructed message
signal is finally delivered to the user of information at the destination. From this description
it is apparent that the design of a digital communication system is rather complex in
conceptual terms but easy to build. Moreover, the system is robust, offering greater tol-
erance of physical effects (e.g., temperature variations, aging, mechanical vibrations) than
its analog counterpart.

In contrast, the design of an analog communication system is simple in conceptual
terms but difficult to build because of stringent requirements on linearity and system ad-
justment. For example, voice communication requires nonlinear distortion products at
least 40 dB below the wanted message signal. In signal-processing terms, the transmitter
consists of a modulator and the receiver consists of a demodulator, the details of which
are determined by the type of CW modulation used.

The conceptual simplicity of analog communications is due to the fact that analog
modulation techniques, exemplified by their wide use in radio and television, make rela-
tively superficial changes to the message signal in order to prepare it for transmission over
the channel. More specifically, there is no significant effort made by the system designer
to tailor the waveform of the transmitted signal to suit the channel at any deeper level.
On the other hand, digital communication theory endeavors to find a finite set of wave-
forms that are closely matched to the characteristics of the channel and which are therefore
more tolerant of channel impairments. In so doing, reliable communication is established
over the channel. In the selection of good waveforms for digital communication over a
noisy channel, the design is influenced solely by the channel characteristics. However, once
the appropriate set of waveforms for transmission over the channel has been selected, the
source information can be encoded into the channel waveforms, and the efficient trans-

Shannon's Information Capacity Theerem 23

mission of information from the source to the user is thereby ensured. In summary, the
use of digital communications provides the capability for information transmission that is
both efficient and reliable.

From this discussion, it is apparent that the use of digital communications requires
a considerable amount of electronic circuitry, but nowadays electronics are inexpensive,
due to the ever-increasing availability of very-large-scale integrated (VLSI) circuits in the
form of silicon chips. Thus although cost considerations used to be a factor in selecting
analog communications over digital communications in the past, that is no longer the case.

Despite the trend toward the ever-increasing use of digital communications, a strong
case can be made for the study of analog communications for two important reasons:

1. As long as we hear and see analog communications around us via radio and televi-
sion, we need to understand how these communications systems work. Moreover,
the study of analog modulation motivates other digital modulation schemes.

2. Analog devices and circuits have a natural affinity for operating at very high speeds
and they consume very little power compared to their digital counterparts. Accord-
ingly, the implementation of very high-speed or very low-power communication sys-
tems dictates the use of an analog approach,

 

 

| Shannon's Information Capacity Theorem

The goal of a communication system designer is to configure a system that transports a
message signal from a source of interest across a noisy channel to a user at the other end
of the channel with the following objective:

The message signal is delivered to the user both efficiently and reliably, subject to
certain design constraints: allowable transmit power, available channel bandwidth,
and affordable cost of building the system.

In the case of a digital communication system, reliability is commonly expressed in terms
of bit error rate (BER) or probability of bit error measured at the receiver output. Clearly,
the smaller the BER, the more reliable the communication system is. A question that comes
to mind in this context is whether it is possible to design a communication system that
operates with zero BER even through the channel is noisy. In an ideal setting, the answer
to this question is an emphatic yes. The answer is embodied in one of Shannon‚Äôs celebrated
theorems,‚Äô¬∞ which is called the information capacity theorem.

Let B denote the channel bandwidth, and let SNR denote the received signal-to-noise
ratio. The information capacity theorem states that ideally these two parameters are related
as

C = B log,(1 + SNR) b/s (1)

where C is the information capacity of the channel. The information capacity is defined
as the maximum rate at which information can be transmitted across the channel without
error; it is measured in bits per second (b/s). For a prescribed channel bandwidth B and
received SNR, the information capacity theorem tells us that a message signal can be
transmitted through the system without error even when the channel is noisy, provided
that the actual signaling rate R in bits per second, at which data are transmitted through
the channel, is less than the information capacity C.

24

BACKGROUND AND PREVIEW

Unfortunately, Shannon‚Äôs information capacity theorem does not tell us how to de-
sign the system. Nevertheless, from a design point of view, the theorem is very valuable
for the following reasons:

1. The information capacity theorem provides a bound on what rate of data transmis-
sion is theoretically attainable for prescribed values of channel bandwidth B and
received SNR, On this basis, we may use the ratio

aC
as a measure of the efficiency of the digital communication system under study. The
closer 7 is to unity, the more efficient the system is.

2. Equation (1) provides a basis for the trade-off between channel bandwidth B and
received SNR, In particular, for a prescribed signaling rate R, we may reduce the
required SNR by increasing the channel bandwidth B, hence the motivation for using
a wideband modulated scheme (e.g., pulse-code modulation) for improved noise
performance.

3. Equation (1) provides an idealized framework for comparing the noise performance
of one modulation scheme against another.

i A Digital Communication Problem

 

When we speak of a digital communication system having a low bit error rate, say, the
implication is that only a small fraction in a long stream of binary symbols is decoded in
error by the receiver. The issue of the receiver determining whether a binary symbol sent
over a noisy channel is decoded in error or not is of fundamental importance to the design
of digital communication systems. It is therefore appropriate briefly to discuss this basic
issue so as to motivate the study of communication systems.

Suppose we have a random binary signal, 7(t), consisting of symbols 1 and 0, that
are equally likely. Symbol 1 is represented by a constant level +1, and symbol 0 is rep-
resented by a constant level ‚Äî1, each of which lasts for a duration T. Such a signal may
represent the output of a digital computer or the digitized version of a speech signal. To
facilitate the transmission of this signal over a communication channel, we employ a simple
modulation scheme known as phase-shift keying. Specifically, the information bearing
signal 1(¬£) is multiplied by a sinusoidal carrier wave A, cos(27f,t), where A, is the carrier
amplitude, f. is the carrier frequency, and t is time. Figure 10a shows.a block diagram of
the transmitter, the output of which is defined by

s(t) = {* cos(27f,t) for symbol 1

2
‚ÄîA, cos(27f.t) for symbol 0 (2)

where 0 =¬¢ = T. The carrier frequency f, is a multiple of 1/T.
The channel is assumed to be distortionless but noisy, as depicted in Figure 10b. The
received signal x(t) is thus defined by

x(t) = s(t) + w(t) (3)

where w(t)-is the additive channel noise.
The receiver consists of a correlator followed by a decision-making device, as de-
picted in Figure 10c. The correlator multiplies the received signal x(t) by a locally generated

A Digital Communication Problem 25

 

Message Transmitted Transmitted + Channel! output
signal m(s) signal s() signal s(2 2 {received signal) 0
~ +
Carrier wave Noise
A, cos (21rf,2) w(t)
{a (b)
Correiator
Received Decision [> Say ify > 0
signal x0) id

 

 

device }‚ÄîS¬ª Otherwise, say 0

 

 

Local carrier
cos (2rf,t) Threshold = 0

{c)

Ficure 10 Elements of a digital communication system. (a) Block diagram of transmitter.
(b) Block diagram of channel. (c) Block diagram of receiver.

carrier cos(27f.t) and then integrates the product over the symbol interval 0 = ¬¢ = T,
producing the output

T
Yr = I x(t) cos(2af.t) dt (4)

QO

Substituting Equations (2) and (3) into (4) and invoking the assumption that the carrier
frequency f, is a multiple of 1/T, we obtain (after the simplification of terms)

+ +wr for symbol 1
v= A (5)
‚Äú3 +w, for symbol 0

where wy is the contribution of the correlator output due to the channel noise w(t). To
reconstruct the original binary signal s(t), the correlator output y; is compared against a
threshold of zero volts by the decision-making device, the operation of which is based on
the following rule:

If the correlator output yr is greater than zero, the receiver outputs symbol 1;
otherwise, it outputs symbol 0.

With this background, we may now discuss/raise some basic issues. First, from Fou-
rier analysis we find that the time-bandwidth product of a pulse signal is constant. This
means that the bandwidth of a rectangular pulse of duration T is inversely proportional
to T, The transmitted signal in Figure 10a consists of the product of this rectangular signal
and the sinusoidal carrier A, cos(2af.t). The multiplication of a signal by a sinusoid has
the effect of shifting the Fourier transform of the signal to the right by f, and to the left
by an equal amount, except for the scaling factor of 1/2. It follows therefore that the
bandwidth of the transmitted signal m(¬¢), and therefore the required channel bandwidth,
is inversely proportional to the reciprocal of the symbol duration T. For the problem at
hand, the reciprocal of T is also the signaling rate of the system in b/s.

26

BACKGROUND AND PREVIEW

There are, however, some other issues that require theoretical considerations:

1. What is the justification for the receiver structure of Figure 10c?

2. The noise contribution wis the value of a random variable W produced by sampling
a certain realization w(t) of the channel noise at time ¬¢ = T in accordance with
Equations (3) and (4). How do we relate the statistics of the random variable W to
the statistical characteristics of the channel noise?

3. The receiver of Figure 10¬¢ makes occasional errors due to the random nature of the
correlator output. That is, the receiver decides in favor of symbol 0 given that symbol
1 was actually transmitted, and vice versa. What is the probability of decision errors?

Moreover, there are some important practical issues that need attention:

1. Channel bandwidth is a highly valuable resource. How do we choose a modulation
scheme that conserves bandwidth in a cost-effective manner? .

2. The binary signal m(t) may include redundant symbols introduced into it through
the use of channel encoding so as to provide protection against channel noise. How
do we design the channel encoder in the transmitter and the channel decoder in the
receiver so as to come very close to Shannon‚Äôs information capacity theorem in a
physically realizable manner?

3. The locally generated carrier in the receiver of Figure 10c is physically separate from
the carrier source used for modulation in the transmitter. How do we synchronize
the receiver to the transmitter with respect to both the carrier phase and symbol
timing so as to justify the use of Equation (4) as the basis of decision-making in the
reconstruction of the original binary signal?

The theoretical and practical issues raised here in the context of the simple digital
communication system of Figure 10 are addressed in the following chapters of the book.

| Historical Notes"

 

A preview of communications would be incomplete without a history of the subject. In
this final section of this introductory chapter we present some historical notes on com-
munications; each paragraph focuses on some important and related events. It is hoped
that this material will provide a sense of inspiration and motivation for the reader.

In 1837, the telegraph was perfected by Samuel Morse, a painter. With the words
‚ÄúWhat hath God wrought,‚Äù transmitted by Morse‚Äôs electric telegraph between Washing-
ton, D.C., and Baltimore, Maryland, in 1844, a completely revolutionary means of real-
time, long-distance communications was triggered. The telegraph is the forerunner of dig-
ital communications in that the Morse code is a variable-length ternary code using an
alphabet of four symbols: a dot, a dash, a letter space, and a word space; short sequences
represent frequent letters, whereas long sequences represent infrequent letters. This type
of signaling is ideal for manual keying. Subsequently, Emile Baudot developed a fixed-
length binary code for telegraphy in 1875. In Baudot‚Äôs telegraphic code, well-suited for
use with teletypewriters, each code word consists of five equal-length code elements, and
each element is assigned one of two possible states: a mark or a space (i.e., symbol 1 or 0
in today‚Äôs terminology}.

In 1864, James Clerk Maxwell formulated the electromagnetic theory of light and
predicted the existence of radio waves; the underlying set of equations bears his name. The

Historical Notes 27

existence of radio waves was established experimentally by Heinrich Hertz in 1887. In
1894, Oliver Lodge demonstrated wireless communication over a relatively short distance
(150 yards). Then, on December 12, 1901, Guglielmo Marconi received a radio signal at
Signal Hill in Newfoundland; the radio signal had originated in Cornwall, England, 1700
miles away across the Atlantic. The way was thereby opened toward a tremendous broad-
eming of the scope of communications. In 1906, Reginald Fessenden, a self-educated aca-
demic, made history by conducting the first radio broadcast.

In 1875, the telephone was invented by Alexander Graham Bell, a teacher of the
deaf. The telephone made real-time transmission of speech by electrical encoding and
replication of sound a practical reality. The first version of the telephone was crude and
weak, enabling people to talk over short distances only. When telephone service was only
a few years old, interest developed in automating it. Notably, in 1897, A. B. Strowger, an
undertaker from Kansas City, Missouri, devised the automatic step-by-step switch that
bears his name; of all the electromechanical switches devised over the years, the Strowger
switch was the most popular and widely used.

In 1904, John Ambrose Fleming invented the vacuum-tube diode, which paved the
way for the invention of the vacuum-tube triode by Lee de Forest in 1906. The discovery
of the triode was instrumental in the development of transcontinental telephony in 1913
and signaled the dawn of wireless voice communications. Indeed, until the invention and
perfection of the transistor, the triode was the supreme device for the design of electronic
amplifiers. ;

In 1918, Edwin H. Armstrong invented the superheterodyne radio receiver; to this
day, almost all radio receivers are of this type. In 1933, Armstrong demonstrated another
revolutionary concept, namely, a modulation scheme that he called frequency modulation
(FM); Armstrong‚Äôs paper making the case for FM radio was published in 1936.

The first all-electronic television system was demonstrated by Philo T. Farnsworth
in 1928, and then by Vladimir K. Zworykin in 1929. By 1939, the British Broadcasting
Corporation (BBC) was broadcasting television on a commercial basis.

In 1928, Harry Nyquist published a classic paper on the theory of signal transmission
in telegraphy. In particular, Nyquist developed criteria for the correct reception of tele-
graph signals transmitted over dispersive channels in the absence of noise. Much of Ny-
quist‚Äôs early work was applied later to the transmission of digital data over dispersive
channels.

In 1937, Alec Reeves invented pulse-code modulation (PCM) for the digital encoding
of speech signals. The technique was developed during World War II to enable the en-
cryption of speech signals; indeed, a full-scale, 24-channel system was used in the field by
the United States military at the end of the war. However, PCM had to await the discovery
of the transistor and the subsequent development of large-scale integration of circuits for
its commercial exploitation.

In 1943, D. O. North devised the matched filter for the optimum detection of a
known signal in additive white noise. A similar result was obtained in 1946 independently
by J. H. Van Vleck and D. Middleton, who coined the term matched filter.

In 1947, the geometric representation of signals was developed by V. A. Kotel‚Äônikov
in a doctoral dissertation presented before the Academic Council of the Molotov Energy
Institute in Moscow. This method was subsequently brought to full fruition by John M.
Wozencraft and Irwin M. Jacobs in a landmark textbook published in 1965.

In 1948, the theoretical foundations of digital communications were laid by Claude
Shannon in a paper entitled ‚ÄúA Mathematical Theory of Communication.‚Äù Shannon‚Äôs
paper was received with immediate and enthusiastic acclaim. It was perhaps this response

28

BACKGROUND AND PREVIEW

that emboldened Shannon to amend the title of his paper to ‚Äú‚ÄúThe Mathematical Theory
of Communication‚Äù when it was reprinted a year later in a book co-authored with Warren
Weaver. It is noteworthy that prior to the publication of Shannon‚Äôs 1948 classic paper, it
was believed that increasing the rate of information transmission over a channel would
increase the probability of error; the communication theory community was taken by
surprise when Shannon proved that this was not true, provided that the transmission rate
was below the channel capacity. Shannon‚Äôs 1948 paper was followed by some significant
advances in coding theory, which include the following:

¬Æ* Development of the first nontrivial error-correcting codes by M. J. E. Golay in 1949
and Richard W. Hamming in 1950.

¬Æ Development of turbo codes by C. Berrou, A. Glavieux, and P. Thitimajshima in
1993; turbo codes provide near-optimum error-correcting coding and decoding per-
formance in the Shannon sense.

The transistor was invented in 1948 by Walter H. Brattain, John Bardeen, and Wil-
liam Shockley at Bell Laboratories. The first silicon integrated circuit (IC) was produced
by Robert Noyce in 1958. These landmark innovations in solid-state devices and integrated
circuits led to the development of very-large-scale integrated (VLSI) circuits and single-
chip microprocessors, and with them the nature of signal processing and the telecommu-
nications industry changed forever.

The invention of the transistor in 1948 spurred the application of electronics to
switching and digital communications. The motivation was to improve reliability, increase
capacity, and reduce cost. The first call through a stored-program system was placed in
March 1958 at Bell Laboratories, and the first commercial] telephone service with digital
switching began in Morris, Illinois, in June 1960. The first T-1 carrier system transmission
was installed in 1962 by Bell Laboratories.

During the period 1943 to 1946, the first electronic digital computer, called the
ENIAG, was built at the Moore School of Electrical Engineering of the University of Penn-
sylvania under the technical direction of J. Presper Eckert, Jr., and John W. Mauchly.
However, John von Neumann‚Äôs contributions were among the earliest and most funda-
mental to the theory, design, and application of digital computers, which go back to the
first draft of a report written in 1945. Computers and terminals started communicating
with each other over long distances in the early 1950s. The links used were initially voice-
grade telephone channels operating at low speeds (300 to 1200 b/s). Various factors have
contributed to a dramatic increase in data transmission rates; notable among them are the
idea of adaptive equalization, pioneered by Robert Lucky in 1965, and efficient modula-
tion techniques, pioneered by G. Ungerboeck in 1982. Another idea widely employed in
computer communications is that of automatic repeat-request (ARQ). The ARQ method
was originally devised by H. C. A. van Duuren during World War II and published in
1946. It was used to improve radio-telephony for telex transmission over long distances.

From 1950 to 1970, various studies were made on computer networks. However,
the most significant of them in terms of impact on computer communications was the
Advanced Research Project Agency Network (ARPANET), first put into service in 1971.
The development of ARPANET was sponsored by the Advanced Research Projects Agency
of the U.S. Department of Defense. The pioneering work in packet switching was done on
ARPANET. In 1985, ARPANET was renamed the Internet. The turning point in the evo-
lution of the Internet occurred in 1990 when Tim Berners-Lee proposed a hypermedia
software interface to the Internet, which he named the World Wide Web.‚Äù Thereupon, in

Notes and References 29

the space of only about two years, the Web went from nonexistence to worldwide popu-
larity, culminating in its commercialization in 1994. How do we explain the explosive
growth of the Internet? We may answer this question by offering these reasons:"

¬ª Before the Web exploded into existence, the ingredients for its creation were already
in place. In particular, thanks to VLSI, personal computers (PCs) had already become
ubiquitous in homes throughout the world, and they were increasingly equipped with
modems for interconnectivity to the outside world.

¬Æ For about two decades, the Internet had grown steadily (albeit within a confined
community of users), reaching a critical threshold of user-value based electronic mail
and file transfer.

¬ª Standards for document description and transfer, hypertext markup language
(HTML), and hypertext transfer protocol (HTTP) had been adopted.

Thus, everything needed for creating the Web was already in place except for two critical
ingredients: a simple user interface and a brilliant service concept.

In 1955, John R. Pierce proposed the use of satellites for communications. This
proposal was preceded, however, by an earlier paper by Arthur C. Clark that was pub-
lished in 1945, also proposing the idea of using an Earth-orbiting satellite as a relay point
for communication between two Earth stations. In 1957, the Soviet Union launched Sput-
nik I, which transmitted telemetry signals for 21 days. This was followed shortly by the
launching of Explorer I by the United States in 1958, which transmitted telemetry signals
for about five months. A major experimental step in communications satellite technology
was taken with the launching of Telstar I from Cape Canaveral on July 10, 1962. The
Telstar satellite was built by Bell Laboratories, which had acquired considerable knowl-
edge from pioneering work by Pierce. The satellite was capable of relaying TV programs
across the Atlantic; this was made possible only through the user of maser receivers and
large antennas.

The use of optical means (e.g., smoke and fire signals) for the transmission of infor-
mation dates back to prehistoric times. However, no major breakthrough in optical com-
munications was made until 1966, when K. C. Kao and G. A. Hockham of Standard
Telephone Laboratories, U.K., proposed the use of a clad glass fiber as a dielectric wave-
guide. The /aser (an acronym for light amplification by stimulated emission of radiation)
had been invented and developed in 1959 and 1960. Kao and Hockham pointed out that
(1) the attenuation in an optical fiber was due to impurities in the glass, and (2) the intrinsic
loss, determined by Rayleigh scattering, is very low. Indeed, they predicted that a loss of
20 dB/km should be attainable. This remarkable prediction, made at a time when the
power loss in a glass fiber was about 1000 dB/km, was to be demonstrated later. Nowa-
days, transmission losses as low as 0.1 dB/km are achievable.

The spectacular advances in microelectronics, digital computers, and lightwave sys-
tems that we have witnessed to date, and that will continue into the future, are all re-
sponsible for dramatic changes in the telecommunications environment; many of these
changes are already in place, and more changes will evolve as time goes on.

i NOTES AND REFERENCES

 

1. For essays on an early account of communications and other related disciplines (e.g., elec-
tronics, computers, radar, radio astronomy, satellites), see Overhage (1962); in particular,
see the chapter on ‚ÄúCommunications‚Äù by L. V. Berkner, pp. 35‚Äî50.

30

¬© BACKGROUND AND PREVIEW

2.

10.

11.
12.
13.

The JPEG image coding standard is discussed in the papers by Wallace (1991); see also the
article by T. A. Ramstad in the handbook edited by Madisetti and Williams (1998).

. The discrete cosine transform (DCT) and its inverse for a block of 8 X 8 source image

samples are respectively defined by

Cll P [, Se sy) 00 ‚Äòes cos( 2 al

4
7
f(x, y) = ; 2 > C(u)C(v) Flu, v) cos( 28 | cos 22 al

 

Flu, v) =

 

where

1 .
Clu), Ce) = 475 for 4 = 0 andv = 0
1 otherwise

For a full treatment of the DCT, see Rao and Yip (1990).

. The MPEG-1 video coding standard is discussed in the paper by Gall (1991); see also the

article by A. M. Tekalp in the handbook edited by Madisetti and Williams (1998), which
discusses the follow-up versions of the MPEG video coding standard.

. The MPEG-1 audio coding standard is discussed in the papers by Brandenburg and Stoll

(1994) and Pan (1993); see also the article by P. Noll in the handbook edited by Madisetti
and Williams (1998), which also discusses the follow-up versions of the MPEG audio
coding standard. In particular, the widespread use of the more current standard, MPEG-3
audio, is resulting in a level of piracy that may dwarf the earlier problems of ‚Äúbootleg‚Äù
cassette tapes.

. For a detailed discussion of communication networks, see Tanenbaum (1996).

. The OSI reference model was developed by a subcommittee of the International Organi-

zation for Standardization (ISO) in 1977. For a discussion of the principles involved in
arriving at the seven layers of the OSI model and a description of the layers themselves,
see Tanenbaum (1996).

. SONET was originally proposed by Telcordia Technologies Inc. (then known as Bellcore)

and standardized by the American National Standards Institute (ANSI). Later, CCITT
approved a SONET standard and issued a set of parallel recommendations called synchro-
nous digital hierarchy (SDH). The differences between SONET and SDH are of a minor
nature.

. For a thorough and precise analysis of the propagation of light waves in an optical fiber,

we need to treat it as a dielectric waveguide and use Maxwell‚Äôs equations to carry out the
analysis; such an analysis is highly mathematical in nature. For a readable account of the
analysis, see Chapter 3 of Green, Jr. (1993).

For a semitechnical overview of Shannon‚Äôs theorems on information theory presented in a
highly readable fashion, see the book entitled Silicon Dreams by Lucky (1989).

For a readable account of the history of communications, see Lebow (1995).
For a historical account of the development of the Internet, see Leiner et al. (1997).

For an insightful essay on new telecommunications services and how society reacts to their
development, see Lucky (1997). This paper points to Metcalf‚Äôs law, according to which it
seems as if any new telecommunications service must take a long time for it to build to
universal acceptance. Lucky cites the World Wide Web as a startling counterexample to
Metcalf‚Äôs law and gives the reasons why.

 

RANDOM PROCESSES

This chapter presents an introductory treatment of stationary random processes with
emphasis on second-order statistics. In particular, it discusses the following issues:

b> The notion of a.random process.
b> The requirement that has to be satisfied for a random process to be stationary.

> The partial description of a random process in terms of its mean, correlation, and
covariance functions.

¬ª The conditions that bave to be satisfied for a stationary random process to be ergodic, a
property that enables us to substitute time averages for ensemble averages.

¬ª What happens to a stationary random process when it is transmitted through a linear
time-invariant filter?

> The frequency-domain description of a random process in terms of power spectral density.
> The characteristics of an important type of random process known as a Gaussian process.
¬ª Sources of noise and their narrowband form.

> Rayleigh and Rician distributions, which represent two special probability distributions
that arise in the study of communication systems.

i 1.1L Eetroduction

The idea of a mathematical model used to describe a physical phenomenon is well estab-
lished in the physical sciences and engineering. In this context, we may distinguish two
classes of mathematical models: deterministic and stochastic. A model is said to be deter-
ministic if there is no uncertainty about its time-dependent behavior at any instant of time.
However, in many real-world problems the use of a deterministic model is inappropriate
because the physical phenomenon of interest involves too many unknown factors. Nev-
ertheless, it may be possible to consider a model described in probabilistic terms in that
we speak of the probability of a future value lying between two specified limits. In such a
case, the model is said to be stochastic or random. A brief review of probability theory is
presented in Appendix 1.

Consider, for example, a radio communication system. The received signal in such
a system usually consists of an information-bearing signal component, a random interfer-
ence component, and channel noise. The information-bearing signal component may rep-
resent, for example, a voice signal that, typically, consists of randomly spaced bursts of
energy of random duration. The interference component may represent spurious electro-
magnetic waves produced by other communication systems operating in the vicinity of the

31

32

CHAPTER 1 RANDOM PROCESSES

tadio receiver. A major source of channel noise is thermal noise, which is caused by the
random motion of the electrons in conductors and devices at the front end of the receiver.
We thus find that the received signal is random in nature. Although it is not possible to
predict the exact value of the signal in advance, it is possible to describe the signal in terms
of statistical parameters such as average power and power spectral density, as discussed
in this chapter.

1.2. Mathematical Definition
of a Random Process

 

 

In light of these introductory remarks, it is apparent that random processes have two
properties. First, they are functions of time. Second, they are random in the sense that
before conducting an experiment, it is not possible to exactly define the waveforms that
will be observed in the future.

In describing a random experiment it is convenient to think in terms of a sample
space. Specifically, each outcome of the experiment is associated with a sample point. The
totality of sample points corresponding to the aggregate of all possible outcomes of the
experiment is called the sample space. Each sample point of the sample space is a function
of time. The sample space or ensemble composed of functions of time is called a random
or stochastic process.‚Äô As an integral part of this notion, we assume the existence of a
probability distribution defined over an appropriate class of sets in the sample space, so
that we may speak with confidence of the probability of various events.

Consider, then, a random experiment specified by the outcomes s from some sample
space S, by the events defined on the sample space S, and by the probabilities of these

Sample
space
$s
x(t)
x(t)
0 i Outcome of the
first trial of
the experiment
xglt
2h) xalt)
Outcome of the
second trial of
0 the experiment

 

Outcome of the
ath trial of
+T the experiment

   

 

 

t‚Äî

Ficure 1.1 An ensemble of sample functions.

1.3 Stationary Processes 33

events, Suppose that we assign to each sample point s a function of time in accordance
with the rule:

X(t, 5), -T=rsT (1.1)

where 2T is the total observation interval. For a fixed sample point s;, the graph of the
function X(t, s;) versus time ¬¢ is called a realization or sample function of the random
process. To simplify the notation, we denote this sample function as

x(t) = X(Z, s;) (1.2)

Figure 1.1 illustrates a set of sample functions {x,(t)|j = 1, 2,..., 2}. From this figure,
we note that for a fixed time #, inside the observation interval, the set of numbers

{1 (th), X2(te)s -- + Xaltad} = {Xtes 81), X(ta, $2), -- +, X tes Sud}

constitutes a random variable. Thus we have an indexed ensemble (family) of random
variables {X(¬¢, s)}, which is called a random process. To simplify the notation, the custom-
ary practice is to suppress the s and simply use X(z) to denote a random process. We may
now formally define a random process X(t) as an ensemble of time functions together with
a probability rule that assigns a probability to any meaningful event associated with an
observation of one of the sample functions of the random process. Moreover, we may
distinguish between a random variable and a random process as follows:

¬ª For a random variable, the outcome of a random experiment is mapped into a
number.

¬ª For a random process, the outcome of a random experiment is mapped into a wave-
form that is a function of time.

| 1.3 Stationary Processes

 

In dealing with random processes encountered in the real world, we often find that the
statistical characterization of a process is independent of the time at which observation of
the process is initiated. That is, if such a process is divided into a number of time intervals,
the various sections of the process exhibit essentially the same statistical properties. Such
a process is said to be stationary. Otherwise, it is said to be nonstationary. Generally
speaking, a stationary process arises from a stable physical phenomenon that has evolved
into a steady-state mode of behavior, whereas a nonstationary process arises from an
unstable phenomenon.

To be more precise, consider a random process X(t) that is initiated at t = ‚Äî%. Let
X(t,), X(t2),..., X(t,) denote the random variables obtained by observing the random
process X(t) at times t), h,..., tz, respectively. The joint distribution function of this set
of random variables is Fxie,),..., xie)(%1¬ª ¬´ - + ¬ª Xp). Suppose next we shift all the observation
times by a fixed amount 7, thereby obtaining a new set of random variables X(t, + 7),
X(t, + 7),..., X(t, + 7). The joint distribution function of this latter set of random
variables is Fyi,+1),..., x(e+7)(%1s + + + 5 X) The random process X(t) is said to be stationary
in the strict sense or strictly stationary if the following condition holds:

Fyn... Xtq+9(%1, ey Xe) = Pye, Xie) (X15 sey Xe) (1.3)
for all time shifts 7, all &, and all possible choices of observation times t,,... , f,. In other
words, a random process X(t), initiated at time t = ‚Äî%, is strictly stationary if the joint

distribution of any set of random variables obtained by observing the random process X(t)
is invariant with respect to the location of the origin t = 0. Note that the finite-dimensional

34

CuHapTeR 1 & RANDOM PROCESSES

distributions in Equation (1.3) depend on the relative time separation between random
variables but not on their absolute time. That is, the random process has the same prob-
abilistic behavior through all time.

Similarly, we may say that two random processes X(t) and Y(t) are jointly strictly
stationary if the joint finite-dimensional distributions of the two sets of random variables
X(ty),..., X(t) and Y(e4),..., Y(t}) are invariant with respect to the origin t = 0 for all
k and j and all choices of observation times t),...,t, and Z1,..., t}.

Returning to Equation (1.3), we may distinguish two situations of special interest:

1. For k = 1, we have
Fey) = Fxtern(x) = Fx(x) for all ¬¢ and + (1.4)

That is, the first-order distribution function of a stationary random process is inde-
pendent of time.
2. For k = 2 and 7 = ‚Äît,, we have

Fy xc (*15 x2) = Pro, x2) (%15 X) for all #, and 4 (1.5)

That is, the second-order distribution function of a stationary random process de-
pends only on the time difference between the observation times and not on the
particular times at which the random process is observed.

These two properties have profound implications for the statistical parameterization of a
stationary random process; this issue is discussed in Section 1.4.

& EXAMPLE 1.1

Consider Figure 1.2, depicting three spatial windows located at times t,, t, t;. We wish to
evaluate the probability of obtaining a sample function x(t) of a random process X(t) that
passes through this set of windows, that is, the probability of the joint event

A={a<X) <b}, #¬¢=1,2,3
In terms of the joint distribution function, this probability equals
P(A) = F ye), xte),Xtt3)( Pas bs, b3) ‚Äî Fees) Xt.) (415 2, a3)

Suppose now the random process X(z) is known to be strictly stationary. An implication
of strict stationarity is that the probability of the set of sample functions of this process passing
through the windows of Figure 1.32 is equal to the probability of the set of sample functions
passing through the corresponding time-shifted windows of Figure 1.3b. Note, however, that
it is not necessary that these two sets consist of the same sample functions. a

--=~ by

ww yas, lens ;
\ ,. ~. A posible

AS 793 sample

function

 

So

1*

Ficure 1.2 Illustrating the probability of a joint event.

 

 

1.4 Mean, Correlation, and Covariance Functions 35

bo I,

ay
: a3
8g
t
cay bo ta

I‚Äù

 

{a)

| Lal |
aq bg
a
tg +t
itr bo tgtt

1‚Äù

 

()
FIGURE 1.3 Ilustrating the concept of stationarity in Example 1.1.

and Covariance Functions

| 1.4 Mean, Correlation,

 

Consider a strictly stationary random process X(t). We define the sean of the process X(t)
as the expectation of the random variable obtained by observing the process at some time
t, as shown by

bex(t) = ELX(e)]

. (1.6)
= | . xfrxy(x) dx

where f.(.)(x) is the first-order probability density function of the process. From Equation
(1.4) we deduce that for a strictly stationary random process, fy;(x) is independent of
time ¬¢. Consequently, the mean of a strictly stationary process is a constant, as shown by

bex(t) = bx ‚Äî for all t (1.7)

We define the autocorrelation function of the process X(t) as the expectation of the product
of two random variables, X(z) and X(t), obtained by observing the process X(t) at times
t, and f,, respectively. Specifically, we write

Ryx(t1, 2) = E[X(√©)X(√©)]
¬© po (1.8)
= in in 1X2 fixie x(e,)(X19 X2) dx dx,

where fixi,),xie,)(X1) X2) is the second-order probability density function of the process.
From Equation (1.5), we deduce that for a strictly stationary random process,
Fexie,),xte)(%1, X2) depends only on the difference between the observation times ¬¢, and #3.

36

CHAPTER 1 ¬Æ& RANDOM PROCESSES

This, in turn, implies that the autocorrelation function of a strictly stationary process
depends only on the time difference t, ‚Äî t,, as shown by

Rx, 2) = Rxf(t2 ‚Äî t,) for all t, and (1.9)
Similarly, the autocovariance function of a strictly stationary process X(t) is written as

Cx(ty, to) = El(X(t) ‚Äî bex)(X(2) ‚Äî ex)]
= Rxlt ‚Äî th) - pk
Equation (1.10) shows that, like the autocorrelation function, the autocovariance function
of a strictly stationary process X(t} depends only on the time difference t, ‚Äî 4. This
equation also shows that if we know the mean and autocorrelation function of the process,
we can uniquely determine the autocovariance function. The mean and autocorrelation
function are therefore sufficient to describe the first two moments of the process.
However, two important points should be carefully noted:

(1.10)

1. The mean and autocorrelation function only provide a partial description of the
distribution of a random process X(t).

2. The conditions of Equations (1.7) and (1.9), involving the mean and autocorrelation
function, respectively, are not sufficient to guarantee that the random process X(t)
is strictly stationary.

Nevertheless, practical considerations often dictate that we simply limit ourselves to a
partial description of the process given by the mean and autocorrelation function. The
class of random processes that satisfy Equations (1.7) and (1.9) has been given various
names, such as second-order stationary, wide-sense stationary, or weakly stationary pro-
cesses. Henceforth, we shall simply refer to them as stationary processes.‚Äù

A stationary process is not necessarily strictly stationary because Equations (1.7) and
(1.9) obviously do not imply the invariance of the joint (k-dimensional) distribution of
Equation (1.3) with respect to the time shift 7 for all &. On the other hand, a strictly
stationary process does not necessarily satisfy Equations (1.7) and (1.9) as the first- and
second-order moments may not exist. Clearly, however, the class of strictly stationary
processes with finite second-order moments forms a subclass of the class of all stationary
processes.

PROPERTIES OF THE AUTOCORRELATION FUNCTION

For convenience of notation, we redefine the autocorrelation function of a stationary pro-
cess X(t) as

Ry(t) = E[X(t + X(¬¢)]_ for allt (1.11)
This autocorrelation function has several important properties:

1. The mean-square value of the process may be obtained from R,(7) simply by putting
7 = 0 in Equation (1.11), as shown by

Ry(0) = E[X?(2)] (1.12)
2. The autocorrelation function Rx(7)-is an even function of 7, that is,
Rx(t) = Rx(~7) (1.13)

This property follows directly from the defining equation (1.11). Accordingly, we
may also define the autocorrelation function R(t) as

Rx(z) = E[X(e)X(t ‚Äî 7)]

14 Mean, Correlation, and Covariance Functions 37

Ry) Slowly fluctuating

random process

  
  
 
  

Rapidly fluctuating
random process

 

t

ie)

Ficure 1.4 Illustrating the autocorrelation functions of slowly and rapidly fluctuating random
processes,

3. The autocorrelation function Ry(7) has its maximum magnitude at 7 = 0, that is,
|Rx(7)| = Rx(0) (1.14)

To prove this property, consider the nonnegative quantity

E[(X(¬¢ + 7) + X(2))?] = 0
Expanding terms and taking their individual expectations, we readily find that

E[X2(t + 7)] + 2E[X(t + X(t] + ELX2(2)] = 0
which, in light of Equations (1.11) and (1.12), reduces to
2Rx(0) + 2Rx(7) = 0

Equivalently, we may write

Rx(0) S Rx(t) S Rx(0)
from which Equation (1.14) follows directly.

The physical significance of the autocorrelation function Rx(7) is that it provides a
means of describing the interdependence of two random variables obtained by observing
a random process X(t) at times 7 seconds apart. It is therefore apparent that the more
rapidly the random process X(t) changes with time, the more rapidly will the autocorre-
lation function Rx(z7) decrease from its maximum Rx(0) as 7 increases, as illustrated in
Figure 1.4. This decrease may be characterized by a decorrelation time 7, such that for
T > 7%, the magnitude of the autocorrelation function Rx(7) remains below some prescribed
value. We may thus define the decorrelation time 7) of a stationary process X(t) of zero
mean as the time taken for the magnitude of the autocorrelation function Rx(7) to decrease
to 1 percent, say, of its maximum value R(0).

¬© EXAMPLE 1.2 Sinusoidal Wave with Random Phase
Consider a sinusoidal signal with random phase, defined by
X(t) = A cos(2rft + ¬©) (1.15)

where A and f, are constants and is a random variable that is uniformly distributed over
the interval [~7, a], that is,

1
fl0) 42m? 7S OS‚Ñ¢ (1.16)

0, elsewhere

38

CuHaprer 1. & RANDOM PROCESSES

 

Figure 1.5 Autocorrelation function of a sine wave with random phase.

This means that the random variable ¬© is equally likely to have any value @ in the interval
[-a, a]. Each value of @ corresponds to a sample in the sample space of the random process
X(@).

The process X(z) defined by Equations (1.15) and (1.16) may represent a locally gen-
erated carrier in the receiver of a communication system, which is used in demodulation of
the received signal. In particular, the random variable @ denotes the phase difference between
this locally generated carrier and the sinusoidal carrier wave used to modulate the message
signal in the transmitter.

The autocorrelation function of X(2) is

Rx(7) = EEX + 1) X(2)]
= E[A? cos(2af¬¢ + 2afr + ¬©) cos(2aft + @)]

 

 

At At
aw Elcos(4arf,t + 2af.r + 20)] + a E[cos(27f,7)]

A? {‚Äù 1 At
=> su cos(4afi.t + 2af.r + 28) d√© + ‚Äî cos(2af-7)
2 Jew lor 2

The first term integrates to zero, and so we get

Rx(n = a cos(2-1f,7) (1.17)

which is plotted in Figure 1.5. We see therefore that the autocorrelation function of a sinu-
soidal wave with random phase is another sinusoid at the same frequency in the ‚Äú7 domain‚Äù
rather than the original time domain. <

¬ª EXAMPLE 1.3. Random Binary Wave

Figure 1.6 shows the sample function x(t) of a process X(t) consisting of a random sequence
of binary symbols 1 and 0. The following assumptions are made:

1. The symbols 1 and 0 are represented by pulses of amplitude +A and ‚ÄîA volts, respec-
tively, and duration T seconds.

2, The pulses are not synchronized, so the starting time ¬£, of the first complete pulse for
positive time is equally likely to lie anywhere between zero and T seconds, That is, ty
is the sample value of a uniformly distributed random variable T,, with its probability
density function defined by .

1
frltd) =y7? OS HST
0, elsewhere

3. During any time interval (x - 1)T <t ‚Äî #, < nT, where 7 is an integer, the presence
of a 1 or a 0 is determined by tossing a fair coin; specifically, if the outcome is heads,

1.4 Mean, Correlation, and Covariance Functions 39

xt)

 

oak ode

FiGuRE 1.6 Sample function of random binary wave.

 

we have a 1 and if the outcome is tails, we have a 0. These two symbols are thus equally
likely, and the presence of a 1 or 0 in any one interval is independent of all other
intervals.

Since the amplitude levels ‚ÄîA and +A occur with equal probability, it follows imme-
diately that E[X(t}] = 0 for all t, and the mean of the process is therefore zero.

To find the autocorrelation function R(t, ¬£), we have to evaluate E[X(z,)X(z))], where
X(#,) and_X(t,) are random variables obtained by observing the random process X(t) at times
t, and #;, respectively.

Consider first the case when |#, ‚Äî t;| > T. Under this condition the random variables
X(t,) and X(t, occur in different pulse intervals and are therefore independent. We thus have

E(X(t,)X(4)] = ELX(e,)ELX(4)] = 0, [te ‚Äî &| > T

Consider next the case when | #, ‚Äî i;| < T, with t, = 0 and t; < f,. In such a situation
we observe from Figure 1.6 that the random variables X(t,) and X(¬¢,) occur in the same pulse
interval if and only if the delay tz satisfies the condition tz < T ‚Äî |t, ‚Äî #;|. We thus obtain
the conditional expectation:

A? ty<T‚Äî | - &|
E[X(t)X(e)ltal = 4,‚Äù ‚Äò
0, elsewhere
Averaging this result over all possible values of tz, we get

T‚Äî | tytit

6 A‚Äôf. TAta) dty

T- [tet] 42
= = dt,
j pa

= a(t | te = al), [te t,| <T

ELX(t)X(4)] = [

 

By similar reasoning for any other value of t,, we conclude that the autocorrelation function
of a random binary wave, represented by the sample function shown in Figure 1.6, is only a
function of the time difference rT = t, ‚Äî t,, as shown by

in|
Rx() = a(t - lal), In]<T (1.18)

0, |r] =T

This result is plotted in Figure 1.7. <

CHAPTER 1 RANDOM PROCESSES

Ryfr)

Ae

T
-T ie] T

FIGURE 1.7 Autocorrelation function of random binary wave.

s Cross-CORRELATION FUNCTIONS

Consider next the more general case of two random processes X(t) and Y(¬¢) with auto-
correlation functions Ry(√©, u) and Ry(t, ), respectively. The two cross-correlation func-

tions of X(t) and Y(t) are defined by

Ryy(t, 4) = ELX(t)¬•(w)] (1.19)
and

Ryx(t, 4) = E[Y(√©)X(w)] (1.20)

where ¬¢ and w denote two values of time at which the processes are observed. In this case,
the correlation properties of the two random processes X(t) and Y(t) may be displayed
conveniently in matrix form as follows:

Ryxlt, uw) Ryle, ¬∞)

Rem) = an u) ‚ÄîRylt, #)

which is called the correlation matrix of the random processes X(t) and Y(z). If the random
processes X(t) and Y(¬¢) are each stationary and, in addition, they are jointly stationary,
then the correlation matrix can be written as

ee |

Re) = I eix(t) Rolo)

(1,21)

where T= t ‚Äî u.
The cross-correlation function is not generally an even function of 7 as was true for
the autocorrelation function, nor does it have a maximum at the origin. However, it does

‚Äòobey a certain symmetry relationship as follows (see Problem 1.9):

Rxy(7) = Ryx(-7) (1.22)

& ExaMpLe 1.4 Quadrature-Modulated Processes

Consider a pair of quadrature-modulated processes X,(t) and X,(t) that are related to a sta-
tionary process X(t) as follows:

X(t) = X(t) cos(2rft + O)

X,(t) = X(t) sin(27f.t + @)

1.5. Ergodic Processes 41

where f, is a carrier frequency, and the random variable ¬© is uniformly distributed over the
interval [0, 27]. Moreover, @ is independent of X(#). One cross-correlation function of X,(z)
and X,(t) is given by

Rial) = BI ()X2(t ‚Äî 7)
E[X(t)X(t ‚Äî 1) cos(2arf.t + ¬©) sin(2afit ~ 2af.7 + 0)]
E[X()X(t ‚Äî AIE[cos(2af,t + @) sin(2aft ‚Äî 2rfr + O)] (1.23)
= 4Rx(DE|sin(4af.t ‚Äî 2af.r +20) ‚Äî sin(2af,7)]
‚Äî3Rx(7) sin(2af-7)

u

where, in the last line, we have made use of the uniform distribution of the random variable
¬Æ representing phase. Note that at 7 = 0, the factor sin(27f,7) is zero and therefore

Ri2(0) = ELX(t)X2(2)]
=0

This shows that the random variables obtained by simultaneously observing the quadrature-
modulated processes X,(¬¢) and X(t) at some fixed value of time ¬¢ are orthogonal to each
other. <

| 1.5 Ergodic Processes

The expectations or ensemble averages of a random process X(¬¢) are averages ‚Äú‚Äòacross the
process.‚Äù For example, the mean of a random process X(t) at some fixed time t, is the
expectation of the random variable X(t,) that describes all possible values of the sample
functions of the process observed at time t = ¬¢,. Naturally, we may also define long-term
sample averages, or time averages that are averages ‚Äúalong the process.‚Äù We are therefore
interested in relating ensemble averages to time averages, for time averages represent a
practical means available to us for the estimation of ensemble averages of a random pro-
cess. The key question, of course, is: When can we substitute time averages for ensemble
averages? To explore this issue, consider the sample function x(t) of a stationary process
X(t), with the observation interval defined as -T = t = T. The DC value of x(t) is defined
by the time average

1 T

ot I x(t) dt (1.24)

HAT) =

Clearly, the time average y,(T) is a random variable, as its value depends on the obser-
vation interval and which particular sample function of the random process X(t) is picked
for use in Equation (1.24). Since the process X(t) is assumed to be stationary, the mean of
the time average y2,,(T) is given by (after interchanging the operation of expectation and
integration):

ElyAT -if) E[x(t)] de
1/7
~ 2T Jor

= Mx

bx dt (1.25)

42

CHAPTER 1 RANDOM PROCESSES

where jtx is the mean of the process X(t). Accordingly, the time average 2,(T) represents
an unbiased estimate of the ensemble-averaged mean x. We say that the process X(t) is
ergodic in the mean if two conditions are satisfied:

¬ª The time average y,(T) approaches the ensemble average jx in the limit as the
observation interval T approaches infinity; that is,
lim BAT) = bx

> The variance of ,(T), treated as a random variable, approaches zero in the limit as
the observation interval T approaches infinity; that is,

lim var[y,(T)] = 0
T$00

The other time average of particular interest is the autocorrelation function R,(7, T)
defined in terms of the sample function x(t) observed over the interval -T = t = T.
Following Equation (1.24), we may formally define the time-averaged autocorrelation
function of a sample function x(¬£) as follows:

1 (7
R. == I 1,2

x(T, T) oT Jer x(t + r)x(t) d√© (1.26)
This second time-average should also be viewed as a random variable with a mean and
variance of its own. In a manner similar to ergodicity of the mean, we say that the process
x(t) is ergodic in the autocorrelation function if the following two limiting conditions are
satisfied:

lim R,(7, T) = Rx(7}

Too

lim var[R,(7, T)] = 0
Toe

We could, of course, go on in a similar way to define ergodicity in the most general
sense by considering higher-order statistics of the process X(t). In practice, however, er-
godicity in the mean and ergodicity in the autocorrelation function, as described here, are
often (but not always) considered to be adequate. Note also that the use of Equations
(1.24) and (1.26) to compute the time averages yz,(T) and R,(t, T) requires that the process
X(t) be stationary. In other words, for a random process to be ergodic, it has to be sta-
tionary; however, the converse is not necessarily true.

-6 Transmission of a Random Process

[Tiros a

Through a Linear Time-Invariant Filter

 

Suppose that a random process X(t) is applied as input to a linear time-invariant filter of
impulse response b(t), producing a new random process Y(t) at the filter output, as in
Figure 1.8. In general, it is difficult to describe the probability distribution of the output
random process Y(t), even when the probability distribution of the input random process
X(t) is completely specified for ‚Äî-% <¬¢< ¬´,

In this section, we determine the time-domain form of the input-output relations of
the filter for defining the mean and autocorrelation functions of the output random process
¬•(¬£) in terms of those of the input X(t), assuming that X(t) is a stationary process.

1.6 Transmission of a Random Process Through a Linear Time-Invariant Filter 43

 

impulse
xX()‚Äî>) so response >> F(s)
we)

 

 

 

Ficure 1.8 Transmission of a random process through a linear time-invariant filter.

The transmission of a process through a linear-time-invariant filter is governed by
the convolution integral; for a review of this operation, see Appendix 2. For the problem
at hand, we may thus express the output random process Y(t) in terms of the input random
process X(t) as

Y(t) = if b(m)X(t ‚Äî 14) dry
where 1, is the integration variable. Hence, the mean of Y(√©) is
By(t) = ELY(√©)]
co (1.27)
= a [ b(7,)X(t ‚Äî 74) an

Provided that the expectation E[X(t)] is finite for all t and the system is stable, we may
interchange the order of expectation and integration in Equation (1.27) and so write

v(t) = | birdBIXte ~ m1 dn
(1.28)

= in b(t )ux(t ‚Äî 1) dr

When the input random process X(t) is stationary, the mean yx(t) is a constant px, so
that we may simplify Equation (1.28) as follows:

by = px [ h(1,) dr
= pxH(0)

(1.29)

where H(0) is the zero-frequency (DC) response of the system. Equation (1.29) states that
the mean of the random process Y(t) produced at the output of a linear time-invariant
system in response to X(t) acting as the input process is equal to the mean of X(t) multiplied
by the DC response of the system, which is intuitively satisfying.

Consider next the autocorrelation function of the output random process Y(t). By
definition, we have

Ryt, u) = E[Y(¬£)¬•(w)]

where ¬¢ and # denote two values of the time at which the output process is observed. We
may therefore use the convolution integral to write

Ry{t, #) = elf b(r)X(t ‚Äî 1%) dry ia b(1)X(u ‚Äî 1) ar] (1.30)

44 CHAPTER 1 & RANDOM PROCESSES

Here again, provided that the mean-square value E[X7(z)] is finite for all t and the system
is stable, we may interchange the order of the expectation and the integrations with respect
to 7, and 7, in Equation (1.30), obtaining

Rut, =| dnbin) | droblnJELXte ‚Äî 1)Xtu ~ nl
. : (1.31)
= [ dr h(7,) ia dth(m%)Rx(t ‚Äî %, # ‚Äî 7)

When the input X(f) is a stationary process, the autocorrelation function of X(t) is only a
function of the difference between the observation times t ‚Äî 7 and # ‚Äî 75. Thus, putting
7 = t‚Äî u in Equation (1.31), we may write

Ry{t) = in i: h(t, )b(m)Rx(t ‚Äî 1%] + %) dy dm (1.32)

On combining this result with that involving the mean py, we see that if the input to a
stable linear time-invariant filter is a stationary process, then the output of the filter is also
a stationary process.

Since Ry(0) = E[Y?(¬£)}, it follows that the mean-square value of the output random
Process Y(t) is obtained by putting rT = 0 in Equation (1.32). We thus get the result

eye = [fo atrvbladRadna ‚Äî 1) dry dry (1.33)

which is a constant.

| 1.7 Power Spectral Density

 

Thus far we have considered the characterization of stationary processes in linear systems
in the time domain. We turn next to the characterization of random processes in linear
systems by using frequency-domain ideas. In particular, we wish to derive the frequency-
domain equivalent to the result of Equation (1.33) defining the mean-square value of the
filter output.

By definition, the impulse response of a linear time-invariant filter is equal to the
inverse Fourier transform of the frequency response of the system; a review of the Fourier
transform is presented in Appendix 2. Using H(f) to denote the frequency response of the
system, we may thus write

b(t) = i: A(f) exp(j2afn) df (1.34)

Substituting this expression for /(7) into Equation (1.33), we get

evan = [|] | [HUN expl npn) af Porat ‚Äî 1) dr, dr,
(1.35)

=] apni { deabirs) [Rule ~ 1) explj2afn) dx

1.7 Power Spectral Density 45

In the last integral on the right-hand side of Equation (1.35), define a new variable
T=m-% ,

Then we may rewrite Equation (1.35) in the form

Ey = {_ dptaie) | drabln) expli2afn) {| Rule) expl-i2mfn) dr (1.36)

However, the middle integral on the right-hand side in Equation (1.36) is simply H*(f),
the complex conjugate of the frequency response of the filter, and so we may simplify this
equation as

E[Y*(t)] = [. df |H(f)|? [. Ryx(7) exp(‚Äîj2af7) dr (1.37)

where | H(f)| is the magnitude response of the filter. We may further simplify Equation
(1.37) by recognizing that the last integral is simply the Fourier transform of the auto-
correlation function Rx(r) of the input random process X(t). This prompts us to introduce
the definition of a new parameter

Sx(f) = [. Rx(1) exp(‚Äîj2af7) dr (1.38)

The function Sy(f) is called the power spectral density, or power spectrum, of the station-
ary process X(t). Thus substituting Equation (1.38) into (1.37), we obtain the desired
relation:

Evo] = [LAU Se(f) df (1.39)

Equation (1.39) states that the mean-square value of the output of a stable linear time-
invariant filter in response to a stationary process is equal to the integral over all frequen-
cies of the power spectral density of the input process multiplied by the squared magnitude
response of the filter. This is the desired frequency-domain equivalent to the time-domain
relation of Equation (1.33).

To investigate the physical significance of the power spectral density, suppose that
the random process X(t) is passed through an ideal narrowband filter with a magnitude
response centered about the frequency f., as shown in Figure 1.9; that is,

14, If + fl <34f

1.40
0, [f+ fl>af (1-40)

LHUP)| -{

 

we 0 I. te
Af af

FicURE 1.9 Magnitude response of ideal narrowband filter.

46

CHAPTER 1 RANDOM PROCESSES

where Af is the bandwidth of the filter. Then from Equation (1.39) we find that if the
filter bandwidth Af is sufficiently small compared to the midband frequency f, and Sx(f)
is a continuous function, the mean-square value of the filter output is approximately

E[Y*(z)] ~ (2Af)Sx(f-) (1.41)

The filter, however, passes only those frequency components of the input random process
X(t) that lie inside a narrow frequency band of width Af centered about the frequency
+f.. Thus Sx(f,) represents the frequency density of the average power in the random
process X(z), evaluated at the frequency f = f,. The dimensions of the power spectral
density are therefore in watts per Hertz (W/Hz).

a PROPERTIES OF THE POWER SPECTRAL DENSITY

The power spectral density S,{f) and the autocorrelation function Ry(7) of a stationary
process X(t) form a Fourier-transform pair with 7 and f as the variables of interest, as
shown by the pair of-relations

2

Sx(f) = i . Rx(7) exp(‚Äîj2afr) dr (1.42)

Relr) = | Self) exp( mfr) af (1.43)

Equations (1.42) and (1.43) are:basic relations in the theory of spectral analysis of random
processes, and together they constitute what are usually called the Einstein‚ÄîWiener‚Äî
Khintchine relations?

The Einstein~Wiener-Khintchine relations show that if either the autocorrelation
function or power spectral density of a randoin process is known, the other can be found
exactly. But these functions display different aspects of the correlation information about
the process, It is commonly accepted that for practical purposes, the power spectral density
is the more useful ‚Äú‚Äòparameter.‚Äù

We now wish to use this pair of relations to derive some general properties of the
power spectral density of a stationary process.

Property 1

The zero-frequency value of the power spectral density of a stationary process equals the
total area under the graph of the autocorrelation function; that is,

Sx(0) = [- Rx(1) dr (1.44)

This property follows directly from Equation (1.42) by putting f = 0.

Property 2
The mean-square value of a stationary process equals the total area under the graph of
the power spectral density; that is,

E[X?(t)] = in Sx(f) df (1.45)

This property follows directly from Equation (1.43) by putting r = 0 and noting that
Ry (0) = E[X*(2)].

1.7 Pewer Spectral Density 47

Property 3
The power spectral density of a stationary process is always nonnegative; that is,
Sx(f} = 0 for all f (1.46)
This property is an immediate consequence of the fact that, in Equation (1.41), the
mean-square value E[Y?(t)] must always be nonnegative.
Property 4

The power spectral density of a real-valued random process is an even function of fre-
quency; that is,

Sxl-f) = Sx(f) (1.47)

This property is readily obtained by substituting ‚Äîf for f in Equation (1.42):
Sx(‚Äîf) = [ Rule) exp(2afr) dr
Next, substituting ‚Äî7 for 1, and recognizing that Ryx(‚Äî7) = Rx(7), we get

Sx(-f) = in Rx(7) exp(‚Äîj2a7fr) dr = Sx(f)

which is the desired result.

Property 5

The power spectral density, appropriately normalized, has the properties usually associated
with a probability density function.

The normalization we have in mind here is with respect to the total area under the
graph of the power spectral density (i.e., the mean-square value of the process). Consider
then the function

bx(f) = a (1.48)
FF sein ag
In light of Properties 2 and 3, we note that px(f) = 0 for all f. Moreover, the total area

under the function px(f) is unity. Hence, the normalized form of the power spectral den-
sity, as defined in Equation (1.48), behaves similar to a probability density function.

Sx(f)
Sx

¬Æ ExamMPLe 1.5 Sinusoidal Wave with Random Phase (continued)

Consider the random process X(t) = A cos(2af.¬¢ + ¬©), where @ is a uniformly distributed
random variable over the interval [‚Äî7, 7]. The autocorrelation function of this random pro-
cess is given by Equation (1.17), which is reproduced here for convenience:

2
Rx(7) = < cos(2 af,7)

48

Craprer | & RANDOM PROCESSES

Self)

ae Ae
y OF +f) ‚Äú7 Af -f.)

 

he 0 fi t

FiGuRE 1.10 Power spectral density of sine wave with random phase; 6(f) denotes the delta
function at f = 0.

Let 8(f) denote the delta function at f = 0; for the definition of the delta function and its
properties, see Appendix 2. Taking the Fourier transform of both sides of the relation defining
Rx(7), we find that the power spectral density of the sinusoidal process X(z) is

Sx(f) = = [Sf ‚Äî f) + 6 + #3] (1.49)

which consists of a pair of delta functions weighted by the factor A7/4 and located at +f, as
illustrated in Figure 1.10. We note that the total area under a delta function is one. Hence,
the total area under Sx(f) is equal to A?/2, as expected. <

& EXAMPLE 1.6 Random Binary Wave (continued)

Consider again a random binary wave consisting of a sequence of 1s and Os represented by
the values +A and ‚ÄîA, respectively. In Example 1.3 we showed that the autocorrelation
function of this random process has a triangular waveform, as shown by

;
x(n) = a(s - i) ri< 7
0, |r| =T

The power spectral density of the process is therefore

[|

Sx(f) = { ai - 4) exp(‚Äîj2afr) dr

-T
Using the Fourier transform of a triangular function (see Table A6.3), we obtain
Sx(f) = A¬∞*T sinc?(fT) (1.50)

which is plotted in Figure 1.11. Here again we see that the power spectral density is nonneg-
ative for all f and that it is an even function of f. Noting that Rx(0) = A? and using Property
2, we find that the total area under Sx(f), or the average power of the random binary wave
described here, is A?, which is intuitively satisfying. 4

The result of Equation (1.50) may be generalized as follows. We note that the energy
spectral density (i.e., the squared magnitude of the Fourier transform) of a rectangular
pulse g(√©) of amplitude.A and duration T is given by

‚Ç¨,(f) = A*T? sinc*(fT) (1.51)
We may therefore rewrite Equation (1.50) in terms of ‚Ç¨,(f) simply as
‚Ç¨,(f)
Sx(f) = = (1.52)

T

1.7 Power Spectral Density 49

Sef}

 

 

2 1 0 1 2

T T T T

FiGure 1,11 Power spectral density of random binary wave.

Equation (1.52) states that for a random binary wave in which binary symbols 1 and 0
are represented by pulses g(t) and ‚Äîg(t), respectively, the power spectral density Sx(f) is
equal to the energy spectral density &,(f) of the symbol shaping pulse g(t), divided by the
symbol duration T.

¬Æ ExaMe_e 1.7 Mixing of a Random Process with a Sinusoidal Process

A situation that often arises in practice is that of mixing (i.e., multiplication) of a stationary
process X(t) with a sinusoidal wave cos(2 aft + ¬©), where the phase @ is a random variable
that is uniformly distributed over the interval [0, 27]. The addition of the random phase ¬Æ
in this manner merely recognizes the fact that the time origin is arbitrarily chosen when
X(t) and cos(2af,t + @) come from physically independent sources, as is usually the case. We
are interested in determining the power spectral density of the random process Y(t), defined
by

Y(t) = X(t) cos(2aft + O) (1.53)

Using the definition of autocorrelation function of a stationary process and noting that the
random variable ¬© is independent of X(t), we find that the autocorrelation function of Y(t) is
given by
Ry(7) = ELY(e + 7) ¬•(2)]

= E[X(t + 7) cos(Qaf.t + 2af.7 + O)X(t) cos(2af.t + O)]

= E[X(t + X(t) ]Elcos(2af,t + 2af,7 + ¬©) cos(2aft + 0)] (1.54)

= $Rx(r)E[cos(2rf,r) + cos(4aft + 2rf.7r + 20)]
$Rx(7) cos(2af.7)

Because the power spectral density is the Fourier transform of the autocorrelation function,
we find that the power spectral densities of the random processes X(t) and Y(t) are related as
follows:

Sof) = USF - f) + Sxlf + fl (1.55)

According to Equation (1.55), the power spectral density of the random process Y(t) defined
in Equation (1.53) is obtained as follows: We shift the given power spectral density Sy({f) of
random process X(t) to the right by f., shift it to the left by fa add the two shifted power
spectra, and divide the result by 4. <

50

CHAPTER 1 RANDOM PROCESSES

& RELATION AMONG THE POWER SPECTRAL DENSITIES
OF THE INPUT AND OUTPUT RANDOM PROCESSES

Let Sx(f) denote the power spectral density of the output random process Y(t) obtained
by passing the random process X(z)} through a linear filter of frequency response H(f).
Then, recognizing by definition that the power spectral density of a random process is
equal to the Fourier transform of its autocorrelation function and using Equation (1.32),
we obtain

Sup) = [_ Ryln) expl‚Äîfaaf) dr
(1.56)

= in in in h(7)h(m)Rx(t ‚Äî 1% + 2) exp(‚Äîj2mfr) dr dry dr

Let tr ‚Äî 7 + 72 = 7%, or, equivalently, r = 1) + 7, ‚Äî 7. Then by making this substitution
in Equation (1.56), we find that S,(f) may be expressed as the product of three terms: the
frequency response H(f) of the filter, the complex conjugate of H(f), and the power spec-
tral density Sx(f) of the input random process X(t): We may thus simplify Equation (1.56)
as

Sy(f) = HUP\A*(PSx(f) (1.57)

Finally, since | H(f)|* = H(f)H*(f), we find that the relationship among the power spectral
densities of the input and output random processes is expressed in the frequency domain
by writing

Sy(f) = |H(f)|?Sx(f) (1.58)

Equation (1.58) states that the power spectral density of the output process Y(t) equals
the power spectral density of the input process X(t) multiplied by the squared magnitude
response of the filter. By using this relation, we can therefore determine the effect of passing
a random process through a stable, linear, time-invariant, filter. In computational terms,
Equation (1.58) is usually easier to handle than its time-domain counterpart of Equation
(1.32), involving the autocorrelation function.

= RELATION AMONG THE POWER SPECTRAL DENSITY
AND THE MAGNITUDE SPECTRUM OF A SAMPLE FUNCTION

We now wish to relate the power spectral density 5(f) directly to the spectral properties
of a sample function x(t) of a stationary process X(z) that is ergodic. For the sample
function x(t) to be Fourier transformable, however, it must be absolutely integrable; that
is :

[ |x(t)| dt < 0 (1.59)
This condition can never be satisfied by any stationary sample function x(t) of infinite
duration. In order to use the Fourier transform technique, we consider a truncated segment

of x(t), defined over the observation interval -T <= t < T, say. Thus, using X(f, T) to
denote the Fourier transform of the truncated sample function so defined, we may write

T
X(f, T) = [. x(t) exp(‚Äîj2aft) dt (1.60)

1.7 Power Spectral Density 51

Assuming that the process x(t) is also ergodic, we may evaluate the autocorrelation
function Rx(7) of X(¬¢) using the time-average formula (see Section 1.5)
T

Rx(t) = lim = x(t + r)x(t) dt (1.61)

It is customary to view the sample function x(f) as a power signal (i.e., a signal with finite
average power). Hence, we may formulate the following Fourier-transform pair:

1 {7 1
aT [. x(t + r)x(t) dt = oT |X(f, T)|? (1.62)

The parameter on the left-hand side is a time-averaged autocorrelation function. The pa-
rameter on the right-hand side is called the periodogram, whose dimensions are the same
as those of the power spectral density. This terminology is a misnomer, however, since the
periodogram is a function of frequency, not period. Nevertheless, it has wide usage. The
quantity was first used by statisticians to look for periodicities such as seasonal trends in
data.

Using the formula for the inverse Fourier transform in the Fourier-transform pair of
Equation (1.62), we may express the time-averaged autocorrelation function of the sample
function x(¬¢) in terms of the periodogram as

T

‚Äú1
aT Sop x(t + t)x(t) dt = in oT |X(f, T)|* exp(j2afT) df (1.63)

Hence, substituting Equation (1.63) into | 1.61), we get

Rx(1) = lim J oT
For a fixed value of the frequency f, the periodogram is a random variable in that
its value varies in a random manner from one sample function of the random process to
another. Thus, for a given sample function x(t), the periodogram does not converge in any
statistical sense to a limiting value as T tends to infinity. As such, it would be incorrect to
interchange the order of the integration and limiting operations in Equation (1.64). Sup-
pose, however, that we take the expectation of both sides of Equation (1.64) over the
ensemble of all sample functions of the random process and recognize that for an ergodic
process the autocorrelation function Rx(7) is unchanged by such an operation. Then, since
each sample function of an ergodic process eventually takes on n√©arly all the modes of
behavior of each other sample function, we may thus write

|X(f, T)|* exp(j2mfr) df (1.64)

oe

Rx(7) = lim Al |X(f, T)|?] exp(j2aft) df (1.65)

T-20 Yoo

Now we may interchange the order of the integration and limiting operations and so obtain

ret = fo ftim Zetixy, NP} explana dp (1.66

Hence, comparing Equations (1.66) and (1.43), we obtain the desired relation between
the power spectral density S,(f) of an ergodic process and the squared magnitude spectrum
|X(f, T)|? of a truncated sample function of the process:

1
Sx(f) = lim ap FLIX, T)/7]
7 (1.67)

T 2
= lim a 2| [. x(t) exp(‚Äîj2aft) dt

Too 2T

 

 

52

CuHarTerR 1 & RANDOM PROCESSES

It is important to note that in Equation (1.67) it is not possible to let T‚Äî ¬© before taking
the expectation. Equation (1.67) provides the mathematical basis for estimating‚Äò the power
spectral density of an ergodic random process, given a sample function x(t) of the process
observed over the interval [‚ÄîT, T].

= CRross-SPECTRAL DENSITIES

Just as the power spectral density provides a measure of the frequency distribution of a
single random process, cross-spectral densities provide a measure of the frequency inter-
relationship between two random processes. In particular, let X(t) and Y(t) be two jointly
stationary processes with their cross-correlation functions denoted by Rxy(7) and Ry (7).
We then define the cross-spectral densities Syy(f) and Syx(f) of this pair of random pro-
cesses to be the Fourier transforms of their respective cross-correlation functions, as
shown by

Sxy(f) = if Ryy(1) exp(‚Äîj2afr) dr (1.68)

and

Syx(f) = in Ryx(1) exp(‚Äîj2afr) dr (1.69)

The cross-correlation functions and cross-spectral densities thus form Fourier-transform
pairs. Accordingly, using the formula for inverse Fourier transformation we may also
write

Rxyylt) = if Sxy(f) exp( j2afr) df (1.70)

and

Ryx(7) = in Syx(f) exp(j2aft) af (1.71)

The cross-spectral densities Syy(f) and Syx(f) are not necessarily real functions of
the frequency #. However, substituting the relationship

Ryy(7) = Ryx(‚Äî7}

into Equation (1.68) and then using Equation (1.69) we find that Sxy(f) and Syx(f) are
related by

Sxy(f) = Syx(-f) = Syx(f) (1.72)

& EXAMPLE 1.8

Suppose that the random processes X(t) and Y(z) have zero mean, and they are individually
stationary. Consider the sum random process

Z(t) = X(t) + Y(n)

The problem is to determine the power spectral density of Z(t).

1.7 Power Spectral Density 53

The autocorrelation function of Z() is given by
Rz{t, u) = E[Z()Z(u)]
= Ef(X(t) + Y(#))(X(#) + Y(z))]
. = ElX(√©)X(z)] + ELX(t)¬•(w)] + E[Y(2)X(w)] + ELY(2)Y¬•(x)]
= Rx(t, 4) + Ryylt, 4) + Ryx(t, 4) + Rylt, #)
Defining 7 = t ~ u, we may therefore write
Rz(t) = Rx(z) + Rxylt) + Ryx(7) + Ry(7) (1.73)
when the random processes X(t) and Y(¬¢) are also jointly stationary. Accordingly, taking the
Fourier transform of both sides of Equation (1.73), we get
S2{f) = Sx(f) + Sxv(f) + Syx(f) + Sy(f) (1.74)

We thus see that the cross-spectral densities Sxy(f) and Syx(f) represent the spectral compo-
nents that must be added to the individual power spectral densities of a pair of correlated
random processes in order to obtain the power spectral density of their sum.

When the stationary processes X(t) and Y(t) are uncorrelated, the cross-spectral densities
Sxy(f) and Syx(f) are zero, and so Equation (1.74) reduces as follows:

Salf) = Sx{f} + Sy(f) (1.75)

We may generalize this latter result by stating that when there is a multiplicity of zero-mean
stationary processes that are uncorrelated with each other, the power spectral density of their
sum is equal to the sum of their individual power spectral densities. <

 

&¬Æ EXAMPLE 1.9

Consider next the problem of passing two jointly stationary processes through a pair of sep-
arate, stable, linear, time-invariant filters, as shown in Figure 1.12. In particular, suppose that
the random process X(¬¢) is the input to the filter of impulse response /7,(t) and that the random
process Y(f) is the input to the filter of impulse response h(t). Let V(z) and Z(t) denote the
random processes at the respective filter outputs. The cross-correlation function of V(¬¢) and
Z(t) is therefore

Ryalt, w) = E[V(a)Z{u)]

= elf hy(7) X(t ‚Äî %) dr ia h(m)¬•(u ‚Äî 7) ar]

ope (1.76)
= i if hy(t)bo( m)E[X(t ‚Äî 71) Y(t ‚Äî m)] dn dr

= in [. b(t )holtm)Revlt ‚Äî Ty # ‚Äî %) dry dry
where Rxy(#, u) is the cross-correlation function of X(t) and Y(z). Because the input random
processes are jointly stationary (by hypothesis), we may set r= ¬¢ ‚Äî # and so rewrite Equation

(1.76) as follows:

Ryz(t) = [. [. by(t)ha(m)Raylt ‚Äî 1 + 2) dry dr (1.77)

wo hO Lev YQ 0 LZ)

Ficure 1.12 A pair of separate linear time-invariant filters.

 

 

 

 

 

54 CHAPTER 1 # RANDOM PROCESSES

Taking the Fourier transform of both sides of Equation (1.77) and using a procedure
similar to that which led to the development of Equation (1.39), we finally get

Sv2lf) = Ai f)Hi(f)Sxv(f) (1.78)

where H,(f) and H,(f) are the frequency responses of the respective filters in Figure 1.12, and
H3(f) is the complex conjugate of H,(f). This is the desired relationship berween the cross-
spectral density of the output processes and that of the input processes. <4

i 1.8 Gaussian Process

The material we have presented on random processes up to this point in the discussion
has been of a fairly general nature. In this section, we consider an important family of
random processes known as Gaussian processes.*

Let us suppose that we observe a random process X(f) for an interval that starts at
time t = 0 and lasts until t = T. Suppose also that we weight the random process X(t) by
some function g(t) and then integrate the product g(t)X(t) over this observation interval,
thereby obtaining a random variable Y defined by

Y= I gle)X(t) dt (1.79)

We refer to Y as a linear functional of X(t). The distinction between a function and a
functional should be carefully noted. For example, the sum Y = =‚Äú,a;X;, where the a; are
constants and the X; are random variables, is a linear fection of the X;; for each observed
set of values for the random variables X;, we have a corresponding value for the random
variable Y. On the other hand, in Equation (1.79) the value of the random variable Y
depends on the course of the argument function g(t)X(t) over the entire observation in-
terval from 0 to T. Thus a functional is a quantity that depends on the entire course of
one or more functions rather than on a number of discrete variables. In other words, the
domain of a functional is a set or space of admissible functions rather than a region of a
coordinate space.

If in Equation (1.79) the weighting function g(t) is such that the mean-square value
of the random variable Y is finite, and if the random variable Y is a Gaussian-distributed
random variable for every g(¬£) in this class of functions, then the process X(¬£) is said to be
a Gaussian process. In other words, the process X(t) is a Gaussian process if every linear
functional of X(t) is a Gaussian random variable.

We say that the random variable Y has a Gaussian distribution if its probability
density function has the form

 

 

1 (y = By)?
frly) Vines op Ja3 (1.80)

where jy is the mean and o¬• is the variance of the random variable Y. A plot of this
probability density function is given in Figure 1.13 for the special case when the Gaussian
random variable Y is normalized to have a mean py of zero and a variance a? of one, as
shown by

 

_1 _¬•
fr(y) ‚Ñ¢ Feo ")

Such a normalized Gaussian distribution is commonly written as N(0, 1).

L8 Gaussian Process 55

FY)

0,6 --

 

 

 

Ficure 1.13 Normalized Gaussian distribution.

A Gaussian process has two main virtues. First, the Gaussian process has many
properties that make analytic results possible; we will discuss these properties later in the
section. Second, the random processes produced by physical phenomena are often such
that a Gaussian model is appropriate. Furthermore, the use of a Gaussian model to describe
the physical phenomena is usually confirmed by experiments. Thus the frequent occurrence
of physical phenomena for which a Gaussian model is appropriate, together with the ease
with which a Gaussian process is handled mathematically, make the Gaussian process very
important in the study of communication systems.

@ CENTRAL Limir THEOREM

The central limit theorem provides the mathematical justification for using a Gaussian
process as a model for a large number of different physical phenomena in which the
observed random variable, at a particular instant of time, is the result of a large number
of individual random events. To formulate this important theorem, let X,,i = 1,2,...,
N, be a set of random variables that satisfies the following requirements:

1. The X; are statistically independent.

2. The X; have the same probability distribution with mean px and variance 7%.

The X; so described are said to constitute a set of independently and identically distributed
(i.i.d.) random variables. Let these random variables be normalized as follows:

¬•,=+(%)- px), i=1,2,...,N
ox
so that we have
EY] = 0

and

Define the random variable

56

CHAPTER 1 3 RANDOM PROCESSES

The central limit theorem states that the probability distribution of Vx approaches a nor-
malized Gaussian distribution N(0, 1) in the limit as the number of random variables N
approaches infinity.

It is important to realize, however, that the central limit theorem gives only the
‚Äúlimiting‚Äù form of the probability distribution of the normalized random variable Vx as
N approaches infinity. When N is finite, it is sometimes found that the Gaussian limit gives
a relatively poor approximation for the actual probability distribution of Vx even though
N may be quite large.

PROPERTIES OF A GAUSSIAN PROCESS

A Gaussian process has some useful properties that are described in the sequel.

Property 1
If a Gaussian process X(t) is applied to a stable linear filter, then the random process Y(t)
developed at the output of the filter is also Gaussian.

This property is readily derived by using the definition of a Gaussian process based
on Equation (1.79). Consider the situation depicted in Figure 1.8, where we have a linear
time-invariant filter of impulse response 4(t), with the random process X(t) as input and
the random process Y(t) as output. We assume that X(t) is a Gaussian process. The random
processes Y(t) and X(t) are related by the convolution integral

n= [pie nx 1) dt, 0O=<t< (1.81)

We assume that the impulse response /(¬¢) is such that the mean-square value of the output
random process Y(¬¢) is finite for all ¬¢ in the range 0 = ¬¢ < ¬ª for which Y(z) is defined. To
demonstrate that the output process Y(t} is Gaussian, we must show that any linear func-
tional of it is a Gaussian random variable. That is, if we define the random variable

Z= [ev [pe ~ ox 1) dr dt (1.82)

then Z must be a Gaussian random variable for every function gy(t), such that the mean-
square value of Z is finite. Interchanging the order of integration in Equation (1.82), we
get

Z= [. etoxi X(7) (1.83)

where

= if gy(t)b(t ‚Äî 1) dr (1.84)

Since X(t) is a Gaussian process by hypothesis, it follows from Equation (1.83) that Z
must be a Gaussian random variable. We have thus shown that if the input X(t) to a linear
filter is a Gaussian process, then the output Y(t) is also a Gaussian process. Note, however,

1.8 Gaussian Process 57

that although our proof was carried out assuming a time-invariant linear filter, this prop-
erty is true for any arbitrary stable linear system.

Property 2

Consider the set of random variables or samples X(t,), X(t2),..., X(t,), obtained by
observing a random process X(t) at times ty, th,..., t,. If the process X(t) is Gaussian,
then this set of random variables is jointly Gaussian for any n, with their n-fold joint
probability density function being completely determined by specifying the set of means

Bx) = E[X(@)], i= 1,2,...,0

and the set of covariance functions
Cyx(tz, t) = EU(X(t) ‚Äî Bx) X(t) ‚Äî xe), k,i=1,2,...,0

Let the #-by-1 vector X denote the set of random variables X(t,), ..., X(t,,) derived from
the Gaussian process X(t) by sampling it at times t,,..., t,. Let x denote a value of X.
According to Property 2, the random vector X has a multivariate Gaussian distribution
defined in matrix form as

 

 

1 1
Foxit) os X()(¬•1y + + + ¬ª Xa) (2m)"2ar2 exo 3 (x ‚Äî pyZix - ¬ª) (1.85)

where the superscript T denotes transposition and
p = mean vector
_ T
a [M1 May +++ Mn
Y = covariance matrix
= {Cx(te, teins
x1 = invetse of covariance matrix

A = determinant of covariance matrix ¬•

Property 2 is frequently used as the definition of a Gaussian process. However, this
definition is more difficult to use than that based on Equation (1.79) for evaluating the
effects of filtering on a Gaussian process.

We may extend Property 2 to two (or more) random processes as follows. Consider
the composite set of tandom variables X(t), X(t),..., X(t), Y(#1), Y(u2),...5 Y(4,)
obtained by observing a random process X(t) at times {t,, i= 1,2,..., 7}, and a second
random process Y(z) at times {#,, k = 1, 2,..., mm}. We say that the processes X(t) and
Y(t) are jointly Gaussian if this composite set of random variables is jointly Gaussian for
any # and m, Note that in addition to the mean and correlation functions of the random
processes X(t) and Y(t) individually, we must also know the cross-covariance function

EU(X(t) ‚Äî bextep) (Y(t) ‚Äî Myegs)] = Ryuvltis te) ‚Äî Moxiepb vey)
for any pair of observation instants (t;, #,). This additional knowledge is embodied in the
cross-correlation function, Ryy(t;, 4,), of the two processes X(t) and Y(t).
Property 3

If a Gaussian process is stationary, then the process is also strictly stationary.

This follows directly from Property 2.

58 CHAPTER 1 @ RANDOM PROCESSES

Property 4

If the random variables X(t,), X(to),..., X(t,), obtained by sampling a Gaussian process
X(t) at times ty, tz, ..., t,, are uncorrelated, that is,

E((X(t) ‚Äî pxiy)(X(E) ‚Äî exuy)] = 9, i#Xk
then these random variables are statistically independent.

The uncorrelatedness of X(t,),..., X(t,,) means that the covariance matrix & is a
diagonal matrix as shown by

of te)

where ¬∞
of = E[(X(t) - E[X(t)))", 7 = 1,2,...50

Under this condition, the multivariate Gaussian distribution of Equation (1.85) simplifies
to

where X; = X(¬¢,) and

 

fale) = a= exp( - &
xA%; Ino, ‚ÄòP 2a?

In words, if the Gaussian random variables X(¬¢,),..., X(t,,) are uncorrelated, then they
are statistically independent, which, in turn, means that the joint probability density func-
tion of this set of random variables can be expressed as the product of the probability
density functions of the individual random variables in the set.

i 1.9 Noise

The term noise is used customarily to designate unwanted signals that tend to disturb the
transmission and processing of signals in communication systems and over which we have
incomplete control. In practice, we find that there are many potential sources of noise in
a communication system. The sources of noise may be. external to the system (e.g., at-
mospheric noise, galactic noise, man-made noise), or internal to the system. The second
category includes an important type of noise that arises from spontaneous fluctuations of
current or voltage in electrical circuits.¬Æ This type of noise represents a basic limitation on
the transmission or detection of signals in communication systems involving the use of
electronic devices. The two most common examples of spontaneous fluctuations in elec-
trical circuits are shot noise and thermal noise, which are described in the sequel.

@ SHOT NOISE

Shot noise arises in electronic devices such as diodes and transistors because of the discrete
nature of current flow in these devices. For example, in a photodetector circuit a current

19 Noise 59

pulse is generated every time an electron is emitted by the cathode due to incident light
from a source of constant intensity. The electrons are naturally emitted at random times
denoted by 7, where ~% < k < &, It is assumed that the random emissions of electrons
have been going on for a long time. Thus, the total current flowing through the photo-
detector may be modeled as an infinite sum of current pulses, as shown by

X= S bt - x) (1.86)
ae
where A(t ‚Äî 7) is the current pulse generated at time 7. The process X(t) defined by
Equation (1.86) is a stationary process called shot noise.

The number of electrons, N(t), emitted in the time interval [0, ¬¢] constitutes a discrete
stochastic process, the value of which increases by one each time an electron is emitted.
Figure 1.14 shows a sample function of such a process, Let the mean value of the number
of electrons, v, emitted between times ¬¢ and t + fo be defined by

E[v] = Ato (1.87)

The parameter A is a constant called the rate of the process. The total number of electrons
emitted in the interval [t, ¬¢ + ty], that is,
p= N(t + to) ‚Äî N(t)
follows a Poisson distribution with a mean value equal to Afy. In particular, the probability
that & electrons are emitted in the interval [¬¢, t + fo] is defined by
1)
P(v = k) = re eM k=O, 1... (1.88)

Unfortunately, a detailed statistical characterization of the shot-noise process X(t)
defined in Equation (1.86) is a difficult mathematical task. Here we simply quote the results
pertaining to the first two moments of the process:

¬ª The mean of X(z) is
By =A [ h(t) dt (1.89)

where A is the rate of the process and /(t) is the waveform of a current pulse.

 

 

 

N(t)
6k ‚Äî
oo at =
4 ‚Äî
) |
| ‚Äî‚Äî‚Äî!
3 i 1 |
i |
i
2k cS |
| \
| i
1+ oo | i
hod i
hbo i
a

0 T% ¬´% Ts T] Ts t¬¢

Ficure 1.14 Sample function of a Poisson counting process.

60

CHAPTER 1 8 RANDOM PROCESSES

¬ª The autocovariance function of X(t) is
Cxy(7}) =A [ h(t)b(t + 7) dt (1.90)

This second result is known as Campbell‚Äôs theorem.

For the special case of a waveform h(f) consisting of a rectangular pulse of amplitude
A and duration T, the mean of the shot-noise process X(z) is AAT, and its autocovariance
function is

MAXT‚Äî |r|), [7] <T

Cal) = {0 |r| =T

which has a triangular form similar to that shown in Figure 1.7.

THERMAL NOISE

Thermal noise is the name given to the electrical noise arising from the random motion of
electrons in a conductor. The mean-square value of the thermal noise voltage Vr ap-
pearing across the terminals of a resistor, measured in a bandwidth of Af Hertz, is, for all
practical purposes, given by

E[V3nx] = 4&TR Af volts? (1.91)

where k is Boltzmann‚Äôs constant equal to 1.38 X 1077? joules per degree Kelvin, T is the
absolute temperature in degrees Kelvin, and R is the resistance in ohms. We may thus
model a noisy resistor by the Th√©venin equivalent circuit consisting of a noise voltage
generator of mean-square value E[V#y] in series with a noiseless resistor, as in Figure
1.15¬¢. Alternatively, we may use the Norton equivalent circuit consisting of a noise current
generator in parallel with a noiseless conductance, as in Figure 1.156. The mean-square
value of the noise current generator is

2 = 1 2.
Ellin] = 3 ElVin (1.92)
= 4kTG Af amps‚Äù
where G = 1/R is the conductance. It is also of interest to note that because the number
of electrons in a resistor is very large and their random motions inside the resistor are
statistically independent of each other, the central limit theorem indicates that thermal
noise is Gaussian distributed with zero mean.

ELtRy] G

ElVin

(a) (b)

Figure 1.15 Models of a noisy resistor. (2) Th√©venin equivalent circuit. (b) Norton equivalent
circuit.

1.9 Noise 61

Noise calculations involve the transfer of power, and so we find that the use of the
maximum-power transfer theorem is applicable to such calculations. This theorem states
that the maximum possible power is transferred from a source of internal resistance R to
a load of resistance R; when R; = R. Under this matched condition, the power produced
by the source is divided equally between the internal resistance of the source and the load
resistance, and the power delivered to the load is referred to as the available power. Ap-
plying the maximum-power transfer theorem to the Th√©venin equivalent circuit of Figure
1.154 or the Norton equivalent circuit of Figure 1.156, we find that a noisy resistor pro-
duces an available noise power equal to kT Af watts.

e Waite NOISE

The noise analysis of communication systems is customarily based on an idealized form
of noise called white noise, the power spectral density of which is independent of the
operating frequency. The adjective white is used in the sense that white light contains equal
amounts of all frequencies within the visible band of electromagnetic radiation. We express
the power spectral density of white noise, with a sample function denoted by w(t), as

N
Sw(f) = > (1.93)
which is illustrated in Figure 1.164. The dimensions of No are in watts per Hertz. The
parameter N, is usually referenced to the input stage of the receiver of a communication
system. Jt may be expressed as

No = kT, (1.94)

where & is Boltzmann‚Äôs constant and T, is the equivalent noise temperature of the receiver.‚Äù
The equivalent noise temperature of a system is defined as the temperature at which a
noisy resistor has to be maintained such that, by connecting the resistor to the input of a
noiseless version of the system, it produces the same available noise power at the output
of the system as that produced by all the sources of noise in the actual system. The im-
portant feature of the equivalent noise temperature is that it depends only on the param-
eters of the system.

Since the autocorrelation function is the inverse Fourier transform of the power
spectral density, it follows that for white noise

= No

Rela) = 5

&(7) (1.95)

Swf) Ry)

N,
Ng 3 &(r)

 

 

ie} . ie}
(a) ()

Figure 1.16 Characteristics of white noise. (2) Power spectral density. (b) Autocorrelation
function.

62

CHAPTER 1 RANDOM PROCESSES

That is, the autocorrelation function of white noise consists of a delta function weighted
by the factor No/2 and occurring at t = 0, as in Figure 1.166. We note that Ry(7) is zero
for + # 0. Accordingly, any two different samples of white noise, no matter how closely
together in time they are taken, are uncorrelated. If the white noise w(t) is also Gaussian,
then the two samples are statistically independent. In a sense, white Gaussian noise rep-
resents the ultimate in ‚Äúrandomness.‚Äù

Strictly speaking, white noise has infinite average power and, as such, it is not phys-
ically realizable. Nevertheless, white noise has simple mathematical properties exemplified
by Equations (1.93) and (1.95), which make it useful in statistical system analysis.

The utility of a white noise process is parallel to that of an impulse function or delta
function in the analysis of linear systems. Just as we may observe the effect of an impulse
only after it has been passed through a system with a finite bandwidth, so it is with white
noise whose effect is observed only after passing through a similar system. We may state,
therefore, that as long as the bandwidth of a noise process at the input of a system is
appreciably larger than that of the system itself, then we may model the noise process as
white noise.

¬Æ EXAMPLE 1.10 Ideal Low-Pass Filtered White Noise

Suppose that a white Gaussian noise w(t) of zero mean and power spectral density No/2 is
applied to an ideal low-pass filter of bandwidth B and passband magnitude response of one.
The power spectral density of the noise (¬£) appearing at the filter output is therefore (see

Figure 1.17)
No
Sif)= 4? ‚ÄúBSF <8 (1.96)
0, |f|>B

The autocorrelation function of v(t) is the inverse Fourier transform of the power spectral
density shown in Figure 1.174:

B
No .
Rat) = ‚Äú= exp(j2af7) df
J ‚ÄúB2 (1.97)
= NoB sinc(2Br)

This autocorrelation function is plotted in Figure 1.17. We see that Rx(7) has its maximum
value of NoB at the origin, and it passes through zero at 7 = +k/2B, where k = 1, 2, 3,---.

Sw) Ry(s)
2 NoB

    

 

-B 0 B ‚Äò1. 3
B 2B

 

{a) 0)

Ficure 1.17 Characteristics of low-pass filtered white noise. (2) Power spectral density. (b) Auto-
correlation function.

1.9 Noise 63

Since the input noise w(t) is Gaussian (by hypothesis), it follows that the band-limited
noise 7(t) at the filter output is also Gaussian. Suppose now that 7(t) is sampled at the rate of
2B times per second. From Figure 1.176, we see that the resulting noise samples are uncor-
related and, being Gaussian, they are statistically independent. Accordingly, the joint proba-
bility density function of a set of noise samples obtained in this way is equal to the product
of the individual probability density functions. Note that each such noise sample has a mean
of zero and variance of NoB. <j

m Examece 1.11 Correlation of White Noise with a Sinusoidal Wave

Consider the sample function

+

w'(t) = a) w(t) cos(2f-t) dt (1.98)
which is the output of a correlator with white Gaussian noise w(t) and sinusoidal wave
V2/T cos(27f.t) as inputs; the scaling factor V2/T is included here to make the sinusoidal
wave input have unit energy over the interval 0 <= ¬¢ = T. (This problem was encountered in
the Background and Preview chapter but was not elaborated on at that time.) With the noise

w(t) having zero mean, it immediately follows that the correlator output tw‚Äô (t) has zero mean,
too. The variance of the correlator output is defined by

2 (T(t
G=E [25 [ w{t,) cos(2zf,t,)w(t2) cos(Zaf.to) dt, at, |
=2 i ‚Äò i "E t 2 2 dt, d
=F Io Jo [vo(ty)w(t2)] cos(2rf-t,) cos(2af.t,) dt, dt,

2 T rT
= =[ [ Ryty, t2)-cos(27f,t)) cos(2aft,) dt, dt,

where Ryw(t,, t2) is the autocorrelation function of the white noise w(t). But from Equation
(1.95):

N,
Rwlt, b) = > Alt, ‚Äî t3)

where No/2 is the power spectral density of the white noise w(t). Accordingly, we may simplify
the expression for the variance o‚Äù as

aN f' S's ‚Äî t) cos(2af,t,) cos(2af,t2) dt, dt.
= 3 Po Jo i 2) cos(2af,t1) cos(2af.tz) dt, dt,
We now invoke the sifting property of the delta function, namely,

in g(t) 5(t) dt = g(0)

where g(t) is a continuous function of time, assuming the value g(0) at time t = 0. Hence, we
may further simplify o‚Äù as

N 2,7,
= 2.5 d
a 7 Fd, 98 (Qaf.t) dt (1.99)
No
2

where it is assumed that the frequency f, of the sinusoidal wave input is an integer multiple
of the reciprocal of T.

64 CHAPTER I RANDOM PROCESSES

Sy(f)

 

Sf.-B fe Sf. +B 0 F.-B fp fp +B

 

(a)

FIGURE 1.18 (a) Power spectral density of narrowband noise. (b) Sample function of narrow-
band noise.

: 1.10 Narrowband Noise

The receiver of a communication system usually includes some provision for preprocessing
the received signal. The preprocessing may take the form of a narrowband filter whose
bandwidth is just large enough to pass the modulated component of the received signal
essentially undistorted but not so large as to admit excessive noise through the receiver.
The noise process appearing at the output of such a filter is called narrowband noise. With
the spectral components of narrowband noise concentrated about some midband fre-
quency +f, as in Figure 1.182, we find that a sample function n(¬¢) of such a process appears
somewhat similar to a sine wave of frequency f,, which undulates slowly in both amplitude
and phase, as illustrated in Figure 1.185.

To analyze the effects of narrowband noise on the performance of a communication
system, we need‚Äòa mathematical representation of it. Depending on the application of
interest, there are two specific representations of narrowband noise:

1. The narrowband noise is defined in terms of a pair of components called the in-phase
and quadrature components.

2. The narrowband noise is defined in terms of two other components called the en-
velope and phase.

These two representations are described in what follows. For now it suffices to say that
given the in-phase and quadrature components, we may determine the envelope and phase
components, and vice versa: Moreover, in their own individual ways, the two represen-
tations are not only basic to the noise analysis of communication systems but also to the
characterization of narrowband noise itself.

1.11 Representation of Narrowband Noise
in Terms of In-Phase and Quadrature Components

 

 

Consider a narrowband noise n(z) of bandwidth 2B centered on frequency f,, as illustrated
in Figure 1.18. In light of the theory of band-pass signals and systems presented in Ap-
pendix 2, we may represent 7(¬¢) in the canonical (standard) form:

n(t) = n,(t) cos(2mf.t) ‚Äî o(t) sin(2af,t) (1.100)

1.11 Ie-Phase and Quadrature Components 65

where x,(t) is called the in-phase component of n(t), and no(t) is called the quadrature
component of n(t). Both m,(t) and mo(t) are low-pass signals. Except for the midband
frequency f,, these two components are fully representative of the narrowband noise n(t}.

Given the narrowband noise x(t), we may extract its in-phase and quadrature com-
ponents using the scheme shown in Figure 1.194. It is assumed that the two low-pass filters
used in this scheme are ideal, each having a bandwidth equal to B (i.e., one-half the band-
width of the narrowband noise n(t}). The scheme of Figure 1.194 follows from the rep-
resentation of Equation (1.100). We may, of course, use this equation directly to generate
the narrowband noise x(¬¢), given its in-phase and quadrature components, as shown in
Figure 1.196, The schemes of Figures 1.194 and 1.196 may thus be viewed as narrowband
noise analyzer and synthesizer, respectively.

The in-phase and quadrature components of a narrowband noise have important
properties that are summarized here:

1, The in-phase component m,(t) and quadrature component no(t) of narrowband noise
n(t) have zero mean.

2. If the narrowband noise ¬ª(t) is Gaussian, then its in-phase component n;(t) and
quadrature component Q(t) are jointly Gaussian.

3. If the narrowband noise x(t) is stationary, then its in-phase component n;(t) and
quadrature component Q(t) are jointly stationary.

4. Both the in-phase component #,(¬¢) and quadrature component 79(t) have the same
power spectral density, which is related to the power spectral density Sy(f) of the
narrowband noise a(t) as

 

Suif ~ fe} + Swf + fs B=f=B

1.101
0, otherwise ( )

Suf) = Syolf) = {
where it is assumed that Sx(f) occupies the frequency interval f. - B=|f|<f,+B,
and f, > B.

5. The in-phase component #;(t) and quadrature component 79(t) have the same vari-
ance as the narrowband noise n(t).

6. The cross-spectral density of the in-phase and quadrature components of narrow-
band noise x(t) is purely imaginary, as shown by

Sunglf) = ‚ÄîSnon(f) (1.102)
ISMf +f.) -SNf-‚Äîf), ‚Äî-BsfsB

0, otherwise

+

 
 

Low-pass
filter

  

nytt) y(t)

  
  
 
     
 

nt) 2 cos (2mf, 1) cos (27rf.2} nd)
Low-pass t ¬Æ
fitter ng) agit
~2 sin (2nf,i) sin (2af..2)
(a) {b)

Figure 1.19 (a) Extraction of in-phase and quadrature components of a narrowband process.
(b) Generation of a narrowband process from its in-phase and quadrature components.

66 CHAPTER 1 ¬© RANDOM PROCESSES

7, If the narrowband noise v(t) is Gaussian and its power spectral density Sy(?) is sym-
metric about the mid-band frequency f,, then the in-phase component #;(t) and
quadrature component 7o(t) ate statistically independent.

For further discussions of these properties, the reader is referred to Problems 1.28 and
1.29.

¬Æ EXAMPLE 1.12 Ideal Band-Pass Filtered White Noise

Consider a white Gaussian noise of zero mean and power spectral density No/2, which is
passed through an ideal band-pass filter of passband magnitude response equal to one, mid-
band frequency f., and bandwidth 2B. The power spectral density characteristic of the filtered
noise #(t) will therefore be as shown in Figure 1.20¬¢. The problem is to determine the auto-
correlation functions of (t) and its in-phase and quadrature components.

The autocorrelation function of (¬¢) is the inverse Fourier transform of the power spec-
tral density characteristic shown in Figure 1.20a:

ft B No . ff tB No .

Ry(7) = Lo. 2 exp(j2afr) df + Ios > exp(j2afr) df
= NpB sine(2B1)[exp(‚Äîj2-af-7) + exp(i2nf7 (1.103)
== 2NoB sinc(2Br) cos(27f,7)

which is plotted in Figure 1.206.

The spectral density characteristic of Figure 1.20a is symmetric about +f.. Therefore,
we find that the corresponding spectral density characteristic of the in-phase noise component

 

  
     

Tt 0 fe m
2B

(@)

Sy, (f= Sy ff)

No

 

 

 

-B O B

 

(b) {ec}

FIGURE 1.20 Characteristics of ideal band-pass filtered white noise. (2) Power spectral density.
(b) Autocorrelation function. (c) Power spectral density of in-phase and quadrature components.

1.12 Envelope and Phase Components 67

n,(t) or the quadrature noise component #o(¬¢) is as shown in Figure 1.21c. The autocorrelation
function of 7;() or not) is therefore (see Example 1.10):

Rut) = Ry, (7) = 2NoB sinc(2B7) (1.104)

4
1.12 Representation of Narrowband Noise
| in Terms of Envelope and Phase Components

In Section 1.11 we considered the representation of a narrowband noise n(¬£) in terms of
its in-phase and quadrature components. We may also represent the noise m(t) in terms of
its envelope and phase components as follows:

n(t) = r(t}) cos[2af.t + ¬•(2)] (1.105)
where
r(t) = [pi (t) + np(e)]‚Äô? (1.106)
and
= tan-1| Za)
y(t) = tan | ale | (1.107)

The function r(t) is called the envelope of n(t), and the function s(t) is called the phase of
nit).

The envelope r(t) and phase y(¬¢) are both sample functions of low-pass random
processes. As illustrated in Figure 1.18), the time interval between two successive peaks
of the envelope r(t) is approximately 1/B, where 2B is the bandwidth of the narrowband
noise n(t).

The probability distributions of 7(t) and #(t) may be obtained from those of 7,(t)
and no(t) as follows. Let N; and Ng denote the random variables obtained by observing
(at some fixed time) the random processes represented by the sample functions n;(t) and
no(t), respectively. We note that N; and No are independent Gaussian random variables
of zero mean and variance o*, and so we may express their joint probability density func-
tion by

 

1 nt + nh
fryng(tr Ng) = Imot exo( - 32 (1.108)
Accordingly, the probability of the joint event that N; lies between 2, and n, + dn; and
that Ng lies between mg and #g + dig (i.e., the pair of random variables N, and Ng lies
jointly inside the shaded area of Figure 1.212) is given by

1 ne + ne
Fix,nol"¬ª MQ) dn; dng = Ino¬Æ en 8) dn; dng (1.109)
Define the transformation (see Figure 1.212)
ny, = r cosy (1.110)
Ao =r sings - (1.111)

In a limiting sense, we may equate the two incremental areas shown shaded in Figures
1.21¬¢@ and 1.21b and thus write

dn, dng = 1 dr di (1.112)

68

CHAPTER 1 @ RANDOM PROCESSES

 

 

 

 

 

‚Äî>| day; Na
im dr
ang | a

i) nna‚Äù + 0 eens eae‚Äù
|

| |

‚Äù i rool

i

]

aby i

% #\,
0 nr ie] Ay

{a) (b)

FIGURE 1.21 Illustrating the coordinate system for representation of narrowband noise: (a) in
terms of in-phase and quadrature components, and (b) in terms of envelope and phase.

Now, let R and ¬• denote the random variables obtained by observing (at some time f) the
random processes represented by the envelope 7(¬¢) and phase #/(¬¢), respectively. Then,
substituting Equations (1.110)-(1.112) into (1.109), we find that the probability of the
random variables R and lying jointly inside the shaded area of Figure 1.21b is equal to

(ex dr d
Imo Pag} ¬•
That is, the joint probability density function of R and ¬• is

r r?
faovlt, ¬•) = yO exp(- 5) (1.113)

This probability density function is independent of the angle #, which means that the
tandom variables R and W are statistically independent. We may thus express fa,y(7, ¬•)
as the product of fg(r) and fy(ip). In particular, the random variable V representing phase
is uniformly distributed inside the range 0 to 27, as shown by

1

fol) 412g? OS USA (1.114)
0, elsewhere
This leaves the probability density function of the random variable R as
r 7 =0
felr) = Jer OP aot (1.115)
0, elsewhere

where a‚Äù is the variance of the original narrowband noise n(¬¢). A random variable having
the probability density function of Equation (1.115) is said to be Rayleigh distributed.‚Äô
For convenience of graphical presentation, let

v= (1.116)

;
tog
fulv) = ofglr) (1.117)

1.13 Sine Wave Plus Narrowband Noise 69

 

 

0 1 2 3

FicureE 1.22 Normalized Rayleigh distribution.

Then we may rewrite the Rayleigh distribution of Equation (1.115) in the normalized form

vy
fo(v) = ver(-5), veo (1.118)
0, elsewhere

Equation (1.118) is plotted in Figure 1.22. The peak value of the distribution fy(v) occurs
at v=1 and is equal to 0.607. Note also that, unlike the Gaussian distribution, the Rayleigh
distribution is zero for negative values of v. This is because the envelope 7(t) can assume
only nonnegative values.

i 1.13 Sine Wave Plus Narrowband Noise

Suppose next that we add the sinusoidal wave A cos(27f,t) to the narrowband noise n(t),
where A and f, are both constants. We assume that the frequency of the sinusoidal wave
is the same as the nominal carrier frequency of the noise. A sample function of the sinu-
soidal wave plus noise is then expressed by

x(t) = A cos(2mf.t) + v(t) (1.119)

Representing the narrowband noise x(t) in terms of its in-phase and quadrature compo-
nents, we may write

 

 

 

x(t) = n}(t) cos(2rf.t) ‚Äî molt) sin(27f,2) (1.120)
where
ni(t) = A + n,(t) (1.121)
We assume that a(t) is Gaussian with zero mean and variance o*. Accordingly, we may
state the following:
1. Both 7;(t) and Q(t) are Gaussian and statistically independent.
2. The mean of #;(t) is A and that of #9(t) is zero.
3. The variance of both #/(t) and no(t) is o¬∞.

We may therefore express the joint probability density function of the random variables
Ni and No, corresponding to #}(t) and #o(t), as follows:

1 ni ~ AP + n2
fri.nglth 9) = 5 exp| (ni aT 8 (1.122)

 

 

70

CHAPTER 1 & RANDOM PROCESSES

Let r(#) denote the envelope of x(t) and y(t) denote its phase. From Equation (1.120),
we thus find that

r(t) = {[ni(e)P + m(e)y? (1.123)
and
w(t) = en | 20 | (1.124)
y(t)

Following a procedure similar to that described in Section 1.12 for the derivation of the
Rayleigh distribution, we find that the joint probability density function of the random
variables R and‚Äô, corresponding to r(t) and y(t) for some fixed time #, is given by

2+ A? ‚Äî 2A
fault, ¬•) = i exp( PS Brent)

(1.125)
We see that in this case, however, we cannot express the joint probability density function
fr vlr, #) asa product fa(7)fu(W). This is because we now have a term involving the values
of both random variables multiplied together as 7 cos ¬•. Hence, R and W are dependent
random variables for nonzero values of the amplitude A of the sinusoidal wave component.

We are interested, in particular, in the probability density function of R. To determine
this probability density function, we integrate Equation (1.125) over all possible values of
Ww obtaining the marginal density

ar

falr) = 0 frawlt, W) dp

24 A2\ 2" A
one ¬∞xP (-" 2 ) j &xP (4 ¬£os v) ay

The integral in the right-hand side of Equation (1.126) can be identified in terms of the
defining integral for the modified Bessel function of the first kind of zero order (see Ap-
pendix 3); that is,

(1.126)

 

 

1 ag
I(x) = on [, exp(x cosy) di (1.127)

Thus, letting x = Ar/o‚Äù, we may rewrite Equation (1.126) in the compact form:

felt) =, exp ae *Yio(45) (1.128)

This relation is called the Rician distribution.‚Äù
As with the Rayleigh distribution, the graphical presentation of the Rician distribu-
tion is simplified by putting

 

 

var (1.129)
a= A (1.130)

ren
folv) = ofzl(r) (1.131)

1.14 Computer Experiments: Flat-Fading Channel 71

0.6 a=0

Fy)

 

 

 

 

FIGURE 1,23 Normalized Rician distribution.

Then we may express the Rician distribution of Equation (1.128) in the normalized form
2

folv) = ver(-¬• ; 2 Yiotaw (1.132)

 

which is plotted in Figure 1.23 for the values 0, 1, 2, 3, 5, of the parameter a. Based on
these curves, we may make the following observations:

1, When a is zero, the Rician distribution reduces to the Rayleigh distribution.

2. The envelope distribution is approximately Gaussian in the vicinity of v= a when a
is large, that is, when the sine-wave amplitude A is large compared with o, the square
root of the average power of the noise m(¬¢).

1.14 Computer Experiments:

Flat-Fading Channel
In this section we use computer simulations to study a multipath channel characterized by
Rayleigh fading, examples of which arise in wireless communications and long-range radio
transmission via the ionosphere. Fading occurs because of interference between different
versions of the transmitted signal, which reach the receiver at correspondingly different
times. The net result is that the received signal can vary widely in both amplitude and
phase. Under certain conditions, the statistical time-varying nature of the received signal‚Äôs
envelope is closely described by a Rayleigh distribution as demonstrated herein.

Figure 1.24 presents a model of a multipath channel. It consists of a large collection
of scatterers randomly positioned in space, whereby a single incident beam is converted
into a correspondingly large number of scattered beams at the receiving antenna. The
transmitted signal is set equal to A cos(27f-t). It is assumed that all the scattered beams
travel at the same mean velocity. However, they differ from each other in amplitude and
phase by virtue of differences in path loss and path delay. Thus the Ath scattered beam is

>given by A, cos(27f,t + @,), where the amplitude A, and phase @, are random variables
that vary slowly with time. Moreover, the ¬©, are all independent of one another and

 

 

 

 

 

72

CHapTer 1 RANDOM PROCESSES

      
 

    
   

 
 

Random
medium

Scattered
beams

Incident

 

Transmitting Receiving
antenna ¬∞ antenna

Ficure 1.24 Model of a multipath channel.

uniformly distributed inside the interval [0, 277]. The type of fading exhibited by the mul-
tipath channel described herein is referred to as ‚Äúflat fading‚Äù because the spectral char-
acteristics of the transmitted signal are completely preserved at the channel output. How-
ever, the strength of the channel output changes with time due to random fluctuations in
the gain of the channel! caused by the multipath phenomenon.

Summing the contributions of all the scatterers, assumed to be N in number, we may
express the random process representing the received signal as

N
X(t) = >) A, cos(2af.t + @,) (1.33)
k=l
which may be rewriten in the equivalent form
X(t) = X; cos(2af,t) ‚Äî Xg sin(2afZ) (1.134)
where the X; and Xq are respectively defined by
N
X, = >, A, cos Oy (1.135)
kat
and
N
Xo= 3D) A, sin @¬Æ (1.136)
√©=1

For convenience of presentation and without loss of generality, we may assume that A,
lies in the closed interval [~‚Äî1, 1] for all z.

Experiment 1. Gaussian Distributions

From the central limit theorem we note that as the number of scatterers, N, approaches
infinity, both X; and Xg should approach Gaussian random variables. To test the validity
of this statement, the probability distributions of the in-phase component X; and quad-
rature component Xg are computed for N = 10, 100, 1000, and 10,000. To test the
validity of the central limit theorem, we need a measure of the goodness-of-fit that tests
the equivalence of the measured probability distribution of the sampled data for varying

1.14 Computer Experiments: Flat-Fading Channel 73

N to the theoretical Gaussian distribution. One way of performing such a test is to use
central moments of a distribution (up to order 4) to define the following two parameters:

2
U3
= 1.137
Pr ua ( )
and
f= (1.138)
He

where fz, Hs, and 14 are the second, third, and fourth central moments, respectively. The
parameters f, and > together provide a measure of the skezwness of the distribution under
test. The closer the values 8, and B, for the measured distribution are to the corresponding
ones for the theoretical distribution, the better is the goodness-of-fit. For a Gaussian ran-
dom variable X of mean px and variance o% we have

He = 0%
3 = 0
Ha = 30%
which yield
8, = 0
and
B= 3

Table 1.1 presents the values of 8, and 8, computed for both the in-phase component X;
and quadrature component Xg for varying N. Comparing these values with the corre-
sponding ones for a Gaussian distribution, we clearly see that as the number of scatterers,
N, increases the distributions of both X; and Xg do approach a zero-mean Gaussian
distribution in accordance with the central limit theorem.

| Taste 1.1 B Values for in-phase and quadrature components

(a) Measured Distribution

 

 

 

 

 

 

 

 

 

 

Number of Scatterers, N

 

 

 

 

10 100 1000 10,000
In-phase component X; Ba 0.2443 0.0255 0.0065 0.0003
fe 2.1567 2.8759 2.8587 3.0075

 

 

0.0874 0.0017 0.0004 0.0000

Bi
fo 1.9621 2.7109 3.1663 3.0135 |

(b) Theoretical Distribution: Gaussian
B, =0
B, = 3

 

Quadrature component Xo

 

 

 

 

 

 

 

 

 

74

CHAPTER 1 = RANDOM PROCESSES

Experiment 2. Rayleigh Distribution

In Equation (1.134) the random process X(t) is expressed in terms of its in-phase and
quadrature components. Equivalently, we may express X(¬¢) in terms of its envelope and
phase as

X(t) = R cos(2af,t + V) (1.139)
where
R= Vx? + XB (1.140)
and
Xx
= tan72{ (2
WY = tan (32) (1.141)

Note that in the experiments considered here the in-phase component X;, quadrature
component Xo, envelope R, and phase ¬• are all independent of time.

If X; and Xp approach Gaussian random variables for increasing N, then from the
theory presented in Section 1.12 we note that the envelope R will approach a Rayleigh
distribution, and the phase VW will approach a uniform distribution. In Figure 1.25 we
present the actual probability density function of r for data generated for the case of
N = 10,000, with 100 histograms and 100 ensemble averages being computed. This figure
also includes the theoretical curve. There is close agreement between these two curves,
substantiating the assertion that the envelope R of the received signal approaches a Ray-
leigh distribution.

Figure 1.26 illustrates the effect of Rayleigh fading on the waveform of the received
signal x(t}, a sample function of X(t), for the case of a sinusoidal transmitted signal with
unit amplitude (i.e., A = 1) and frequency f, = 1 MHz. Specifically, the transmitted signal
and the corresponding received signal are shown in parts a and b of Figure 1.26, respec-
tively. Comparing these two waveforms, we see that transmission through the multipath

 

9.774 Tt 4

0.6

 

 

 

 

FIGURE 1.25 Probability density function of the envelope of random process X(¬¢): comparing
theory and experiment.

1.15 Summary and Discussion 75

 

Amplitude
lo}

  

 

 

 

0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Time(s) x 10&

(a)

 

Amplitude

 

 

 

al
a
oa

=i
0 0.5 1 1.5 2 2.5 3 3.5
Time(s) x 108
(4)

FIGURE 1.26 Effect of Rayleigh fading on a sinusoidal wave. (a) Input sinusoidal wave.
(b) Waveform of the resulting signal.

channel of Figure 1,24 has resulted in a received signal whose amplitude and phase com-
ponents vary randomly with time, as expected.

[1.15 Summary and Discussion

Much of the material presented in this chapter has dealt with the characterization of a
particular class of random processes known to be stationary and ergodic. The implication
of (wide-sense) stationarity is that we may develop a partial description of a random
process in terms of two ensemble-averaged parameters: (1) a mean that is independent of
time, and (2) an autocorrelation function that depends only on the difference between the
times at which two observations of the process are made.‚Äô¬∞ Ergodicity enables us to use
time averages as ‚Äúestimates‚Äù of these parameters. The time averages are computed using
a sample function (i.e., single realization) of the random process.

Another important parameter of a random process is the power spectral density. The
autocorrelation function and the power spectral density constitute a Fourier-transform
pair. The formulas that define the power spectral density in terms of the autocorrelation
function and vice versa are known as the Einstein-Wiener-Khintchine relations.

In Table 1.2 we present a graphical summary of the autocorrelation functions and
power spectral densities of important random processes. All the processes described in this
table are assumed to have zero mean and unit variance. This table should give the reader
a feeling for (1) the interplay between the autocorrelation function and power spectral
density of a random process, and (2) the role of linear filtering in shaping the autocorre-
lation function or, equivalently, the power spectral density of a white noise process.

The latter part of the chapter dealt with a noise process that is Gaussian and nar-
rowband, which is the kind of filtered noise encountered at the front end of an idealized
form of communication receiver. Gaussianity means that the random variable obtained by

76 CHAPTER | & RANDOM PROCESSES

densities of random processes of zero mean and unit variance

 

 

 

 

TABLE 1.2. Graphical summary of autocorrelation functions and power spectral

 

Type of Process, X(t)
Sinusoidal process of unit

frequency and random
phase

Random binary wave of unit
symbol-duration

RC low-pass filtered white
noise

Ideal low-pass filtered white
noise

Ideal band-pass filtered
white noise

RLC-filtered white noise

 

 

 

 

Autocorrelation Function, Rx(t)

Power Spectral Density, Sx(f)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2.9
~0.5 a 0.5
f
L
-0.5 0 05 f
0.5F
kL
i
‚Äú1.0 0 10. Ff

 

 

 

 

 

 

 

Notes and References 77

observing the output of the filter at some fixed time has a Gaussian distribution. The
narrowband nature of the noise means that it may be represented in terms of an in-phase
and a quadrature component. These two components are both low-pass, Gaussian pro-
cesses, each with zero mean and a variance equal to that of the original narrowband noise.
Alternatively, a Gaussian narrowband noise may be represented in terms of a Rayleigh-
distributed envelope and a uniformly distributed phase. Each of these representations has
its own specific area of application, as shown in subsequent chapters of the book.

i NoTES AND REFERENCES

1. For a rigorous treatment of random processes, sce the classic books of Doob (1953), Lo√©ve
(1963), and Cram√©r and Leadbetter (1967).

2. There is another important class of random processes commonly encountered in practice,
the mean and autocorrelation function of which exhibit periodicity, as in

Hx(ty + T) = px(ty)
Rx(ty + T, th + T) = Rx(ty, tr)

for all t; and t,. A random process X(t) satisfying this pair of conditions is said to be
cyclostationary (in the wide sense). Modeling the process X(z) as cyclostationary adds a
new dimension, namely, period T to the partial description of the process. Examples of
cyclostationary processes include a television signal obtained by raster-scanning a random
video field, and a modulated process obtained by varying the amplitude, phase, or fre-
quency of a sinusoidal carrier. For detailed discussion of cyclostationary processes, see
Pranks (1969), pp. 204-214, and the paper by Gardner and Franks (1975).

3. Traditionally, Equations (1.42) and (1.43) have been referred to in the literature as the
Wiener-Khintchine relations in recognition of pioneering work done by Norbert Wiener
and A. I. Khintchine; for their original papers, see Wiener (1930) and Khintchine (1934).
A discovery of a forgotten paper by Albert Einstein on time-series analysis (delivered at the
Swiss Physical Society‚Äôs February 1914 meeting in Basel) reveals that Einstein had discussed
the autocorrelation function and its relationship to the spectral content of a time series
many years before Wiener and Khintchine. An English translation of Einstein‚Äôs paper is
reproduced in the IEEE ASSP Magazine, vol. 4, October 1987. This particular issue also
contains articles by W. A. Gardner and A. M. Yaglom, which elaborate on Einstein‚Äôs
original work.

4. For further details of power spectrum estimation, see Blackman and Tukey (1958), Box
and Jenkins (1976), Marple (1987), and Kay (1988).

5. The Gaussian distribution and associated Gaussian process are named after the great math-
ematician C. F. Gauss. At age 18, Gauss invented the method of least squares for finding
the best value of a sequence of measurements of some quantity. Gauss later used the method
of least squares in fitting orbits of planets to data measurements, a procedure that was
published in 1809 in his book entitled Theory of Motion of the Heavenly Bodies. In con-
nection with the error of observation, he developed th√© Gaussian distribution. This distri-
bution is also known as the normal distribution. Partly for historical reasons, mathemati-
cians commonly use the term normal, while engineers and physicists commonly use the
term Gaussian.

6. For a detailed treatment of electrical noise, see Van der Ziel (1970) and the collection of
papers edited by Gupta (1977).

An introductory treatment of shot noise is presented in Helstrom (1990). For a more de-
tailed treatment, see the paper by Yue, Luganani, and Rice (1978).

78 CHAPTER 1 & RANDOM PROCESSES

10.

i PROBLEMS

Thermal noise was first studied experimentally by J. B. Johnson in 1928, and for this reason
it is sometimes referred to as the Johnson noise. Johnson‚Äôs experiments were confirmed
theoretically by Nyquist (1928).

. The noisiness of a receiver may also be measured in terms of the so-called noise figure. The

relationship between the noise figure and the equivalent noise temperature is developed in
Chapter 8.

. The Rayleigh distribution is named after the English physicist J. W. Strutt, Lord Rayleigh.
. The Rician distribution is named in honor of Stephen O. Rice for the original contribution

reported in a pair of papers published in 1944 and 1945, which are reproduced in Wax
(1954). ;

The statistical characterization of communication systems presented in this book is con-
fined to the first two moments, mean and autocorrelation function (equivalently, autoco-
variance function) of the pertinent random process. However, when a random process is
transmitted through a nonlinear system, valuable information is contained in higher-order
moments of the resulting output process. The parameters used to characterize higher-order
moments in the time domain are called cumulants, and their multidimensional Fourier
transforms are called polyspectra. For a discussion of higher-order cumulants and polys-
pectra and their estimation, see the paper by Nikias and Raghuveer (1987).

Stationarity and Ergodicity
1.1 Consider a random process X(t) defined by

X(t) = sin(27f,t)

in which the frequency f, is a random variable uniformly distributed over the interval
[0, W]. Show that X(t) is nonstationary. Hint: Examine specific sample functions of the
random process X(z) for the frequency f = W/4, W/2, and W, say.

1.2 Consider the sinusoidal process

1.3

X(t) = A cos(27rf.t)
where the frequency f, is constant and the amplitude A is uniformly distributed:
‚Äî fy Osa=l
fala) = {3 otherwise

Determine whether or not this process is strictly stationary.
A random process X(t) is defined by

X(t) = A cos(27rf,t)
where A is a Gaussian-distributed random variable of zero mean and variance 04. This
random process is applied to an ideal integrator, producing the output

Y(t) = [ xa dr

(a) Determine the probability density function of the output ¬•(#) at a particular time t,.
(b) Determine whether or not Y(t) is stationary.
(c) Determine whether or not Y(t) is ergodic.

1.4 Let X and Y be statistically independent Gaussian-distributed random variables, each with

zero mean and unit variance. Define the Gaussian process
Z(t) = X cos(2at) + ¬• sin(27)

Problems 79

(a) Determine the joint probability density function of the random variables Z(t,) and
Z(t) obtained by observing Z(¬£) at times t, and t,, respectively.
(b) Is the process Z(t) stationary? Why?

Correlation and Spectral Density Functions

1.5 Prove the following two properties of the autocorrelation function Rx(r) of a random
process X(t):
(a) If X(t) contains a DC component equal to A, then Rx(7) will contain a constant
component equal to A‚Äù.
(b) If X(t) contains a sinusoidal component, then Rx(7) will also contain a sinusoidal
component of the same frequency.
1.6 The square wave x(t) of Figure P1.6 of constant amplitude A, period To, and delay tz,
represents the sample function of a random process X(t). The delay is random, described
by the probability density function

1
= Fy == 57
frAta) = 4 ¬∞¬∞ .
0, otherwise

(a) Determine the probability density function of the random variable X(r,) obtained by
observing the random process X(t) at time t,.

(b) Determine the mean and autocorrelation function of X(t) using ensemble-averaging.

(c) Determine the mean and autocorrelation function of X(t) using time-averaging.

(d) Establish whether or not X(¬¢) is stationary. In what sense is it ergodic?

xt)

tule

FIcure P1.6

1.7 A binary wave consists of a random sequence of symbols 1 and 0, similar to that described
in Example 1.3, with one basic difference: symbol 1 is now represented by a pulse of
amplitude A volts and symbol 0 is represented by zero volts. All other parameters are the
same as before. Show that for this new random binary wave X(z):

(a) The autocorrelation function is

A? A? |7|
fy+S (1-40 <
4¬∞ 4 ( pT) l<t
A?
4¬∞?

Rx(7) =
|7|=T

(b) The power spectral density is
A? AT.
Sx(f) = ve af) + es sinc?(fT)

What is the percentage power contained in the DC component of the binary wave?

80 CuapTeR 1 & RANDOM PROCESSES

1.8 A random process Y(t) consists of a DC component of 3/2 volts, a periodic component
g(t), and a random component X(t). The autocorrelation function of Y(t) is shown in
Figure P1.8.

(a) What is the average power of the periodic component g(t}?
(b) What is the average power of the random component X(#)}?

Ry (7)
Wolts)?

 

 

 

 

 

FIGURE P1.8

1.9 Consider a pair of stationary processes X(t) and Y(t). Show that the cross-correlations
Rxy(r) and Ryx(z) of these processes have the following properties:
(a) Rxy(7) = Ryx(‚Äî7)
(b) | Rxv{7) | = Z[Rx(0) + Ry(0)]
where Ry(7) and Ry(7) are the autocorrelation functions of X(t) and Y(t), respectively.
1.10 Consider two linear filters connected in cascade as in Figure P1.10. Let X(t) be a stationary
process with autocorrelation function Rx(r). The random process appearing at the first
filter output is V(t) and that at the second filter output is Y(t).
(a) Find the autocorrelation function of Y(t).
(b) Find the cross-correlation function Ryy(r) of V(t) and Y(t).

Vv
XQ‚Äî ‚Äî Aft) 0

FIGURE P1.10

 

 

 

 

1.11 A stationary process: X(¬¢) is applied to a linear time-invariant filter of impulse response
A(t), producing an output Y(¬¢).
(a) Show that the cross-correlation function Ryx(7) of the output Y(t) and the input X(t)
is equal to the impulse response 4(r) convolved with the autocorrelation function
Rx(7) of the input, as shown by

Ryx(1) = if h(u)Rx(t ‚Äî u) du
Show that the second cross-correlation function Rx (7) equals
Ryy(t} = if h(‚Äîu)Rx(1 ‚Äî 4) du

(b) Find the cross-spectral densities Syx(f) and Sxy(f)}.

Problems 81

(c) Assuming that X(t) is a white noise process with zero mean and power spectral density
N,/2, show that

Ryx(a) = 32 b(n)

Comment on the practical significance of this result.
1.12 The power spectral density of a random process X(t) is shown in Figure P1.12. It consists

of a delta function at f = 0 and a triangular component.

(a) Determine and sketch the autocorrelation function Rx(1) of X(t).

(b) What is the DC power contained in X(t)?

(c) What is the AC power contained in X(t)?

(d) What sampling rates will give uncorrelated samples of X(t)? Are the samples statis-
tically independent?

Sf)
6(f)

1.0

fo 0 to
Figure P1.12

1.13 A pair of noise processes 1,(t) and ,(t) are related by
ni(t) = n,(t) cos(2afit + 6) ~ 2,(t) sin(2af,t + 0)
where f, is a constant, and @ is the value of a random variable ¬© whose probability density
function is defined by

i
fol¬Æ) = 422¬∞
0, otherwise

Os¬¢@527

The noise process ,(t) is stationary and its power spectral density is as shown in Figure
P1.13. Find and plot the corresponding power spectral density of 72(t).

Sy ,f)

 

 

-W Q w
Figure P1.13

1.14 A random telegraph signal X(t), characterized by the autocorrelation function
Rx(z) = exp(‚Äî2v|7|)

82

Cuaprer 1 & RANDOM PROCESSES

where vis a constant, is applied to the low-pass RC filter of Figure P1.14. Determine the
power spectral density and autocorrelation function of the random process at the filter
output.

R
Input Cc f Output

Figure P1.14

1.15 A running integrator is defined by

where x(t) is the input, y(t) is the output, and T is the integration period. Both x(#) and
y(t) are sample functions of stationary processes X(t) and Y(i), respectively. Show that
the power spectral density of the integrator output is related to that of the integrator input
as

Sof) =-T? sinc(fT)Sx(f)

1.16 A zero-mean stationary process X(t) is applied to a linear filter whose impulse response
is defined by a truncated exponential:

ae~‚Ñ¢. Osr1sT
b(t) = ;
) {√© otherwise

Show that the power spectral density of the filter output Y(t) is defined by

ae
Sy(f) a + 4p f? (1

 

 

2 exp(‚ÄîaT) cos(2mfT) + exp(‚Äî2aT))Sx(f)

where Sx(f) is the power spectral density of the filter input.
1.17 The output of an oscillator is described by

X(t) = A cos(2nft ‚Äî ¬©)

where A is a constant, and f and ¬© are independent random variables. The probability
density function of @ is defined by

1
fol¬Æ) = 420‚Äù
0, otherwise

Os 0527

Find the power spectral density of X(t) in terms of the probability density function of the
frequency f. What happens to this power spectral density when the frequency f assumes
a constant value?

Problems 83

Gaussian Processes

1.18 A stationary, Gaussian process X(t) has zero mean and power spectral density Sx(f).
Determine the probability density function of a random variable obtained by observing
the process X(t) at some time t,.

1.19 A Gaussian process X(t) of zero mean and variance oX is passed through a full-wave
rectifier, which is described by the input-output relation of Figure P1.19. Show that the
probability density function of the random variable Y(t,), obtained by observing the ran-
dom process Y(t) at the rectifier output at time f,, is as follows:

21. a =0
Fryugly) = T Ox P 2% I? y=

0, y<0

 

 

Ficure P1.19

1.20 Let X(t) be a zero-mean, stationary, Gaussian process with autocorrelation function
Rx(7). This process is applied to a square-law device, which is defined by the input-output
relation

Y(t) = X?(2)
where Y(t) is the output.
(a) Show that the mean of Y(t) is Rx(0).
(b) Show that the autocovariance function of Y(t) is 2R3(r).

1.21 A stationary, Gaussian process X(t) with mean px and variance 7% is passed through two
linear filters with impulse responses /,(t) and /2(t), yielding processes Y(t) and Z(t), as
shown in Figure P1.21.

(a) Determine the joint probability density function of the random variables Y(t,) and
Z(t).

(b) What conditions are necessary and sufficient to ensure that Y(t,) and Z(t,) are statis-
tically independent?

 

AO > yi)

 

X(1)

hl) pe 20)

 

 

 

Ficure P1.21

84 Cwaprer 1 & RANDOM PROCESSES

1.22 A stationary, Gaussian process X(t) with zero mean and power spectral density Sx(f) is
applied to a linear filter whose impulse response 4(¬¢) is shown in Figure P1,22. A sample
Y is taken of the random process at the filter output at time T.

(a) Determine the mean and variance of Y.
(b) What is the probability density function of Y?

A(t)

me

Q T
Figure P1.22

Noise
1.23 Consider a white Gaussian noise process of zero mean and power spectral density No/2
that is applied to the input of the high-pass RL filter shown in Figure P1.23.

(a) Find the autocorrelation function and power spectral density of the random process
at the output of the filter.

(b) What are the mean and variance of this output?

Input L Output

FiGure P1.23

1.24 A white noise w(t) of power spectral density No/2 is applied to a Butterworth low-pass
filter of order n, whose magnitude response is defined by

1
¬© [L + (f/f)?
(a) Determine the noise equivalent bandwidth for this low-pass filter. (See Appendix 2
for the definition of noise equivalent bandwidth.)
(b) What is the limiting value of the noise equivalent bandwidth as # approaches infinity?
1.25 The shot-noise process X(t) defined by Equation (1.86) is stationary. Why?
1.26 White Gaussian noise of zero mean and power spectral density N,/2 is applied to the

filtering scheme shown in Figure P1.26a. The frequency responses of these two filters are
shown in Figure P1.26b. The noise at the low-pass filter output is denoted by n(¬¢).

(a) Find the power spectral density and the autocorrelation function of n(t).
(b) Find the mean and variance of x(t).

|Hif)|

White
noise

Problems 85

(c) What is the rate at which w(t) can be sampled so that the resulting samples are essen-
tially uncorrelated?

 

 

 

 

HAC f
Band-pass Low-pass 1.0
filter filter Output
Hf) HAfY n)
f
cos (27rf,t) fe 0 Jf . i 0 . f
2B 2B
(a) )

Ficure P1.26

1.27 Let X(t) be a stationary process with zero mean, autocorrelation function Ry(7), and
power spectral density $x(f). We are required to find a linear filter with impulse response
h(t), such that the filter output has the same statistical characteristics as X(t) when the
input is white noise of power spectral density N,/2.
(a) Determine the condition which the impulse response /(t) must satisfy to achieve this
requirement.
(b) What is the corresponding condition on the frequency response H(f) of the filter?

Narrowband Noise

1.28 In the noise analyzer of Figure 1.192, the low-pass filters are ideal with a bandwidth equal
to one-half that of the narrowband noise x(t) applied to the input. Using this scheme,
derive the following results:

(a) Equation (1.101), defining the power spectral densities of the in-phase noise com-
ponent #;(¬¢) and quadrature noise component (tf) in terms of the power spectral
density of n(t).

(b) Equation (1.102), defining the cross-spectral densities of 2;(t) and no(t).

1.29 Assume that the narrowband noise n(t) is Gaussian and its power spectral density Sy(f)
is symmetric about the midband frequency f.. Show that the in-phase and quadrature
components of x(t) are statistically independent.

1.30 The power spectral density of a narrowband noise n(t) is as shown in Figure P1.30. The
carrier frequency is 5 Hz.

(a) Find the power spectral densities of the in-phase and quadrature components of n(t).

(b) Find their cross-spectral densities.

Sy(f)
(W/Hz)

 

F(Hz)

 

~7 -5-4 0 45 7
Figure P1.30

86 CHAPTER 1 & RANDOM PROCESSES

1.31 Consider a Gaussian noise (t) with zero mean and the power spectral density Sy(f)
shown in Figure P1.31.
(a) Find the probability density function of the envelope of x(t).
(b) What are the mean and variance of this envelope?

Swf)

 

Sf 9 L |
2B

Figure P1.31

Computer Experiments

1.32. In this computer experiment we study the statistical characterization of a random process

X(t) defined by
X(t) = A cos(2mrf,t + ¬©) + Wit)

where the phase @ of the sinusoidal component is a uniformly distributed random variable
over the interval [‚Äîa, 7], and Wit) is a white Gaussian noise component of zero mean
and power spectral density No/2. The two components of X(t) are statistically indepen-
dent; hence the autocorrelation function of X(t) is

A Ni
Rx(7) = ~~ cos(2af,1) + ‚Äî ar)
2 2

This equation shows that for |7| > 0 the autocorrelation function Rx(z) has the same

sinusoidal waveform as the signal component of X(t).

The purpose of this computer experiment is to perform the computation of Rx(7)
using two different methods:

(a) Ensemble averaging. Generate M = 50 randomly picked realizations of the process
X(t). Hence compute the product x(t + 7)x(t) for some fixed time t, where x(t) is a
realization of X(t). Repeat the computation of x(t + 7)x(t) for the M realizations of
X(t), and thereby compute the average of these computations over M. Repeat this
sequence of computations for- different values of 7.

(b) Time averaging. Compute the time-averaged autocorrelation function

T

R,(1, T) = == zt + q)x(t) dt

where x(t) is a particular realization of X(t), and 2T is the total observation interval.
For this computation, use the Fourier-transform pair:

Raley T) = 5 |Xalf)|?

Problems 87

where | Xr(f)|?/2T is the periodogram of the process X(t). Specifically, compute the
Fourier transform X7(f) of the time-windowed function
(t) x(t), -T<xt=T
x(t) =
r 0, otherwise

Hence compute the inverse Fourier transform of | Xr(f){7/2T.

Compare the results of your computation of Rx(7) using these two approaches.

1.33 In this computer experiment we continue the study of the multipath channel described in
Section 1.14. Specifically, consider the situation where the received signal includes a line-
of-sight component, as shown by

N
X(t) = 3 A, cos(2aft + @,) + 4 cos(2nfz)
a=
where a cos(27f,t) is the directly received component. Following the material presented

in Section 1.14, compute the envelope of X(¬£) for N = 10,000, and a = 0, 1, 2, 3, 5.
Compare your results with the Rician distribution studied in Section 1.13.

 

CONTINUOUS-WAVE
MODULATION

In this chapter we study continuous-wave modulation, which is basic to the operation of
analog communication systems. The chapter is divided into two related parts. In the first
part we study the time-domain and frequency-domain descriptions of two basic families of
continuous-wave modulation: 7

¬ª Amplitude modulation, in which the amplitude of a sinusoidal carrier is varied in
accordance with an incoming message signal.

¬Æ Angle modulation, in which the instantaneous frequency or phase of the sinusoidal carrier
is varied in accordance with the message signal.

The second part of the chapter focuses on the effects of channel noise on the performance
of the receivers pertaining to these modulation schemes.

Advantages and disadvantages of the different methods of continuous-wave
modulation are highlighted in light of the material presented herein.

i 2.1 Introduction

The purpose of a communication system is to transmit information-bearing signals through
a communication channel separating the transmitter from the receiver. Information-
bearing signals are also referred to as baseband signals. The term baseband is used to
designate the band of frequencies representing the original signal as delivered by a source
of information. The proper use of the communication channel requires a shift of the range
of baseband frequencies into other frequency ranges suitable for transmission, and a cor-
responding shift back to the original frequency range after reception. For example, a radio
system must operate with frequencies of 30 kHz and upward, whereas the baseband signal
usually contains frequencies in the audio frequency range, and so some form of frequency-
band shifting must be used for the system to operate satisfactorily. A shift of the range of
frequencies in a signal is accomplished by using modulation, which is defined as the process
by which some characteristic of a carrier is varied in accordance with a modulating wave
(signal). A common form of the carrier is a sinusoidal wave, in which case we speak ofa
continuous-wave modulation‚Äô process. The baseband signal is referred to as the modulat-
ing wave, and the result of the modulation process is referred to as the modulated wave.
Modulation is performed at the transmitting end of the communication system. At the
receiving end of the system, we usually require the original baseband signal to be restored.
This is accomplished by using a process known as demodulation, which is the reverse of.
the modulation process.

In basic signal-processing terms, we thus find that the transmitter of an analog com-
munication system consists of a modulator and the receiver consists of a demodulator, as

88

2.1 Introduction 89

Message _ Modulated Channel Estimate of
signal Medulater wave output = Demodulater -‚Äîs message signal

(b)

 

 

 

 

Sinusoidal
carrier wave

(a)

Figure 2.1 Components of a continuous-wave modulation system: (a) transmitter, and (b)
receiver.

depicted in Figure 2.1. In addition to the signal received from the transmitter, the receiver
input includes channel noise. The degradation in receiver performance due to channel noise
is determined by the type of modulation used.

In this chapter we study two families of continuous-wave (CW) modulation systems,
namely, amplitude modulation and angle modulation. In amplitude modulation, the am-
plitude of the sinusoidal carrier wave is varied in accordance with the baseband signal. In
angle modulation, the angle of the sinusoidal carrier wave is varied in accordance with the
baseband signal. Figure 2.2 displays the waveforms of amplitude-modulated and angle-
modulated signals for the case of sinusoidal modulation. Parts (a) and (6) of the figure
show the sinusoidal carrier and modulating waves, respectively. Parts (c) and (d) show the

(a)

 

(¬ª)

 

(a) Time ‚Äî>

Figure 2.2 Illustrating AM and FM signals produced by a single tone. (2) Carrier wave. (b)
Sinusoidal modulating signal. (c) Amplitude-modulated signal. (2) Frequency-modulated signal.

90 CHAPTER 2 @ CONTINUOUS-WAVE MODULATION

corresponding amplitude-modulated and frequency-modulated waves, respectively; fre-
quency modulation is a form of angle modulation. This figure clearly illustrates the basic
differences between amplitude modulation and angle modulation, which are discussed in
what follows.

| 2.2 Amplitude Modulation

Consider a sinusoidal carrier wave c(t) defined by
c(t) = A, cos(2af,t) (2.1)

where A, is the carrier amplitude and f, is the carrier frequency. To simplify the exposition
without affecting the results obtained and conclusions reached, we have assumed that the
phase of the carrier wave is zero in Equation (2.1). Let s(t) denote the baseband signal
that carries the specification of the message. The source of carrier wave c(z) is physically
independent of the source responsible for generating m(t). Amplitude modulation (AM) is
defined as a process in which the amplitude of the carrier wave c(t) is varied about a mean
value, linearly with the baseband signal m(t). An amplitude-modulated (AM) wave may
thus be described, in its most general form, as a function of time as follows:

s(t) = A,[1 + &,m(t)] cos(2af.t) (2.2)

where k, is a constant called the amplitude sensitivity of the modulator responsible for the
generation of the modulated signal s(¬¢). Typically, the carrier amplitude A, and the message
signal m(t) are measured in volts, in which case &, is measured in volt‚Ñ¢*.

Figure 2.34 shows a baseband signal m(z), and Figures 2.3 and 2.3¬¢ show the cor-
responding AM wave s(t) for two values of amplitude sensitivity &, and a carrier amplitude
A, = 1 volt. We observe that the envelope of s(t) has essentially the same shape as the

baseband signal m(t) provided that two requirements are satisfied:
1, The amplitude of ¬£,#m(t) is always less than unity, that is,
|km(t)|<1 for allt (2.3)

This condition is illustrated in Figure 2.36; it ensures that the function 1 + kit)
is always positive, and since an envelope is a positive function, we may express the
envelope of the AM wave s(t) of Equation (2.2) as A,[1 + k,m(t)]. When the am-
plitude sensitivity &, of the modulator is large enough to make | k,m(t)| > 1 for any
t, the carrier wave becomes overmodulated, resulting in carrier phase reversals when-
ever the factor 1 + &,1m(t) crosses zero. The modulated wave then exhibits envelope
distortion, as in Figure 2.3c. It is therefore apparent. that by avoiding overmodula-
tion, a one-to-one relationship is maintained between the envelope of the AM wave
and the modulating wave for all values of time-~a useful feature, as we shall see later
on. The absolute maximum value of k,7(f) multiplied by 100 is referred to as the
percentage modulation.

2. The carrier frequency f, is much greater than the highest frequency component W of
the message signal m(t), that is

>> Ww (2.4)

We call W the message bandwidth. If the condition of Equation (2.4) is not satisfied,
an envelope cannot be visualized (and therefore detected) satisfactorily.

2.2 Amplitude Modulation 91

mit)

 

(a)

s(t) sft)

 

 

 

 

 

 

 

 

 

(b) {c}

Figure 2.3 [lustrating the amplitude modulation process. (a) Baseband signal m(t). (b) AM
wave for | k,nt(t)| < 1 for all t. (c) AM wave for |k,m(t)| > 1 for some t.

From Equation (2.2), we find that the Fourier transform of the AM wave s(¬£) is given
by

sin = Sear ‚Äî y+ or + p+ MEM + Mr + 2S)

Suppose that the baseband signal m(¬£) is band-limited to the interval -W = f = W, as in
Figure 2.4a. The shape of the spectrum shown in this figure is intended for the purpose of
illustration only. We find from Equation (2.5) that the spectrum S(f) of the AM wave is
as shown in Figure 2.46 for the case when f, > W. This spectrum consists of two delta
functions weighted by the factor A,/2 and occurring at +f., and two versions of the
baseband spectrum translated in frequency by +f, and scaled in amplitude by &,A./2.
From the spectrum of Figure 2.46, we note the following:

1. As a result of the modulation process, the spectrum of the message signal s(t) for
negative frequencies extending from ‚Äî W to 0 becomes completely visible for positive
(i.e., measurable) frequencies, provided that the carrier frequency satisfies the con-
dition f, > W; herein lies the importance of the idea of ‚Äúnegative‚Äù frequencies.

2. For positive frequencies, the portion of the spectrum of an AM wave lying above the
carrier frequency f, is referred to as the upper sideband, whereas the symmetric
portion below f, is referred to as the lower sideband. For negative frequencies, the
upper sideband is represented by the portion of the spectrum below ~f, and the
lower sideband by the portion above ‚Äîf,. The condition f, > W ensures that
the sidebands do not overlap.

92

CHAPTER 2 CoNTINUOUS-WAVE MODULATION

M(f) s(f)

WwW

3.

  
     
    
   

Al -
Far -6

 

M(O)
$k, AMO)
Upper _‚Äî Upper
sideband sideband
0 Ww f fe fw f
{a} ()

FIGURE 2.4 (a) Spectrum of baseband signal. (b) Spectrum of AM wave.

For positive frequencies, the highest frequency component of the AM wave equals
f+ W, and the lowest frequency component equals f. ‚Äî W. The difference between
these two frequencies defines the transmission bandwidth B for an AM wave, which
is exactly twice the message bandwidth W, that is,

Br = 2W ; (2.6)

@ VIRTUES AND LIMITATIONS OF AMPLITUDE MODULATION

Amplitude modulation is the oldest method of performing modulation. Its greatest virtue
is the simplicity of implementation:

¬ª In the transmitter, amplitude modulation is accomplished using a nonlinear device.

For example, in the switching modulator discussed in Problem 2.3, the combined
sum of the message signal and carrier wave is applied to a diode, with the carrier
amplitude being large enough to swing across the characteristic curve of the diode.
Fourier analysis of the voltage developed across a resistive load reveals the generation
of an AM component, which may be extracted by means of a band-pass filter.

In the receiver, amplitude demodulation is also accomplished using a nonlinear de-
vice. For example, we may use a simple and yet highly effective circuit known as the
envelope detector, which is discussed in Problem 2.5. The circuit consists of a diode
connected in series with the parallel combination of a capacitor and load resistor.
Some version of this circuit is found in most commercial AM radio receivers. Pro-
vided that the carrier frequency is high enough and the percentage modulation is less
than 100 percent, the demodulator output developed across the load resistor is nearly
the same as the envelope of the incoming AM wave, hence the name ‚Äúenvelope
detector.‚Äù

Recall, however, that transmitted power and channel bandwidth are our two primary

communication resources, and they should be used efficiently. In this context, we find that
the standard form of amplitude modulation defined in Equation (2.2) suffers from two
major limitations:

1.

Amplitude modulation is wasteful of power. The carrier wave c(t) is completely
independent of the information-bearing signal m(t). The transmission of the carrier
wave therefore represents a waste of power, which means that in amplitude modu-
lation only a fraction of the total transmitted power is actually affected by m(t).

2.3 Linear Modulation Schemes 93

2. Amplitude modulation is wasteful of bandwidth. The upper and lower sidebands of
an AM wave are uniquely related to each other by virtue of their symmetry about
the carrier frequency; hence, given the magnitude and phase spectra of either side-
band, we can uniquely determine the other. This means that insofar as the transmis-
sion of information is concerned, only one sideband is necessary, and the commu-
nication channel therefore needs to provide only the same bandwidth as the baseband
signal. In light of this observation, amplitude modulation is wasteful of bandwidth
as it requires a transmission bandwidth equal to twice the message bandwidth.

To overcome these limitations, we must make certain modifications: suppress the
carrier and modify the sidebands of the AM wave. These modifications naturally result in
increased system complexity. In effect, we trade system complexity for improved use of
communication resources. The basis of this trade-off is linear modulation, which is dis-
cussed in the next section. In a strict sense, full amplitude modulation does not qualify as
linear modulation because of the presence of the carrier wave.

i 2.3 Linear Modulation Schemes

In its most general form, linear modulation is defined by
s(t) = s,(t) cos(2mf.2) 7 Sq(t) sin(27f.t) (2.7)

where s;(t} is the in-phase component of the modulated wave s(t), and so{t) is its quad-
rature component, Equation (2.7) is recognized as the canonical representation of a nar-
rowband signal, which is discussed in detail in Appendix 2. In linear modulation, both
s(t) and so{t) are low-pass signals that are linearly related to the message signal 7(t).

Indeed, depending on how these two components of s(z) are defined, we may identify
three types of linear modulation involving a single message signal:

1. Double sideband-suppressed carrier (DSB-SC) modulation, where only the upper and
lower sidebands are transmitted.

2. Single sideband (SSB) modulation, where only one sideband (the lower sideband or
the upper sideband) is transmitted.

3. Vestigial sideband (VSB) modulation, where only a vestige (i.e., trace) of one of the
sidebands and a correspondingly modified version of the other sideband are
transmitted.

Table 2.1 presents a summary of the definitions of these three special forms of linear
modulation: There are two important points to note from Table 2.1:

1. The in-phase component s;{Z) is solely dependent on the message signal ##(f}.

2. The quadrature component so(t) is a filtered version of m(t). The spectral modifi-
cation of the modulated wave s(t) is solely due to so(t).

To be more specific, the role of the quadrature component (if present) is merely to interfere
with the in-phase component, so as to reduce or eliminate power in one of the sidebands
of the modulated signal s(t), depending on how the quadrature component is defined.

94 CuapTer 2 & CONTINUOUS-WAVE MODULATION

i Taste 2.1 Different forms of linear modulation

 

 

 

 

In-Phase Quadrature
Component Component
Type of Modulation 5,(t) Sq(t) Comments
DSB-SC m(t) 0 m(t) = message signal
SSB:*
(a) Upper sideband im(t) 4ya(t) *(t) = Hilbert transform of m(t)
transmitted .
(b) Lower sideband dm(t) ‚Äîjeh(t)
transmitted
VSB:
(a) Vestige of lower sideband im(t) din‚Äô (t) m‚Äô(t) = output of the filter of
transmitted frequency response Ho(f)
(b) Vestige of upper mit) ‚Äî}m'(t) due to m(t).
sideband transmitted For the definition of Ho(f),

see Eq. (2.16)

 

 

 

 

 

 

"For the mathematical description of single sideband modulation, see Problem 2.16.

# DOUBLE SIDEBAND-SUPPRESSED CARRIER (DSB-SC) MODULATION

 

This form of linear modulation is generated by using a product modulator that simply
multiplies the message signal m1(t) by the carrier wave A, cos(27f,2), as illustrated in Figure
2.5a. Specifically, we write

 

s(t) = Agn(t) cos(27f,t) (2.8)
mit)
DSB-SC
Baseband Product
signal m(e) modulator 72" modulated wave
s(t) = A,m{2) cos (2af,,4)

 

 

 

 

I ,

Carrier
A, 008 (2f,4)

(a) (b)

s(t)

Phase reversals

 

 

 

{c)

FIGURE 2.5 (a) Block diagram of product modulator. (b) Baseband signal. (c) DSB-SC modu-
lated wave.

2.3 Linear Modulation Schemes 95
MUP) stp)

M(O) $A-AM(O)

 

 

Ww 0 W | _| 0 _ he _
ow 2w
(a) (b)
FiGureE 2.6 (a) Spectrum of baseband signal. (b) Spectrum of DSB-SC madulated wave.

Figure 2.5¬¢ shows the modulated signal s(¬¢) for the arbitrary message waveform of Figure
2.5b. The modulated signal s(t) undergoes a phase reversal whenever the message signal
m(t) crosses zero. Consequently, the envelope of a DSB-SC modulated signal is different
from the message signal; this is unlike the case of an AM wave that has a percentage
modulation less than 100 percent.

From Equation (2.8), the Fourier transform of s(t) is obtained as

S(f) = 5 AdMUr ‚Äî f) + M(f + fo] (2.9)

For the case when the baseband signal m(z) is limited to the interval -W = f = W, as in
Figure 2.6¬¢, we thus find that the spectrum S(f) of the DSB-SC wave s(t) is as illustrated
in Figure 2.6b. Except for a change in scale factor, the modulation process simply translates
the spectrum of the baseband signal by +f.. Of course, the transmission bandwidth re-
quired by DSB-SC modulation is the same as that for amplitude modulation, namely, 2W.

COHERENT DETECTION

The baseband signal m(t) can be uniquely recovered from a DSB-SC wave s(t) by first
multiplying s(t) with a locally generated sinusoidal wave and then low-pass filtering the
product, as in Figure 2.7. It is assumed that the local oscillator signal is exactly coherent
or synchronized, in both frequency and phase, with the carrier wave c(t) used in the prod-
uct modulator to generate s(¬¢). This method of demodulation is known as coherent detec-
tion or synchronous demodulation.

It is instructive to derive coherent detection as a special case of the more general
demodulation process using a local oscillator signal of the same frequency but arbitrary
phase difference @, measured with respect to the carrier wave c(t). Thus, denoting the local

 

utd

Product

modulator

Low-pass

so filter

> uv, (0)

 

 

 

hai cos (2rf,t + 6)

 

Local
oscillator

 

 

 

Figure 2.7 Coherent detector for demodulating DSB-SC modulated wave.

96

CHAPTER 2. & CONTINUOUS-WaAVE MODULATION

vif)
$4, Az M(O) cos

4A, AL MO)

 

 

an ee ee

 

Ficure 2.8 Illustrating the spectrum of a product modulator output with a DSB-SC modulated
wave as input.

oscillator signal by A‚Äò cos(2af.t + #), and using Equation (2.8) for the DSB-SC wave s(2),
we find that the product modulator output in Figure 2.7 is

u(t) = Al cos(2af,t + )s(t)
= A.A! cos(2 aft) cos(2mf,t + b)m(t) (2.10)

= 5 AAL cos(4af,t + b)m(t) + 5 AAL cos @ m(t)
The first term in Equation (2.10) represents a DSB-SC modulated signal with a carrier
frequency 2f., whereas the second term is proportional to the baseband signal #(t). This
is further illustrated by the spectrum V(f) shown in Figure, 2.8, where it is assumed that
the baseband signal 7n(2) is limited to the interval ~ W = f = W. It is therefore apparent
that the first term in Equation (2.10) is removed by the low-pass filter in Figure 2.7,
provided that the cut-off frequency of this filter is greater than W but less than 2f, ‚Äî W.
This requirement is satisfied by choosing f, > W. At the filter ourput we then obtain a
signal given by

v(t) = 5 AA! cos & m({t) (2.11)

The demodulated signal v,(t) is therefore proportional to 7(z) when the phase error @
is a constant. The amplitude of this demodulated signal is maximum when ¬¢ = 0, and
it is minimum (zero) when @ = +2/2. The zero demodulated signal, which occurs for
= +72, represents the quadrature null effect of the coherent detector. Thus the phase
error ¬¢ in the local oscillator causes the detector output to be attenuated by a factor equal
to cos @. As long as the phase error ¬¢ is constant, the detector provides an undistorted
version of the original baseband signal s(t). In practice, however, we usually find that the
phase error ¬¢ varies randomly with time, due to random variations in the communication
channel, The result is that at the detector output, the multiplying factor cos ¬¢ also varies
randomly with time, which is obviously undesirable. Therefore, provision must be made
in the system to maintain the local oscillator in the receiver in perfect synchronism, in both
frequency and phase, with the carrier wave used to generate the DSB-SC modulated signal
in the transmitter. The resulting system complexity is the price that must be paid for
suppressing the carrier wave to save transmitter power.

a CosTas RECEIVER

One method of obtaining a practical synchronous receiver system, suitable for demodu-
lating DSB-SC waves, is to use the Costas receiver? shown in Figure 2.9. This receiver

2.3 Linear Modulation Schemes 97

 

Echannet
1
SA, cos ¬¢ m(t)
Product Low-pass ‚Äú Demodulated
modulator [> filter signal

 

 

 

cos (2arf.t+ 6)

Voltage-

Phase
controlled discriminator
oscillator

 

 

 

 

 

DSB-SC signal
A, cos (2rrf,t) m(t)

 

‚Äî90¬∞
phase-shifter

 

| sin (2af,1 + 6)

 

Product Low-pass
modulator > filter

 

 

 

 

44, sin @ m(d)

 

@-channe!

FIGURE 2.9 Costas receiver.

consists of two coherent detectors supplied with the same input signal, namely, the incom-
ing DSB-SC wave A, cos(2af,z)m(t), but with individual local oscillator signals that are
in phase quadrature with respect to each other. The frequency of the local oscillator is
adjusted to be the same as the carrier frequency f., which is assumed known a priori. The
detector in the upper path is referred to as the in-phase coherent detector or I-channel,
and that in the lower path is referred to as the quadrature-phase coherent detector or
Q-channel. These two detectors are coupled together to form a negative feedback system
designed in such a way as to maintain the local oscillator synchronous with the carrier
wave. .

To understand the operation of this receiver, suppose that the local oscillator signal
is of the same phase as the carrier wave A,.cos(27f.t) used to generate the incoming
DSB-SC wave. Under these conditions, we find that the I-channel output contains the
desired demodulated signal m(t), whereas the Q-channel output is zero due to the quad-
rature null effect of the Q-channel. Suppose next that the local oscillator phase drifts from
its proper value by a small angle @ radians. The I-channel output will remain essentially
unchanged, but there will now be some signal appearing at the O-channel output, which
is proportional to sin @ = ¬¢ for small ¬¢. This Q-channel output will have the same polarity
as the I-channel output for one direction of local oscillator phase drift and opposite po-
larity for the opposite direction of local oscillator phase drift. Thus, by combining the
I- and Q-channel outputs in a phase discriminator (which consists of a multiplier followed
by a low-pass filter), as shown in Figure 2.9, a DC control signal is obtained that auto-
matically corrects for local phase errors in the voltage-controlled oscillator.

It is apparent that phase control in the Costas receiver ceases with modulation and
that phase-lock has to be reestablished with the reappearance of modulation. This is not
a serious problem when receiving voice transmission, because the lock-up process normally
occurs so rapidly that no distortion is perceptible.

QUADRATURE-CARRIER MULTIPLEXING

The quadrature null effect of the coherent detector may also be put to good use in the
construction of the so-called quadrature-carrier multiplexing or quadrature-amplitude

98

CHaPTeER 2 CONTINUOUS-WAVE MODULATION

Message
signal m,(z)

Message
signal nm (1)

 

   
  

Product + Multiplexed Product Low-
le) pass
modulator signal s() modulator filter Agmy¬Æ

 

= Ap cos (Zaft) t~ 2 cos (27f,1)

 

 

 

 

 

 

 

 

 

 

 

 

90¬∞ Multiplexed 90¬∞
90¬∞ ional ~
phase-shifter signal s() phase-shifter
| A, Sin (2af,2) | 2sin (2af,0)
Product Product Low-pass A
modulator modulator [> filter Acari
fa) {b)

Figure 2.10 Quadrature-carrier multiplexing system. (¬¢) Transmitter. (b) Receiver.

modulation (QAM). This scheme enables two DSB-SC modulated waves (resulting from
the application of two physically independent message signals) to occupy the same channel
bandwidth, and yet it allows for the separation of the two message signals at the receiver
output. It is therefore a bandwidth-conservation scheme.

A block diagram of the quadrature-carrier multiplexing system is shown in Figure
2.10. The transmitter part of the system, shown in Figure 2.10a, involves the use of two
separate product modulators that are supplied with two carrier waves of the same fre-
quency but differing in phase by ‚Äî90 degrees. The transmitted signal s(t) consists of the
sum of these two product modulator outputs, as shown by

s(t) = Agny(t) cos(2af.t) + Agma{t) sin(27f.2) (2,12)

where #,(t) and #,(t) denote the two different message signals applied to the product
modulators. Thus s(t) occupies a channel bandwidth of 2W centered at the carrier fre-
quency f., where W is the message bandwidth of m4(t) or m,(t). According to Equation
(2.12), we may view A,m,(t) as the in-phase component of the multiplexed band-pass
signal s(t) and ‚ÄîAgm,(t} as its quadrature component.

The receiver part of the system is shown in Figure 2.106. The multiplexed signal s(t)
is applied simultaneously to two separate coherent detectors that are supplied with two
local carriers of the same frequency but differing in phase by ‚Äî90 degrees. The output of
the top detector is A,m,(t), whereas the output of the bottom detector is A,2(t). For the
system to operate satisfactorily, it is important to maintain the correct phase and frequency
relationships between the local oscillators used in the transmitter and receiver parts of the
system.

To maintain this synchronization, we may send a pilot signal outside the passband
of the modulated signal. In this method, the pilot signal typically consists of a low-power
sinusoidal tone whose frequency and phase are related to the carrier. wave c(¬£); at the
receiver, the pilot signal is extracted by means of a suitably tuned circuit and then trans-
lated to the correct frequency for use in the coherent detector.

@ SINGLE-SIDEBAND MODULATION

In single-sideband modulation, only the upper or lower sideband is transmitted. We may
generate such a modulated wave by using the frequency-discrimination method that con-
sists of two stages:

2.3 Linear Modulation Schemes 99

¬ª The first stage is a product modulator, which generates a DSB-SC modulated wave.

¬Æ The second stage is a band-pass filter, which is designed to pass one of the sidebands
of this modulated wave and suppress the other.

From a practical viewpoint the most severe requirement of SSB generation using the fre-
quency discrimination method arises from the unwanted sideband. The nearest frequency
component of the unwanted sideband is s√©parated from the desired sideband by twice the
lowest frequency component of the message (modulating) signal. The implication here is
that for the generation of an SSB modulated signal to be possible, the message spectrum
must have an energy gap centered at the origin, as illustrated in Figure 2.114. This require-
ment is naturally satisfied by voice signals, whose energy gap is about 600 Hz wide (i.e.,
it extends from ~300 to +300 Hz). Thus, assuming that the upper sideband is retained,
the spectrum of the SSB modulated signal is as shown in Figure 2.11.

In designing the band-pass filter used in the frequency-discriminator for generating
a SSB-modulated wave, we must meet the three basic requirements:

¬ª The desired sideband lies inside the passband of the filter.
¬Æ The unwanted sideband lies inside the stopband of the filter.

> The filter‚Äôs transition band, which separates the passband from the stopband, is twice
the lowest frequency component of the message signal.

This kind of frequency discrimination usually requires the use of highly selective filters,
which can only be realized in practice by means of crystal resonators.

To demodulate a SSB modulated signal s(t), we may use a coherent detector, which
multiplies s(t) by a locally generated carrier and then low-pass filters the product. This
method of demodulation assumes perfect synchronism between the oscillator in the co-
herent detector and the oscillator used to supply the carrier wave in the transmitter. This
requirement is usually met in one of two ways:

¬Æ A low-power pilot carrier is transmitted in addition to the selected sideband.

¬ª A highly stable oscillator, tuned to the same frequency as the carrier frequency, is
used in the receiver.

In the latter method, it is inevitable that there would be some phase error ¬¢ in the local
oscillator output with respect to the carrier wave used to generate the incoming SSB mod-
ulated wave. The effect of this phase error is to introduce a phase distortion in the de-
modulated signal, where each frequency component of the original message signal under-
goes a constant phase shift . This phase distortion is tolerable in voice communications,

Wap) Isl
<< \ / ~ J f

‚Äúfe O f, font me o- fo\p yp fethh
‚Äî| kK Energy gap oe
{a) (6)

 

FiGuRE 2.11 (a) Spectrum of a message signal m(t) with an energy gap of width 2f,, centered
on the origin. (b) Spectrum of corresponding SSB signal containing the upper sideband.

100

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

because the human ear is relatively insensitive to phase distortion. In particular, the pres-
ence of phase distortion gives rise to a Donald Duck voice effect. In the transmission of
music and video signals, on the other hand, the presence of this form of waveform distor-
tion is utterly unacceptable.

VESTIGIAL SIDEBAND MODULATION

In vestigial sideband (VSB) modulation, one of the sidebands is partially suppressed and
a vestige of the other sideband is transmitted to compensate for that suppression. A popular
method for generating a VSB-modulated wave is to use the frequency discrimination
method. First, we generate a DSB-SC modulated wave and then pass it through a band-
pass filter, as shown in Figure 2.12; it is the special design of the band-pass filter that
distinguishes VSB modulation from SSB modulation. Assuming that a vestige of the lower
sideband is transmitted, the frequency response H(f) of the band-pass filter takes the form
shown in Figure 2.13. To simplify matters, only the response for positive frequencies is
shown here. This frequency response is normalized, so that at the carrier frequency f, we
have |H(j,)| = 1/2. The important feature to note from Figure 2.13 is that the cutoff
portion of the frequency response around the carrier frequency f, exhibits odd symmetry.
That is, inside the transition interval f. ‚Äî f, =| f| =f. + f, the following two conditions
are satisfied:

1. The sum of the values of the magnitude response | H(f)| at any two frequencies
equally displaced above and below f, is unity.
2. The phase response arg(H(f)) is linear. That is, H(f) satisfies the condition

Hf-fJ+Hf+fp=1 for-Wsfsw (2.13)

Note also that outside the frequency band of interest (ie., | f| > f, + W), the frequency
response H(f) may have an arbitrary specification. Accordingly, the transmission band-
width of VSB modulation is

Br=W+ f, (2.14)

where W is the message bandwidth, and f, is the width of the vestigial sideband.
According to Table 2.1, the VSB modulated wave is described in the time domain as

1 1 v
s(t) = 3 Agn(t) cos(27f.t) + 3 Agm'(t) sin(27f,t) (2.15)
where the plus sign corresponds to the transmission of a vestige of the upper sideband,
and the minus sign corresponds to the transmission of a vestige of the lower sideband. The
signal m‚Äô(t) in the quadrature component of s(t) is obtained by passing the message signal

DSB-SC

modulated
‚Äúee Band-pass modulated
reo
filter, H(f) wave

Ficure 2.12 Filtering scheme for the generation of VSB modulated wave.

    
  
 

Product
modulator

Message
signal m(2)

 

 

A, cos (2nf,1)
carrier wave

2.3 Linear Medulation Schemes 101

ACAI

1.0

0.5

 

 

i |
[ I
| !
i |
| |
!

 

o So-fo fe Sotho i+w ,

FIGURE 2.13 Magnitude response of VSB filter; only the positive-frequency portion is shown.

m(t) through a filter whose frequency response Ho(f) satisfies the following requirement
(see Problem 2.20):

Ho(f) = Hf - f) - Hf + f)] for -Wsfsw (2.16)

Figure 2.14 displays a plot of the frequency response Ho(f), scaled by 1/. The role of the
quadrature component determined by Ho(f) is to interfere with the in-phase component
in Equation (2.15) so as to partially reduce power in one of the sidebands of the modulated
wave s(t) and retain simply a vestige of the other sideband, as desired.

It is of interest to note that SSB modulation may be viewed as a special case of
VSB modulation. Specifically, when the vestigial sideband is reduced to zero (i.e., we set
f. = 0), the modulated wave s(t) of Equation (2.15) takes the limiting form of a single-
sideband modulated wave.

TELEVISION SIGNALS

A discussion of vestigial sideband modulation would be incomplete without a mention of
its role in commercial television (TV) broadcasting. The exact details of the modulation
format used to transmit the video signal characterizing a TV system are influenced by two
factors:

1. The video signal exhibits a large bandwidth and significant low-frequency content,
which suggest the use of vestigial sideband modulation.

2. The circuitry used for demodulation in the receiver should be simple and therefore
inexpensive; this suggests the use of envelope detection, which requires the addition
of a carrier to the VSB-modulated wave.

 

 

 

FiGuRE 2.14 Frequency response of a filter for producing the quadrature component of the
VSB modulated wave.

102

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

With regard to point 1, however, it should be stressed that although there is indeed
a basic desire to conserve bandwidth, in commercial TV broadcasting the transmitted
signal is not quite VSB modulated. The reason is that at the transmitter the power levels
are high, with the result that it would be expensive to rigidly control the filtering of side-
bands. Instead, a VSB filter is inserted in each receiver, where the power levels are low,
The overall performance is the same as conventional vestigial-sideband modulation, except
for some wasted power and bandwidth. These remarks are illustrated in Figure 2.15. In
particular, Figure 2.15a shows the idealized spectrum of a transmitted TV signal. The
upper sideband, 25 percent of the lower sideband, and the picture carrier are transmitted,
The frequency response of the VSB filter used to do the required spectrum shaping in the
receiver is shown in Figure 2.150.

The channel bandwidth used for TV broadcasting in North America is 6 MHz, as
indicated in Figure 2.15. This channel bandwidth not only accommodates the bandwidth
requirement of the VSB modulated video signal but also provides for the accompanying
sound signal that modulates a carrier of its own. The values presented on the frequency
axis in Figures 2.15a and 2,15 pertain to a specific TV channel. According to this figure,
the picture carrier frequency is at 55.25 MHz, and the sound carrier frequency is at 59.75
MHz. Note, however, that the information content of the TV signal lies in a baseband
spectrum extending from 1.25 MHz below the picture carrier to 4.5 MHz above it.

With regard to point 2, the use of envelope detection (applied to a VSB modulated

 

 

 

 

 

 

 

go ‚Äî>\ Nine 4.5 MHz > 0.25 MHz
Sa \
$s K‚Äî 0.75 MHz |
BE OS 7 \
28 ;
af |
gs
awa
So L Picture ! Sound
Ee carrier | carrier
5.2 |
ES 1
se !
= li
ol MHz
‚Äú64 56 58 60 Mle)
{a)
Picture Sound
carrier carrier
2 ol
a
@
a=!
&
g O55
So
Zz
0 AY F(MHz)

 

(b)

FIGURE 2.15 (a) Idealized magnitude spectrum of a transmitted TV signal. (b) Magnitude re-
sponse of VSB shaping filter in the receiver.

2.4 Frequency Translation 103

wave plus carrier) produces waveform distortion in the video signal recovered at the de-
tector output. The distortion is produced by the quadrature component of the VSB mod-
ulated wave; this issue is discussed next.

The use of the time-domain description given in Equation (2.15) enables the deter-
mination of the waveform distortion caused by the envelope detector. Specifically, adding
the carrier component A, cos(27f,t) to the VSB-modulated wave of Equation (2.15), the
latter being scaled by a factor k,, modifies the modulated signal applied to the envelope
detector input as ¬∞

s(f) = ad + + komt cos(2af,t) ¬£ ; k,Agn'(t) sin(27f.2) (2.17)

where the constant k, determines the percentage modulation. The envelope detector out-
put, denoted by a(t), is therefore

1 2 1 25 1/2
a(t) = ad|1 + 3 kam) + 3 kam] }

2> V2 2.18
1 ; kywm'(t) (2-18)
= A,| 1+ =k m(t)|4¬¢1 +
1+ i kmi(t)

2 @

Equation (2.18) indicates that the distortion is contributed by m‚Äô(t), which is responsible
for the quadrature component of the incoming VSB-modulated signal. This distortion can
be reduced by using two methods:

¬ª Reducing the percentage modulation to reduce the amplitude sensitivity k,.
¬Æ Increasing the width of the vestigial sideband to reduce m'(t),

Both methods are in fact used in practice. In commercial TV broadcasting, the width of
the vestigial sideband (which is about 0.75 MHz, or one-sixth of a full sideband) is deter-
mined to keep the distortion due to m'(¬¢) within tolerable limits when the percentage
modulation is nearly 100.

 

| 2.4 Frequency Translation

The basic operation involved in single-sideband modulation is in facta form of frequency
translation, which is why single-sideband modulation is sometimes referred to as frequency
changing, mixing, or heterodyning. This operation is clearly illustrated in the spectrum of
the signal shown in Figure 2.11b compared to that of the original message signal in Figure
2.112. Specifically, we see that a message spectrum occupying the band from f, to f;, for
positive frequencies in Figure 2.114 is shifted upward by an amount equal to the carrier
frequency f, in Figure 2.11), and the message spectrum for negative frequencies is trans-
lated downward in a symmetric fashion.

The idea of frequency translation described herein may be generalized as follows.
Suppose that we have a modulated wave s,(t) whose spectrum is centered on a carrier
frequency f,, and the requirement is to translate it upward in frequency such that its carrier
frequency is changed from f, to a new value f,. This requirement may be accomplished
using the mixer shown in Figure 2.16. Specifically, the mixer is a device that consists of a
product modulator followed by a band-pass filter.

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

 

 
      

Product

Modulated wave s, (+)
modulator

with carrier frequency f,

Band-pass > Modulated wave s(t)
filter with carrier frequency f,

    

 

Ag, cos (2nf et)

FIGURE 2.16 Block diagram of mixer.

To explain the action of the mixer, consider the situation depicted in Figure 2.17,
where, for the purpose of illustration, it is assumed that the mixer input s;(¬¢) is an AM
signal with carrier frequency f, and bandwidth 2.W. Part (a) of Figure 2.17 displays the
AM spectrum S,(f} assuming that f; > W. Part (B) of the figure displays the spectrum
S'(f) of the resulting signal s‚Äô(t) at the product modulator output.

The signal s‚Äô(t) may be viewed as the sum of two modulated components: one com-
ponent represented by the shaded spectrum in Figure 2.17b, and the other component
represented by the unshaded spectrum in this figure. Depending on whether the incoming
carrier frequency f, is translated upward or downward, we may identify two different
situations, as described here:

Up conversion. In this case the translated carrier frequency f, is greater than the
incoming carrier frequency f,, and the required local oscillator frequency fr is there-

fore defined by
h=fAth
or :
f=h-h
oy

Pet

{@)

if

     
 
  

       

ZIN Zin 0 ich
eS) LD La

FiGuRE 2.17 (a) Spectrum of modulated signal s,(¬¢) at the mixer input. (b) Spectrum of the
corresponding signal s‚Äô(t) at the output of the product modulator in the mixer.

2.5 Frequency-Division Multiplexing 105

The unshaded part of the spectrum in Figure 2.17b defines the wanted modulated
signal s2(t), and the shaded part of this spectrum defines the iznage signal associated
with s,(t), For obvious reasons, the mixer in this case is referred to as a frequency-
up converter,

Down conversion. In this second case the translated carrier frequency f, is smaller
than the incoming carrier frequency f,, and the required oscillator frequency f; is
therefore defined by

h=fi-f

or

fh=fA-kh

The picture we have this time is the reverse of that pertaining to up conversion. In
particular, the shaded part of the spectrum in Figure 2.176 defines the wanted mod-
ulated signal s(t), and the unshaded part of this spectrum defines the associated
image signal. The mixer is now referred to as a frequency-down converter. Note that
in this case the translated carrier frequency f; has to be larger than W (i.e., one half
of the bandwidth of the modulated signal) to avoid sideband overlap.

The purpose of the band-pass filter in the mixer of Figure 2.16 is to pass the wanted
modulated signal s,(t) and eliminate the associated image signal. This objective is achieved
by aligning the midband frequency of the filter with the translated carrier frequency f, and
assigning it a bandwidth equal to that of the incoming modulated signal s,(t).

It is important to note that mixing is a linear operation. Accordingly, the relation of
the sidebands of the incoming modulated wave to the carrier is completely preserved at
the mixer output.

| 2.5 Frequency-Division Multiplexing

 

 

 

Another important signal processing operation is multiplexing, whereby a number of in-
dependent signals can be combined into a composite signal suitable for transmission over
a common channel. Voice frequencies transmitted over telephone systems, for example,
range from 300 to 3100 Hz. To transmit a number of these signals over the same channel,
the signals must be kept apart so that they do not interfere with each other, and thus they
can be separated at the receiving end. This is accomplished by separating the signals either
in frequency or in time. The technique of separating the signals in frequency is referred to
as frequency-division multiplexing (FDM), whereas the technique of separating the signals
in time is called time-division multiplexing (TDM). In this section, we discuss FDM sys-
tems, and TDM systems are discussed in Chapter 3.

A block diagram of an FDM system is shown in Figure 2.18. The incoming message
signals are assumed to be of the low-pass type, but their spectra do not necessarily have
nonzero values all the way down to zero frequency. Following each signal input, we have
shown a low-pass filter, which is designed to remove high-frequency components that do
not contribute significantly to signal representation but are capable of disturbing other
message signals that share the common channel. These low-pass filters may be omitted
only if the input signals are sufficiently band limited initially. The filtered signals are applied

 

 

 

 

 

 

 

 

 

 

 

 

 

106 CHAPTER 2 & CONTINUOUS-WAVE MODULATION
Message ‚Äî_ Low-pass Band-pass Band-pass Low-pass Message
inputs filters Modulators filters filters Demoduiators filters outputs
l‚Äî > LP oe MOD -‚Äî > BP BP = DEM -‚Äî‚Äî‚Äî= (LP |. 1
2‚Äî> (LP BP ‚Äî=> LP 2
Common
channel | ~
N-‚Äî‚Äî=> LP BP > LP o> N

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Carrier Carer
supply supply

  

[.

 

 

 

 

 

Transmitter Receiver

FiGureE 2.18 Block diagram of FDM system.

to modulators that shift the frequency ranges of the signals so as to occupy mutually
exclusive frequency intervals. The necessary carrier frequencies needed to perform these
frequency translations are obtained from a carrier supply. For the modulation, we may
use any one of the methods described in previous sections of this chapter. However, the
most widely used method of modulation in frequency-division multiplexing is single side-
band modulation, which, in the case of voice signals, requires a bandwidth that is ap-
proximately equal to that of the original voice signal. In practice, each voice input is usually
assigned a bandwidth of 4 kHz. The band-pass filters following the modulators are used
to restrict the band of each modulated wave to its prescribed range. The resulting band-
pass filter outputs are next combined in parallel to form the input to the common channel.
At the receiving terminal, a bank of band-pass filters, with their inputs connected in par-
allel, is used to separate the message signals on a frequency-occupancy basis. Finally, the
original message signals are recovered by individual demodulators. Note that the FDM
system shown in Figure 2.18 operates in only one direction. To provide for two-way
transmission, as in telephony, for example, we have to completely duplicate the multi-
plexing facilities, with the components connected in reverse order and with the signal
waves proceeding from right to left.

¬Æ EXaMPLeE 2.1

The practical implementation of an FDM system usually involves many steps of modulation
and demodulation, as illustrated in Figure 2.19. The first multiplexing step combines 12 voice
inputs into a basic group, which is formed by having the ath input modulate a carrier at
frequency f, = 60 + 4n kHz, where # = 1, 2,..., 12. The lower sidebands are then selected
by band-pass filtering and combined to form a group of 12 lower sidebands (one for each
voice input). Thus the basic group occupies the frequency band 60 to 108 kHz. The next step
in the FDM hierarchy involves the combination of five basic groups into a supergroup. This
is accomplished by using the xth group to modulate a carrier of frequency f, = 372 + 48"
kHz, where n = 1,2,..., 5. Here again the lower sidebands are selected by filtering and then

2.6 Angle Modulation 107

Carrier frequencies (in kHz) Carrier frequencies

 

 

 

 

 

 

 

 

 

 

 

 

 

 

of voice inputs (in KHz) of groups
\ 108 kHz
108 {12 | 6i2‚Äî[ 5 ]o02 KH
p= 504
104‚Äî il 564 4
100 + 10 516‚Äî, 3 ‚Äúse
96 7 9 468 2 408
92 8 420 1 ao
7 20 312
88 7
84 6 Supergroup
80 5 of ¬ß groups
76‚Äî 4
72‚Äî 3
68 ‚Äî-| 2
4 kHz a eS
} 60
Basic group of 12
9 voice inputs

Voice band

FIGURE 2.19 Illustrating the modulation Steps in an FDM system.

and mastergroups are combined into very large groups. <q
-| 2.6 Angle Modulation
teen nnnnniiinnnan we

In the previous sections of this chapter, we investigated the effect of slowly varying the
amplitude of a sinusoidal carrier wave in accordance with the basehand (information-
carrying) signal. There is another way of modulating a sinusoidal carrier wave, namely,
angle modulation in which the angle of the carrier wave is varied according to the baseband
signal. In this method of modulation, the amplitude of the catrier wave is maintained
constant. An important feature of angle modulation is that it can provide better discrim-
ination against noise and interference than amplitude modulation. As will be shown later
in Section 2.7, however, this improvement in performance is achieved at the expense of
increased transmission bandwidth; that is, angle modulation provides us with a practical

means of exchanging channel bandwidth for improved noise performance. Such a trade-
off is not possible with amplitude modulation, regardless of its form.

# Basic DEFINITIONS

Let 6,(¬¢) denote the angle of a modulated sinusoidal carrier, assumed to be a function of
the message signal. We express the resulting angle-modulated wave as

s(t) = A. cos[@,(t)] (2.19)

108

CHaPprer 2 & CONTINUOUS-WAVE MODULATION

where A, is the carrier amplitude. A complete oscillation occurs whenever 6,(t) changes
by 27 radians. If 6,{f) increases monotonically with time, the average frequency in Hertz,
over an interval from f to t + At, is given by

6,(t + At) ‚Äî 6,(t)

Qa At (2.20)

fadt) =

We may thus define the instantaneous frequency of the angle-modulated signal s(t) as
follows:

 

At
. 6,(¬¢ + At) ‚Äî 6,(t)
~ fm, | 2m At (2.21)
= Ai d6,(t)
2n dt

Thus, according to Equation (2.19), we may interpret the angle-modulated signal
s(t) as a rotating phasor of length A. and angle 6,(t). The angular velocity of such a phasor
is d6,(t\/dt measured in radians per second, in accordance with Equation (2.21). In the
simple case of an unmodulated carrier, the angle 6,(t) is

6,(t) = 2nf.t + 6,

and the corresponding phasor rotates with a constant angular velocity equal to 2af,. The
constant @, is the value of @,(f) at t = 0.

There are an infinite number of ways in which the angle 0,(t) may be varied in some
manner with the message (baseband) signal. However, we shall consider only two com-
monly used methods, phase modulation and frequency modulation, defined as follows:

1. Phase modulation (PM) is that form of angle modulation in which the angle 0,(t) is
varied linearly with the message signal m(t), as shown by

6,(t) = 2nf.t + kpmlt) (2.22)

The term 27f,t represents the angle of the unmodulated carrier; and the constant k,
represents the phase sensitivity of the modulator, expressed in radians per volt on
the assumption that m(¬¢) is a voltage waveform. For convenience, we have assumed
in Equation (2.22) that the angle of the unmodulated carrier is zero at t = 0. The
phase-modulated signal s(t) is thus described in the time domain by

s(t) = A, cos[2af.t + Rpmi(t)] (2.23)

b

Frequency modulation (FM) is that form of angle modulation in which the instan-
taneous frequency f,(t) is varied linearly with the message signal m(t), as shown by

filt) = f, + Rem?) (2.24)

The term f, represents the frequency of the unmodulated carrier, and the constant
ky represents the frequency sensitivity of the modulator, expressed in Hertz per volt

2.7 Frequency Modulation 109

A Phase Modulating . " Frequency
vet Integrator |) modulator [> FM wave wave Differentiator modulator

A, cos (2rf.) A, COS (2r7f,1)
{a) (b)

 

   
  

ae PM wave

 

 

 

 

FIGURE 2.20 [lustrating the relationship between frequency modulation and phase modulation.
(a) Scheme for generating an FM wave by using a phase modulator. (b) Scheme for generating a
PM wave by using a frequency modulator.

on the assumption that m/(t) is a voltage waveform. Integrating Equation (2.24) with
respect to time and multiplying the result by 27, we get
La

0,(t) = Zaft + 2k, I, m(r) dr (2.25)

where, for convenience, we have assumed that the angle of the unmodulated carrier
wave is zero at tf = 0. The frequency-modulated signal is therefore described in the
time domain by

s(t) = A, cos] 2 + amy | mt) ir| (2.26)

A consequence of allowing the angle 0,(t) to become dependent on the message signal
m(t) as in Equation (2.22) or on its integral as in Equation (2.25) is that the zero crossings
of a PM signal or FM signal no longer have a perfect regularity in their spacing; zero
crossings refer to the instants of time at which a waveform changes from a negative to a
positive value or vice versa. This is one important feature that distinguishes both PM and
FM signals from an AM signal. Another important difference is that the envelope of a PM
or FM signal is constant (equal to the carrier amplitude), whereas the envelope of an AM
signal is dependent on the message signal.

Comparing Equation (2.23) with (2.26) reveals that an FM signal may be regarded
as a PM signal in which the modulating wave is J7m(7) dz in place of m(t). This means
that an FM signal can be generated by first integrating ¬ª(t) and then using the result as
the input to a phase modulator, as in Figure 2.204. Conversely, a PM signal can be gen-
erated by first differentiating m(t) and then using the result as the input to a frequency
modulator, as in Figure 2.20b. We may thus deduce all the properties of PM signals from
those of FM signals and vice versa. Henceforth, we concentrate our attention on FM

signals.

j 2.7 Frequency Modulation

The FM signal s(t) defined by Equation (2.26) is a nonlinear function of the modulating
signal m(t), which makes frequency modulation a nonlinear modulation process. Conse-
quently, unlike amplitude modulation, the spectrum of an FM signal is not related in a
simple manner to that of the modulating signal; rather, its analysis is much more difficult
than that of an AM signal.

110) =CuHaprer2 8 CoNTINUOUS-WAVE MODULATION

How then can we tackle the spectral analysis of an FM signal? We propose to provide
an empirical answer to this important question by proceeding in the following manner:

> We consider the simplest case possible, namely, that of a single-tone modulation that
produces a narrowband FM signal.

> We next consider the more general case also involving a single-tone modulation, but
this time the FM signal is wideband.

We could, of course, go on and consider the more elaborate case of a multitone FM signal,
However, we propose not to do so, because our immediate objective is to establish an
empirical relationship between the transmission bandwidth of an FM signal and the mes-
sage bandwidth. As we shall subsequently see, the two-stage spectral analysis described
here provides us with enough insight to propose a solution to the problem.

Consider then a sinusoidal modulating signal defined by

m(t) = A,, cos(27f,,t) (2.27)
The instantaneous frequency of the resulting FM signal equals

fit) = fo + RyAm cos(27f,,t)

= f, + Af cos(27f,,t) (2.28)

where
Af = kpAm (2.29)

The quantity Af is called the frequency deviation, representing the maximum departure

of the instantaneous frequency of the FM signal from the carrier frequency f.. A funda-

mental characteristic of an FM signal is that the frequency deviation Af is proportional

to the amplitude of the modulating signal and is independent of the modulation frequency.
Using Equation (2.28), the angle 6,(¬¢) of the FM signal is obtained as

0(¬¢) = 27 [, f;(t) dr

A
= laf.t + a sin(27f,,,t)
The ratio of the frequency deviation Af to the modulation frequency f,,, is commonly called
the modulation index of the FM signal. We denote it by B, and so write

(2.30)

Af
= 2.31
B 1, (2.31)
and

6(t) = 2arf.t + B sin(2afat) (2.32)

From Equation (2.32) we see that, in a physical sense, the parameter 8 represents the phase
deviation of the FM signal, that is, the maximum departure of the angle 6;(¬¢) from the
angle 27f,t of the unmodulated carrier; hence, 6 is measured in radians.

The FM signal itself is given by

s(t) = A, cos[2af,t + B sin(27f,,t)] (2.33)

2.7 Frequency Modulation 111

Depending on the value of the modulation index 8, we may distinguish two cases of
frequency modulation: :

> Narrowband FM, for which ¬£ is small compared to one radian.
¬ª Wideband FM, for which 8 is large compared to one radian.

These two cases are considered next, in that order.

sg NARROWBAND FREQUENCY MODULATION

Consider Equation (2.33), which defines an FM signal resulting from the use of a sinusoidal
modulating signal. Expanding this relation, we get

s(t) = A, cos(27f,t) cos[B sin(27f,,t)] + A, sin(27f.t) sin[B sin(27f,,t)] (2.34)

Assuming that the modulation index B is small compared to one radian, we may use the
following approximations:

cos[B sin(2nf,t)] ~ 1
and
sin[B sin(2af,2)] ~ B sin(2af,¬¢)
Hence, Equation (2.34) simplifies to
s(t) = A, cos(2af.t) ‚Äî BA, sin(2af,t) sin(2af,2) (2.35)

Equation (2.35) defines the approximate form of a narrowband FM signal produced by a
sinusoidal modulating signal A,, cos(27f,,t). From this representation we deduce the mod-
ulator shown in block diagram form in Figure 2.21. This modulator involves splicting the
carrier wave A, cos(27f,t) into two paths. One path is direct; the other path contains a
‚Äî90 degree phase-shifting network and a product modulator, the combination of which
generates a DSB-SC modulated signal. The difference between these two signals produces
a narrowband FM signal, but with some distortion.

Ideally, an FM signal has a constant envelope and, for the case of a sinusoidal mod-
ulating signal of frequency f,,, the angle 6,(¬¢) is also sinusoidal with the same frequency.

Product ‚Äî
modulator

Narrowband
FM wave

Modulating

wave Integrator

 
   

 

A, sin (21f,2)

   

-$0¬∞
phase-shifter

Carrier wave
A, COS (27,2)

  

 

 

 

Narrowband phase modulator

FicuRE 2.21 Block diagram of a method for generating a narrowband FM signal.

112 CHAPTER 2 CONTINUOUS-WAVE MODULATION

But the modulated signal produced by the narrowband modulator of Figure 2.21 differs
from this ideal condition in two fundamental respects:

1. The envelope contains a residual amplitude modulation and, therefore, varies with
time.

2. For a sinusoidal modulating wave, the angle 6,(¬¢) contains harmonic distortion in
the form of third- and higher-order harmonics of the modulation frequency f,,.

However, by restricting the modulation index to 8 = 0.3 radians, the effects of residual
AM and harmonic PM are limited to negligible levels.
Returning to Equation (2.35), we may expand it as follows:

s(t) = A, cos(2af,t) + ; BAJ{cos[2a(f, + fxl√©] ‚Äî cos[2a(f. ‚Äî fad√©l} (2.36)

This expression is somewhat similar to the corresponding one defining an AM signal,
which is as follows:

Sam(t) = A, cos(2mft) + + mA.tcosi2n(f + fixlt] + cos2a(f. ‚Äî fnd√©l} (2.37)

where p. is the modulation factor of the AM signal. Comparing Equations (2.36) and
(2.37), we see that in the case of sinusoidal modulation, the basic difference between an
AM signal and a narrowband FM signal is that the algebraic sign of the lower side fre-
quency in the narrowband FM is reversed. Thus, a narrowband FM signal requires essen-
tially the same transmission bandwidth (i.e., 2f,,) as the AM signal,

We may represent the narrowband FM signal with a phasor diagram as shown in
Figure 2.224, where we have used the carrier phasor as reference. We see that the resultant

Sum of side-
frequency phasors

    
 
 
      

Resultant

Upper side-
frequency

  

. frequency
Carrier

 

(a}

   
 
  

 

Upper side- ‚Äò
frequency ‚Äò
Carrier Sum of
hi side-frequency
phasors

Lower side-
frequency

{b)

FIGURE 2.22 A phasor comparison of narrowband FM and AM waves for sinusoidal modula-
tion. (2) Narrowband FM wave. (b) AM wave.

2.7 Frequency Modulation 113

of the two side-frequency phasors is always at right angles to the carrier phasor. The effect
of this is to produce a resultant phasor representing the narrowband FM signal that is
approximately of the same amplitude as the carrier phasor, but out of phase with respect
to it. This phasor diagram should be contrasted with that of Figure 2.225, representing
an AM signal. In this latter case we see that the resultant phasor representing the AM
signal has an amplitude that is different from that of the carrier phasor but always in phase
with it.

@ WIDEBAND FREQUENCY MODULATION

We next wish to determine the spectrum of the single-tone FM signal of Equation (2.33)
for an arbitrary value of the modulation index . In general, an FM signal produced by a
sinusoidal modulating signal, as in Equation (2.33), is in itself nonperiodic unless the
carrier frequency f, is an integral multiple of the modulation frequency f,,. However, we
may simplify matters by using the complex representation of band-pass signals described
in Appendix 2. Specifically, we assume that the carrier frequency f. is large enough (com-
pared to the bandwidth of the FM signal) to justify rewriting this equation in the form

s(t) = Re[A, exp(j2af.t + j6 sin(27f,,t))]
= Re[s(z) exp( j27f.2)]
where 5(t) is the complex envelope of the FM signal s(t), defined by
5(t) = A, exp[j6 sin(277,,2)] (2.39)

Thus, unlike the original FM signal s(z), the complex envelope (t) is a periodic function
of time with a fundamental frequency equal to the modulation frequency f,,. We may
therefore expand &(t) in the form of a complex Fourier series as follows:

(2.38)

a(t) = s c, exp( j270f,,t) (2.40)

where the complex Fourier coefficient c, is defined by
V2 fy
CG. = fn | ¬ß(t) exp(‚Äîj2anf,,t) at

-1/2f,,
U2F py (2.41)

= fade | | expliB sin 2nft) ‚Äî j2mnfl de

Define a new variable:
x = 2nfat (2.42)

Hence, we may rewrite Equation (2.41) in the new form
A. [‚Äù ups
Cn = A: | exp[j(@ sin x ‚Äî nx)] dx (2.43)

The integral on the right-hand side of Equation (2.43), except for a scaling factor, is
recognized as the nth order Bessel function of the first kind* and argument . This function
is commonly denoted by the symbol J,,(8), as shown by

JB) = 5 | expli(B sin x ~ x) de (2.44)

114

CHAPTER 2 CONTINUOUS-WAVE MODULATION

Accordingly, we may reduce Equation (2.43) to

co, = A.],AB) (2.45)

Substituting Equation (2.45) in (2.40), we get, in terms of the Bessel function J,,(8), the
following expansion for the complex envelope of the FM signal:

3(t)= A, dX JalB) exp(j2rnf,,t) (2.46)

nao

Next, substituting Equation (2.46) in (2.38), we get

s(t) = A, * Rel > JAB) exp[j2a(f. + nfl (2.47)

PES me 00

Interchanging the order of summation and evaluation of the real part in the right-hand
side of Equation (2.47), we finally get

cy

s(t) =A, >) JAB) cos[2a(f, + nfn)t] (2.48)

n= ‚Äî00

This is the desired form for the Fourier series representation of the single-tone FM signal
s(t) for an arbitrary value of 8. The discrete spectrum of s(t) is obtained by taking the
Fourier transforms of both sides of Equation (2.48); we thus have

sip) = 52 DBI ~ fe - fn) + BUF + fe + rfl (2.49)

In Figure 2.23 we have plotted the Bessel function J,,(8) versus the modulation index
B for different positive integer values of 7. We can develop further insight into the behavior

1.06
0.8
0.6

04

JAB)

0.2

 

-0.2

 

 

-0.4
FIGURE 2.23 Plots of Bessel functions of the first kind for varying order.

2.7 Frequency Modulation 115
of the Bessel function J,(8) by making use of the following properties (see Appendix 3 for
more details):

1. JAB) = (-1)7J_,(B) for all , both positive and negative (2.50)

2. For small values of the modulation index 8, we have

Jo(B) = 1
af
Ji(B) = 7) (2.51)

3. > J2s) = 1 (2.52)

Thus, using Equations (2.49)-(2.52) and the curves of Figure 2.23, we may make
he following observations:

1. The spectrum of an FM signal contains a carrier component and an infinite set of
side frequencies located symmetrically on either side of the carrier at frequency sep-
arations of fins 2fns Sf ¬∞** ~ In this respect, the result is unlike that which prevails
in an AM system, since in an AM system a sinusoidal modulating signal gives rise
to only one pair of side frequencies.

For the special case of B small compared with unity, only the Bessel coefficients J,(B)

and J,() have significant values, so that the FM signal is effectively composed of a

carrier and a single pair of side frequencies at f. + f,,. This situation corresponds to

the special case of narrowband FM that was considered earlier.

3. The amplitude of the carrier component varies with 8 according to Jo(8). That is,
unlike an AM signal, the amplitude of the carrier component of an FM signal is
dependent on the modulation index 8. The physical explanation for this property is
that the envelope of an FM signal is constant, so that the average power of such a
signal developed across a 1-ohm resistor is also constant, as shown by

1

P= ze (2,53)

N

When the carrier is modulated to generate the FM signal, the power in the side
frequencies may appear only at the expense of the power originally in the carrier,
thereby making the amplitude of the carrier component dependent on 8. Note that
the average power of an FM signal may also be determined from Equation (2.48),
obtaining

Pasa? > 716) (2.54)

n= ‚Äî 00

Substituting Equation (2.52) into (2.54), the expression for the average power P
reduces to Equation (2.53), and so it should.

> EXAMPLE 2.2

In this example, we wish to investigate the ways in which variations in the amplitude and
frequency of a sinusoidal modulating signal affect the spectrum of the FM signal. Consider

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

 

1.0
f
B=10 ‚Äî>| 2af ke
(a)
1.0
A | | in f
B=2.0 >| 2af kk
¬Æ)
1.0
atl | inva f

 

B=5.0 te >| fn
‚Äî 2af aon
(c)

FIGURE 2.24 Discrete amplitude spectra of an FM signal, normalized with respect to the carrier
amplitude, for the case of sinusoidal modulation of fixed frequency and varying amplitude. Only
the spectra for positive frequencies are shown.

first the case when the frequency of the modulating signal is fixed, but its amplitude is varied,
producing a corresponding variation in the frequency deviation Af. Thus, keeping the mod-
ulation frequency f,, fixed, we find that the amplitude spectrum of the resulting FM signal is
as shown plotted in Figure 2.24 for 8 = 1, 2, and 5. In this diagram we have normalized the
spectrum with respect to the unmodulated carrier amplitude.

Consider next the case when the amplitude of the modulating signal is fixed; that is, the
frequency deviation Af is maintained constant, and the modulation frequency f,, is varied.
In this case we find that the amplitude spectrum of the resulting FM signal is as shown
plotted in Figure 2.25 for B = 1, 2, and 5. We see that when Af is fixed and is increased,
we have an increasing number of spectral lines crowding into the fixed frequency interval
fp - Af <|f |< f+ Af. That is, when 6 approaches infinity, the bandwidth of the FM wave

2.7 Frequency Modulation 117

 

 

p=10 DAF

{a)

1.0

Z ba,

2.0 hk 24g ‚Äî‚Äî‚Äî>}

 

u

(b)

1.0

: li! L, ive ;
‚Ñ¢ es _‚Äî

FiGurE 2.25 Discrete amplitude spectra of an FM signal, normalized with respect to the carrier
amplitude, for the case of sinusoidal modulation of varying frequency and fixed amplitude. Only
the spectra for positive frequencies are shown.

 

approaches the limiting value of 2Af, which is an important point to keep in mind for later
discussion.

@ TRANSMISSION BANDWIDTH OF FM SIGNALS

In theory, an FM signal contains an infinite number of side frequencies so that the band-
width required to transmit such a signal is similarly infinite in extent. In practice, however,
we find that the FM signal is effectively limited to a finite number of significant side
frequencies compatible with a specified amount of distortion. We may therefore specify
an effective bandwidth required for the transmission of an FM signal. Consider first the

118

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

case of an FM signal generated by a single-tone modulating wave of frequency f,,- In such
an FM signal, the side frequencies that are separated from the carrier frequency f. by an
amount greater than the frequency deviation Af decrease rapidly toward zero, so that the
bandwidth always exceeds the total frequency excursion, but nevertheless is limited. Spe-
cifically, for large values of the modulation index f, the bandwidth approaches, and is
only slightly greater than, the total frequency excursion 2Af in accordance with the situ-
ation shown in Figure 2.25. On the other hand, for small values of the modnlation index
B, the spectrum of the FM signal is effectively limited to the carrier frequency f, and one
pair of side frequencies at f, + fi. 80 that the bandwidth approaches 2f,,. We may thus
define an approximate rule for the transmission bandwidth of an FM signal generated by
a single-tone modulating signal of frequency f,, as follows:

Br = 2Af + 2f,.= 2as( + t) (2.55)

This empirical relation is known as Carson's rule.*

For an alternative assessment of the bandwidth requirement of an FM signal, we
may use a definition based on retaining the maximum number of significant side frequen-
cies whose amplitudes are all greater than some selected value. A convenient choice for
this value is 1 percent of the unmodulated carrier amplitude. We may thus define the
transmission bandwidth of an FM wave as the separation between the two frequencies
beyond which none of the side frequencies is greater than 1 percent of the carrier amplitude
obtained when the modulation is removed. That is, we define the transmission bandwidth
as 2tmaxfns Where f,, is the modulation frequency and #,,¬ª is the largest value of the
integer # that satisfies the requirement |J,(B)| > 0.01. The value of #,,.x varies with the
modulation index 8 and can be determined readily from tabulated values of the Bessel
function J,,(B). Table 2.2. shows the total number of significant side frequencies (including
both the upper and lower side frequencies) for different values of , calculated on the 1
percent basis explained herein. The transmission bandwidth By calculated using this pro-
cedure can be presented in the form of a universal curve by normalizing it with respect to
the frequency deviation Af and then plotting it versus B. This curve is shown in Figure
2.26, which is drawn as a best fit through the set of points obtained by using Table 2.2.
In Figure 2.26 we note that as the modulation index f is increased, the bandwidth occupied

TABLE 2.2 Number of significant side
frequencies of a wideband FM signal for varying

 

 

 

 

 

 

modulation index
Modulation Index Number of Significant Side Frequencies

B 2tynax
0.1 2
0.3 4
0.5 4
1.0 : 6
2.0 8
5.0. 16

10.0 28

20.0 50

30.0 70

 

2.7 Frequency Modulation 119

40 -

 

 

 

 

 

{il
0.1 0.2 04 6 810 2 4 6 810 20 40

FiGure 2.26 Universal curve for evaluating the 1 percent bandwidth of an FM wave.

by the significant side frequencies drops toward that over which the carrier frequency
actually deviates. This means that small values of the modulation index # are relatively
more extravagant in transmission bandwidth than are the larger values of p.

Consider next the more general case of an arbitrary modulating signal m(z) with its
highest frequency component denoted by W. The bandwidth required to transmit an
FM signal generated by this modulating signal is estimated by using a worst-case tone-
modulation analysis. Specifically, we first determine the so-called deviation ratio D, defined
as the ratio of the frequency deviation Af, which corresponds to the maximum possible
amplitude of the modulation signal s(t), to the highest modulation frequency W; these
conditions represent the extreme cases possible. The deviation ratio D plays the same role
for nonsinusoidal modulation that the modulation index B plays for the case of sinusoidal
modulation. Then, replacing 8 by D and replacing f,, with W, we may use Carson‚Äôs rule
given by Equation (2.55) or the universal curve of Figure 2.26 to obtain a value for the
transmission bandwidth of the FM signal. From a practical viewpoint, Carson‚Äôs rule some-
what underestimates the bandwidth requirement of an FM system, whereas using the uni-
versal curve of Figure 2.26 yields a somewhat conservative result. Thus, the choice of a
transmission bandwidth that lies between the bounds provided by these two rules of thumb
is acceptable for most practical purposes.

B EXAMPLE 2.3

In North America, the maximum value of frequency deviation Af is fixed at 75 kHz for
commercial FM broadcasting by radio. If we take the modulation frequency W = 15 kHz,
which is typically the ‚Äúmaximum‚Äù audio frequency of interest in FM transmission, we find
that the corresponding value of the deviation ratio is

D=-7=5

15

Using Carson‚Äôs rule of Equation (2.55), replacing B by D, and replacing f,, by W, the ap-
proximate value of the transmission bandwidth of the FM signal is obtained as

By = 2(75 + 15) = 180 kHz

126

CHAPTER 2 ¬Æ CONTINUOUS-WAVE MODULATION

On the other hand, use of the curve of Figure 2.26 gives the transmission bandwidth of the
FM signal to be

By = 3.2Af = 3.2 % 75 = 240 kHz

In practice, a bandwidth of 200 kHz is allocated to each FM transmitter. On this basis,
Carson‚Äôs rule underestimates the transmission bandwidth by 10 percent, whereas the universal
curve of Figure 2.26 overestimates it by 20 percent. q

= GENERATION OF FM SIGNALS

There are essentially two basic methods of generating frequency-modulated signals,
namely, direct FM and indirect FM. In the direct method the carrier frequency is directly
varied in accordance with the input baseband signal, which is readily accomplished using
a voltage-controlled oscillator. In the indirect method, the modulating signal is first used
to produce a narrowband FM signal, and frequency multiplication is next used to increase
the frequency deviation to the desired level, The indirect method is the preferred choice
for frequency modulation when the stability of carrier frequency is of major concern as in
commercial radio broadcasting, as described next.

Indirect FM‚Äô

A simplified block diagram of an indirect FM system is shown in Figure 2.27. The
message (baseband) signal m(¬¢) is first integrated and then used to phase-modulate a
crystal-controlled oscillator; the use of crystal control provides frequency stability. To
minimize the distortion inherent in the phase modulator, the maximum phase deviation
or modulation index B is kept small, thereby resulting in a narrowband FM signal; for the
implementation of the narrow-band phase modulator, we may use the arrangement de-
scribed in Figure 2.21, The narrowband FM signal is next multiplied in frequency by means
of a frequency multiplier so as to produce the desired wideband FM signal.

A frequency multiplier consists of a nonlinear device followed by a band-pass filter,
as shown in Figure 2.28. The implication of the nonlinear device being memoryless is that
it has no energy-storage elements. The input-output relation of such a device may be
expressed in the general form

v(t) = ays(t) + ays2(t) + +++ + a,s*(¬£) (2.56)

where 44, do,..., 4, are coefficients determined by the operating point of the device, and
n is the bighest order of nonlinearity. In other words, the memoryless nonlinear device is
an ath power-law device. The input s(t) is an FM signal defined by

s(t) = A, cos] 2 + amk, [ m(r) ar|

 

Baseband Narrowband :
signal Integrator -‚Äîs| phase -‚Äîs| Paiene ‚Äî> FM signal
mt) modulator

ft

Crystal-
controlled
oscillator

 

 

 

 

 

 

FIGURE 2.27 Block diagram of the indirect method of generating a wideband FM signal.

2.7 Frequency Modulation 121

 

FM signal s‚Äò(z) with
carrier frequency wf,
and modulation
index ng

FM signal s()
with carrier frequency
f, and modulation
index 8

ut) Band-pass filter
with midband
frequency af,

Memoryless

‚Äî‚Äî‚Äî‚Äî a " :
nonlinear device

 

 

 

 

Figure 2.28 Block diagram of frequency multiplier.

whose instantaneous frequency is
filt) = fo + Remit) (2.57)

The mid-band frequency of the band-pass filter in Figure 2.28 is set equal to #f,, where f,
is the carrier frequency of the incoming FM signal s(t). Moreover, the band-pass filter is
designed to have a bandwidth equal to ¬ª times the transmission bandwidth of s(t). In
Section 2.8 dealing with nonlinear effects in FM systems, we describe the spectral contri-
butions of such nonlinear terms as the second- and third-order terms in the input-output
relation of Equation (2.56). For now it suffices to say that after band-pass filtering of the
nonlinear device‚Äôs output v(t), we have a new FM signal defined by

s(t) = Al cos| 2a + 2ank; [ m(z7) ar| (2.58)

whose instantaneous frequency is
FAt) = nf. + nkymi(t) (2.59)

Thus, comparing Equation (2.59) with (2.57), we see that the nonlinear processing circuit
of Figure 2.28 acts as a frequency multiplier. The frequency multiplication ratio is deter-
mined by the highest power x in the input-output relation of Equation (2.56), character-
izing the memoryless nonlinear device.

6 DEMODULATION OF FM SIGNALS

 

Frequency demodulation is the process that enables us to recover the original modulating
signal from a frequency-modulated signal. The objective is to produce a transfer charac-
teristic that is the inverse of that of the frequency modulator, which can be realized directly
or indirectly. Here we describe a direct method of frequency demodulation involving the
use of a popular device known as a frequency discriminator, whose instantaneous output
amplitude is directly proportional to the instantaneous frequency of the input FM signal.
In Section 2.14, we describe an indirect method of frequency demodulation that uses
another popular device known as a phase-locked loop.

Basically, the frequency discriminator consists of a slope circuit followed by an en-
velope detector. An ideal slope circuit is characterized by a frequency response that is
purely imaginary, varying linearly with frequency inside a prescribed frequency interval.
Consider the frequency response depicted in Figure 2.294, which is defined by

 

; Br Br By
+ <fsf4+‚Äî
jamal f+ ), fe~ FSH SLAG

 

H,(f) = jana(f +f 22), 4-2 Br (2.60)

<fs-f.+
7) 3 BFS ET G

0, elsewhere

122

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

Af)

  
    

Slope = 27a

 

@ (b)

 

(c)

FIGURE 2.29 (a) Frequency response of ideal slope circuit. (b) Frequency response of the slope
circuit's complex low-pass equivalent. (c) Frequency response of the ideal slope circuit comple-
mentary to that of part (a).

where a is a constant. We wish to evaluate the response of this slope circuit, denoted by
s(t), which is produced by an FM signal s(t) of carrier frequency f, and transmission
bandwidth Br. It is assumed that the spectrum of s(t) is essentially zero outside the fre-
quency interval f. ‚Äî Bp/2 =|f| =f. + Br/2.

We may simplify the analysis of the frequency discriminator by invoking the iso-
morphism between a real-valued band-pass filter and a correspondirig complex-valued
low-pass filter. This isomorphism is discussed in Appendix 2. According to the material
presented in that appendix, we may replace the band-pass filter with frequency response
H,(f) with an equivalent low-pass filter with frequency response H,(f) by doing two
things:

1. We shift H,(f) to the right by f., where f, is the midband frequency of the band-
pass filter; this operation aligns the translated frequency response of the equivalent
low-pass filter with that of the band-pass filter.

2. We set A,(f ‚Äî f.) equal to 2H,(f) for f > 0.

Thus for the problem at hand we get

 

Af - f.) =2H(f), f>0 (2.61)
Hence, using Equations (2.60) and (2.61), we get
7 : Br Br Br
Fy(f) = jena f a) ), 2 =f59 (2.62)
elsewhere

>

which is plotted in Figure 2.29b.

2.7 Frequency Modulation 123

The incoming FM signal s(t) is defined by Equation (2.26), which is reproduced here
for convenience:

s(t) = A, cos] 2a + 2ak, [, m(7) ar|

Given that the carrier frequency f, is high compared to the transmission bandwidth of the
FM signal s(t), the complex envelope of s(¬¢) is

a(t) = A, exp, [ m(r) ar| (2.63)

Let ¬ß,(¬¢) denote the complex envelope of the response of the slope circuit defined by
Figure 2.294 due to &(t). Then, following the material presented in Appendix 2, we may
express the Fourier transform of 5,(t) as follows:

= 1. =
Sif) = 5 AUAS(F)
(2.64)
ry = Br B
janal f+ + Bap, 7 shez
0, elsewhere

where 5(f) is the Fourier transform of 3(t). From Fourier analysis we know that multipli-
cation of the Fourier transform of a signal by j27f is equivalent to differentiating the
signal in the time domain; see item 8 of Table A6.2. Hence, from Equation (2.64) we
deduce

 

$,(t) = | ) + jaBrili | (2.65)
Substituting Equation (2.63) into (2.65), we get
¬ß,(f) = jmBoaA.| 1 + 2 mt exp| jam, [, m(z) ar| (2.66)
T

The desired response of the slope circuit is therefore
silt) = Re[8,(¬¢) exp( j27f1)|
2k 1 a) (2.67)
= 7ByAJ1+ B, m(t) | cos} 2af.t + 2k, [, m(t) dr + 3

The signal s,(t) is a hybrid-modulated signal in which both amplitude and frequency of
the carrier wave vary with the message signal m(t). However, provided that we choose
2h
Br
then we may use an envelope detector to recover the amplitude variations and thus, except
for a bias term, obtain the original message signal. The resulting envelope-detector output
is therefore

m(t)| <1 for all ¬¢

 

 

|5,(t)| = mByaA, E + a mi (2.68)

The bias term 7B7aA, in the right-hand side of Equation (2.68) is proportional to
the slope a of the transfer function of the slope circuit. This suggests that the bias may be

124 CHAPTER 2 CONTINUOUS-WAVE MODULATION

 

Slope circuit Envelope
Hf) > detector

 

Baseband

FM wave
signal

 

Slope circuit Envelope
Hf) > detector

 

 

 

 

 

Figure 2.30 Block diagram of frequency discriminator.

removed by subtracting from the envelope-detector output | ¬ß,(t)| the output of a second
envelope detector preceded by the complementary slope circuit with a frequency response
H,(f) as described in Figure 2.29¬¢. That is, the two slope circuits are related by

A,(f) = Hy(‚Äîf) (2.69)

Let s(t) denote the response of the complementary slope circuit produced by the incoming
FM signal s(t). Then, following a procedure similar to that just described, we may write

|3,(¬¢)| = mya) _ ake mt (2.70)
Br

where ¬ß,(t) is the complex envelope of the signal s,(t). The difference between the two

envelopes in Equations (2.68) and (2.70) is

solt) = |81()| ‚Äî |$2(2)|
4ak aA m(t)

(2.71)

which is a scaled version of the original message signal 72(¬¢) and free from bias.

We may thus model the ideal frequency discriminator as a pair of slope circuits with
their complex transfer functions related by Equation (2.69), followed by envelope detectors
and finally a summer, as in Figure 2.30. This scheme is called a balanced frequency
discriminator.

FM STEREO MULTIPLEXINGS

Stereo multiplexing is a form of frequency-division multiplexing (FDM) designed to trans-
mit two separate signals via the same carrier. It is widely used in FM radio broadcasting
to send two different elements of a program (e.g., two different sections of an orchestra,
a vocalist and an accompanist) so as to give a spatial dimension to its perception by a
listener at the receiving end.

The specification of standards for FM stereo transmission is influenced by two
factors:

1. The transmission has to operate within the allocated FM broadcast channels.
2. It has to be compatible with monophonic radio receivers.

The first requirement sets the permissible frequency parameters, including frequency de-
viation. The second requirement constrains the way in which the transmitted signal is
configured.

Figure 2.314 shows the block diagram of the multiplexing system used in an FM
stereo transmitter. Let ,(t) and m,(t) denote the signals picked up by left-hand and

2.7 Frequency Modulation 125

Mairixer

    
 

Frequency
doubler

 

cos (22f.1)
{a)

Matrixer

 

mm) + mm)

    

 

 

 

 

 
  
   

 

 

 

 

 

 

 

 

 

 

 

Baseband
LPF 2m, (0)
BPF
m(t) centered at Baseband 2m,@)
2f,= 38 kHz m,(t) ~ m,(t)
Frequency
doubler
Narrowband
filter tuned to
F-= 19 kHz

 

 

 

(b)

Ficure 2.31 (a) Multiplexer in transmitter of FM stereo. (6) Demultiplexer in receiver of FM
stereo.

right-hand microphones at the transmitting end of the system. They are applied to a
simple matrixer that generates the sum signal, m,(t) + m,(t), and the difference signal,
m,(t) ‚Äî m,(t), The sum signal is left unprocessed in its baseband form; it is available for
monophonic reception. The difference signal and a 38-kHz subcarrier (derived from a 19-
kHz crystal oscillator by frequency doubling) are applied to a product modulator, thereby
producing a DSB-SC modulated wave. In addition to the sum signal and this DSB-SC
modulated wave, the multiplexed signal m(t) also includes a 19-kHz pilot to provide a
reference for the coherent detection of the difference signal at the stereo receiver. Thus the
multiplexed signal is described by

m(t) = [11,(t) + m,(t)] + [ot ‚Äî m,(t)] cos(4af.¬£) + K cos(27f.t) (2.72)

where f, = 19 kHz, and K is the amplitude of the pilot tone. The multiplexed signal m(t)
then frequency-modulates the main carrier to produce the transmitted signal. The pilot is
allotted between 8 and 10 percent of the peak frequency deviation; the amplitude K in
Equation (2.72) is chosen to satisfy this requirement.

At a stereo receiver, the multiplexed signal 7(t) is recovered by frequency demodu-
lating the incoming FM wave. Then m(t) is applied to the demultiplexing system shown

126 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

in Figure 2.31. The individual components of the multiplexed signal m/(t) are separated
by the use of three appropriate filters. The recovered pilot (using a narrowband filter tuned
to 19 kHz) is frequency doubled to produce the desired 38-kHz subcarrier. The availability
of this subcarrier enables the coherent detection of the DSB-SC modulated wave, thereby
recovering the difference signal, ,(t) ‚Äî ,(t). The baseband low-pass filter in the top
path of Figure 2.310 is designed to pass the sum signal, #,(t) + ,(t). Finally, the simple
matrixer reconstructs the left-hand signal #;(t) and right-hand signal ¬ª,(t), except for
scaling factors, and applies them to their respective speakers.

| 2.8 Nonlinear Effects in FM Systems

In the preceding two sections, we studied frequency modulation theory and methods for
its generation and demodulation. We complete the discussion of frequency modulation by
considering nonlinear effects in FM systems.

Nonlinearities, in one form or another, are present in all electrical networks. There
are two basic forms of nonlinearity to consider:

 

1. The nonlinearity is said to be strong when it is introduced intentionally and in a
controlled manner for some specific application. Examples of strong nonlinearity
include square-law modulators, hard-limiters, and frequency multipliers.

2. The nonlinearity is said to be weak when a linear performance is desired, but non-
linearities of a parasitic nature arise due to imperfections. The effect of such weak
nonlinearities is to limit the useful signal levels in a system and thereby become an
important design consideration.

In this section we examine the effects of weak nonlinearities on frequency modulation,
Consider a communications channel, the transfer characteristic of which is defined
by the nonlinear input‚Äîoutput relation

v(t) = ayy(t) + aur (t) + asvi(t) (2.73)

where u,(t) and v,(t) are the input and output signals, respectively, and a, a2, and a3 are
constants; Equation (2.73) is a truncated version of Equation (2.56) used in the context
of frequency multiplication. The channel described in Equation (2.73) is said to be me-
moryless in that the output signal v,(t) is an instantaneous function of the input signal
v,(t) (Le., there is no energy storage involved in the description). We wish to determine the
effect of transmitting-a frequency-modulated wave through such a channel. The FM signal
is defined by

u(t) = A, cosl2aft + o(4)]

where

b(t).= ak, I, m(r) dr

For this input signal, the use of Equation (2.73) yields

v,(t) = a, A, cos[2af.t + b(t)] + 4.A2 cos*[2af.t + b(2)]

4
+ a3A3 cos(2af.t + 6(2)] a

2.8 Nonlinear Effects in FM Systems 127

Expanding the squared and cubed cosine terms in Equation (2.74) and then collecting
common terms, we get

1
v(t) = 5 a,A2 + (aa. + 2 ¬´At cos[2af.t + o(t)]

+ 5 aA? cos[4af.t + 2¬¢ (t)] (2.75)

+ 5 a,A2 cos[6af.t + 3(t)]

Thus the channel output consists of a DC component and three frequency-modulated
signals with carrier frequencies f., 2f., and 3f.; the latter components are contributed by
the linear, second-order, and third-order terms of Equation (2.73), respectively.

To extract the desired FM signal from the channel output v,(¬¢), that is, the particular
component with carrier frequency, f,, it is necessary to separate the FM signal with this
carrier frequency from the one with the closest carrier frequency, 2f,. Let Af denote the
frequency deviation of the incoming FM signal v,(t), and W denote the highest frequency
component of the message signal s(t). Then, applying Carson‚Äôs rule and noting that the
frequency deviation about the second harmonic of the carrier frequency is doubled, we
find that the necessary condition for separating the desired FM signal with the carrier
frequency f, from that with the carrier frequency 2f, is

2f,- QAf+W)>f+Af+w

f. > 3Af + 2W (2.76)

Thus, by using a band-pass filter of midband frequency f, and bandwidth 2Af + 2W, the
channel output is reduced to

u(t) = (aa. + 3 aA?) cos[2af.t + o(t)] (2.77)

We see therefore that the only effect of passing an FM signal through a channel with
amplitude nonlinearities, followed by appropriate filtering, is simply to modify its ampli-
tude. That is, unlike amplitude modulation, frequency modulation is not affected by dis-
tortion produced by transmission through a channel with amplitude nonlinearities, It is
for this reason that we find frequency modulation used in microwave radio systems: It
permits the use of highly nonlinear amplifiers and power transmitters, which are particu-
larly important to producing a maximum power output at radio frequencies.

An FM system is extremely sensitive to phase nonlinearities, however, as we would
intuitively expect. A common type of phase nonlinearity that is encountered in microwave
radio systems is known as AM-to-PM conversion. This is the result of the phase charac-
teristic of repeaters or amplifiers used in the system being dependent on the instantaneous
amplitude of the input signal. In practice, AM-to-PM conversion is characterized by a
constant K, which is measured in degrees per dB and may be interpreted as the peak phase
change at the output for a 1-dB change in envelope at the input. When an FM wave is
transmitted through a microwave radio link, it picks up spurious amplitude variations due
to noise and interference during the course of transmission, and when such an FM wave
is passed through a repeater with AM-to-PM conversion, the output will contain unwanted

128 CHAPTER 2 CoNTINUOUS-WAVE MODULATION

phase modulation and resultant distortion. It is therefore important to keep the AM-to.
PM conversion at a low level. For example, for a good microwave repeater, the AM-to.
PM conversion constant K is less than 2 degrees per dB.

| 2.9 Superheterodyne Receiver‚Äô

In a broadcasting system, irrespective of whether it is based on amplitude modulation or
frequency modulation, the receiver not only has the task of demodulating the incoming
modulated signal, but it is also required to perform some other system functions:

¬ª Carrier-frequency tuning, the purpose of which is to select the desired signal (ije,,
desired radio or TV station).

¬ª Filtering, which is required to separate the desired signal from other modulated sig-
nals that may be picked up along the way.

¬ª Amplification, which is intended to compensate for the loss of signal power incurred
in the course of transmission.

The superheterodyne receiver, or superbet as it is often referred to, is a special type of
receiver that fulfills all three functions, particularly the first two, in an elegant and practical
fashion. Specifically, it overcomes the difficulty of having to build a tunable highly selective
and variable filter. Indeed, practically all radio and TV receivers now being made are of
the superheterodyne type.

Basically, the receiver consists of a radio-frequency (RF) section, a mixer and local
oscillator, an intermediate-frequency (IF) section, demodulator, and power amplifier. Typ-
ical frequency parameters of commercial AM and FM radio receivers are listed in Table
2.3. Figure 2.32 shows the block diagram of a superheterodyne receiver for amplitude
modulation using an envelope detector for demodulation.

The incoming amplitude-modulated wave is picked up by the receiving antenna and
amplified in the RF section that is tuned to the carrier frequency of the incoming wave.
The combination of mixer and local oscillator (of adjustable frequency) provides a het-
erodyning function, whereby the incoming signal is converted to a predetermined fixed
intermediate frequency, usually lower than the incoming carrier frequency. This frequency
translation is achieved without disturbing the relation of the sidebands to the carrier; see
Section 2.4. The result of the heterodyning is to produce an intermediate-frequency carrier

defined by
fu = fro ‚Äî fre (2.78)

where fio is the frequency of the local oscillator and fpr is the carrier frequency of the
incoming RF signal. We refer to ftp as the intermediate frequency (IF), because the signal

TABLE 2.3 Typical frequency parameters of AM and FM
radio receivers

 

AM Radio FM Radio
RF carrier range 0.535-1.605 MHz 88-108 MHz
Midband frequency of IF section 0.455 MHz 10.7 MHz

TF bandwidth 10 kHz 200 kHz

 

 

 

 

 

2.9 Superheterodyne Receiver 129

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Antenna
a
/ Loudspeaker
7
RF . IF Envelope Audio
section am) Mixer > section | detector >| amplifier >
7
/
La
/
/ Common (~) Local
7 tuning ~~~ oscillator

Ficure 2.32 Basic elements of an AM radio receiver of the superheterodyne type.

is neither at the original input frequency nor at the final baseband frequency. The mixer‚Äî
local oscillator combination is sometimes referred to as the first detector, in which case
the demodulator is called the second detector.

The IF section consists of one or more stages of tuned amplification, with a band-
width corresponding to that required for the particular type of modulation that the receiver
is intended to handle. The IF section provides most of the amplification and selectivity in
the receiver. The output of the IF section is applied to a demodulator, the purpose of which
is to recover the baseband signal. If coherent detection is used, then a coherent signal
source must be provided in the receiver. The final operation in the receiver is the power
amplification of the recovered message signal.

In a superheterodyne receiver the mixer will develop an intermediate frequency out-
put when the input signal frequency is greater or less than the local oscillator frequency
by an amount equal to the intermediate frequency. That is, there are two input frequencies,
namely, | fro + f|, which will result in fy at the mixer output. This introduces the
possibility of simultaneous reception of two signals differing in frequency by twice the
intermediate frequency. For example, a receiver tuned to 0.65 MHz and having an IF of
0.455 MHz is subject to an image interference at 1.56 MHz; indeed, any receiver with this
value of IF, when tuned to any station, is subject to image interference at a frequency of
0.910 MHz higher than the desired station. Since the function of the mixer is to produce
the difference between two applied frequencies, it is incapable of distinguishing between
the desired signal and its image in that it produces an IF output from either one of them.
The only practical cure for image interference is to employ highly selective stages in the
RF section (i.e., between the antenna and the mixer) in order to favor the desired signal
and discriminate against the undesired or image signal. The effectiveness of suppressing
unwanted image signals increases as the number of selective stages in the RF section in-
creases, and as the ratio of intermediate to signal frequency increases.

The basic difference between AM and FM superheterodyne receivers lies in the use
of an FM demodulator such as limiter-frequency discriminator. In an FM system, the
message information is transmitted by variations of the instantaneous frequency of a
sinusoidal carrier wave, and its amplitude is maintained constant. Therefore, any varia-
tions of the carrier amplitude at the receiver input must result from noise or interference.
An amplitude limiter, following the IF section, is used to remove amplitude variations
by clipping the modulated wave at the IF section output almost to the zero axis. The
resulting rectangular wave is rounded off by a band-pass filter that suppresses har-
monics of the carrier frequency. Thus the filter output is again sinusoidal, with an ampli-
tude that is practically independent of the carrier amplitude at the receiver input (see
Problem 2.42),

130 CHAPTER 2 CONTINUOUS-WAVE MODULATION

| 2.10 Noise in CW Modulation Systems

 

Up to this point in our discussion we have focused attention on the characterization of
continuous-wave (CW) modulation techniques, entirely from a deterministic perspective,
In the remainder of the chapter, we study the effects of channel noise on the reception of
CW modulated signals and thereby develop a deeper understanding of the behavior of
analog communications.

To undertake such a study we follow the customary practice by formulating two
models:

1. Channel model, which assumes a communication channel that is distortionless but
perturbed by additive white Gaussian noise (AWGN).

2. Receiver model, which assumes a receiver consisting of an ideal band-pass filter fol-
lowed by an ideal demodulator appropriate for the application at hand; the band-
pass filter is used to minimize the effect of channel noise.

These simplifying assumptions are made in order to obtain a basic understanding of the
way in which noise affects the performance of the receiver. Moreover, they provide a
framework for the comparison of different CW modulation-demodulation schemes.

Figure 2.33 shows the noisy receiver model that combines the above two assump-
tions. In this figure, s(t) denotes the incoming modulated signal and w(t) denotes the
channel noise. The received signal is therefore made up of the sum of s(¬¢) and w(t); this is
the signal that the receiver has to work on. The band-pass filter in the model of Figure
2.33 represents the combined filtering action of the tuned amplifiers used in the actual
receiver for the purpose of signal amplification prior to demodulation. The bandwidth of
this band-pass filter is just wide enough to pass the modulated signal s(t) without distor-
tion. As for the demodulator in the model of Figure 2.33, its details naturally depend on
the type of modulation used.

# SIGNAL-TO-NOISE RATIOS: BASIC DEFINITIONS

Let the power spectral density of the noise #(t) be denoted by No/2, defined for both
positive and negative frequencies; that is, No is the average noise power per unit bandwidth
measured at the front end of the receiver. We also assume that the band-pass filter in the
receiver model of Figure 2.33 is ideal, having a bandwidth equal to the transmission band-
width By of the modulated signal s(t) and a midband frequency equal to the carrier fre-
quency f.. The latter assumption is justified for double sideband‚Äîsuppressed carrier (DSB-
SC) modulation, full amplitude modulation (AM), and frequency modulation (FM); the
cases of single sideband (SSB) modulation and vestigial sideband (VSB) modulation require
special considerations. Taking the midband frequency of the band-pass filter to be the
same as the carrier frequency f., we may model the power spectral density Sx(f) of the
noise x(t), resulting from the passage of the white noise w(t) through the filter, as shown

Modulated + x()
signal or Bane bass >>| Demodulator oe
s(t)
+

Noise
wit)

 

 

 

 

 

 

 

 

FIGURE 2.33 Receiver model.

2:10 Noise in CW Modulation Systems 131

in Figure 2.34. Typically, the carrier frequency f. is large compared to the transmission
bandwidth By. We may therefore treat the filtered noise n(t) as a narrowband noise rep-
resented in the canonical form

n(t) = n,(t) cos(2af.t) ‚Äî molt) sin(27f.t) (2.79)

where #;(t) is the in-phase noise component and Q(t) is the quadrature noise component,
both measured with respect to the carrier wave A, cos(27rf,t). The filtered signal x(t) avail-
able for demodulation is defined by :

x(t) = s(t) + n(t) (2.80)

The details of s(t) depend on the type of modulation used. In any event, the average noise
power at the demodulator input is equal to the total area under the curve of the power
spectral density 5,(f). From Figure 2.34 we readily see that this average noise power is
equal to NoBy. Given the format of s(¬¢), we may also determine the average signal power
at the demodulator input. With the demodulated signal s(t) and the filtered noise (n(t)
appearing additively at the demodulator input in accordance with Equation (2.80), we
may go on to define an input signal-to-noise ratio, (SNR);, as the ratio of the average
power of the modulated signal s(t) to the average power of the filtered noise n(t).

A more useful measure of noise performance, however, is the output signal-to-noise
ratio, (SNR) 0, defined as the ratio of the average power of the demodulated message signal
to the average power of the noise, both measured at the receiver output. The output signal-
to-noise ratio provides an intuitive measure for describing the fidelity with which the de-
modulation process in the receiver recovers the message signal from the modulated signal
in the presence of additive noise. For such a criterion to be well defined, the recovered
message signal and the corruptive noise component must appear additively at the demod-
ulator output. This condition is perfectly valid in the case of a receiver using coherent
detection. On the other hand, when the receiver uses envelope detection as in full AM or
frequency discrimination as in FM, we have to assume that the average power of the filtered
noise u(t) is relatively low to justify the use of output signal-to-noise ratio as a measure of
receiver performance.

The output signal-to-noise ratio depends, among other factors, on the type of mod-
ulation used in the transmitter and the type of demodulation used in the receiver. Thus it
is informative to compare the output signal-to-noise ratios for different modulation-
demodulation systems. However, for this comparison to be of meaningful value, it must
be made on an equal basis as described here:

¬ª¬Æ The modulated signal s(z) transmitted by each system has the same average power.

¬ª The channel noise w(t) has the same average power measured in the message band-
width W.

 

fe ie) te

Ficure 2.34 Idealized characteristic of band-pass filtered noise.

132 CHAPTER 2 CONTINUOUS-WAVE MODULATION

 

 

 

 

Message signal with Low-pass
the same power as the filter of |} Output
modulated wave 4 bandwidth W
Noise
w(t)

FiGure 2.35 The baseband transmission model, assuming a message signal of bandwidth W,
used for calculating the channel signal-to-noise ratio.

Accordingly, as a frame of reference we define the channel signal-to-noise ratio, (SNR)c,
as the ratio of the average power of the modulated signal to the average power of channel
noise in the message bandwidth, both measured at the receiver input. This definition is
illustrated in Figure 2.35.

For the purpose of comparing different continuous-wave (CW) modulation systems,
we normalize the receiver performance by dividing the output signal-to-noise ratio by the
channel signal-to-noise ratio. We thus define a figure of merit for the receiver as follows:

: .. _ (SNR)o
Figure of merit (SNR)c (2.81)
Clearly, the higher the value of the figure of merit, the better will the noise performance
of the receiver be. The figure-of merit may.equal one, be less than one, or be greater than
one, depending on the type of modulation used, which will become apparent from the
discussion that follows.

2.11 Noise in Linear Receivers
Using Coherent Detection

 

 

From Sections 2.2 and 2.3 we recall that the demodulation of an amplitude-modulated
wave depends on whether the carrier is suppressed or not. When the carrier is suppressed
we usually require the use of coherent detection, in which case the receiver is linear. On
the other hand, when the amplitude modulation includes transmission of the carrier, de-
modulation is accomplished simply by using an envelope detector, in which case the re-
ceiver is nonlinear. In this section we study the effect of noise on the performance of a
linear receiver. The more difficult case of a nonlinear receiver is deferred to Section 2.12.

Consider the case of DSB-SC modulation Figure 2.36 shows the model of a DSB-SC
receiver using a coherent detector. The use of coherent detection requires multiplication
of the filtered signal x(z) by.a locally generated sinusoidal wave cos(27f,t) and then low-
pass filtering the product. To simplify the analysis, we assume that the amplitude of the
locally generated sinusoidal wave is unity. For this demodulation scheme to operate sat-
isfactorily, however, it is necessary that the local oscillator be synchronized both in phase
and in frequency with the oscillator generating the carrier wave in the transmitter. We
assume that this synchronization has been achieved.

The DSB-SC component of the filtered signal x(¬¢) is expressed as

s(t) = CA, cos(27f.t)m(#) (2.82)
where A, cos(27f,t) is the sinusoidal carrier wave and m(t) is the message signal. In the
expression for s(t) in Equation (2.82) we have included a system-dependent scaling factor

C, the purpose of which is to ensure that the signal component s(¬¢) is measured in the same
units as the additive noise component x(t). We assume that 772(¬£) is the sample function of

Coherent datector

 

 

 

2.11 Noise in Linear Receivers Using Coherent Detection 133
v(t) '

L .| Low-pass ‚Äî y

I

DSB-sc __.* Band-pass [7
signal s(t) filter filter
* ]
- |
Noise te (20.0

w(t}

Product
modulator

 

 

 

 

 

 

 

q

 

 

Local -
oscillator

 

 

 

FIGURE 2.36 Model of DSB-SC receiver using coherent detection.

a stationary process of zero mean, whose power spectral density Su(f} is limited to a
maximum frequency W; that is, W is the message bandwidth. The average power P of the
message signal is the total area under the curve of power spectral density, as shown by

Ww
p= | suf) df (2.83)

The carrier wave is statistically independent of the message signal. To emphasize this
independence, the carrier should include a random phase that is uniformly distributed over
27 radians. In the defining equation for s(t) this random phase angle has been omitted for
convenience of presentation. Using the result of Example 1.7 of Chapter 1 on a modulated
random process, we may express the average power of the DSB-SC modulated signal
component s(¬¢) as C?A2P/2. With a noise spectral density of No/2, the average noise power
in the message bandwidth W is equal to WNo. The channel signal-to-noise ratio of the
DSB-SC modulation system is therefore

C¬∞A2P
2WNo
where the constant C? in the numerator ensures that this ratio is dimensionless.
Next, we wish to determine the output signal-to-noise ratio of the system. Using the
narrowband representation of the filtered noise 7(t), the total signal at the coherent detec-
tor input may be expressed as
x(t) = s(t) + n(t)
CA, cos(2mf,t)m(t) + a(t) cos(2af.t) ‚Äî no(t) sin(2af.t)
where m;(t) and #o(t) are the in-phase and quadrature components of m(¬¢) with respect to

the carrier. The output of the product-modulator component of the coherent detector is
therefore

(SNR)cpsp = (2.84)

(2.85)

u(t) = x(t) cos(27f,2)
3CA g(t) + 5n;(t)
+ 3[CA ant) + 2,(t)] cos(4af.t) ‚Äî }no(t) sin(4f.t)
The low-pass filter in the coherent detector in Figure 2.36 removes the high-frequency
components of v(z), yielding the receiver output ;

y(t) = ZCAan(t) + 3m(t) (2.86)
Equation (2.86) indicates the following:

1. The message signal #(t) and in-phase noise component #;(¬£) of the filtered noise n(t)
appear additively at the receiver output.

134

CHAPTER 2 CONTINUOUS-WAVE MODULATION

2. The quadrature component z(t) of the noise n(t) is completely rejected by the co-
herent detector.

These two results are independent of the input signal-to-noise ratio. Thus, coherent detec.
tion distinguishes itself from other demodulation techniques in an important property: The
output message component is unmutilated and the noise component always appears ad-
ditively with the message, irrespective of the input signal-to-noise ratio.

The message signal component at the receiver output is CA m(t)/2. Therefore, the
average power of this component may be expressed as C*A2P/4, where P is the average
power of the original message signal m(t) and C is the system-dependent scaling factor
referred to earlier.

In the case of DSB-SC modulation, the band-pass filter in Figure 2.36 has a band-
width B; equal to 2W in order to accommodate the upper and lower sidebands of the
modulated signal s(#). It follows therefore that the average power of the filtered noise n(z)
is 2WNp. From the discussion of narrowband noise presented in Section 1.11, we know
that the average power of the (low-pass) in-phase noise component #;(¬¢) is the same as
that of the (band-pass) filtered noise n(t). Since from Equation (2.86) the noise component
at the receiver output is #,(#)/2, it follows that the average power of the noise at the receiver
output is

(5)¬∞2WNo = 3WNo

The output signal-to-noise for a DSB-SC receiver using coherent detection is therefore

 

C7A2P/4
(SNR)o, pspsc = WN, /2- os
_ C*A2P $7
2WNo
Using Equations (2.84) and (2.87), we obtain the figure of merit
(SNR)o (2.88)

 

(SNR)c DSB-SC -

Note that the factor C? is common to both the output and channel signal-to-noise ratios,
and therefore cancels out in evaluating the figure of merit.

Following through the noise analysis of a coherent detector for SSB, we find that,
despite the fandamental differences between it and the coherent detector for DSB-SC mod-
ulation, the figure of merit is exactly the same for both of them; see Problem 2.49.

The important conclusions to-be drawn from the discussions presented in this section
and Problem 2.49 are two-fold:

1. For the same average transmitted or modulated signal power and the same average
noise power in the message bandwidth, a coherent SSB receiver wili have exactly the
same output signal-to-noise ratio as a coherent DSB-SC receiver.

2. In both cases, the noise performance of the receiver is exactly the same as that ob-
tained by simply transmitting the message signal in the presence of the same channel
noise. The only effect of the modulation process is to translate the message signal to
a different frequency band to facilitate its transmission over a band-pass channel.

Simply put, neither DSB-SC modulation nor SSB modulation offers the means for a trade-
off between improved noise performance and increased channel bandwidth. This is a se
rious problem when high quality of reception is a requirement.

2.12 Noise in AM Receivers Using Envelope Detection 135

2,12 Noise in AM Receivers
Using Envelope Detection

The next noise analysis we perform is for an amplitude modulation (AM) system using an
envelope detector in the receiver, as shown in the model of Figure 2.37. In a full AM signal,
both sidebands and the carrier wave are transmitted, as shown by

s(t) = A,[1 + k,en(t)] cos(27f.t) (2.89)

where A, cos(27f,t) is the carrier wave, 7(t) is the message signal, and &, is a constant
that determines the percentage modulation. In the expression for the amplitude-modulated
signal component s(t) given in Equation (2.89), we see no need for the use of a scaling
factor, because it is reasonable to assume that the carrier amplitude A, has the same units
as the additive noise component.

The average power of the carrier component in the AM signal s(t) is A2/2. The
average power of the information-bearing component A,k,mm(t) cos(27f,t) is A2k2P/2,
where P is the average power of the message signal m(z). The average power of the full
AM signal s(t) is therefore equal to A2(1 + &&P)/2. As for the DSB-SC system, the average
power of noise in the message bandwidth is WNp. The channel signal-to-noise ratio for
AM is therefore

A2(1 + k2P)
(SNR)cam = 2WN, (2.90)

To evaluate the output signal-to-noise ratio, we first represent the filtered noise x(t)
in terms of its in-phase and quadrature components. We may therefore define the filtered
signal x(t) applied to the envelope detector in the receiver model of Figure 2.37 as follows:

x(t) = s(t) + n(t)

[A. + A,k,mlt) + nj(t)] cos(2af.t) ‚Äî ng(t) sin(27f,t) (2.91)

It is informative to represent the components that comprise the signal x(t) by means of
phasors, as in Figure 2.384. From this phasor diagram, the receiver output is readily ob-
tained as

envelope of x(z)
{[Act+ Ak,malt) + mye)? + 2o(e)y‚Äô?

(0) (2.92)

i

The signal y(z) defines the output of an ideal envelope detector. The phase of x(t) is of no
interest to us, because an ideal envelope detector is totally insensitive to variations in the
phase of x(z).

The expression defining y(t) is somewhat complex and needs to be simplified in some
manner to permit the derivation of insightful results. Specifically, we would like to ap-
proximate the output y(t) as the sum of a message term plus a term due to noise. In general,
this is quite difficult to achieve. However, when the average carrier power is large com-

 

 

Output
-‚Äî‚Äî‚Äî‚Äî‚Äî¬ª- signal
>t)

AM signal Band-pass x() Envelope
s(t) filter detector

 

 

 

 

 

 

Noise
w(t)

FiGure 2.37 Model of AM receiver.

136

CuHarrer 2 & CONTINUOUS-WAVE MODULATION

   

Resultant y(}

 
 

A. (1 + &,m()] nt)

{a)

r{(z)

(b)
Ficure 2.38 (a) Phasor diagram for AM wave plus narrowband noise for the case of high car-

Tier-to-noise ratio. (b) Phasor diagram for AM wave plus narrowband noise for the case of low
carrier-to-noise ratio.

pared with the average noise power, so that the receiver is operating satisfactorily, then
the signal term A,[1 + k,on(t)] will be large compared with the noise terms m;(¬¢) and no(t},
at least most of the time. Then we may approximate the output y(t) as (see Problem 2.51):

y(t) = A, + A,R,m(t) + 1,(t) (2.93)

The presence of the DC or constant term A, in the envelope detector output y(t) of
Equation (2.93) is due to demodulation of the transmitted carrier wave. We may ignore
this term, however, because it bears no relation whatsoever to the message signal 7(t), In
any case, it may be removed simply by means of a blocking capacitor. Thus if we neglect
the DC term A, in Equation (2.93), we find that the remainder has, except for scaling
factors, a form similar to the output of a DSB-SC receiver using coherent detection. Ac-
cordingly, the output signal-to-noise ratio of an AM receiver using an envelope detector
is approximately

A2k2P

(SNR)o,am ‚Ñ¢ 2WN, (2.94)

Equation (2.94) is, however, valid only if the following two conditions are satisfied:

1. The average noise power is small compared to the average carrier power at the
envelope detector input.

2. The amplitude sensitivity &, is adjusted for a percentage modulation less than or
equal to 100 percent.

Using Equations (2.90) and (2.94), we obtain the following figure of merit for amplitude
modulation:
(SNR)o _ kop
(SNR)cl ay 1 + R2P

 

(2.95)

Thus, whereas the figure of merit of a DSB-SC receiver or that of an SSB receiver using
coherent detection is always unity, the corresponding figure of merit of an AM receiver
using envelope detection is always less than unity. In other words, the noise performance
of a full AM receiver is always inferior to that of a DSB-SC receiver. This is due to the

2.12 Noise in AM Receivers Using Envelope Detection 137

wastage of transmitter power, which results from transmitting the carrier as a component
of the AM wave.

> EXAMPLE 2.4 Single-Tone Modulation

Consider the special case of a sinusoidal wave of frequency f,, and amplitude A,, as the
modulating wave, as shown by .

m(t) = A,, cos(2arf,,t)
The corresponding AM wave is
s(t) = A[1 + p cos(2xf,,t)] cos(2Zarf.t)

where 4. = &,A,,, is the modulation factor. The average power of the modulating wave m(t) is
(assuming a load resistor of 1 ohm)

 

 

P= $A2,
Therefore, using Equation (2.95), we get
1 aye
(SNR)}o _ 2 kaAm
(SNR) AM 7 1 242
t+ 2 keAry (2.96)
2
2+ we

When ¬ª = 1, which corresponds to 100 percent modulation, we get a figure of merit equal
to. 1/3. This means that, other factors being equal, an AM system (using envelope detection)
must transmit three times as much average power as a suppressed-carrier system (using co-
herent detection) to achieve the same quality of noise performance.

@ THRESHOLD EFFECT

When the carrier-to-noise ratio is small compared with unity, the noise term dominates
and the performance of the envelope detector changes completely from that just described.
In this case it is more convenient to represent the narrowband noise n(t) in terms of its
envelope 7(t) and phase y(t), as shown by

n(t) = r(t) cos[2mft + w(t)] (2.97)

The corresponding phasor diagram for the detector input x(t) = s{t) + n(t) is shown in
Figure 2.386, where we have used the noise envelope as reference, because it is now the
dominant term. To the noise phasor r(t) we have added a phasor representing the signal
term A,[1 + k,y(¬¢)], with the angle between them being equal to the phase #(t) of the
noise v(t). In Figure 2.386 it is assumed that the carrier-to-noise ratio is so low that the
carrier amplitude A, is small compared with the noise envelope r(t), at least most of the
time. Then we may neglect the quadrature component of the signal with respect to the
noise, and thus find from Figure 2.386 that the envelope detector output is

y(t) = r(t) + A, cos[W(z)] + ARagn(t) cos[i(¬£)] (2.98)

This relation reveals that when the carrier-to-noise ratio is low, the detector output has
no component strictly proportional to the message signal m/(t). The last term of the ex-
pression defining y(t) contains the message signal 7(t) multiplied by noise in the form of

138

CHAPTER 2 CONTINUOUS-WAVE MODULATION

cos[i(t)}. From Section 1.11 we recall that the phase f(t) of the narrowband noise n(7) is
uniformly distributed over 27 radians. It follows therefore that we have a complete loss
of information in that the detector output does not contain the message signal m(t) at all.
The loss of a message in an envelope detector that operates at a low carrier-to-noise ratia
is referred to as the threshold effect. By threshold we mean a value of the carrier-to-noise
ratio below which the noise performance of a detector deteriorates much more rapidly
than proportionately to the carrier-to-noise ratio. It is important to recognize that every
nonlinear detector (e.g., envelope detector) exhibits a threshold effect. On the other hand,
such an effect does not arise in a coherent detector.

A rigorous mathematical analysis of the threshold effect for the general case of an
AM wave is beyond the scope of this book. In the next subsection we simplify matters by
considering the case of an unmodulated carrier. Despite this simplification, we can still
develop a great deal of insight into the threshold effect experienced in an envelope detector,

General Formula for (SNR) in Envelope Detection*
Consider an envelope detector whose input signal is defined by
x(t) = A, cos(2mf,t) + n(t) (2.99)

where A, cos(2zf.t) is the unmodulated carrier and x(t) is the sample function of band-
limited, zero-mean, white Gaussian noise N(t). The power spectral density of N(t) is

No <
sty=4q rif ~ f= W (2.100)
0 otherwise

Representing the narrowband noise n(t) in terms of its in-phase component 1;(t) and
quadrature component 7o({t), we may express the noisy signal at the detector input as

x(t) = (A, + 2,(t)) cos(2af,t) ‚Äî no(t) sin(2af,t) (2.101)

The noise components 7;(t) and 9(t) are zero-mean, jointly Gaussian, mutually indepen-
dent low-pass random processes with identical power spectral densities (see Equation
1.101):

Sxif - f) + Sf +f) for| fl = W

2.102
0 otherwise (

Sulf) = Sno(f) = {

For the problem at hand, the input signal consists of an unmodulated carrier with
average power equal to A2/2. The average noise power at the detector input is

 

 

a2, = 2WNo (2.103)
The carrier-to-noiser ratio is therefore defined by
A2/2
p= a3
OX
ye (2.104)
~ 4AWNo

We may think of p as an input signal-to-noise ratio for the problem described herein.

*A reader who is not interested in the mathematical details of how noise affects the envelope detection of an
AM signal may skip the material up to Eq. (2.124) and read the two limiting cases of the formula in that equation.

2.12 Noise in AM Receivers Using Exvelope Detection 139

However, determination of the output signal-to-noise ratio is a more difficult un-
dertaking because the envelope detector output

yt) = ViA, ¬• alt)? + nd(2) (2.105)

is a nonlinear combination of signal and noise terms. With no clear-cut separation between
signal and noise at the detector output y(t), how then do we isolate the contribution of
the signal s(t) to y(t) from the contribution due to the noise (t)? To resolve this issue, we
adopt a heuristic approach based on signal averaging. Specifically, we introduce the fol-
lowing two definitions: 4

1. The mean output signal, s,, is the difference between the expectation of y(¬¢) in the
combined presence of signal and noise and the expectation of y(z) in the presence of
noise alone, as shown by

So = Efy(t)] ‚Äî Ely.(t) (2.106)
where y(t) is itself defined by Equation (2.105) and - is defined by
Yolt) = Vakit) + nit) (2.107)

2. The mean output noise power is the difference between the mean-square value of the
detector output y(t) and the square of the mean value of y(z), as shown by

var[y(z)] = Ely7(e)] ‚Äî (Ely(t)])? (2.108)
On this basis, we define the output signal-to-noise ratio as
83
var[y(t)]

From Section 1.12, we recall that the envelope detector output due to noise alone is
Rayleigh distributed; that is

y y?
fry) = Jeon(-25). y=0 (2.110)

0 otherwise

 

(SNR)g = (2.109)

The expectation of y,(t) is therefore
Elyo(t)] = E yfy,ly) dy
2 2
y BA
-| oF, e(-3 | dy

From the definition of the gamma function for real positive values of the argument x, we
have

(2,111)

 

T(x) = [ z*~! exp(‚Äîz) dz (2.112)
We may therefore rewrite Equation (2.111) as

Biylt] = V2ost(3)

7
- fen

(2.113)

140

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

where we have used the value I'(3/2) = V7/2. To calculate the mean signal s, at the
detector output, we also need the expectation of y(t). Due to the combined presence of
signal and noise, we recall from Section 1.13 that y(t) is Rician distributed, as shown by

2 2
y yi t Ac\, (Ay
ely) = 5 o% exo( 20%, Jo(42 for y = 0 (2.114)
0 otherwise

 

where Io(+) is the modified Bessel function of the first kind of zero order (see Appendix 3),

Hence,
2 2 2
: -! 2 Uy + Ac\, (Ady
E[y(z)] [, oh eo 2a}, )io( 42 dy (2.115)

Putting A,y/o% = 4 and recognizing that p = A2/2on, we may recast this expectation in
the form

 

 

Ely(t)] = aa exp(‚Äîp) [, ‚Äú exo) a du (2.116)

The integral in Equation (2.116) can be written in a concise form by using confluent
hypergeometric functions; see Appendix 4. In particular, using the integral representation

- Timl2)( (m1
[ uw" exp(‚ÄîB?u*\Iy(u) du = yes (a(2 ‚Äòsga)) (2.117}

 

with m = 3, T(/2) = Val2 and b? = 1/4p, we may express the expectation of y(¬¢) in
terms of the confluent hypergeometric function ,F,(3/2;13p) as

Ely] = ie On exp(-al( (3st) ) (2.118)

We may further simplify matters by using the following identity:
exp(‚Äî4)(1F,(a38;#)) = 1Fi(B ‚Äî 0583‚Äîn) (2.119)

and so finally express the expectation of y(t) in the concise form

Efy(z)] = ia ox(sh(-s1-¬ª) (2.120)

Thus using Equations (2.113) and (2.120) in Equation (2.106) yields the mean output

signal as
$= it Fy} ‚Äî=31;- - i) (2.121)
o 2 Ont 11 2? 3‚ÄîP " .

whose dependence on the standard deviation oy of the noise x(t) is testimony to the
intermingling of signal and noise at the detector output.
Following a similar procedure, we may express the mean-square value of the detector

output y(z) as
3 2 2
3 y y* + Az Ay
*b (1 = J, oF | Qa, Joo ot dy (2.122)

= 2on(Fi(-151;-p))

 

 

2.12 Noise in AM Receivers Using Envelope Detection 141

Hence using. Equations (2.120) and (2.122) in Equation (2.108} yields the mean output
noise power as .

varly(e)] = 208( sits) - a8 (‚Äîda- )) ) (2.123)

Finally, using Equations (2.121) and (2.123) in Equation (2.109) yields the output
signal-to-noise ratio for the envelope detection problem at hand as

(1A(-$::-0) 7 1)
1 2
(ms (‚Äî18s-0)) - (vr,(-}- ))

Equation (2.124) is the general formula for the output signal-to-noise of an envelope
detector whose input consists of an unmodulated carrier and band-limited, white Gaussian
noise. Two limiting cases of this general formula are of particular interest:

(SNR)o =

 

(2.124)

1. Large carrier-to-noise ratio, For large p, we may use the following asymptotic for-
mula (see Appendix 4)

1
in(‚Äîdas- ) = Je for p> & (2.125)

Moreover, the following identity
1F,{~1;1;-p) = 1+ p (2.126)

holds exactly for all p. Accordingly, the use of Equations (2.125) and (2.126) in
Equation (2.124) yields the following approximate formula for the output signal-to-
noise ratio:

(SNR)o =p for p> ~ (2.127)

where we have ignored contributions due to p‚Äô‚Äù and p¬∞ in the numerator of Equation
(2.124) as being subdominant compared to p for large p. Equation (2.127) shows
that for large carrier-to-noise p the envelope detector behaves like a coherent detector,
in that the output signal-to-noise ratio is proportional to the input signal-to-noise
ratio.

2. Small carrier-to-noise ratio. For small p, we have (see Appendix 4)

a

F,(a;c3‚Äîp) = 1 ‚Äî =P for p‚Äî> 0 (2.128)

Hence, using this asymptotic formula, we may approximate the output signal-to-
noise ratio for small p as

mp?
16 ~ 4 (2.129)
= 0,91p? for p> 0

(SNR)o ~

where, in the denominator, we have ignored contributions due to p and p? as being
subdominant compared to p¬∞ for small p. Equation (2.129) shows that for a small
carrier-to-noise ratio, the output signal-to-noise ratio of the envelope detector is pro-
portional to the squared input signal-to-noise ratio.

142 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

qT

     
     
 

10-
5 7 (SNR)o = p
zs -
= LL
DB
gs [
s
¬ª le
@ o¬£
2 c
Zz -
i= =
bo
a (SNR)g = 0.91p?
a
5
¬© ol

TTT

 

Pot a tititl fof ttt tii Lt tii his)
1.0 10.0 100.0

Carrier-to-noise ratio, p

0.01

 

Ficure 2.39 Output signal-to-noise ratio of an envelope detector for varying carrier-to-noise
ratio.

The conclusions drawn from the two limiting cases considered herein are that an
envelope detector favors strong signals and penalizes weak signals. The phenomenon of
weak signals being penalized by the detector is referred to as weak signal suppression,
which is a manifestation of the threshold effect.

Using the formula of Equation (2.124), in Figure 2,39 we have plotted the output
signal-to-noise ratio (SNR)o of the envelope detector versus the carrier-to-noise ratio p
using tabulated values of confluent hypergeometric functions. This figure also includes the
two asymptotes for large p and small p. From Figure 2.39 we see that the output signal-
to-noise ratio deviates from a linear behavior around a carrier-to-noise ratio of 10 dB (i.e,
p= 10).

| 2.13 Noise in FM Receivers

 

Finally, we turn our attention to the noise analysis of a frequency modulation (FM) system,
for which we use the receiver model shown in Figure 2.40. As before, the noise 1(t) is
modeled as white Gaussian noise of zero mean and power spectral density No/2. The

 

 

 

 

 

 

 

 

 

 

ati) v(t) | Baseband
FM Band-pass tees tenet Output
A - -‚Äî‚Äî| low-pass -}‚Äî> |
signal s() > / filter Limiter Discriminator Nike signal
Noise

wilt)

FIGurRE 2.40 Model of an FM receiver.

2.13 Noise in FM Receivers 143

received FM signal s(t) has a carrier frequency f, and transmission bandwidth B., such
that only a negligible amount of power lies outside the frequency band f, + Bz/2 for
positive frequencies, and similarly for negative frequencies.

As in the AM case, the band-pass filter.has a midband frequency f, and bandwidth
By and therefore passes the FM signal essentially without distortion. Ordinarily, Br is
small compared with the midband frequency f., so that we may use the narrowband
representation for #(f), the filtered version of channel noise w(t), in terms of its in-phase
and quadrature components.

In an FM system, the message signal is transmitted by variations of the instantaneous
frequency of a sinusoidal carrier wave, and its amplitude is maintained constant. Therefore,
any variations of the carrier amplitude at the receiver input must result from noise or
interference. The amplitude limiter, following the band-pass filter in the receiver model of
Figure 2.40, is used to remove amplitude variations by clipping the modulated wave at
the filter output almost to the zero axis. The resulting rectangular wave is rounded off by
another band-pass filter that is an integral part of the limiter, thereby suppressing har-
monics of the carrier frequency. Thus, the filter output is again sinusoidal, with an am-
plitude that is practically independent of the carrier amplitude at the receiver input.

The discriminator in the model of Figure 2.40 consists of two components:

1. A slope network ox differentiator with a purely imaginary frequency response that
varies linearly with frequency. It produces a hybrid-modulated wave in which both
amplitude and frequency vary in accordance with the message signal.

2, An envelope detector that recovers the amplitude variation and thus reproduces the
message signal.

The slope network and envelope detector are usually implemented as integral parts of a
single physical unit.

The postdetection filter, labeled ‚Äúbaseband low-pass filter‚Äù in Figure 2.40, has a
bandwidth that is just large enough to accommodate the highest frequency component of
the message signal. This filter removes the out-of-band components of the noise at the
discriminator output and thereby keeps the effect of the output noise to a minimum.

The filtered noise #(t) at the band-pass filter output in Figure 2.40 is defined in terms
of its in-phase and quadrature components by

n{t) = y(t) cos(2af,t) ‚Äî no(t) sin(2af.t)

Equivalently, we may express n(f) in terms of its envelope and phase as

n(t) = r(t) cos[(2mft) + y(t)] (2.130)
where the envelope is
r{t) = [n7(t) + na(z)]'? (2.131)
and the phase is
y(t) = can 2] (2.132)
ny(t)

The envelope r(f) is Rayleigh distributed, and the phase ¬•(z) is uniformly distributed over
27 radians (see Section 1.12).
The incoming FM signal s(t) is defined by

s(t) = A, cos) 2a + 2ak, if m(t) ar| (2.133)

144

CHAPTER 2 CONTINUOUS-WAVE MODULATION

where A, is the carrier amplitude, f, is the carrier frequency, k; is the frequency sensitivity,
and m(t) is the message signal. Note that, as with the standard AM, in FM there is no
need to introduce a scaling factor in the definition of the modulated signal s(t), since it jg
reasonable to assume that its amplitude A, has the same units as the additive noise com.
ponent x(t). To proceed, we define

b(t) = 27k, [vet ar (2.134)

We may thus express s(f) in the simple form
s(t) = A, cos[2af,t + #(¬¢)] (2.135)
The noisy signal at the band-pass filter output is therefore

x(t) = s(t) + n(t)

= A. cos|2af.t + (2)] + r(t) cos[2af.t + wle)] (2.136)

It is informative to represent x(t) by means of a phasor diagram, as in Figure 2.41. In this
diagram we have used the signal term as reference. The phase 6(t) of the resultant phasor
representing x(t) is obtained directly from Figure 2.41 as

r(z) sin[ p(t) ‚Äî $(0)] }
A. + rz) cos[w(t) ‚Äî (2)

The envelope of x(z) is of no interest to us, because any envelope variations at the band-
pass filret output are removed by the limiter.

Our motivation is to determine the error in the instantaneous frequency of the carrier
wave caused by the presence of the filtered noise n(¬¢). With the discriminator assumed
ideal, its output is proportional to 6"(t}/27 where 6'(t) is the derivative of 6(t) with respect
to time. In view of the complexity of the expression defining 6(t), however, we need to
make certain simplifying approximations, so that our analysis may yield useful results.

We assume that the carrier-to-noise ratio measured at the discriminator input is large
compared with unity. Let -R denote the random variable obtained by observing (at some
fixed time) the envelope process with sample function r(¬£) [due to the noise n(Z)]. Then, at
least most of the time, the random variable R is small compared with the carrier amplitude
A,, and so the expression for the phase 6(¬¢) simplifies considerably as follows:

 

At) = H(t) + ean (2.137)

‚ÄúOnly ‚Äî 0) (2.138)

 

alt) = P(t) +

or, using the expression for #(¬£) given in Equation (2.134),

 

 

Alt) = Qk, [ m(r) dr + r(e) sin[w(t) ‚Äî (t)] (2.139)
0 A;
Resultant .
r()
ie a
A yn an A

 

FIGURE 2.41 Phasor diagram for FM wave plus narrowband noise for the case of high carrier‚Äù
to-noise ratio.

2.13 Noise in FM Receivers 145

The discriminator output is therefore

u(t) = i d6(t)
2a. dt
(2.140)

= kym(t) + n4(t)
where the noise term #,(t) is defined by

d
nalt) = >< ¬© tre) sinlwte) ‚Äî (0) (2.141)

We thus see that provided the carrier-to-noise ratio is high, the discriminator output v(t)
consists of the original message signal s(t) multiplied by the constant factor ky, plus an
additive noise component #,(√©). Accordingly, we may use the output signal-to-noise ratio
as previously defined to assess the quality of performance of the FM receiver. Before doing
this, however, it is instructive to see if we can simplify the expression defining the noise
ng(t).

From the phasor diagram of Figure 2.41, we note that the effect of variations in the
phase 1f(t) of the narrowband noise appear referred to the signal term @(t). We know that
the phase y(t) is uniformly distributed over 27 radians. It would therefore be tempting to
assume that the phase difference s(t) ‚Äî (tf) is also uniformly distributed over 27 radians.
If such an assumption were true, then the noise #,(t) at the discriminator output would
be independent of the modulating signal and would depend only on the characteristics of
the carrier and narrowband noise. Theoretical considerations show that this assumption
is justified provided that the carrier-to-noise ratio is high.? Then we may simplify Equation
(2.141) as:

~ 14
‚Äù 2nA, dt

However, from the defining equations for r(¬¢) and y(t), we note that the quadrature com-
ponent #(t) of the filtered noise (t) is

 

ng(t) {r({z) sin[y(e)]) (2,142)

 

no{t) = r(t) sin[ed(t)] (2.143)
Therefore, we may rewrite Equation (2.142) as
_ 1 dagit)
n,(t) nA. dt (2.144)

This means that the additive noise 2,(t) appearing at the discriminator output is deter-
mined effectively by the carrier amplitude A, and the quadrature component no(t) of the
narrowband noise n{t).

The output signal-to-noise ratio is defined as the ratio of the average output signal
power to the average output noise power. From Equation (2.140), we see that the message
component in the discriminator output, and therefore the low-pass filter output, is k m(t).
Hence, the average output signal power is equal to k2P, where P is the average power of
the message signal m(t).

To determine the average output noise power, we note that the noise n,(t) at the
discriminator output is proportional to the time derivative of the quadrature noise com-
ponent (2). Since the differentiation of a function with respect to time corresponds to
multiplication of its Fourier transform by j27f, it follows that we may obtain the noise
Process 7,(t) by passing mo(t) through a linear filter with a frequency response equal to

jauf _ if
2awA, A,

146 CH4pTER 2 # ContTINUOUS-WAVE MODULATION

This means that the power spectral density Sy(f) of the noise q(t) is related to the power
spectral density Sx(f) of the quadrature noise component 79(t) as follows:

f*
SuJAf) = Az Suolf) (2.145)

With the band-pass filter in the receiver model of Figure 2.40 having an ideal fre.
quency response characterized by bandwidth Br and midband frequency f., it follows that
the narrowband noise n({t) will have a power spectral density characteristic that is similarly
shaped. This means that the quadrature component no(t) of the narrowband noise n{(t}
will have the ideal low-pass characteristic shown in Figure 2.424. The corresponding power
spectral density of the noise #,(t) is shown in Figure 2.420; that is,

Nof? By
syifiey ar? IS (2.146)
0, otherwise

In the receiver model of Figure 2.40, the discriminator output is followed by a low-pass
filter with a bandwidth equal to the message bandwidth W. For wideband FM, we usually
find that W is smaller than Bz/2, where By is the transmission bandwidth of the FM
signal. This means that the out-of-band components of noise 74(t) will be rejected. There.
fore, the power spectral density Sx, (f) of the noise 7,(¬£) appearing at the receiver output

is defined by
Nof?
sy fr=d az? lS (2.147)
0, otherwise

as shown in Figure 2.42c. The average output noise power is determined by integrating
the power spectral density Sx(f) from ‚ÄîW to W. We thus get the following result:

w
Average power of output noise = re f‚Äô df
2 J-w

 

 

 

 

2.148
_ 2NoW? (2.148)
3A2
Sygif) Sy MP) Sy (Ff)
No
f
_ Br ie} Br f _ Br 0 By f WW i) W
ra 2 2 2
(a) (b) (c}

FIGURE 2.42 Noise analysis of FM receiver. (a) Power spectral density of quadrature compo-
nent s(t) of narrowband noise nit). (b) Power spectral density of noise a(t) at the discriminator
output. (c) Power spectral density of noise n,(t) at the receiver output.

2.13 Noise in FM Receivers 147

Note that the average output noise power is inversely proportional to the average carrier
power A2/2. Accordingly, in an FM system, increasing the carrier power has a noise-
quieting effect.

Earlier we determined the average output signal power as k3P. Therefore, provided
the carrier-to-noise ratio is high, we may divide this average output signal power by the
average Output noise power of Equation (2.148) to obtain the output signal-to-noise ratio

3A2K7F

2NoW?

The average power in the modulated signal s(t) is A2/2, and the average noise power in

the message bandwidth is WNp. Thus the channel signal-to-noise ratio is

2

2WNo

Dividing the output signal-to-noise ratio by the channel signal-to-noise ratio, we get the

following figure of merit for frequency modulation:
(SNR)o| _ 343P

(SNR)clmy  W?

From Section 2.7 we recall that the frequency deviation Af is proportional to the
frequency sensitivity ky of the modulator. Also, by definition, the deviation ratio D is equal
to the frequency deviation Af divided by the message bandwidth W. In other words, the
deviation ratio D is proportional to the ratio k,P‚Äú7/W. It follows therefore from Equation
(2.151) that the figure of merit of a wideband FM system is a quadratic function of the
deviation ratio. Now, in wideband FM, the transmission bandwidth B- is approximately
proportional to the deviation ratio D. Accordingly, we may state that when the carrier-
to-noise ratio is high, an increase in the transmission bandwidth By provides a correspond-
ing quadratic increase in the output signal-to-noise ratio or figure of merit of the FM
system.The important point to note from this statement is that, unlike amplitude modu-
lation, the use of frequency modulation does provide a practical mechanism for the ex-
change of increased transmission bandwidth for improved noise performance.

(SNR)o,pm = (2.149)

(SNR) cEM = (2.150)

 

(2.151)

¬Æ EXAMPLE 2.5 Single-Tone Modulation
Consider the case of a sinusoidal wave of frequency f,, as the modulating signal, and assume

a peak frequency deviation Af. The modulated FM signal is thus defined by

s(t) = A, cos] 2a + Z sn ft)
Therefore, we may write
t
Qak, ii m(r) dr = ra sin(27rf,,t)

Differentiating both sides with respect to time and solving for m(t), we get

mt) = Af cos(2 aft)
Ry
Hence, the average power of the message signal m(z), developed across a 1-ohm load, is
2
> (af)

 

2

148

CHAPTER 2 # CONTINUOUS-WAVE MODULATION
Substituting this result into the formula for the output signal-to-noise ratio given in Equation
(2.149), we get

3A2(Af)*
(SNR)oem = ‚Äú4NSW>
_ 3A2p"
4N,W

 

where 8 = Af/W is the modulation index. Using Equation (2.151) to evaluate the correspond.

ing figure of merit, we get
3 (2)
mw 2\W

aig
=7A

{(SNR)o
(SNR)c

 

 

(2.152)

I is important to note that the modulation index 8 = Af/W is determined by the bandwidth
W of the postdetection low-pass filter and is not related to the sinusoidal message frequency
fs except insofar as this filter is usually chosen so as to pass the spectrum of the desired
message; this is merely a matter of consistent design. For a specified system bandwidth W, the
sinusoidal message frequency f,, may lie anywhere between 0 and W and would yield the same
output signal-to-noise ratio.

It is of particular interest to compare the noise performance of AM and FM systems,
An insightful way of making this comparison is to consider the figures of merit of the two
systems based on a sinusoidal modulating signal. For an AM system operating with a sinu-
soidal modulating signal and 100 percent modulation, we have (from Example 2.4):

(SNR)o 1

(SNR)clam 3

 

Comparing this figure of merit with the corresponding result described in Equation (2.152)
for an FM system, we see that the use of frequency modulation offers the possibilicy of im-
proved noise performance over amplitude modulation when

3B > 5
that is,
V2
p> Zz = 0.471

We may therefore consider 8 = 0.5 as defining roughly the transition between narrowband
FM and wideband FM. This statement, based on noise considerations, further confirms
a similar observation that was made in Section 2.7 when considering the bandwidth of
FM waves.

@ CAPTURE EFFECT

The inherent ability of an FM system to minimize the effects of unwanted signals (e.g
noise, as just discussed) also applies to interference produced by another frequency:
modulated signal whose frequency content is close to the carrier frequency of the desired
FM wave. However, interference suppression in an FM receiver works well only when the
interference is weaker than the desired FM input. When the interference is the stronge!
one of the two, the receiver locks onto the stronger signal and thereby suppresses the

2.13 Noise in FM Receivers 149

desired FM input. When they are of nearly equal strength, the receiver fluctuates back and
forth between them. This phenomenon is known as the capture effect, which describes
another distinctive characteristic of frequency modulation.

a FM THRESHOLD EFFECT

The formula of Equation (2.149), defining the output signal-to-noise ratio of an FM re-
ceiver, is valid only if the carrier-to-noise ratio, measured at the discriminator input, is
high compared with unity. It is found experimentally that as the input noise power is
increased so that the carrier-to-noise ratio is decreased, the FM receiver breaks. At first,
individual clicks are heard in the receiver output, and as the carrier-to-noise ratio decreases
still further, the clicks rapidly merge into a crackling or sputtering sound. Near the break-
ing point, Equation (2,149) begins to fail by predicting values of output signal-to-noise
ratio larger than the actual ones. This phenomenon is known as the threshold effect.‚Äô The
threshold is defined as the minimum carrier-to-noise ratio yielding an FM improvement
that is not significantly deteriorated from the value predicted by the usual signal-to-noise
formula assuming a small noise power.

For a qualitative discussion of the FM threshold effect, consider first the case when
there is a no signal present, so that the carrier wave is unmodulated. Then the composite
signal at the frequency discriminator input is

x(t) = [A, + n;(t)] cos(2af,t) ‚Äî ao(t) sin(27f,t) (2.153)

where #;(t) and w(t) are the in-phase and quadrature components of the narrowband
noise #(z) with respect to the carrier wave. The phasor diagram of Figure 2.43 displays
the phase relations between the various components of x(t) in Equation (2.153). As the
amplitudes and phases of #;(t) and #9(t) change with time in a random manner, the point
P, [the tip of the phasor representing x(t)] wanders around the point P, (the tip of the
phasor representing the carrier), When the carrier-to-noise ratio is large, ;(t) and not)
are usually much smaller than the carrier amplitude A,, and so the wandering point P, in
Figure 2.43 spends most of its time near point P,. Thus the angle (t) is approximately
#Q(tVA, to within a multiple of 27. When the carrier-to-noise ratio is low, on the other
hand, the wandering point P, occasionally sweeps around the origin and 6(¬¢) increases or
decreases by 27 radians. Figure 2.44 illustrates how in a rough way the excursions in 6(2),
depicted in Figure 2.444, produce impulselike components in 6"(t) = d6/dt. The discrim-
inator output v(t) is equal to 9‚Äô(t)/27. These impulselike components have different heights
depending on how close the wandering point P, comes to the origin O, but all have areas
nearly equal to +27 radians, as illustrated in Figure 2.44b, When the signa! shown in
Figure 2.44b is passed through the postdetection low-pass filter, corresponding but wider
impulselike components are excited in the receiver output and are heard as clicks. The
clicks are produced only when 6(t) changes by +27 radians.

 

FIGURE 2.43 Phasor diagram interpretation of Equation (2.153).

150

CHapren 2 = CONTINUOUS-WAVE MODULATION

 

 

 

 

 

 

 

(b)

Ficure 2.44 Illustrating impulselike components in @‚Äô() = d6(t)/dt produced by changes of 2a
in 6(¬¢); (a) and (b) are graphs of @(#) and @‚Äô(t), respectively.

From the phasor diagram of Figure 2.43, we may deduce the conditions required for
clicks to occur. A positive-going click occurs when the envelope 7(¬¢) and phase y(¬¢) of the
narrowband noise n(t) satisfy the following conditions:

rt) > A,
it) <= we) + dy(e)
dy{t)

‚Äî >
dt 0

These conditions ensure that the phase 6(t) of the resultant phasor x(t) changes by 2a
radians in the time increment dt, during which the phase of the narrowband noise increases
by incremental amount dit). Similarly, the conditions for a negative-going click to occur
are as follows:
r(t) > A,
w(t) > ‚Äîa7 > yt) + able)
dep(t)

ac
dt 0

These conditions ensure that 6(t) changes by ‚Äî27 radians during the time increment di.
The carrier-to-noise ratio is defined by
Az

= 2.154)
P  2ByNo (

 

As pis decreased, the average number of clicks per unit time increases. When this numbet
becomes appreciably large, threshold is said to occur.

2.13 Noise in FM Receivers 151

The output signal-to-noise ratio is calculated as follows:

1. The output signal is taken as the receiver output measured in the absence of noise.
The average output signal power is calculated assuming a sinusoidal modulation that
produces a frequency deviation Af‚Äôequal to B/2, so that the carrier swings back
and forth across the entire input frequency band.

2. The average output noise power is calculated when there is no signal present; that
is, the carrier is unmodulated, with no restriction imposed on the value of the carrier-
to-noise ratio p.

On this heuristic basis, theory‚Äô yields Curve I of Figure 2.45 presenting a plot of the
output signal-to-noise ratio versus the carrier-to-noise ratio when the ratio B/2 W is equal
to 5. This curve shows that the output signal-to-noise ratio deviates appreciably from a
linear function of the carrier-to-noise ratio p when p is less than about 10 dB. Curve II of
Figure 2.45 shows the effect of modulation on the output signal-to-noise ratio when the
modulating signal (assumed sinusoidal) and the noise are present at the same time. The
average output signal power pertaining to curve II may be taken to be effectively the same
as for curve I. The average output noise power, however, is strongly dependent on the
presence of the modulating signal, which accounts for the noticeable deviation of curve I
from curve I. In particular, we find that as p decreases from infinity, the output signal-to-

 

42,‚Äî‚Äî 1 T TT TTT
40

38/-

 

   
  
 
 

6b
(SNR)y = 3p (35);

VO
7

34/- 4
7

a

RD
2h
i
a
1

32
30
2
264
24
22
20

18);-

Output signal-to-noise ratio 10 logip (SNR)g, dB

16;-

Me

12 /-

 

 

10 } | | |
0 2 4 6 8 10 12 14 16 18 20

Carrier-to-noise ratio 10 log, p, dB .

 

FIGURE 2.45 Dependence of output signal-to-noise ratio on input carrier-to-noise ratio for FM
reciever. In curve I, the average output noise power is calculated-assuming an unmodulated car-

rier. In curve II, the average output noise power is calculated assuming a sinusoidally modulated
carrier. Both curves I and II are calculated from theory.

152

CHAPTER 2 = CONTINUOUS-WAVE MODULATION

noise deviates appreciably from a linear function of p when p is about 11 dB. Also when
the signal is present, the resulting modulation of the carrier tends to increase the average
number of clicks per second. Experimentally, it is found that occasional clicks are hearg
in the receiver output at a carrier-to-noise ratio of about 13 dB, which appears to be only
slightly higher than what theory indicates. Also it is of interest to note that the increase jp.
the average number of clicks per second tends to cause the output signal-to-noise ratio to
fall off somewhat more sharply just below the threshold level in the presence of
modulation.

From the foregoing discussion we may conclude that threshold effects in FM receivers
may be avoided in most practical cases of interest if the carrier-to-noise ratio p is equal tg
or greater than 20 or, equivalently, 13 dB. Thus using Equation (2.154) we find that the
loss of message at the discriminator output is negligible if

At
ca > 2
2BrNo ¬∞

or, equivalently, if the average transmitted power A2/2 satisfies the condition

 

Az
F = 20BrNo (2.155)

To use this formula, we may proceed as follows:

1. For a specified modulation index 8 and message bandwidth W, we determine the
transmission bandwidth of the FM wave, Br, using the universal curve of Figure 2.26
or Carson‚Äôs rule.

2. For a specified average noise power per unit bandwidth, No, we use Equation (2.155)
to determine the minimum value of the average transmitted power A;/2 that is nec-
essary to operate above threshold.

@ FM THRESHOLD REDUCTION

In communication systems using frequency modulation, there is particular interest in re-
ducing the noise threshold in an FM receiver so as to satisfactorily operate the receiver
with the minimum signal power possible. Threshold reduction in FM receivers may be
achieved by using an FM demodulator with negative feedback!? (commonly referred tos
an FMEB demodulator), or by using a phase-locked loop demodulator. Such devices art
referred to as extended-threshold demodulators, the idea of which is illustrated in Figure
2.46, The threshold extension shown in this figure is measured with respect to the standard
frequency discriminator {i.e., one without feedback).

The block diagram of an FMEB demodulator‚Äô? is shown in Figure 2.47. We see that
the local oscillator of the conventional FM receiver has been replaced by a voltage
controlled oscillator (VCO) whose instantaneous output frequency is controlled by the
demodulated signal. In order to understand the operation of this receiver, suppose for the
moment that the VCO is removed from the circuit and the feedback loop is left open.
Assume that a wideband EM signal is applied to the receiver input, and a second FM
signal, from the same source but whose modulation index is a fraction smaller, is applied
to the VCO terminal of the mixer. The output of the mixer would consist of the difference
frequency component, because the sum frequency component is removed by the band-past
filter, The frequency deviation of the mixer output would be small, although the frequency
deviation of both input FM waves is large, since the difference between their instantaneous
deviations is small. Hence, the modulation indices would subtract and the resulting FM
wave at the mixer output would have a smaller modulation index. The FM wave with

2.13 Noise in FM Receivers 153

 
 

Output signal-te noise ratio, dB

 

 

I
I
|
{
{
I
|
|
1
i

 

a
Extended Threshold ‚ÄîCarrier-to-noise ratio, dB
threshold

FIGURE 2.46 FM threshold extension.

reduced modulation index may be passed through a band-pass filter, whose bandwidth
need only be a fraction of that required for either wideband FM, and then frequency
demodulated. It is now apparent that the second wideband FM signal applied to the mixer
may be obtained by feeding the output of the frequency discriminator back to the VCO.

It will now be argued that the signal-to-noise ratio of an FMFB receiver is the same
as that of a conventional FM receiver with the same input signal and noise power if the
carrier-to-noise ratio is sufficiently large. Assume for the moment that there is no feed-
back around the demodulator. In the combined presence of an unmodulated carrier
A, cos(27f,t) and narrowband noise

a(t) = n;(t) cos(2af,t) ‚Äî nog(t) sin(27f,t)

the phase of the composite signal x(z) at the limiter-discriminator input is approximately
equal to #Q(t)/A,, assuming that the carrier-to-noise ratio is high. The envelope of x(z) is
of no interest to us, because the limiter removes all variations in the envelope. Thus the
composite signal at the frequency discriminator input consists of a small index phase-
modulated wave with the modulation derived from the component no(t) of noise that is
in phase quadrature with the carrier. When feedback is applied, the VCO generates a
frequency-modulated signal that reduces the phase-modulation index of the wave in the
band-pass filter output, that is, the quadrature component no(¬¢) of noise. Thus we see that
as long as the carrier-to-noise ratio is sufficiently large, the FMFB receiver does not respond
to the in-phase noise component x(t), but that it would demodulate the quadrature noise
component 7 (t) in exactly the same fashion as it would demodulate signal modulation.

Received we Baseband
FM ‚Äî | Mixer Bane pass di Limi et _ low-pass Qutput
wave ilter iscriminator filter signa

Voltage-
controlled

oscillator

 

   

 

 

 

 

  

 

 

 

 

 

Ficure 2.47 FM demodulator with negative feedback.

154

CHAPTER 2. 8 CONTINUOUS-WAVE MODULATION

Signal and quadrature noise are reduced in the same proportion by the applied feedback,
with the result that the baseband signal-to-noise ratio is independent of feedback. For large
carrier-to-noise ratios, the baseband signal-to-noise ratio of an FMFB receiver is then the
same as that of a conventional FM receiver.

The reason that an FMFB receiver is able to extend the threshold is that, unlike g
conventional FM receiver, it uses a very important piece of a priori information, namely,
that even though the carrier frequency of the incoming FM wave will usually have large
frequency deviations, its rate of change will be at the baseband rate. An FMFB demodp.
lator is essentially a tracking filter that can track only the slowly varying frequency of g
wideband FM signal, and consequently it responds only to a narrowband of noise centered
about the instantaneous carrier frequency. The bandwidth of noise to which the FMFB
receiver responds is precisely the band of noise that the VCO tracks. The end result is that
an FMEB receiver is capable of realizing a threshold extension on the order of 5-7 dB,
which represents a significant improvement in the design of minimum power FM systems,

Like the FMFB demodulator, the phase-locked loop (discussed later in Section 2,14)
is also a tracking filter and, as such, the noise bandwidth to which it responds is precisely
the band of noise tracked by the VCO. Indeed, the phase-locked loop demodulator offers
a threshold extension capability with a relatively simple circuit. Unfortunately, the amount
of threshold extension is not predictable by any existing theory, and it depends on signal
parameters. Roughly speaking, improvement by a few (on the order of 2, to 3) decibels ig
achieved in typical applications, which is not as good as an FMFB demodulator.

s PrReE-EMPHASIS AND DE-EMPHASIS IN FM

Equation (2.147) shows that the power spectral density of the noise at the output of an
FM receiver has a square-law dependence on the operating frequency; this is illustrated in
Figure 2.48a. In Figure 2.486, we have included the power spectral density of a typical
message source; audio and video signals typically have spectra of this form. In particular,
we see that the power spectral density of the message usually falls off appreciably at higher
frequencies. On the other hand, the power spectral density of the output noise mcreases
rapidly with frequency. Thus around f = + W, the relative spectral density of the message
is quite low, whereas that of the output noise is quite high in comparison. Clearly, the
message is not using the frequency band allotted to it in an efficient manner. It may appear
that one way of improving the noise performance of the system is to slightly reduce the‚Äô
bandwidth of the postdetection low-pass filter so as to reject a large amount of noise power
while losing only a small amount of message power. Such an approach, however, is usually
not satisfactory because the distortion of the message caused by the reduced filter band-
width, even though slight, may not be tolerable. For example, in the case of music, we
find that although the high-frequency notes contribute only a very small fraction of the
total power, nonetheless, they contribute a great deal from an esthetic viewpoint. .

A more satisfactory approach to the efficient use of the allowed frequency band is
based on the use of pre-emphasis in the transmitter and de-emphasis in the receiver,

8y,(f) Syl f)

Ww 0 Ww f Ww ie] Ww f

Ficure 2.48 (a) Power spectral density of noise at FM receiver output. (b) Power spectral den
sity of a typical message signal.

2.13 Noise in FM Receivers 155

Pre-emphasis FM ? FM De-emphasis Message
ay filter, Hoel f) - transmitter receiver filter, Hy) plus noise
+

Noise
wh}

 

 

 

 

 

 

 

 

FIGURE 2.49 Use of pre-emphasis and de-emphasis in an FM system,

illustrated in Figure 2.49. In this method, we artificially emphasize the high-frequency
components of the message signal prior to modulation in the transmitter, and therefore
before the noise is introduced in the receiver. In effect, the low-frequency and high-
frequency portions of the power spectral density of the message are equalized in such a
way that the message fully occupies the frequency band allotted to it. Then, at the discrim-
inator output in the receiver, we perform the inverse operation by de-emphasizing the
high-frequency components, so as to restore the original signal-power distribution of the
message. In such a process, the high-frequency components of the noise at the discriminator
output are also reduced, thereby effectively increasing the output signal-to-noise ratio of
the system. Such a pre-emphasis and de-emphasis process is widely used in commercial
FM radio transmission and reception.

In order to produce an undistorted version of the original message at the receiver
output, the pre-emphasis filter in the transmitter and the de-emphasis filter in the receiver
must ideally have frequency responses that are the inverse of each other. That is, if H,.(f)
designates the frequency response of the pre-emphasis filter, then the frequency response
Hig.(f) of the de-emphasis filter must ideally be (ignoring transmission delay)

_1_
Hel)‚Äù

This choice of frequency responses makes the average message power at the receiver output
independent of the pre-emphasis and de-emphasis procedure.

From our previous noise analysis in FM systems, assuming a high carrier-to-noise
ratio, the power spectral density of the noise m4(z) at the discriminator output is given by
Equation (2.146). The modified power spectral density of the noise at the de-emphasis
filter output is therefore

Ag f) = ~-W=/f=W (2.156)

Nof? , _Br
Half) /¬∞Sx(f) = 4 az HelAl?s LAL (2.157)
0, otherwise

Recognizing, as before, that the postdetection low-pass filter has a bandwidth W that is,

in general, less than By/2, we find that the average power of the modified noise at the
receiver output is as follows:

Average output noise No i an

=‚Äî Hz f)|? d 2.158

(a with spur nos A2 J-w PH)" af )

Because the average message power at the receiver output is ideally unaffected by the

combined pre-emphasis and de-emphasis procedure, it follows that the improvement in

output signal-to-noise ratio produced by the use of pre-emphasis in the transmitter and

de-emphasis in the receiver is defined by

average Output noise power without pre-emphasis and de-emphasis
average output noise power with pre-emphasis and de-emphasis

 

 

(2.159)

156

CHAPTER 2 CONTINUOUS-WAVE MODULATION

Earlier we showed that the average output noise power without pre-emphasis and dp,
emphasis is equal to (2Nj)W7/3A2); see Equation (2.148). Therefore, after cancellation of
common terms, we may express the improvement factor I as

23

af‚Äù fl Half)|? af

 

 

(2.160)

It must be emphasized that this improvement factor assumes the use of a high carrier-to.
noise ratio at the discriminator input in the receiver.

& EXAMPLE 2.6

Asimple pre-emphasis filter that emphasizes high frequencies and is commonly used in practice
is defined by the frequency-response

if
fo

which is closely realized by the RC-amplifier network shown in Figure 2.50a, provided that
R << rand 2afCr <1 inside the frequency band of interest. The amplifier in Figure 2.50,
is intended to make up for the attenuation introduced by the RC network at low frequencies,
The frequency parameter fy is 1/(2aCr).

The corresponding de-emphasis filter in the receiver is defined by the frequency response

1
1+ iflfo
which can be realized using the simple RC network of Figure 2.50b.

The improvement in output signal-to-noise ratio of the FM receiver, resulting from th√©
combined use of the pre-emphasis and de-emphasis filters of Figure 2.50, is therefore

er. ae
af‚Äù ft df af
3 of wi + (fff) (2.161)
(Wifa)?
3[(Wifo) ~ tan‚Äú (WHF)

In commercial FM broadcasting, we typically have fy = 2.1 kHz, and we may reason-
ably assume W = 15 kHz. This set of values yields I = 22, which corresponds to an improve-
ment of 13 dB in the output signal-to-noise ratio of the receiver. The output signal-to-noise

H,(f) = 1 + =

Fadf)

 

 

r

Amplifier -‚Äî-o Ty
Output input Cc Output
signal signal signal

{a} @)

 
  
  
 

 

 

 

Input
signal

FiGgurk 2.50 (a) Pre-emphasis filter. (b) De-emphasis filter.

2.14 Computer Experiments: Phase-Locked Loop 157

ratio of an FM receiver without pre-emphasis and de-emphasis is typically 40-50 dB. We see,
therefore, that by using the simple pre-emphasis and de-emphasis filters shown in Figure 2.50,
we can realize a significant improvement in the noise performance of the receiver. |

The use of the simple linear pre-emphasis and de-emphasis filters just described is an
example of how the performance of an FM system may be improved by using the differ-
ences between characteristics of signals and noise in the system. These simple filters also
find application in audio tape-recording. Specifically, nonlinear pre-emphasis and de-
emphasis techniques have been applied successfully to tape recording. These techniques!*
(known as Dolby-A, Dolby-B, and DBX systems) use a combination of filtering and dy-
namic range compression to reduce the effects of noise, particularly when the signal level
is low.

2.14 Computer Experiments:
| Phase-Locked Loop

The experimental study presented in this section focuses on the use of a phase-locked loop
for the demodulation of a frequency modulated signal. Before proceeding with the exper-
iments, however, we first present a brief exposition of phase-locked loop theory.

Basically, the phase-locked loop consists of three major components: a multiplier, a
loop filter, and a voltage-controlled oscillator (VCO) connected together in the form of a
feedback system, as shown in Figure 2.51. The VCO is a sinusoidal generator whose
frequency is determined by a voltage applied to it from an external source. In effect, any
frequency modulator may serve as a VCO. We assume that initially we have adjusted the
VCO so that when the control voltage is zero, two conditions are satisfied:

1. The frequency of the VCO is precisely set at the unmodulated carrier frequency f..

2. The VCO output has a 90 degree phase-shift with respect to the unmodulated carrier
wave,

Suppose then that the input signal applied to the phase-locked loop is an FM signal defined
by

s(t) = A, sin[2rf.t + $4(t)]
where A, is the carrier amplitude. With a modulating signal m(t), the angle @;(t) is related

to m(t) by the integral

oi (t) = 2ak, if m(7) dr

Error

 

 

  
 

 

 

 

FM wave el Loop Output
s{t) filter v{)
Feedback
signal Voltage-
ri) controlled
oscillator

 

FIGURE 2.51 Phase-locked loop.

158

CHAPTER 2 @ CONTINUOUS-WAVE MODULATION

where k, is the frequency sensitivity of the frequency modulator. Let the VCO output ip
the phase-locked loop be defined by

r(t) = A, cos|2rf.t + b2(t)]

where A, is the amplitude. With a control voltage v(¬¢) applied to the VCO input, the angle
2(t) is related to v(t) by the integral

,(t) = 2ak, [, u(t) dr (2.162)

where &, is the frequency sensitivity of the VCO, measured in Hertz per volt. The object
of the phase-locked loop is to generate a VCO output 7(¬¢) that has the same phase angle
(except for the fixed difference of 90 degrees) as the input FM signal s(¬¢), The time-varying
phase angle #,(t) characterizing s(t) may be due to modulation by a message signal m/z),
in which case we wish to recover #,(¬£) and thereby produce an estimate of m(t). In other
applications of the phase-locked loop, the time-varying phase angle ¬¢ ,(¬¢) of the incoming
signal s(t) may be an unwanted phase shift caused by fluctuations in the communication
channel; in this latter case, we wish to track @ ,(t) so as to produce a signal with the same
phase angle for the purpose of coherent detection (synchronous demodulation).

a MopeEL OF THE PHASE-LOCKED Loop‚Äù

To develop an understanding of the phase-locked loop, it is desirable to have a model of
the loop. We start by developing a nonlinear model, which is subsequently linearized to
simplify the analysis. According to Figure 2.51, the incoming FM signal s(t) and the VCO
output r(¬¢) are applied to the multiplier, producing two components:

1. A high-frequency component, represented by the double-frequency term
kh, AA, sinl[4rft +.61(¬£) + b2{t)]
2. A low-frequency component represented by the difference-frequency term
kyAA, sinfoilt) ‚Äî ba(t)]
where &,,, is the rultiplier gain, measured in volt‚Ñ¢?.

The loop filter in the phase-locked loop is a low-pass filter, and its response to the high-

_frequency component will be negligible. The VCO also contributes to the attenuation of

this component. Therefore, discarding the high-frequency component (i.e., the double-
frequency term), the input to the loop filter is reduced to

elt) = kwA-A, sin[d(√©)] (2.163)
where ¬¢,(t) is the phase error defined by
b(t) = bilt) ‚Äî ba(√©) (2.164)

√©

= 6,(t) ‚Äî 27k, 3 u(z) dr

The loop filter operates on the error e(¬¢) to produce an output u(t) defined by the convo-
lution integral:

v(t) = [ e(t)h(t ‚Äî 7) dr (2.165)

2.14¬∞ Computer Experiments: Phase-Locked Loop 159

where 4(t) is the impulse response of the loop filter. Using Equations (2.164) and (2.165)
to relate #,(t) and &,(z), we obtain the following nonlinear integro-differential equation
as the descriptor of the dynamic behavior of the phase-locked loop:

apdt) _ deat) ¬∞

ho dT 27Ko EC sin[d,(7)]h(t ‚Äî 7) dr (2.166)

where K, is as loop-gain parameter defined by
Ko = kak AA, (2.167)

The amplitudes A, and A, are both measured in volts, the multiplier gain &,, in volt‚Äù * and
the frequency sensitivity &, in Hertz per volt. Hence, it follows from Equation (2.167) that
Ky has the dimensions of frequency. Equation (2.166) suggests the model shown in Figure
2.52 for a phase-locked loop. In this model we have also included the relationship between
u(t) and e(t) as represented by Equations (2.163) and (2.165). We see that the model of
Figure 2.52 resembles the actual block diagram of Figure 2.51. The multiplier at the input
of the phase-locked loop is replaced by a subtracter and a sinusoidal nonlinearity, and the
VCO by an integrator.

The sinusoidal nonlinearity in the model of Figure 2.52 complicates the task of an-
alyzing the behavior of the phase-locked loop. It would be helpful to linearize this model
to simplify the analysis and yet give a good approximate description of the loop‚Äôs behavior
in certain modes of operation. When the ‚Äòphase error ¬¢,(¬£) is zero, the phase-locked loop
is said to be in phase-lock. When ¬¢,(t) is at all times small compared with one radian, we
miay use the approximation

sin[p.(√©)] = $,(t)

which is accurate to within 4 percent for ,(t) less than 0.5 radians. In this case, the loop
is said to be near phase-lock, and the sinusoidal nonlinearity of Figure 2.52 may be dis-
regarded. Under this condition, v(z) is approximately equal to m(t), except for the scaling
factor k,/k,.

The complexity of the phase-locked loop is determined by the frequency response
H(f) of the loop filter. The simplest form of a phase-locked loop is obtained when
H(f) = 1; that is, there is no loop filter, and the resulting phase-locked loop is referred to
as a first-order phase-locked loop. A major limitation of a first-order phase-locked loop is
that the loop gain parameter Ky controls both the loop bandwidth as well as the hold-in
frequency range of the loop; the hold-in frequency range refers to the range of frequencies

2nKg 2ak,

da Att) vu)

t
fe
0

FicureE 2.52 Nonlinear model of the phase-locked loop.

 

 

 

 

 

 

 

 

 

 

 

 

 

160

CHAPTER 2 CONTINUOUS-WAVE MODULATION

for which the loop remains phase-locked to the input signal. We may overcome this lim:
itation by using a loop filter with the frequency response

a
if
where a is a constant. Then with this loop filter in place and the phase-locked loop oper.

ating in its linear mode, we find from Equation (2.166) that the phase-locked loop behaves
as a second-order feedback system, as shown by the standard frequency response

(f) Giff)
@(f) 1+ 2 eciflf,) + (iflfaY‚Äù
where ¬Æ,(f) and ¬Æ,(f) are the Fourier transforms of @,(t) and ¬¢,(t), respectively. The

system is parameterized by the natural frequency, fs and damping factor, ¬£, which are
respectively defined by

H(f)=1+ (2.168)

 

 

(2.169)

f. = VaK, (2.170)
and
= |Ko
c= [2 (2.171)

The second-order phase-locked loop so described is the subject of the computer experi-
ments presented next.

Experiment 1: Acquisition Mode

When a phase-locked loop is used for coherent detection (synchronous demodulation), the
loop must first lock onto the input signal and then follow the variations of its phase angle
with time. The process of bringing a loop into phase-lock is called acquisition, and the
ensuing process of following angular variations in the input signal is called tracking. In
the acquisition mode and quite possibly the tracking mode, the phase error ¬¢,(¬¢) between
the input signal s(t) and the VCO output r(¬¢) will certainly be large, thereby mandating
the use of the nonlinear model of Figure 2.52. However, a nonlinear analysis of the ac-
quisition process based on this latter model is beyond the scope of this book. In this
experiment, we use computer simulations to study the acquisition process and thereby
develop insight into some of its features.

Consider a second-order phase-locked loop using the loop filter of Equation (2.168)
and having the following parameters:

1
Natural frequency f,, = on Hz

Damping factor ¬£ = 0.3, 0.707, 1.0

To accommodate variation in Z, the filter parameter a is varied in accordance with the
formula

fr
2¬¢

which follows from Equations (2.170) and (2.171). Figure 2.53 presents the variation it
the phase error @,(t) with time for each of the three specified values of damping factor bs

a=

2.14 Computer Experiments: Phase-Locked Loop 161

9S
B

9
w

9
a

Phase error ¬¢,(4), radians
Oo
Ny

o

 

 

 

-0.2 | | |
0 5 10 15 20 25

Time ¬¢, seconds

 

Figure 2.53 Variation of the phase error for three different values of damping factor.

assuming a frequency step of 0.125 Hz. These results show that the damping factor
¬£ = 0.707 gives the best compromise between a fast response time and an underdamped
oscillatory behavior.

Experiment 2: Phase-Plane Portrait

A phase-plane portrait is a family of trajectories, with each trajectory representing a single
solution of Equation (2.166). For the second experiment we plot the phase-plane portrait
of a second-order phase-locked loop for the case of sinusoidal modulation. The system
parameters of the loop are as follows:
. 50
Loop-gain parameter Ky = =~ Hz
2a
Loop-natural frequency f, = So Hz
Pp q YIn n/t

50
Si idal modulation f, (= H:
inusoidal modulation frequency f, IaV/In Zz

 

Figure 2.54 presents the phase-plane portrait of the phase-locked loop adjusted for
critical damping, where the trajectories (frequency error versus phase error) are plotted
for different starting points. From this portrait we make the following observations:

1. For a sinusoidal nonlinearity, the phase-plane portrait is itself periodic with period
2 in the phase error #,, but it is aperiodic in d,/dt.
2. For an initial frequency error
1 dd
K dt
with an absolute value less than or equal to 1, the phase-locked loop is assured of
attaining a stable (equilibrium) point at (0, 0) or (0, 277); the multiplicity of equilib-
rium points is a manifestation of periodicity of the phase-plane portrait.

162

CHAPTER 2 & CONTINUOUS-WAVE MODULATION

Frequency error, (Hz)

 

 

 

Phase error, radians

FicurRE 2.54 Phase-plane portrait for critical damping and sinusoidal modulation.

3. For an initial frequency error
1 doe
K at
with an absolute value equal to 2, we have a saddle point at (0, ar) where the slightest

perturbation applied to the phase-locked loop causes it to shift to the equilibrium
point (0, 0) or (0, 277).

4.2.15 Summary and Discussion

 

In this chapter we studied the principles of continuous-wave (CW) modulation. This an-
alog form of modulation uses a sinusoidal carrier whose amplitude or angle is varied in
accordance with a message signal. We may thus distinguish two families of CW modula-
tion: amplitude modulation and angle modulation.

= AMPLITUDE MODULATION

Amplitude modulation may itself be classified into four types, depending on the spectral
content of the modulated signal. The four types of amplitude modulation and their prac-
tical merits are as follows:

1. Full amplitude modulation (AM), in which the upper and lower sidebands are trans-
mitted in full, accompanied by the carrier wave.

Accordingly, demodulation of an AM signal is accomplished rather simply in the receiver
by using an envelope detector, for example. It is for this reason we find that full AM is
commonly used in commercial AM radio broadcasting, which involves a single powerful
transmitter and numerous receivers that are relatively inexpensive to build.

2.15 Summary and Discussion 163

2. Double sideband-suppressed carrier (DSB-SC) modulation, in which only the upper
and lower sidebands are transmitted.

The suppression of the carrier wave means that DSB-SC modulation requires much less
power than full AM to transmit the same message signal; this advantage of DSB-SC mod-
ulation over full AM is, however, attained at the expense of increased receiver complexity.
DSB-SC modulation is therefore well suited for point-to-point communication involving
one transmitter and one receiver; in this form of communication, transmitted power is at
a premium and the use of a complex receiver is therefore justifiable.

3. Single sideband (SSB) modulation, in which only the upper sideband or lower sideband
is transmitted.

SSB modulation is the optimum form of CW modulation in the sense that it requires the
minimum transmitted power and the minimum channel bandwidth for conveying a mes-
sage signal from one point to another. However, its use is limited to message signals with
an energy gap centered on zero frequency.

4, Vestigial sideband modulation, in which almost all of one sideband and a vestige of
the other sideband are transmitted in a prescribed complementary fashion.

VSB modulation requires a channel bandwidth that is between that required for SSB and
DSB-SC systems, and the saving in bandwidth can be significant if modulating signals with
large bandwidths are being handled, as in the case of television signals and high-speed
data.

DSB-SC, SSB, and VSB are examples of linear modulation, whereas, strictly speaking, full
AM is nonlinear. However, the deviation of full AM from linearity is of a mild sort.
Accordingly, all four forms of amplitude modulation lend themselves readily to spectral
analysis using the Fourier transform.

ANGLE MODULATION

Angle modulation ‚Äòmay be classified into frequency modulation (FM) and phase modula-
tion (PM). In FM, the instantaneous frequency of a sinusoidal carrier is varied in propor-
tion to the message signal. In PM, on the other hand, it is the phase of the carrier that is
varied in proportion to the message signal. The instantaneous frequency is defined as the
derivative of the phase with respect to time, except for the scaling factor 1/(27). Accord-
ingly, FM and PM are closely related to each other; if we know the properties of the one,
we can determine those of the other. For this reason, and because FM is commonly used
in broadcasting, much of the material on angle modulation in the chapter was devoted
to FM.

Unlike amplitude modulation, FM is a nonlinear modulation process. Accordingly,
spectral analysis of FM is more difficult than for AM. Nevertheless, by studying single-
tone FM, we were able to develop a great deal of insight into the spectral properties of
FM. In particular, we derived an empirical rule known as Carson‚Äôs rule for an approximate
evaluation of the transmission bandwidth Bz of FM. According to this rule, Bis controlled
by a single parameter: the modulation index @ for sinusoidal FM, or the deviation ratio
D for nonsinusoidal FM.

NOISE ANALYSIS

We conclude the chapter on CW modulation systems by presenting a comparison of their
noise performances. For this comparison, we assume that the modulation is produced by

164

CHAPTER 2 ¬Æ CONTINUOUS-WAVE MODULATION

7O;-
Iv
60 ~~ ml
8dB
50 [-

Output signal-to-noise ratio, dB

 

 

 

| I |
0 10 20 30 40 50

Channel signal-to-noise ratio, dB

 

FIGURE 2.55 Comparison of the noise performance of various CW modulation systems. Curve [:
Full AM, 4 = 1. Curve Il: DSB-SC, SSB. Curve III: FM, 8 = 2. Curve IV: FM, 8 = 5. (Curves
IIL and IV include 13-dB pre-emphasis, de-emphasis improvement.)

a sinusoidal wave. For the comparison to be meaningful, we also assume that the modu-
lation systems operate with exactly the same channel signal-to-noise ratio. We may thus
plot the output signal-to-noise ratio versus the channel signal-to-noise ratio as in Figure
2.55 for the following modulation schemes:

¬ª Full AM with 100 percent modulation
¬ª Coherent DSB-SC, SSB
¬ª FM with B = 2 and B= 5
Figure 2.55 also includes the AM and FM threshold effects. In making the comparison, it

is informative to keep in mind the transmission bandwidth requirement of the modulation
system in question. In this regard, we use a normalized transmission bandwidth defined by

_ Br
W
where B- is the transmission bandwidth of the modulated signal, and W is the message

bandwidth. Table 2.4 presents the values of B, for the different CW modulation schemes.
From Figure 2.55 and Table 2.4 we make the following observations:

B,

¬ª Among the family of AM systems, SSB modulation is optimum with regard to noise
performance as well as bandwidth conservation.

¬ª The use of FM improves noise performance but at the expense of an excessive trans
mission bandwidth. This assumes that the FM system operates above threshold fot
the noise improvement to be realized.

Notes ard References 165

TABLE 2.4 Values of B,, for

various CW modulation schemes

 

FM
AM, DSB-SC SSB B=2 BPS

 

2 1

oo
=
Nn

 

 

On an important point to conclude the discussion on CW modulation, only frequency

modulation offers the capability to trade off transmission bandwidth for improved noise

pe

tformance. The trade-off follows a square law, which is the best that we can do with

CW modulation (ie., analog communications). In Chapter 3 we describe pulse-code mod-
ulation, which is basic to the transmission of analog information-bearing signals by a
digital communication system, and which can indeed do much better.

I NoTES AND REFERENCES

1.

10.

11.

12.

It appears that the terms continuous wave and heterodyning were first used by Reginald
Fessenden in the early 1900s.

. The Costas receiver is named in honor of its inventor; see the paper by Costas (1956).

. Bessel functions play an important role in the study of both analog and digital communi-
cation systems. They can be of the so-called first kind or second kind. Appendix 3 discusses
mathematical details and properties of both kinds of Bessel functions. A table of Bessel
functions of the first kind is presented in Table A6.5.

. Carson‚Äôs rule for the bandwidth of FM signals is named in honor of its originator; Carson
and Fry (1937) wrote one of the early classic papers on frequency modulation theory.

. The indirect method of generating a wideband FM wave was first proposed by Armstrong
(1936). Armstrong was also the first to recognize the noise-robustness properties of fre-
quency modulation.

. Stereo multiplexing usually involves the use of frequency modulation for radio transmis-
sion. However, it may also be transmitted using amplitude modulation as discussed in
Problem 2.14; for more details, see the paper by Mennie (1978).

. For detailed description of the superheterodyne receiver, see the Radio Engineering Hand-
book edited by Henney (1958, pp. 19-34-19-41).

. The qualitative study of threshold in envelope detection presented here follows Downing
(1964, p. 71).

. For a justification of the critical assumption on which the simplification presented in Equa-
tion (2.142) rests, see Rice (1963).

For a detailed discussion of the threshold effect in FM receivers, see the paper by Rice
(1963) and the book by Schwartz, Bennett, and Stein (1966, pp. 129-163).

Figure 2.45 is adapted from Rice (1963). The validity of the theoretical curve II in this
figure has been confirmed experimentally; see Schwartz, Bennett, and Stein (1966, p. 153).
For some earlier experimental work on the threshold phenomenon in FM, see the paper
by Crosby (1937).

The idea of using feedback around an FM demodulator was originally proposed by Chaffee
(1939).

166 CHAPTER 2 & CONTINUOUS-WAVE MODULATION

13. The treatment of the FMFB demodulator presented in Section 2.13 is based on the pape,
by Enloe (1962); see also Roberts (1977, pp. 166-181).

14. For a detailed discussion of Dolby systems mentioned in the latter part of Section 2.13, seg
Stremler (1990, pp. 732-734).

15. For a full treatment of the nonlinear analysis of a phase-locked loop, see Gardner ( 1979)
Lindsey (1972), and Viterbi (1966).

.

| PROBLEMS

Amplitude Modulation

2.1 Suppose that nonlinear devices are available for which the output current √©, and input
voltage v, are related by

i, = ayy, + asu;
where a; and a3 are constants. Explain how these devices may be used to provide: (a) a
product modulator and (b) an amplitude modulator.

2.2 Figure P2.2 shows the circuit diagram of a square-law modulator. The signal applied to
the nonlinear device is relatively weak, such that it can be represented by a square law:

v,(t) = ayu(t) + anvi(t)

where a, and a, are constants, v;,(t) is the input voltage, and v(t) is the output voltage.
The input voltage is defined by

u(t) = A, cos(2af.t) + mt)

where mm(t) is a message signal and A, cos(27f,t) is the carrier wave.
(a) Evaluate the output voltage v,(z).

(b) Specify the frequency response that the tuned circuit in Figure P2.2 must satisfy in
order to generate an AM signal with f, as the carrier frequency.

(c) What is the amplitude sensitivity of this AM signal?

 

Nonlinear
device

   

 

 

 

mn

A, cos (2arf,1)

Tuned to f,

Figure P2.2

2.3 Figure P2.3a shows the circuit diagram of a switching modulator. Assume that the carrit!
wave c(t) applied to the diode is large in amplitude, so that the diode acts like an ideal
switch: it presents zero impedance when forward biased (i.e., c(t) > 0). We may thus

Problems 167

approximate the transfer characteristic of the diode-load resistor combination by a piece-
wise-linear characteristic defined as (see Figure P2.3)

_ J vilt), c(t) > 0
valt) = {¬∞ > et <0

That is, the load voltage v,(t) varies periodically between the values v,(¬¢) and zero at a
rate equal to the carrier frequency f.. Hence, we may write

u(t) ~ [A, cos(2mf.t) + m(t)]g7,(t)
where g7,(t) is a periodic pulse train defined by

‚Äî1

 

&r(t} = > ste 2a cos[2af,t(2n ‚Äî 1)]

(a) Find the AM wave component red in the output voltage v,[t).

(b) Specify the unwanted components in v(t) that need to be removed by a band-pass
filter of suitable design.

ug
c() =A, cos (27.0)
¬©) ‚ÄîF

mit) C) oR S09 Slope = 1

 

{a) ()
FIGuRE P2.3

2.4 Consider the AM signal

s(t) = Al + yp cos(2f,,t)] cos(27f,t)

produced by a sinusoidal modulating signal of frequency f,,. Assume that the modulation
factor is 4 = 2, and the carrier frequency f, is much greater than f,,. The AM signal s(t)
is applied to an ideal envelope detector, producing the output v(t).

(a) Determine the Fourier series representation of u(t).

(b) What is the ratio of second-harmonic amplitude to fundamental amplitude in v(t)?

Figure P2.5 shows the circuit diagram of an envelope detector. It consists simply of a
diode and resistor-capacitor (RC) filter. On a positive half-cycle of the input signal, the
diode is forward-biased and the capacitor C charges up rapidly to. the peak value of the
input signal, When the input signal falls below this value, the diode becomes reverse-
biased and the capacitor C discharges slowly through the load resistor R;. The discharging
process continues until the next positive half-cycle. Thereafter, the charging-discharging
routine is continued.

(a) Specify the condition that must be satisfied by the capacitor C for it to charge rapidly
and thereby follow the input voltage up to the positive peak when the diode is
conducting.

(b) Specify the condition which the load resistor R; must satisfy so that the capacitor C
discharges slowly between positive peaks of the carrier wave, but not so long that the

168 CHAPTER 2 CONTINUOUS-WAVE MODULATION

capacitor voltage will not discharge at the maximum rate of change of the modulating
wave.

c Ry Output

AM wave
re)
FiGure P2.5

2.6 Consider a square-law detector, using a nonlinear device whose transfer characteristic ig

defined by
va(t) = ayu,(t) + anu;(t}

where a, and a, are constants, v;(¬£) is the input, and v,(t) is the output. The input consists
of the AM wave

vu,(t) = A[1 + k,m(t)] cos(27f.t)

(a) Evaluate the output v2(2).
(b) Find the conditions for which the message signal #z(t) may be recovered from v(t).

2.7 The AM signal
s(t) = AJ[1 + kan(t)] cos(2af.t)
is applied to the system shown in Figure P2.7. Assuming that | k,zn(t)| <1 for all ¬¢ and

the message signal m(t) is limited to the interval ‚Äî-W = f = W and that the cartier
frequency f. > 2W show that #(t) can be obtained from the square-rooter output v;(¬¢}.

vi Low-pass ale) Square
s(Q) ae} Squarer filter T‚Äî‚Äî| Tooter b> 030

ars
ad = 50 val = Vos
FiGcure P2.7

 

 

 

 

2.8 Consider a message signal (2) with the spectrum shown in Figure P2.8. The message
bandwidth W = 1 kHz. This signal is applied to a product modulator, together with a
carrier wave A, cos(2.f,t), producing the DSB-SC modulated signal s(t). The modulated
signal is next applied to a coherent detector. Assuming perfect synchronism between th√©
carrier waves in the modulator and detector, determine the spectrum of the detectot
output when: (a) the carrier frequency f, = 1.25 kHz and (b) the carrier frequency
f, = 0.75 kHz. What is the lowest carrier frequency for which each component of the
modulated signal s(z) is uniquely determined by m(t)?

Problems 169

Mf)
w Oo W f
Ficure P2.8

2.9 Figure P2.9 shows the circuit diagram of a balanced‚Äômodulator. The input applied to the
top AM modulator is #(t], whereas that applied to the lower AM modulator is ‚Äîm(¬¢);
these two modulators have the same amplitude sensitivity. Show that the output s(t) of
the balanced modulator consists of a DSB-SC modulated signal.

 

AM ald

mt) ‚Äî modulator

 

 

 

A, Cos (20,0)

 

  

s(t)

  

Oscillator

 

 

 

A,cos (2nf,2)

 

AM

~ mt) > edulator

 

 

 

FIGURE P2.9

2.10 A DSB-SC modulated signal is demodulated by applying it to a coherent detector.

(a) Evaluate the effect of a frequency error Af in the local carrier frequency of the de-
tector, measured with respect to the carrier frequency of the incoming DSB-SC signal.

(b) For the case of a sinusoidal modulating wave, show that because of this frequency
error, the demodulated signal exhibits beats at the error frequency. Illustrate your
answer with a sketch of this demodulated signal.

2.11 Consider the DSB-SC signal
s(t) = A, cos(2afitim(t)

where A, cos(27f,t) is the carrier wave and m(t) is the message signal. This modulated
signal is applied to a square-law device characterized by

y(t} = s7(t)

The output y(¬£) is next applied to a narrowband filter with a passband magnitude response
of one, midband frequency 2f., and bandwidth Af. Assume that Af is small enough to
treat the spectrum of y(f) as essentially constant inside the passband of the filter.

(a) Determine the spectrum of the square-law device output y(f).
(b) Show that the filter output v(t) is approximately sinusoidal, given by

AZ
v(t) = > E Af cos(47f.t)

where E is the energy of the message signal s(t).


 